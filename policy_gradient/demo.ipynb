{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Policy Gradient Model Demo"
   ],
   "metadata": {
    "id": "Kp-g7Ld4hzON"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_Ke_Eb1dNSt",
    "outputId": "b4669c22-0a76-4034-8142-e6da51710a37"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
      "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.4.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (24.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (3.20.3)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.4.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[atari]) (8.1.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[atari]) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[atari]) (4.66.4)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[atari]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[atari]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[atari]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[atari]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[atari]) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari] gym[accept-rom-license] tensorboardx"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment and Model"
   ],
   "metadata": {
    "id": "b_hZ_EJZh6UJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from gym.wrappers import (\n",
    "    RecordVideo,\n",
    "    RecordEpisodeStatistics,\n",
    "    GrayScaleObservation,\n",
    "    FrameStack,\n",
    "    TransformReward,\n",
    "    AtariPreprocessing,\n",
    ")\n",
    "import gym\n",
    "import cv2\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set manual seed\n",
    "seed = 2339\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "rvR-hFVPdWkR"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# create output folder\n",
    "def get_output_folder(parent_dir, name):\n",
    "    parent_dir = os.path.join(parent_dir, name)\n",
    "    parent_dir = parent_dir + time.strftime(\"-%m-%d-%H-%M\")\n",
    "\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    return parent_dir\n",
    "\n",
    "\n",
    "def always_true(x):\n",
    "    return True\n",
    "\n",
    "\n",
    "class RepeatActionInFramesTakeMaxOfTwo(gym.Wrapper):\n",
    "    def __init__(self, env, h=160, repeat=4):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.repeat = repeat\n",
    "        self.h = h\n",
    "        self.shape = (\n",
    "            h,\n",
    "            env.observation_space.low.shape[1],\n",
    "            env.observation_space.low.shape[2],\n",
    "        )\n",
    "        self.frames = deque(maxlen=2)\n",
    "\n",
    "        if repeat <= 0:\n",
    "            raise ValueError(\"Repeat value needs to be 1 or higher\")\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        for i in range(self.repeat):\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            self.frames.append(observation[: self.h, :, :])\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Open queue into arguments for np.maximum\n",
    "        maximum_of_frames = np.maximum(*self.frames)\n",
    "        return maximum_of_frames, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.frames.clear()\n",
    "        self.frames.append(observation[: self.h, :, :])\n",
    "        return observation[: self.h, :, :]\n",
    "\n",
    "\n",
    "class NormResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Create the new observation space for the env\n",
    "        # Since we are converting to grayscale we set low of 0 and high of 1\n",
    "        self.shape = shape\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=self.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"Change from 255 grayscale to 0-1 scale\"\"\"\n",
    "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        return (observation / 255.0).reshape(self.shape)\n",
    "\n",
    "\n",
    "class ClipRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        def clip(x):\n",
    "            if x < 0:\n",
    "                return -1\n",
    "            if x > 0:\n",
    "                return 1\n",
    "            return 0\n",
    "\n",
    "        vec_clip = np.vectorize(clip)\n",
    "        return vec_clip(reward)\n",
    "\n",
    "\n",
    "def Wrap(env, video_dir):\n",
    "    shape = (84, 84)\n",
    "    env = RepeatActionInFramesTakeMaxOfTwo(env, repeat=4)\n",
    "    env = ClipRewardWrapper(env)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = NormResizeObservation(env, shape)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    env = RecordEpisodeStatistics(env, 100)\n",
    "    env = RecordVideo(env, video_dir, episode_trigger=always_true)\n",
    "    return env"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ-EiKs0dics",
    "outputId": "778213d4-62a0-4bbf-d924-14231732a85d"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class PolicyPi(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(84 * 84 * 4, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        # 9 is number of action in Enduro\n",
    "        self.output = nn.Linear(hidden_dim // 2, 9)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.dropout(outs)\n",
    "        outs = self.hidden2(outs)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.dropout(outs)\n",
    "        logits = self.output(outs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "policy_pi = PolicyPi(hidden_dim=512).to(device)"
   ],
   "metadata": {
    "id": "lCt9jDiXdnr2"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make(\"ALE/Enduro-v5\")\n",
    "output_dir = get_output_folder(\"gym_monitor\", \"PG\")\n",
    "env = Wrap(env, output_dir)"
   ],
   "metadata": {
    "id": "pIH6nO-vdzmi"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "id": "v3OH3z65iEnn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = torch.load(\"policy_pi.pth\")\n",
    "policy_pi.load_state_dict(model)\n",
    "policy_pi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gD8uHt2dpxE",
    "outputId": "17d275ad-7ad6-4d52-8c1a-0f5920216a3c"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PolicyPi(\n",
       "  (hidden): Linear(in_features=28224, out_features=512, bias=True)\n",
       "  (hidden2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=9, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def pick_sample(s):\n",
    "    with torch.no_grad():\n",
    "        # Flatten observation\n",
    "        s_batch = np.expand_dims(s, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
    "        s_batch = torch.flatten(s_batch, start_dim=1)\n",
    "\n",
    "        # Get logits from state\n",
    "        logits = policy_pi(s_batch)\n",
    "        logits = logits.squeeze(dim=0)\n",
    "\n",
    "        # From logits to probability distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample\n",
    "        a = torch.multinomial(probs, num_samples=1)\n",
    "        return a.item()\n",
    "\n",
    "\n",
    "rewards = []\n",
    "frames = 0\n",
    "s = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    a = pick_sample(s)\n",
    "    s, r, term, _ = env.step(a)\n",
    "    done = term\n",
    "\n",
    "    rewards.append(r)\n",
    "    frames += 1\n",
    "\n",
    "    if done:\n",
    "        break"
   ],
   "metadata": {
    "id": "8F4m8BkJd7Ww"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "total_reward = np.sum(rewards)\n",
    "\n",
    "print(\"Total reward: {}; Total time: {};\".format(total_reward, frames))\n",
    "print(\"Estimated in-game final score: ~{}\".format(int((frames - 100) / 100) * 100))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkbJhnqjeguY",
    "outputId": "690a04d0-fb61-48ba-de7a-f3fb9d17eaa8"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total reward: 20; Total time: 833;\n",
      "Estimated in-game final score: ~700\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "H79GDot9elje"
   },
   "execution_count": 8,
   "outputs": []
  }
 ]
}