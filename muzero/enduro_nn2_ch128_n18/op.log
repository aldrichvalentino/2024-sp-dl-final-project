A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-11 23:01:23] [command] train weight_iter_0.pkl 1 1
[2024-05-11 23:02:29] nn step 100, lr: 0.005.
	loss_policy_0: 0.1159
	accuracy_policy_0: 0.64852
	loss_value_0: 0.31995
	loss_policy_1: 0.03113
	accuracy_policy_1: 0.53953
	loss_value_1: 0.06528
	loss_reward_1: 0.02986
	loss_policy_2: 0.03242
	accuracy_policy_2: 0.50115
	loss_value_2: 0.06499
	loss_reward_2: 0.02961
	loss_policy_3: 0.03294
	accuracy_policy_3: 0.5183
	loss_value_3: 0.06491
	loss_reward_3: 0.0295
	loss_policy_4: 0.03329
	accuracy_policy_4: 0.48893
	loss_value_4: 0.06525
	loss_reward_4: 0.0296
	loss_policy_5: 0.03364
	accuracy_policy_5: 0.53395
	loss_value_5: 0.06547
	loss_reward_5: 0.02968
	loss_policy: 0.27931
	loss_value: 0.64585
	loss_reward: 0.14825
[2024-05-11 23:03:32] nn step 200, lr: 0.005.
	loss_policy_0: 0.02734
	accuracy_policy_0: 0.82881
	loss_value_0: 0.17796
	loss_policy_1: 0.01116
	accuracy_policy_1: 0.72988
	loss_value_1: 0.03563
	loss_reward_1: 0.0
	loss_policy_2: 0.01379
	accuracy_policy_2: 0.68531
	loss_value_2: 0.0357
	loss_reward_2: 0.0
	loss_policy_3: 0.01619
	accuracy_policy_3: 0.62184
	loss_value_3: 0.03578
	loss_reward_3: 0.0
	loss_policy_4: 0.0168
	accuracy_policy_4: 0.60086
	loss_value_4: 0.03582
	loss_reward_4: 0.0
	loss_policy_5: 0.01762
	accuracy_policy_5: 0.59311
	loss_value_5: 0.0359
	loss_reward_5: 0.0
	loss_policy: 0.1029
	loss_value: 0.3568
	loss_reward: 1e-05
Optimization_Done 200
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-11 23:09:03] [command] train weight_iter_200.pkl 1 2
[2024-05-11 23:10:01] nn step 300, lr: 0.01.
	loss_policy_0: 0.0948
	accuracy_policy_0: 0.70066
	loss_value_0: 0.19458
	loss_policy_1: 0.02262
	accuracy_policy_1: 0.68135
	loss_value_1: 0.03912
	loss_reward_1: 4e-05
	loss_policy_2: 0.0235
	accuracy_policy_2: 0.67904
	loss_value_2: 0.03918
	loss_reward_2: 5e-05
	loss_policy_3: 0.02488
	accuracy_policy_3: 0.65287
	loss_value_3: 0.03928
	loss_reward_3: 6e-05
	loss_policy_4: 0.02533
	accuracy_policy_4: 0.64076
	loss_value_4: 0.03933
	loss_reward_4: 0.00012
	loss_policy_5: 0.02571
	accuracy_policy_5: 0.64783
	loss_value_5: 0.0393
	loss_reward_5: 0.00025
	loss_policy: 0.21684
	loss_value: 0.39078
	loss_reward: 0.00052
[2024-05-11 23:10:50] nn step 400, lr: 0.01.
	loss_policy_0: 0.06222
	accuracy_policy_0: 0.76693
	loss_value_0: 0.19731
	loss_policy_1: 0.01614
	accuracy_policy_1: 0.74062
	loss_value_1: 0.03957
	loss_reward_1: 4e-05
	loss_policy_2: 0.01744
	accuracy_policy_2: 0.73342
	loss_value_2: 0.0396
	loss_reward_2: 7e-05
	loss_policy_3: 0.01808
	accuracy_policy_3: 0.72119
	loss_value_3: 0.03966
	loss_reward_3: 6e-05
	loss_policy_4: 0.01863
	accuracy_policy_4: 0.70814
	loss_value_4: 0.03968
	loss_reward_4: 0.0001
	loss_policy_5: 0.01908
	accuracy_policy_5: 0.71119
	loss_value_5: 0.03963
	loss_reward_5: 0.00016
	loss_policy: 0.1516
	loss_value: 0.39546
	loss_reward: 0.00043
Optimization_Done 400
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-11 23:14:48] [command] train weight_iter_400.pkl 1 3
[2024-05-11 23:15:48] nn step 500, lr: 0.02.
	loss_policy_0: 0.0602
	accuracy_policy_0: 0.76156
	loss_value_0: 0.1946
	loss_policy_1: 0.0152
	accuracy_policy_1: 0.74711
	loss_value_1: 0.03902
	loss_reward_1: 3e-05
	loss_policy_2: 0.01641
	accuracy_policy_2: 0.73816
	loss_value_2: 0.03907
	loss_reward_2: 5e-05
	loss_policy_3: 0.0173
	accuracy_policy_3: 0.715
	loss_value_3: 0.03911
	loss_reward_3: 3e-05
	loss_policy_4: 0.01772
	accuracy_policy_4: 0.71574
	loss_value_4: 0.03916
	loss_reward_4: 5e-05
	loss_policy_5: 0.01784
	accuracy_policy_5: 0.73104
	loss_value_5: 0.03914
	loss_reward_5: 0.00013
	loss_policy: 0.14466
	loss_value: 0.39009
	loss_reward: 0.00028
[2024-05-11 23:16:39] nn step 600, lr: 0.02.
	loss_policy_0: 0.05258
	accuracy_policy_0: 0.77887
	loss_value_0: 0.19195
	loss_policy_1: 0.01309
	accuracy_policy_1: 0.75754
	loss_value_1: 0.03847
	loss_reward_1: 3e-05
	loss_policy_2: 0.01413
	accuracy_policy_2: 0.74912
	loss_value_2: 0.0385
	loss_reward_2: 3e-05
	loss_policy_3: 0.01502
	accuracy_policy_3: 0.739
	loss_value_3: 0.03851
	loss_reward_3: 4e-05
	loss_policy_4: 0.01538
	accuracy_policy_4: 0.73107
	loss_value_4: 0.03851
	loss_reward_4: 5e-05
	loss_policy_5: 0.01575
	accuracy_policy_5: 0.74289
	loss_value_5: 0.03846
	loss_reward_5: 0.00011
	loss_policy: 0.12595
	loss_value: 0.38441
	loss_reward: 0.00027
Optimization_Done 600
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-11 23:24:17] [command] train weight_iter_600.pkl 1 4
[2024-05-11 23:25:17] nn step 700, lr: 0.04.
	loss_policy_0: 0.07913
	accuracy_policy_0: 0.71801
	loss_value_0: 0.22824
	loss_policy_1: 0.01843
	accuracy_policy_1: 0.71088
	loss_value_1: 0.04555
	loss_reward_1: 3e-05
	loss_policy_2: 0.01915
	accuracy_policy_2: 0.7027
	loss_value_2: 0.04584
	loss_reward_2: 3e-05
	loss_policy_3: 0.02035
	accuracy_policy_3: 0.67891
	loss_value_3: 0.04594
	loss_reward_3: 3e-05
	loss_policy_4: 0.0209
	accuracy_policy_4: 0.68059
	loss_value_4: 0.04605
	loss_reward_4: 5e-05
	loss_policy_5: 0.02124
	accuracy_policy_5: 0.69135
	loss_value_5: 0.04602
	loss_reward_5: 0.00011
	loss_policy: 0.1792
	loss_value: 0.45763
	loss_reward: 0.00025
[2024-05-11 23:26:06] nn step 800, lr: 0.04.
	loss_policy_0: 0.06307
	accuracy_policy_0: 0.75049
	loss_value_0: 0.19331
	loss_policy_1: 0.01484
	accuracy_policy_1: 0.73496
	loss_value_1: 0.03876
	loss_reward_1: 2e-05
	loss_policy_2: 0.01549
	accuracy_policy_2: 0.7325
	loss_value_2: 0.0388
	loss_reward_2: 4e-05
	loss_policy_3: 0.01627
	accuracy_policy_3: 0.714
	loss_value_3: 0.03881
	loss_reward_3: 2e-05
	loss_policy_4: 0.01669
	accuracy_policy_4: 0.71629
	loss_value_4: 0.03884
	loss_reward_4: 4e-05
	loss_policy_5: 0.01694
	accuracy_policy_5: 0.72668
	loss_value_5: 0.03881
	loss_reward_5: 8e-05
	loss_policy: 0.1433
	loss_value: 0.38732
	loss_reward: 0.0002
Optimization_Done 800
[2024-05-11 23:26:10] [command] quit
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-11 23:31:02] [command] train weight_iter_800.pkl 1 5
[2024-05-11 23:32:07] nn step 900, lr: 0.08.
	loss_policy_0: 0.0715
	accuracy_policy_0: 0.73361
	loss_value_0: 0.21017
	loss_policy_1: 0.01574
	accuracy_policy_1: 0.73988
	loss_value_1: 0.04212
	loss_reward_1: 2e-05
	loss_policy_2: 0.01656
	accuracy_policy_2: 0.72891
	loss_value_2: 0.04213
	loss_reward_2: 3e-05
	loss_policy_3: 0.01746
	accuracy_policy_3: 0.71139
	loss_value_3: 0.04216
	loss_reward_3: 2e-05
	loss_policy_4: 0.01743
	accuracy_policy_4: 0.72191
	loss_value_4: 0.04216
	loss_reward_4: 5e-05
	loss_policy_5: 0.01772
	accuracy_policy_5: 0.73219
	loss_value_5: 0.04214
	loss_reward_5: 8e-05
	loss_policy: 0.15642
	loss_value: 0.42087
	loss_reward: 0.00019
[2024-05-11 23:32:51] nn step 1000, lr: 0.08.
	loss_policy_0: 0.05853
	accuracy_policy_0: 0.75643
	loss_value_0: 0.19241
	loss_policy_1: 0.01294
	accuracy_policy_1: 0.7535
	loss_value_1: 0.03853
	loss_reward_1: 1e-05
	loss_policy_2: 0.01322
	accuracy_policy_2: 0.75059
	loss_value_2: 0.03854
	loss_reward_2: 2e-05
	loss_policy_3: 0.0136
	accuracy_policy_3: 0.74248
	loss_value_3: 0.03856
	loss_reward_3: 2e-05
	loss_policy_4: 0.01376
	accuracy_policy_4: 0.74064
	loss_value_4: 0.03856
	loss_reward_4: 3e-05
	loss_policy_5: 0.01369
	accuracy_policy_5: 0.75787
	loss_value_5: 0.03853
	loss_reward_5: 7e-05
	loss_policy: 0.12575
	loss_value: 0.38514
	loss_reward: 0.00016
Optimization_Done 1000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-11 23:36:41] [command] train weight_iter_1000.pkl 2 6
[2024-05-11 23:37:57] nn step 1100, lr: 0.1.
	loss_policy_0: 0.06246
	accuracy_policy_0: 0.76463
	loss_value_0: 0.21244
	loss_policy_1: 0.01372
	accuracy_policy_1: 0.7684
	loss_value_1: 0.04254
	loss_reward_1: 1e-05
	loss_policy_2: 0.01413
	accuracy_policy_2: 0.77193
	loss_value_2: 0.04253
	loss_reward_2: 3e-05
	loss_policy_3: 0.01493
	accuracy_policy_3: 0.75
	loss_value_3: 0.04254
	loss_reward_3: 2e-05
	loss_policy_4: 0.01493
	accuracy_policy_4: 0.75127
	loss_value_4: 0.04255
	loss_reward_4: 3e-05
	loss_policy_5: 0.0152
	accuracy_policy_5: 0.76934
	loss_value_5: 0.04248
	loss_reward_5: 8e-05
	loss_policy: 0.13536
	loss_value: 0.42508
	loss_reward: 0.00018
[2024-05-11 23:38:49] nn step 1200, lr: 0.1.
	loss_policy_0: 0.05913
	accuracy_policy_0: 0.77469
	loss_value_0: 0.21143
	loss_policy_1: 0.01292
	accuracy_policy_1: 0.77635
	loss_value_1: 0.04234
	loss_reward_1: 1e-05
	loss_policy_2: 0.01319
	accuracy_policy_2: 0.77193
	loss_value_2: 0.04234
	loss_reward_2: 3e-05
	loss_policy_3: 0.01378
	accuracy_policy_3: 0.76336
	loss_value_3: 0.0423
	loss_reward_3: 3e-05
	loss_policy_4: 0.01368
	accuracy_policy_4: 0.76605
	loss_value_4: 0.04228
	loss_reward_4: 3e-05
	loss_policy_5: 0.01399
	accuracy_policy_5: 0.77336
	loss_value_5: 0.04221
	loss_reward_5: 9e-05
	loss_policy: 0.12669
	loss_value: 0.4229
	loss_reward: 0.00018
Optimization_Done 1200
[2024-05-11 23:41:12] [command] train weight_iter_1200.pkl 3 7
[2024-05-11 23:42:15] nn step 1300, lr: 0.1.
	loss_policy_0: 0.0559
	accuracy_policy_0: 0.78012
	loss_value_0: 0.20063
	loss_policy_1: 0.01263
	accuracy_policy_1: 0.76324
	loss_value_1: 0.04012
	loss_reward_1: 0.0
	loss_policy_2: 0.01318
	accuracy_policy_2: 0.75252
	loss_value_2: 0.04009
	loss_reward_2: 0.0
	loss_policy_3: 0.01359
	accuracy_policy_3: 0.74875
	loss_value_3: 0.04006
	loss_reward_3: 0.0
	loss_policy_4: 0.01376
	accuracy_policy_4: 0.74637
	loss_value_4: 0.04003
	loss_reward_4: 0.0
	loss_policy_5: 0.01416
	accuracy_policy_5: 0.74746
	loss_value_5: 0.04
	loss_reward_5: 0.0
	loss_policy: 0.12324
	loss_value: 0.40093
	loss_reward: 2e-05
[2024-05-11 23:43:15] nn step 1400, lr: 0.1.
	loss_policy_0: 0.05217
	accuracy_policy_0: 0.80078
	loss_value_0: 0.21003
	loss_policy_1: 0.01155
	accuracy_policy_1: 0.78779
	loss_value_1: 0.04199
	loss_reward_1: 0.0
	loss_policy_2: 0.0119
	accuracy_policy_2: 0.78023
	loss_value_2: 0.04196
	loss_reward_2: 0.0
	loss_policy_3: 0.01209
	accuracy_policy_3: 0.77689
	loss_value_3: 0.04191
	loss_reward_3: 0.0
	loss_policy_4: 0.01199
	accuracy_policy_4: 0.78236
	loss_value_4: 0.04187
	loss_reward_4: 0.0
	loss_policy_5: 0.01227
	accuracy_policy_5: 0.78145
	loss_value_5: 0.04183
	loss_reward_5: 0.0
	loss_policy: 0.11198
	loss_value: 0.4196
	loss_reward: 1e-05
Optimization_Done 1400
[2024-05-11 23:45:35] [command] train weight_iter_1400.pkl 4 8
[2024-05-11 23:46:33] nn step 1500, lr: 0.1.
	loss_policy_0: 0.03561
	accuracy_policy_0: 0.83303
	loss_value_0: 0.20567
	loss_policy_1: 0.00933
	accuracy_policy_1: 0.81225
	loss_value_1: 0.04112
	loss_reward_1: 0.0
	loss_policy_2: 0.00982
	accuracy_policy_2: 0.79336
	loss_value_2: 0.04109
	loss_reward_2: 0.0
	loss_policy_3: 0.01002
	accuracy_policy_3: 0.79559
	loss_value_3: 0.04106
	loss_reward_3: 0.0
	loss_policy_4: 0.01001
	accuracy_policy_4: 0.79426
	loss_value_4: 0.04101
	loss_reward_4: 0.0
	loss_policy_5: 0.01046
	accuracy_policy_5: 0.79555
	loss_value_5: 0.04098
	loss_reward_5: 0.0
	loss_policy: 0.08524
	loss_value: 0.41093
	loss_reward: 1e-05
[2024-05-11 23:47:32] nn step 1600, lr: 0.1.
	loss_policy_0: 0.03203
	accuracy_policy_0: 0.84232
	loss_value_0: 0.20339
	loss_policy_1: 0.00796
	accuracy_policy_1: 0.82586
	loss_value_1: 0.04067
	loss_reward_1: 0.0
	loss_policy_2: 0.00832
	accuracy_policy_2: 0.8185
	loss_value_2: 0.04062
	loss_reward_2: 0.0
	loss_policy_3: 0.00853
	accuracy_policy_3: 0.81838
	loss_value_3: 0.04058
	loss_reward_3: 0.0
	loss_policy_4: 0.00847
	accuracy_policy_4: 0.81607
	loss_value_4: 0.04053
	loss_reward_4: 0.0
	loss_policy_5: 0.00887
	accuracy_policy_5: 0.81902
	loss_value_5: 0.04049
	loss_reward_5: 0.0
	loss_policy: 0.07418
	loss_value: 0.40628
	loss_reward: 1e-05
Optimization_Done 1600
[2024-05-11 23:49:52] [command] train weight_iter_1600.pkl 5 9
[2024-05-11 23:50:53] nn step 1700, lr: 0.1.
	loss_policy_0: 0.02168
	accuracy_policy_0: 0.86967
	loss_value_0: 0.19846
	loss_policy_1: 0.00644
	accuracy_policy_1: 0.84352
	loss_value_1: 0.03968
	loss_reward_1: 0.0
	loss_policy_2: 0.00683
	accuracy_policy_2: 0.83357
	loss_value_2: 0.03964
	loss_reward_2: 0.0
	loss_policy_3: 0.00699
	accuracy_policy_3: 0.83105
	loss_value_3: 0.0396
	loss_reward_3: 0.0
	loss_policy_4: 0.00706
	accuracy_policy_4: 0.82898
	loss_value_4: 0.03955
	loss_reward_4: 0.0
	loss_policy_5: 0.00747
	accuracy_policy_5: 0.83178
	loss_value_5: 0.03951
	loss_reward_5: 0.0
	loss_policy: 0.05647
	loss_value: 0.39643
	loss_reward: 1e-05
[2024-05-11 23:51:44] nn step 1800, lr: 0.1.
	loss_policy_0: 0.02074
	accuracy_policy_0: 0.87566
	loss_value_0: 0.20594
	loss_policy_1: 0.00598
	accuracy_policy_1: 0.8507
	loss_value_1: 0.04117
	loss_reward_1: 0.0
	loss_policy_2: 0.00641
	accuracy_policy_2: 0.83977
	loss_value_2: 0.04111
	loss_reward_2: 0.0
	loss_policy_3: 0.00649
	accuracy_policy_3: 0.84135
	loss_value_3: 0.04106
	loss_reward_3: 0.0
	loss_policy_4: 0.0066
	accuracy_policy_4: 0.84041
	loss_value_4: 0.04101
	loss_reward_4: 0.0
	loss_policy_5: 0.00692
	accuracy_policy_5: 0.83998
	loss_value_5: 0.04096
	loss_reward_5: 0.0
	loss_policy: 0.05315
	loss_value: 0.41126
	loss_reward: 1e-05
Optimization_Done 1800
[2024-05-11 23:53:55] [command] train weight_iter_1800.pkl 6 10
[2024-05-11 23:54:56] nn step 1900, lr: 0.1.
	loss_policy_0: 0.01689
	accuracy_policy_0: 0.88219
	loss_value_0: 0.20078
	loss_policy_1: 0.00606
	accuracy_policy_1: 0.84191
	loss_value_1: 0.04013
	loss_reward_1: 0.0
	loss_policy_2: 0.00678
	accuracy_policy_2: 0.82801
	loss_value_2: 0.04008
	loss_reward_2: 0.0
	loss_policy_3: 0.00712
	accuracy_policy_3: 0.81779
	loss_value_3: 0.04002
	loss_reward_3: 0.0
	loss_policy_4: 0.00751
	accuracy_policy_4: 0.80566
	loss_value_4: 0.03996
	loss_reward_4: 0.0
	loss_policy_5: 0.00788
	accuracy_policy_5: 0.80348
	loss_value_5: 0.03989
	loss_reward_5: 0.0
	loss_policy: 0.05224
	loss_value: 0.40087
	loss_reward: 1e-05
[2024-05-11 23:55:59] nn step 2000, lr: 0.1.
	loss_policy_0: 0.01543
	accuracy_policy_0: 0.88914
	loss_value_0: 0.19407
	loss_policy_1: 0.00531
	accuracy_policy_1: 0.85043
	loss_value_1: 0.0388
	loss_reward_1: 0.0
	loss_policy_2: 0.00576
	accuracy_policy_2: 0.8366
	loss_value_2: 0.03874
	loss_reward_2: 0.0
	loss_policy_3: 0.00593
	accuracy_policy_3: 0.8274
	loss_value_3: 0.03867
	loss_reward_3: 0.0
	loss_policy_4: 0.00617
	accuracy_policy_4: 0.81693
	loss_value_4: 0.0386
	loss_reward_4: 0.0
	loss_policy_5: 0.0064
	accuracy_policy_5: 0.81248
	loss_value_5: 0.03852
	loss_reward_5: 0.0
	loss_policy: 0.04499
	loss_value: 0.38741
	loss_reward: 1e-05
Optimization_Done 2000
[2024-05-11 23:58:16] [command] train weight_iter_2000.pkl 7 11
[2024-05-11 23:59:18] nn step 2100, lr: 0.1.
	loss_policy_0: 0.01628
	accuracy_policy_0: 0.87258
	loss_value_0: 0.18699
	loss_policy_1: 0.00549
	accuracy_policy_1: 0.84014
	loss_value_1: 0.0374
	loss_reward_1: 0.0
	loss_policy_2: 0.00609
	accuracy_policy_2: 0.82559
	loss_value_2: 0.03732
	loss_reward_2: 0.0
	loss_policy_3: 0.00644
	accuracy_policy_3: 0.81035
	loss_value_3: 0.03723
	loss_reward_3: 0.0
	loss_policy_4: 0.00673
	accuracy_policy_4: 0.79748
	loss_value_4: 0.03715
	loss_reward_4: 0.0
	loss_policy_5: 0.00717
	accuracy_policy_5: 0.79518
	loss_value_5: 0.03706
	loss_reward_5: 0.0
	loss_policy: 0.0482
	loss_value: 0.37315
	loss_reward: 1e-05
[2024-05-12 00:00:19] nn step 2200, lr: 0.1.
	loss_policy_0: 0.0135
	accuracy_policy_0: 0.8835
	loss_value_0: 0.17397
	loss_policy_1: 0.00454
	accuracy_policy_1: 0.8499
	loss_value_1: 0.03479
	loss_reward_1: 0.0
	loss_policy_2: 0.00496
	accuracy_policy_2: 0.83502
	loss_value_2: 0.03472
	loss_reward_2: 0.0
	loss_policy_3: 0.00518
	accuracy_policy_3: 0.8259
	loss_value_3: 0.03463
	loss_reward_3: 0.0
	loss_policy_4: 0.00529
	accuracy_policy_4: 0.81635
	loss_value_4: 0.03454
	loss_reward_4: 0.0
	loss_policy_5: 0.00561
	accuracy_policy_5: 0.81025
	loss_value_5: 0.03444
	loss_reward_5: 0.0
	loss_policy: 0.03909
	loss_value: 0.3471
	loss_reward: 1e-05
Optimization_Done 2200
[2024-05-12 00:02:33] [command] train weight_iter_2200.pkl 8 12
[2024-05-12 00:03:36] nn step 2300, lr: 0.1.
	loss_policy_0: 0.02558
	accuracy_policy_0: 0.83426
	loss_value_0: 0.17243
	loss_policy_1: 0.00744
	accuracy_policy_1: 0.79762
	loss_value_1: 0.03448
	loss_reward_1: 0.0
	loss_policy_2: 0.00783
	accuracy_policy_2: 0.78854
	loss_value_2: 0.03439
	loss_reward_2: 0.0
	loss_policy_3: 0.00778
	accuracy_policy_3: 0.78814
	loss_value_3: 0.0343
	loss_reward_3: 0.0
	loss_policy_4: 0.00803
	accuracy_policy_4: 0.78328
	loss_value_4: 0.03422
	loss_reward_4: 0.0
	loss_policy_5: 0.00834
	accuracy_policy_5: 0.77064
	loss_value_5: 0.03413
	loss_reward_5: 0.0
	loss_policy: 0.06499
	loss_value: 0.34395
	loss_reward: 1e-05
[2024-05-12 00:04:36] nn step 2400, lr: 0.1.
	loss_policy_0: 0.02243
	accuracy_policy_0: 0.8465
	loss_value_0: 0.17208
	loss_policy_1: 0.00662
	accuracy_policy_1: 0.8125
	loss_value_1: 0.0344
	loss_reward_1: 0.0
	loss_policy_2: 0.00694
	accuracy_policy_2: 0.80762
	loss_value_2: 0.03432
	loss_reward_2: 0.0
	loss_policy_3: 0.00696
	accuracy_policy_3: 0.80438
	loss_value_3: 0.03423
	loss_reward_3: 0.0
	loss_policy_4: 0.00699
	accuracy_policy_4: 0.79764
	loss_value_4: 0.03414
	loss_reward_4: 0.0
	loss_policy_5: 0.00739
	accuracy_policy_5: 0.78404
	loss_value_5: 0.03406
	loss_reward_5: 0.0
	loss_policy: 0.05734
	loss_value: 0.34324
	loss_reward: 1e-05
Optimization_Done 2400
[2024-05-12 00:06:50] [command] train weight_iter_2400.pkl 9 13
[2024-05-12 00:07:49] nn step 2500, lr: 0.1.
	loss_policy_0: 0.02236
	accuracy_policy_0: 0.86027
	loss_value_0: 0.16571
	loss_policy_1: 0.00654
	accuracy_policy_1: 0.83109
	loss_value_1: 0.03307
	loss_reward_1: 0.00072
	loss_policy_2: 0.00684
	accuracy_policy_2: 0.82777
	loss_value_2: 0.03292
	loss_reward_2: 0.00071
	loss_policy_3: 0.00684
	accuracy_policy_3: 0.81658
	loss_value_3: 0.03279
	loss_reward_3: 0.00063
	loss_policy_4: 0.00691
	accuracy_policy_4: 0.81152
	loss_value_4: 0.0326
	loss_reward_4: 0.00076
	loss_policy_5: 0.0072
	accuracy_policy_5: 0.80242
	loss_value_5: 0.03242
	loss_reward_5: 0.00066
	loss_policy: 0.05668
	loss_value: 0.32952
	loss_reward: 0.00348
[2024-05-12 00:08:50] nn step 2600, lr: 0.1.
	loss_policy_0: 0.02112
	accuracy_policy_0: 0.8698
	loss_value_0: 0.1718
	loss_policy_1: 0.00621
	accuracy_policy_1: 0.84145
	loss_value_1: 0.03432
	loss_reward_1: 0.00059
	loss_policy_2: 0.00643
	accuracy_policy_2: 0.83588
	loss_value_2: 0.03418
	loss_reward_2: 0.00054
	loss_policy_3: 0.0064
	accuracy_policy_3: 0.83039
	loss_value_3: 0.03405
	loss_reward_3: 0.00053
	loss_policy_4: 0.00648
	accuracy_policy_4: 0.82457
	loss_value_4: 0.03388
	loss_reward_4: 0.0006
	loss_policy_5: 0.00688
	accuracy_policy_5: 0.81436
	loss_value_5: 0.03371
	loss_reward_5: 0.00058
	loss_policy: 0.05352
	loss_value: 0.34194
	loss_reward: 0.00284
Optimization_Done 2600
[2024-05-12 00:10:55] [command] train weight_iter_2600.pkl 10 14
[2024-05-12 00:11:56] nn step 2700, lr: 0.1.
	loss_policy_0: 0.019
	accuracy_policy_0: 0.8783
	loss_value_0: 0.15888
	loss_policy_1: 0.00591
	accuracy_policy_1: 0.84773
	loss_value_1: 0.03168
	loss_reward_1: 0.00097
	loss_policy_2: 0.00612
	accuracy_policy_2: 0.84201
	loss_value_2: 0.03147
	loss_reward_2: 0.00097
	loss_policy_3: 0.00623
	accuracy_policy_3: 0.83949
	loss_value_3: 0.03127
	loss_reward_3: 0.00092
	loss_policy_4: 0.00635
	accuracy_policy_4: 0.82951
	loss_value_4: 0.03106
	loss_reward_4: 0.00097
	loss_policy_5: 0.0066
	accuracy_policy_5: 0.82055
	loss_value_5: 0.03086
	loss_reward_5: 0.00088
	loss_policy: 0.05022
	loss_value: 0.31523
	loss_reward: 0.0047
[2024-05-12 00:12:57] nn step 2800, lr: 0.1.
	loss_policy_0: 0.0176
	accuracy_policy_0: 0.884
	loss_value_0: 0.15521
	loss_policy_1: 0.00529
	accuracy_policy_1: 0.86066
	loss_value_1: 0.03094
	loss_reward_1: 0.00087
	loss_policy_2: 0.00552
	accuracy_policy_2: 0.85326
	loss_value_2: 0.03074
	loss_reward_2: 0.00084
	loss_policy_3: 0.00546
	accuracy_policy_3: 0.85012
	loss_value_3: 0.03053
	loss_reward_3: 0.00082
	loss_policy_4: 0.00561
	accuracy_policy_4: 0.84014
	loss_value_4: 0.03032
	loss_reward_4: 0.00083
	loss_policy_5: 0.00582
	accuracy_policy_5: 0.83109
	loss_value_5: 0.03012
	loss_reward_5: 0.0008
	loss_policy: 0.0453
	loss_value: 0.30786
	loss_reward: 0.00415
Optimization_Done 2800
[2024-05-12 00:15:12] [command] train weight_iter_2800.pkl 11 15
[2024-05-12 00:16:13] nn step 2900, lr: 0.1.
	loss_policy_0: 0.01435
	accuracy_policy_0: 0.90447
	loss_value_0: 0.15307
	loss_policy_1: 0.00456
	accuracy_policy_1: 0.88033
	loss_value_1: 0.03055
	loss_reward_1: 0.00076
	loss_policy_2: 0.00471
	accuracy_policy_2: 0.87482
	loss_value_2: 0.03038
	loss_reward_2: 0.00078
	loss_policy_3: 0.00485
	accuracy_policy_3: 0.87006
	loss_value_3: 0.03019
	loss_reward_3: 0.00076
	loss_policy_4: 0.005
	accuracy_policy_4: 0.86432
	loss_value_4: 0.03004
	loss_reward_4: 0.00072
	loss_policy_5: 0.00534
	accuracy_policy_5: 0.85672
	loss_value_5: 0.02985
	loss_reward_5: 0.00072
	loss_policy: 0.03881
	loss_value: 0.30409
	loss_reward: 0.00373
[2024-05-12 00:17:14] nn step 3000, lr: 0.1.
	loss_policy_0: 0.01269
	accuracy_policy_0: 0.90869
	loss_value_0: 0.15008
	loss_policy_1: 0.0041
	accuracy_policy_1: 0.884
	loss_value_1: 0.02996
	loss_reward_1: 0.00075
	loss_policy_2: 0.00422
	accuracy_policy_2: 0.87957
	loss_value_2: 0.02981
	loss_reward_2: 0.00071
	loss_policy_3: 0.00427
	accuracy_policy_3: 0.87867
	loss_value_3: 0.02965
	loss_reward_3: 0.00066
	loss_policy_4: 0.00445
	accuracy_policy_4: 0.87625
	loss_value_4: 0.0295
	loss_reward_4: 0.0007
	loss_policy_5: 0.00473
	accuracy_policy_5: 0.86717
	loss_value_5: 0.02935
	loss_reward_5: 0.00069
	loss_policy: 0.03446
	loss_value: 0.29835
	loss_reward: 0.0035
Optimization_Done 3000
[2024-05-12 00:18:56] [command] train weight_iter_3000.pkl 12 16
[2024-05-12 00:19:53] nn step 3100, lr: 0.1.
	loss_policy_0: 0.01424
	accuracy_policy_0: 0.91078
	loss_value_0: 0.15344
	loss_policy_1: 0.0046
	accuracy_policy_1: 0.88684
	loss_value_1: 0.03063
	loss_reward_1: 0.0007
	loss_policy_2: 0.00478
	accuracy_policy_2: 0.87699
	loss_value_2: 0.03048
	loss_reward_2: 0.00069
	loss_policy_3: 0.0048
	accuracy_policy_3: 0.87725
	loss_value_3: 0.03033
	loss_reward_3: 0.00068
	loss_policy_4: 0.00507
	accuracy_policy_4: 0.86854
	loss_value_4: 0.03018
	loss_reward_4: 0.00068
	loss_policy_5: 0.00537
	accuracy_policy_5: 0.86482
	loss_value_5: 0.03002
	loss_reward_5: 0.0007
	loss_policy: 0.03885
	loss_value: 0.30507
	loss_reward: 0.00346
[2024-05-12 00:20:53] nn step 3200, lr: 0.1.
	loss_policy_0: 0.01318
	accuracy_policy_0: 0.91346
	loss_value_0: 0.14558
	loss_policy_1: 0.0041
	accuracy_policy_1: 0.88877
	loss_value_1: 0.02904
	loss_reward_1: 0.0007
	loss_policy_2: 0.00423
	accuracy_policy_2: 0.88512
	loss_value_2: 0.02888
	loss_reward_2: 0.00068
	loss_policy_3: 0.00426
	accuracy_policy_3: 0.88547
	loss_value_3: 0.02873
	loss_reward_3: 0.00062
	loss_policy_4: 0.00444
	accuracy_policy_4: 0.87803
	loss_value_4: 0.0286
	loss_reward_4: 0.00065
	loss_policy_5: 0.00466
	accuracy_policy_5: 0.87436
	loss_value_5: 0.02847
	loss_reward_5: 0.00061
	loss_policy: 0.03487
	loss_value: 0.28931
	loss_reward: 0.00325
Optimization_Done 3200
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-12 00:49:21] [command] train weight_iter_3200.pkl 14 17
[2024-05-12 00:50:24] nn step 3300, lr: 0.1.
	loss_policy_0: 0.01492
	accuracy_policy_0: 0.90998
	loss_value_0: 0.16059
	loss_policy_1: 0.00471
	accuracy_policy_1: 0.88721
	loss_value_1: 0.03205
	loss_reward_1: 0.00052
	loss_policy_2: 0.00506
	accuracy_policy_2: 0.87885
	loss_value_2: 0.03189
	loss_reward_2: 0.0005
	loss_policy_3: 0.00522
	accuracy_policy_3: 0.8752
	loss_value_3: 0.03173
	loss_reward_3: 0.00051
	loss_policy_4: 0.00572
	accuracy_policy_4: 0.86391
	loss_value_4: 0.03158
	loss_reward_4: 0.00049
	loss_policy_5: 0.0062
	accuracy_policy_5: 0.85096
	loss_value_5: 0.03144
	loss_reward_5: 0.00047
	loss_policy: 0.04184
	loss_value: 0.31928
	loss_reward: 0.00249
[2024-05-12 00:51:13] nn step 3400, lr: 0.1.
	loss_policy_0: 0.01282
	accuracy_policy_0: 0.91959
	loss_value_0: 0.15257
	loss_policy_1: 0.00401
	accuracy_policy_1: 0.89766
	loss_value_1: 0.03044
	loss_reward_1: 0.00047
	loss_policy_2: 0.0041
	accuracy_policy_2: 0.89178
	loss_value_2: 0.0303
	loss_reward_2: 0.00049
	loss_policy_3: 0.00435
	accuracy_policy_3: 0.89188
	loss_value_3: 0.03016
	loss_reward_3: 0.00046
	loss_policy_4: 0.00457
	accuracy_policy_4: 0.88066
	loss_value_4: 0.03003
	loss_reward_4: 0.00047
	loss_policy_5: 0.00495
	accuracy_policy_5: 0.87172
	loss_value_5: 0.02986
	loss_reward_5: 0.00048
	loss_policy: 0.03481
	loss_value: 0.30335
	loss_reward: 0.00237
Optimization_Done 3400
[2024-05-12 00:53:46] [command] train weight_iter_3400.pkl 15 18
[2024-05-12 00:54:53] nn step 3500, lr: 0.1.
	loss_policy_0: 0.01698
	accuracy_policy_0: 0.90945
	loss_value_0: 0.16582
	loss_policy_1: 0.00474
	accuracy_policy_1: 0.89314
	loss_value_1: 0.03313
	loss_reward_1: 1e-05
	loss_policy_2: 0.00508
	accuracy_policy_2: 0.88789
	loss_value_2: 0.03305
	loss_reward_2: 1e-05
	loss_policy_3: 0.00537
	accuracy_policy_3: 0.88684
	loss_value_3: 0.03295
	loss_reward_3: 1e-05
	loss_policy_4: 0.00586
	accuracy_policy_4: 0.88186
	loss_value_4: 0.03285
	loss_reward_4: 1e-05
	loss_policy_5: 0.00647
	accuracy_policy_5: 0.8766
	loss_value_5: 0.03277
	loss_reward_5: 1e-05
	loss_policy: 0.04451
	loss_value: 0.33057
	loss_reward: 3e-05
[2024-05-12 00:55:59] nn step 3600, lr: 0.1.
	loss_policy_0: 0.01347
	accuracy_policy_0: 0.91844
	loss_value_0: 0.15537
	loss_policy_1: 0.00367
	accuracy_policy_1: 0.90441
	loss_value_1: 0.03105
	loss_reward_1: 0.0
	loss_policy_2: 0.00375
	accuracy_policy_2: 0.90188
	loss_value_2: 0.03095
	loss_reward_2: 0.0
	loss_policy_3: 0.00382
	accuracy_policy_3: 0.90613
	loss_value_3: 0.03087
	loss_reward_3: 0.0
	loss_policy_4: 0.00417
	accuracy_policy_4: 0.89734
	loss_value_4: 0.03077
	loss_reward_4: 0.0
	loss_policy_5: 0.00439
	accuracy_policy_5: 0.89662
	loss_value_5: 0.03068
	loss_reward_5: 0.0
	loss_policy: 0.03327
	loss_value: 0.30969
	loss_reward: 1e-05
Optimization_Done 3600
[2024-05-12 00:58:21] [command] train weight_iter_3600.pkl 16 19
[2024-05-12 00:59:25] nn step 3700, lr: 0.1.
	loss_policy_0: 0.018
	accuracy_policy_0: 0.8858
	loss_value_0: 0.11604
	loss_policy_1: 0.00484
	accuracy_policy_1: 0.86299
	loss_value_1: 0.02308
	loss_reward_1: 0.00298
	loss_policy_2: 0.00498
	accuracy_policy_2: 0.85693
	loss_value_2: 0.02267
	loss_reward_2: 0.00305
	loss_policy_3: 0.00538
	accuracy_policy_3: 0.84795
	loss_value_3: 0.02224
	loss_reward_3: 0.00302
	loss_policy_4: 0.00559
	accuracy_policy_4: 0.8473
	loss_value_4: 0.0217
	loss_reward_4: 0.00312
	loss_policy_5: 0.00598
	accuracy_policy_5: 0.84229
	loss_value_5: 0.02113
	loss_reward_5: 0.00313
	loss_policy: 0.04477
	loss_value: 0.22685
	loss_reward: 0.0153
[2024-05-12 01:00:30] nn step 3800, lr: 0.1.
	loss_policy_0: 0.01001
	accuracy_policy_0: 0.91232
	loss_value_0: 0.11319
	loss_policy_1: 0.00276
	accuracy_policy_1: 0.89975
	loss_value_1: 0.02272
	loss_reward_1: 0.00246
	loss_policy_2: 0.00295
	accuracy_policy_2: 0.88727
	loss_value_2: 0.02265
	loss_reward_2: 0.00238
	loss_policy_3: 0.00317
	accuracy_policy_3: 0.87943
	loss_value_3: 0.0226
	loss_reward_3: 0.0023
	loss_policy_4: 0.00335
	accuracy_policy_4: 0.87568
	loss_value_4: 0.02245
	loss_reward_4: 0.00243
	loss_policy_5: 0.00348
	accuracy_policy_5: 0.87418
	loss_value_5: 0.02222
	loss_reward_5: 0.00244
	loss_policy: 0.02572
	loss_value: 0.22583
	loss_reward: 0.01202
Optimization_Done 3800
[2024-05-12 01:02:54] [command] train weight_iter_3800.pkl 17 20
[2024-05-12 01:03:54] nn step 3900, lr: 0.1.
	loss_policy_0: 0.03738
	accuracy_policy_0: 0.79627
	loss_value_0: 0.1169
	loss_policy_1: 0.00885
	accuracy_policy_1: 0.77621
	loss_value_1: 0.02343
	loss_reward_1: 0.00308
	loss_policy_2: 0.00873
	accuracy_policy_2: 0.77496
	loss_value_2: 0.02345
	loss_reward_2: 0.00298
	loss_policy_3: 0.00905
	accuracy_policy_3: 0.76881
	loss_value_3: 0.02354
	loss_reward_3: 0.00286
	loss_policy_4: 0.00921
	accuracy_policy_4: 0.76484
	loss_value_4: 0.02349
	loss_reward_4: 0.00302
	loss_policy_5: 0.00941
	accuracy_policy_5: 0.76373
	loss_value_5: 0.02338
	loss_reward_5: 0.00308
	loss_policy: 0.08263
	loss_value: 0.23421
	loss_reward: 0.01502
[2024-05-12 01:04:54] nn step 4000, lr: 0.1.
	loss_policy_0: 0.02602
	accuracy_policy_0: 0.83035
	loss_value_0: 0.10989
	loss_policy_1: 0.0064
	accuracy_policy_1: 0.81377
	loss_value_1: 0.02211
	loss_reward_1: 0.00279
	loss_policy_2: 0.00634
	accuracy_policy_2: 0.80807
	loss_value_2: 0.02226
	loss_reward_2: 0.00274
	loss_policy_3: 0.00638
	accuracy_policy_3: 0.80869
	loss_value_3: 0.0224
	loss_reward_3: 0.0026
	loss_policy_4: 0.0066
	accuracy_policy_4: 0.80273
	loss_value_4: 0.02247
	loss_reward_4: 0.00275
	loss_policy_5: 0.00679
	accuracy_policy_5: 0.79984
	loss_value_5: 0.0225
	loss_reward_5: 0.00286
	loss_policy: 0.05852
	loss_value: 0.22162
	loss_reward: 0.01374
Optimization_Done 4000
[2024-05-12 01:07:11] [command] train weight_iter_4000.pkl 18 21
[2024-05-12 01:08:04] nn step 4100, lr: 0.1.
	loss_policy_0: 0.03565
	accuracy_policy_0: 0.79305
	loss_value_0: 0.12159
	loss_policy_1: 0.00838
	accuracy_policy_1: 0.7798
	loss_value_1: 0.02452
	loss_reward_1: 0.00345
	loss_policy_2: 0.00862
	accuracy_policy_2: 0.77189
	loss_value_2: 0.02475
	loss_reward_2: 0.00331
	loss_policy_3: 0.00883
	accuracy_policy_3: 0.76363
	loss_value_3: 0.0249
	loss_reward_3: 0.00325
	loss_policy_4: 0.00891
	accuracy_policy_4: 0.7598
	loss_value_4: 0.025
	loss_reward_4: 0.00332
	loss_policy_5: 0.00907
	accuracy_policy_5: 0.76104
	loss_value_5: 0.02504
	loss_reward_5: 0.00346
	loss_policy: 0.07946
	loss_value: 0.2458
	loss_reward: 0.01679
[2024-05-12 01:09:04] nn step 4200, lr: 0.1.
	loss_policy_0: 0.03268
	accuracy_policy_0: 0.80738
	loss_value_0: 0.12124
	loss_policy_1: 0.00786
	accuracy_policy_1: 0.79072
	loss_value_1: 0.02453
	loss_reward_1: 0.00339
	loss_policy_2: 0.00795
	accuracy_policy_2: 0.78623
	loss_value_2: 0.02476
	loss_reward_2: 0.00324
	loss_policy_3: 0.008
	accuracy_policy_3: 0.77824
	loss_value_3: 0.02498
	loss_reward_3: 0.00319
	loss_policy_4: 0.00799
	accuracy_policy_4: 0.77979
	loss_value_4: 0.02512
	loss_reward_4: 0.00329
	loss_policy_5: 0.00829
	accuracy_policy_5: 0.77717
	loss_value_5: 0.02524
	loss_reward_5: 0.00346
	loss_policy: 0.07278
	loss_value: 0.24587
	loss_reward: 0.01656
Optimization_Done 4200
[2024-05-12 01:11:18] [command] train weight_iter_4200.pkl 19 22
[2024-05-12 01:12:21] nn step 4300, lr: 0.1.
	loss_policy_0: 0.03423
	accuracy_policy_0: 0.81805
	loss_value_0: 0.13315
	loss_policy_1: 0.00819
	accuracy_policy_1: 0.80174
	loss_value_1: 0.02681
	loss_reward_1: 0.00384
	loss_policy_2: 0.00832
	accuracy_policy_2: 0.79453
	loss_value_2: 0.02698
	loss_reward_2: 0.00363
	loss_policy_3: 0.00841
	accuracy_policy_3: 0.78811
	loss_value_3: 0.02711
	loss_reward_3: 0.00357
	loss_policy_4: 0.00835
	accuracy_policy_4: 0.79021
	loss_value_4: 0.02724
	loss_reward_4: 0.00375
	loss_policy_5: 0.00876
	accuracy_policy_5: 0.78666
	loss_value_5: 0.02735
	loss_reward_5: 0.00386
	loss_policy: 0.07626
	loss_value: 0.26865
	loss_reward: 0.01865
[2024-05-12 01:13:23] nn step 4400, lr: 0.1.
	loss_policy_0: 0.03269
	accuracy_policy_0: 0.82621
	loss_value_0: 0.13409
	loss_policy_1: 0.00763
	accuracy_policy_1: 0.81131
	loss_value_1: 0.02703
	loss_reward_1: 0.00371
	loss_policy_2: 0.00768
	accuracy_policy_2: 0.80779
	loss_value_2: 0.02723
	loss_reward_2: 0.00348
	loss_policy_3: 0.00783
	accuracy_policy_3: 0.80062
	loss_value_3: 0.0274
	loss_reward_3: 0.00348
	loss_policy_4: 0.00784
	accuracy_policy_4: 0.79879
	loss_value_4: 0.02753
	loss_reward_4: 0.0036
	loss_policy_5: 0.00819
	accuracy_policy_5: 0.79002
	loss_value_5: 0.02763
	loss_reward_5: 0.00373
	loss_policy: 0.07186
	loss_value: 0.2709
	loss_reward: 0.018
Optimization_Done 4400
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-12 01:28:32] [command] train weight_iter_4400.pkl 21 23
[2024-05-12 01:29:43] nn step 4500, lr: 0.1.
	loss_policy_0: 0.02762
	accuracy_policy_0: 0.84289
	loss_value_0: 0.1413
	loss_policy_1: 0.00704
	accuracy_policy_1: 0.82615
	loss_value_1: 0.02832
	loss_reward_1: 0.00259
	loss_policy_2: 0.00755
	accuracy_policy_2: 0.81346
	loss_value_2: 0.0283
	loss_reward_2: 0.00235
	loss_policy_3: 0.00766
	accuracy_policy_3: 0.80727
	loss_value_3: 0.02827
	loss_reward_3: 0.00231
	loss_policy_4: 0.00783
	accuracy_policy_4: 0.80523
	loss_value_4: 0.02827
	loss_reward_4: 0.00244
	loss_policy_5: 0.00852
	accuracy_policy_5: 0.78332
	loss_value_5: 0.02822
	loss_reward_5: 0.00248
	loss_policy: 0.06622
	loss_value: 0.28268
	loss_reward: 0.01216
[2024-05-12 01:30:48] nn step 4600, lr: 0.1.
	loss_policy_0: 0.02121
	accuracy_policy_0: 0.87035
	loss_value_0: 0.13639
	loss_policy_1: 0.00532
	accuracy_policy_1: 0.85227
	loss_value_1: 0.02732
	loss_reward_1: 0.00221
	loss_policy_2: 0.00571
	accuracy_policy_2: 0.84303
	loss_value_2: 0.02734
	loss_reward_2: 0.00199
	loss_policy_3: 0.00587
	accuracy_policy_3: 0.84002
	loss_value_3: 0.0273
	loss_reward_3: 0.00199
	loss_policy_4: 0.00607
	accuracy_policy_4: 0.83271
	loss_value_4: 0.02725
	loss_reward_4: 0.00212
	loss_policy_5: 0.00664
	accuracy_policy_5: 0.81561
	loss_value_5: 0.02728
	loss_reward_5: 0.00226
	loss_policy: 0.05082
	loss_value: 0.27288
	loss_reward: 0.01057
Optimization_Done 4600
[2024-05-12 01:33:11] [command] train weight_iter_4600.pkl 22 24
[2024-05-12 01:34:11] nn step 4700, lr: 0.1.
	loss_policy_0: 0.01413
	accuracy_policy_0: 0.92383
	loss_value_0: 0.15455
	loss_policy_1: 0.00414
	accuracy_policy_1: 0.90611
	loss_value_1: 0.03087
	loss_reward_1: 0.001
	loss_policy_2: 0.00452
	accuracy_policy_2: 0.89701
	loss_value_2: 0.03076
	loss_reward_2: 0.00095
	loss_policy_3: 0.00494
	accuracy_policy_3: 0.89326
	loss_value_3: 0.03068
	loss_reward_3: 0.00089
	loss_policy_4: 0.00524
	accuracy_policy_4: 0.89221
	loss_value_4: 0.03059
	loss_reward_4: 0.00096
	loss_policy_5: 0.00599
	accuracy_policy_5: 0.87793
	loss_value_5: 0.03052
	loss_reward_5: 0.00099
	loss_policy: 0.03895
	loss_value: 0.30798
	loss_reward: 0.00479
[2024-05-12 01:35:13] nn step 4800, lr: 0.1.
	loss_policy_0: 0.00958
	accuracy_policy_0: 0.93596
	loss_value_0: 0.14584
	loss_policy_1: 0.00286
	accuracy_policy_1: 0.9216
	loss_value_1: 0.02914
	loss_reward_1: 0.00088
	loss_policy_2: 0.00305
	accuracy_policy_2: 0.91766
	loss_value_2: 0.02906
	loss_reward_2: 0.00078
	loss_policy_3: 0.00338
	accuracy_policy_3: 0.91646
	loss_value_3: 0.02897
	loss_reward_3: 0.00075
	loss_policy_4: 0.00361
	accuracy_policy_4: 0.91227
	loss_value_4: 0.02889
	loss_reward_4: 0.00079
	loss_policy_5: 0.00416
	accuracy_policy_5: 0.90195
	loss_value_5: 0.02882
	loss_reward_5: 0.00082
	loss_policy: 0.02665
	loss_value: 0.29071
	loss_reward: 0.00402
Optimization_Done 4800
[2024-05-12 01:37:34] [command] train weight_iter_4800.pkl 23 25
[2024-05-12 01:38:37] nn step 4900, lr: 0.1.
	loss_policy_0: 0.05053
	accuracy_policy_0: 0.80072
	loss_value_0: 0.11761
	loss_policy_1: 0.01263
	accuracy_policy_1: 0.76354
	loss_value_1: 0.02343
	loss_reward_1: 0.00151
	loss_policy_2: 0.01295
	accuracy_policy_2: 0.75834
	loss_value_2: 0.02326
	loss_reward_2: 0.00142
	loss_policy_3: 0.01358
	accuracy_policy_3: 0.75221
	loss_value_3: 0.02312
	loss_reward_3: 0.00144
	loss_policy_4: 0.01399
	accuracy_policy_4: 0.74541
	loss_value_4: 0.02292
	loss_reward_4: 0.0014
	loss_policy_5: 0.01464
	accuracy_policy_5: 0.73643
	loss_value_5: 0.02276
	loss_reward_5: 0.0014
	loss_policy: 0.11832
	loss_value: 0.23309
	loss_reward: 0.00717
[2024-05-12 01:39:42] nn step 5000, lr: 0.1.
	loss_policy_0: 0.0351
	accuracy_policy_0: 0.85393
	loss_value_0: 0.12408
	loss_policy_1: 0.00912
	accuracy_policy_1: 0.82471
	loss_value_1: 0.02472
	loss_reward_1: 0.00154
	loss_policy_2: 0.00949
	accuracy_policy_2: 0.81701
	loss_value_2: 0.0245
	loss_reward_2: 0.00147
	loss_policy_3: 0.00978
	accuracy_policy_3: 0.81738
	loss_value_3: 0.02429
	loss_reward_3: 0.00141
	loss_policy_4: 0.01011
	accuracy_policy_4: 0.81584
	loss_value_4: 0.02412
	loss_reward_4: 0.00141
	loss_policy_5: 0.0109
	accuracy_policy_5: 0.79855
	loss_value_5: 0.02398
	loss_reward_5: 0.00144
	loss_policy: 0.08451
	loss_value: 0.24569
	loss_reward: 0.00727
Optimization_Done 5000
[2024-05-12 01:42:01] [command] train weight_iter_5000.pkl 24 26
[2024-05-12 01:42:54] nn step 5100, lr: 0.1.
	loss_policy_0: 0.05122
	accuracy_policy_0: 0.79256
	loss_value_0: 0.13122
	loss_policy_1: 0.01289
	accuracy_policy_1: 0.75535
	loss_value_1: 0.02631
	loss_reward_1: 0.00378
	loss_policy_2: 0.01332
	accuracy_policy_2: 0.75449
	loss_value_2: 0.02619
	loss_reward_2: 0.00362
	loss_policy_3: 0.01375
	accuracy_policy_3: 0.74744
	loss_value_3: 0.02608
	loss_reward_3: 0.0035
	loss_policy_4: 0.01431
	accuracy_policy_4: 0.74244
	loss_value_4: 0.02588
	loss_reward_4: 0.00383
	loss_policy_5: 0.01473
	accuracy_policy_5: 0.73633
	loss_value_5: 0.0257
	loss_reward_5: 0.00381
	loss_policy: 0.1202
	loss_value: 0.26139
	loss_reward: 0.01855
[2024-05-12 01:43:45] nn step 5200, lr: 0.1.
	loss_policy_0: 0.04027
	accuracy_policy_0: 0.81646
	loss_value_0: 0.12169
	loss_policy_1: 0.01012
	accuracy_policy_1: 0.78062
	loss_value_1: 0.02447
	loss_reward_1: 0.00319
	loss_policy_2: 0.01048
	accuracy_policy_2: 0.77781
	loss_value_2: 0.02447
	loss_reward_2: 0.00309
	loss_policy_3: 0.01066
	accuracy_policy_3: 0.77688
	loss_value_3: 0.02445
	loss_reward_3: 0.00291
	loss_policy_4: 0.01122
	accuracy_policy_4: 0.7699
	loss_value_4: 0.02437
	loss_reward_4: 0.00315
	loss_policy_5: 0.01158
	accuracy_policy_5: 0.76428
	loss_value_5: 0.02433
	loss_reward_5: 0.00316
	loss_policy: 0.09433
	loss_value: 0.24378
	loss_reward: 0.0155
Optimization_Done 5200
[2024-05-12 01:45:57] [command] train weight_iter_5200.pkl 25 27
[2024-05-12 01:47:01] nn step 5300, lr: 0.1.
	loss_policy_0: 0.06072
	accuracy_policy_0: 0.7798
	loss_value_0: 0.13996
	loss_policy_1: 0.01491
	accuracy_policy_1: 0.75195
	loss_value_1: 0.02813
	loss_reward_1: 0.00399
	loss_policy_2: 0.01528
	accuracy_policy_2: 0.74623
	loss_value_2: 0.02824
	loss_reward_2: 0.00367
	loss_policy_3: 0.01589
	accuracy_policy_3: 0.73684
	loss_value_3: 0.02828
	loss_reward_3: 0.00367
	loss_policy_4: 0.01634
	accuracy_policy_4: 0.73012
	loss_value_4: 0.02826
	loss_reward_4: 0.00373
	loss_policy_5: 0.01693
	accuracy_policy_5: 0.71982
	loss_value_5: 0.02824
	loss_reward_5: 0.00399
	loss_policy: 0.14007
	loss_value: 0.28112
	loss_reward: 0.01906
[2024-05-12 01:48:08] nn step 5400, lr: 0.1.
	loss_policy_0: 0.05378
	accuracy_policy_0: 0.80092
	loss_value_0: 0.1429
	loss_policy_1: 0.01321
	accuracy_policy_1: 0.77232
	loss_value_1: 0.02875
	loss_reward_1: 0.00384
	loss_policy_2: 0.01332
	accuracy_policy_2: 0.77182
	loss_value_2: 0.02882
	loss_reward_2: 0.00355
	loss_policy_3: 0.01384
	accuracy_policy_3: 0.76414
	loss_value_3: 0.02893
	loss_reward_3: 0.00351
	loss_policy_4: 0.01426
	accuracy_policy_4: 0.75754
	loss_value_4: 0.02902
	loss_reward_4: 0.00376
	loss_policy_5: 0.01502
	accuracy_policy_5: 0.74959
	loss_value_5: 0.02905
	loss_reward_5: 0.00389
	loss_policy: 0.12343
	loss_value: 0.28747
	loss_reward: 0.01854
Optimization_Done 5400
[2024-05-12 01:50:27] [command] train weight_iter_5400.pkl 26 28
[2024-05-12 01:51:20] nn step 5500, lr: 0.1.
	loss_policy_0: 0.05621
	accuracy_policy_0: 0.8017
	loss_value_0: 0.15561
	loss_policy_1: 0.01394
	accuracy_policy_1: 0.77043
	loss_value_1: 0.0313
	loss_reward_1: 0.00456
	loss_policy_2: 0.01445
	accuracy_policy_2: 0.76486
	loss_value_2: 0.03133
	loss_reward_2: 0.00441
	loss_policy_3: 0.0152
	accuracy_policy_3: 0.75633
	loss_value_3: 0.03142
	loss_reward_3: 0.00425
	loss_policy_4: 0.01581
	accuracy_policy_4: 0.74924
	loss_value_4: 0.03134
	loss_reward_4: 0.00461
	loss_policy_5: 0.01678
	accuracy_policy_5: 0.73834
	loss_value_5: 0.03123
	loss_reward_5: 0.0047
	loss_policy: 0.13238
	loss_value: 0.31222
	loss_reward: 0.02254
[2024-05-12 01:52:21] nn step 5600, lr: 0.1.
	loss_policy_0: 0.0443
	accuracy_policy_0: 0.82428
	loss_value_0: 0.14662
	loss_policy_1: 0.01136
	accuracy_policy_1: 0.79467
	loss_value_1: 0.0295
	loss_reward_1: 0.00426
	loss_policy_2: 0.01167
	accuracy_policy_2: 0.78951
	loss_value_2: 0.02965
	loss_reward_2: 0.00396
	loss_policy_3: 0.01207
	accuracy_policy_3: 0.78816
	loss_value_3: 0.02964
	loss_reward_3: 0.0039
	loss_policy_4: 0.01268
	accuracy_policy_4: 0.77916
	loss_value_4: 0.02969
	loss_reward_4: 0.00435
	loss_policy_5: 0.01359
	accuracy_policy_5: 0.76619
	loss_value_5: 0.02971
	loss_reward_5: 0.00439
	loss_policy: 0.10567
	loss_value: 0.29481
	loss_reward: 0.02085
Optimization_Done 5600
[2024-05-12 01:54:45] [command] train weight_iter_5600.pkl 27 29
[2024-05-12 01:55:49] nn step 5700, lr: 0.1.
	loss_policy_0: 0.07087
	accuracy_policy_0: 0.77598
	loss_value_0: 0.15515
	loss_policy_1: 0.01679
	accuracy_policy_1: 0.74812
	loss_value_1: 0.03116
	loss_reward_1: 0.00459
	loss_policy_2: 0.01742
	accuracy_policy_2: 0.74
	loss_value_2: 0.03141
	loss_reward_2: 0.00423
	loss_policy_3: 0.01824
	accuracy_policy_3: 0.73004
	loss_value_3: 0.03154
	loss_reward_3: 0.00432
	loss_policy_4: 0.01916
	accuracy_policy_4: 0.72201
	loss_value_4: 0.03166
	loss_reward_4: 0.00425
	loss_policy_5: 0.0202
	accuracy_policy_5: 0.71178
	loss_value_5: 0.03172
	loss_reward_5: 0.0047
	loss_policy: 0.16269
	loss_value: 0.31264
	loss_reward: 0.02208
[2024-05-12 01:56:54] nn step 5800, lr: 0.1.
	loss_policy_0: 0.05928
	accuracy_policy_0: 0.80859
	loss_value_0: 0.15967
	loss_policy_1: 0.01436
	accuracy_policy_1: 0.77873
	loss_value_1: 0.03215
	loss_reward_1: 0.00451
	loss_policy_2: 0.0149
	accuracy_policy_2: 0.77207
	loss_value_2: 0.03243
	loss_reward_2: 0.00424
	loss_policy_3: 0.01573
	accuracy_policy_3: 0.76166
	loss_value_3: 0.03258
	loss_reward_3: 0.00427
	loss_policy_4: 0.01653
	accuracy_policy_4: 0.75932
	loss_value_4: 0.03275
	loss_reward_4: 0.00446
	loss_policy_5: 0.0176
	accuracy_policy_5: 0.74625
	loss_value_5: 0.03291
	loss_reward_5: 0.00471
	loss_policy: 0.1384
	loss_value: 0.32248
	loss_reward: 0.02219
Optimization_Done 5800
[2024-05-12 01:59:15] [command] train weight_iter_5800.pkl 28 30
[2024-05-12 02:00:19] nn step 5900, lr: 0.1.
	loss_policy_0: 0.07335
	accuracy_policy_0: 0.76006
	loss_value_0: 0.16358
	loss_policy_1: 0.01718
	accuracy_policy_1: 0.73459
	loss_value_1: 0.03327
	loss_reward_1: 0.00606
	loss_policy_2: 0.01788
	accuracy_policy_2: 0.72484
	loss_value_2: 0.03375
	loss_reward_2: 0.00565
	loss_policy_3: 0.01883
	accuracy_policy_3: 0.71293
	loss_value_3: 0.03417
	loss_reward_3: 0.00565
	loss_policy_4: 0.01957
	accuracy_policy_4: 0.7002
	loss_value_4: 0.03451
	loss_reward_4: 0.00586
	loss_policy_5: 0.02078
	accuracy_policy_5: 0.69008
	loss_value_5: 0.03489
	loss_reward_5: 0.00628
	loss_policy: 0.1676
	loss_value: 0.33416
	loss_reward: 0.0295
[2024-05-12 02:01:25] nn step 6000, lr: 0.1.
	loss_policy_0: 0.05988
	accuracy_policy_0: 0.79324
	loss_value_0: 0.15882
	loss_policy_1: 0.01435
	accuracy_policy_1: 0.76494
	loss_value_1: 0.03224
	loss_reward_1: 0.00543
	loss_policy_2: 0.01489
	accuracy_policy_2: 0.75541
	loss_value_2: 0.03271
	loss_reward_2: 0.00501
	loss_policy_3: 0.01554
	accuracy_policy_3: 0.74824
	loss_value_3: 0.03305
	loss_reward_3: 0.00521
	loss_policy_4: 0.01618
	accuracy_policy_4: 0.74055
	loss_value_4: 0.03336
	loss_reward_4: 0.00562
	loss_policy_5: 0.01708
	accuracy_policy_5: 0.73311
	loss_value_5: 0.03379
	loss_reward_5: 0.00564
	loss_policy: 0.13792
	loss_value: 0.32396
	loss_reward: 0.02691
Optimization_Done 6000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-12 02:06:10] [command] train weight_iter_6000.pkl 29 31
[2024-05-12 02:07:15] nn step 6100, lr: 0.09.
	loss_policy_0: 0.08021
	accuracy_policy_0: 0.78775
	loss_value_0: 0.18293
	loss_policy_1: 0.01909
	accuracy_policy_1: 0.76
	loss_value_1: 0.03725
	loss_reward_1: 0.00623
	loss_policy_2: 0.02023
	accuracy_policy_2: 0.74508
	loss_value_2: 0.03784
	loss_reward_2: 0.0059
	loss_policy_3: 0.0213
	accuracy_policy_3: 0.73443
	loss_value_3: 0.03843
	loss_reward_3: 0.00606
	loss_policy_4: 0.02239
	accuracy_policy_4: 0.7284
	loss_value_4: 0.03886
	loss_reward_4: 0.00631
	loss_policy_5: 0.02414
	accuracy_policy_5: 0.71053
	loss_value_5: 0.03923
	loss_reward_5: 0.0067
	loss_policy: 0.18735
	loss_value: 0.37453
	loss_reward: 0.03121
[2024-05-12 02:08:06] nn step 6200, lr: 0.09.
	loss_policy_0: 0.06909
	accuracy_policy_0: 0.80453
	loss_value_0: 0.17875
	loss_policy_1: 0.01636
	accuracy_policy_1: 0.77604
	loss_value_1: 0.03643
	loss_reward_1: 0.00584
	loss_policy_2: 0.01749
	accuracy_policy_2: 0.76795
	loss_value_2: 0.03704
	loss_reward_2: 0.00547
	loss_policy_3: 0.01826
	accuracy_policy_3: 0.75979
	loss_value_3: 0.03765
	loss_reward_3: 0.00565
	loss_policy_4: 0.01949
	accuracy_policy_4: 0.74811
	loss_value_4: 0.03817
	loss_reward_4: 0.0059
	loss_policy_5: 0.02071
	accuracy_policy_5: 0.73918
	loss_value_5: 0.03871
	loss_reward_5: 0.0063
	loss_policy: 0.16139
	loss_value: 0.36676
	loss_reward: 0.02917
Optimization_Done 6200
[2024-05-12 02:10:24] [command] train weight_iter_6200.pkl 30 32
[2024-05-12 02:11:26] nn step 6300, lr: 0.09.
	loss_policy_0: 0.06683
	accuracy_policy_0: 0.79982
	loss_value_0: 0.17256
	loss_policy_1: 0.01598
	accuracy_policy_1: 0.77148
	loss_value_1: 0.0351
	loss_reward_1: 0.00574
	loss_policy_2: 0.01725
	accuracy_policy_2: 0.76234
	loss_value_2: 0.03562
	loss_reward_2: 0.00547
	loss_policy_3: 0.01836
	accuracy_policy_3: 0.7507
	loss_value_3: 0.03603
	loss_reward_3: 0.00561
	loss_policy_4: 0.01917
	accuracy_policy_4: 0.74037
	loss_value_4: 0.03632
	loss_reward_4: 0.00594
	loss_policy_5: 0.02047
	accuracy_policy_5: 0.73086
	loss_value_5: 0.03671
	loss_reward_5: 0.00614
	loss_policy: 0.15806
	loss_value: 0.35234
	loss_reward: 0.0289
[2024-05-12 02:12:30] nn step 6400, lr: 0.09.
	loss_policy_0: 0.05373
	accuracy_policy_0: 0.82781
	loss_value_0: 0.17082
	loss_policy_1: 0.0135
	accuracy_policy_1: 0.79834
	loss_value_1: 0.03475
	loss_reward_1: 0.00558
	loss_policy_2: 0.01452
	accuracy_policy_2: 0.78758
	loss_value_2: 0.0353
	loss_reward_2: 0.00534
	loss_policy_3: 0.01551
	accuracy_policy_3: 0.77898
	loss_value_3: 0.0357
	loss_reward_3: 0.00542
	loss_policy_4: 0.01653
	accuracy_policy_4: 0.77016
	loss_value_4: 0.03618
	loss_reward_4: 0.00582
	loss_policy_5: 0.01798
	accuracy_policy_5: 0.75359
	loss_value_5: 0.03654
	loss_reward_5: 0.00615
	loss_policy: 0.13177
	loss_value: 0.3493
	loss_reward: 0.02831
Optimization_Done 6400
[2024-05-12 02:14:52] [command] train weight_iter_6400.pkl 31 33
[2024-05-12 02:15:49] nn step 6500, lr: 0.09.
	loss_policy_0: 0.08144
	accuracy_policy_0: 0.7873
	loss_value_0: 0.16671
	loss_policy_1: 0.01989
	accuracy_policy_1: 0.75221
	loss_value_1: 0.03382
	loss_reward_1: 0.00486
	loss_policy_2: 0.02187
	accuracy_policy_2: 0.73582
	loss_value_2: 0.03422
	loss_reward_2: 0.00448
	loss_policy_3: 0.02314
	accuracy_policy_3: 0.72418
	loss_value_3: 0.03454
	loss_reward_3: 0.00461
	loss_policy_4: 0.0246
	accuracy_policy_4: 0.71393
	loss_value_4: 0.03484
	loss_reward_4: 0.00487
	loss_policy_5: 0.02618
	accuracy_policy_5: 0.70256
	loss_value_5: 0.03501
	loss_reward_5: 0.0053
	loss_policy: 0.19712
	loss_value: 0.33914
	loss_reward: 0.02411
[2024-05-12 02:16:49] nn step 6600, lr: 0.09.
	loss_policy_0: 0.06527
	accuracy_policy_0: 0.81787
	loss_value_0: 0.16972
	loss_policy_1: 0.01686
	accuracy_policy_1: 0.78021
	loss_value_1: 0.03434
	loss_reward_1: 0.00509
	loss_policy_2: 0.01847
	accuracy_policy_2: 0.77043
	loss_value_2: 0.03465
	loss_reward_2: 0.00467
	loss_policy_3: 0.01992
	accuracy_policy_3: 0.75918
	loss_value_3: 0.03488
	loss_reward_3: 0.00468
	loss_policy_4: 0.02143
	accuracy_policy_4: 0.7477
	loss_value_4: 0.03513
	loss_reward_4: 0.00495
	loss_policy_5: 0.023
	accuracy_policy_5: 0.73391
	loss_value_5: 0.03541
	loss_reward_5: 0.00551
	loss_policy: 0.16494
	loss_value: 0.34412
	loss_reward: 0.02489
Optimization_Done 6600
[2024-05-12 02:19:07] [command] train weight_iter_6600.pkl 32 34
[2024-05-12 02:20:09] nn step 6700, lr: 0.09.
	loss_policy_0: 0.08411
	accuracy_policy_0: 0.77166
	loss_value_0: 0.1692
	loss_policy_1: 0.02138
	accuracy_policy_1: 0.72836
	loss_value_1: 0.03436
	loss_reward_1: 0.00577
	loss_policy_2: 0.02293
	accuracy_policy_2: 0.71707
	loss_value_2: 0.03483
	loss_reward_2: 0.00529
	loss_policy_3: 0.02454
	accuracy_policy_3: 0.70045
	loss_value_3: 0.03531
	loss_reward_3: 0.00545
	loss_policy_4: 0.02587
	accuracy_policy_4: 0.68781
	loss_value_4: 0.03558
	loss_reward_4: 0.00577
	loss_policy_5: 0.02725
	accuracy_policy_5: 0.67895
	loss_value_5: 0.03585
	loss_reward_5: 0.00599
	loss_policy: 0.20608
	loss_value: 0.34513
	loss_reward: 0.02827
[2024-05-12 02:21:11] nn step 6800, lr: 0.09.
	loss_policy_0: 0.07598
	accuracy_policy_0: 0.79379
	loss_value_0: 0.17146
	loss_policy_1: 0.01945
	accuracy_policy_1: 0.75246
	loss_value_1: 0.0348
	loss_reward_1: 0.00574
	loss_policy_2: 0.02093
	accuracy_policy_2: 0.74059
	loss_value_2: 0.03519
	loss_reward_2: 0.00528
	loss_policy_3: 0.02231
	accuracy_policy_3: 0.73059
	loss_value_3: 0.03559
	loss_reward_3: 0.00556
	loss_policy_4: 0.02394
	accuracy_policy_4: 0.71658
	loss_value_4: 0.03594
	loss_reward_4: 0.00577
	loss_policy_5: 0.02546
	accuracy_policy_5: 0.70285
	loss_value_5: 0.03633
	loss_reward_5: 0.00614
	loss_policy: 0.18806
	loss_value: 0.34932
	loss_reward: 0.02849
Optimization_Done 6800
[2024-05-12 02:23:19] [command] train weight_iter_6800.pkl 33 35
[2024-05-12 02:24:10] nn step 6900, lr: 0.09.
	loss_policy_0: 0.09401
	accuracy_policy_0: 0.78674
	loss_value_0: 0.18308
	loss_policy_1: 0.02412
	accuracy_policy_1: 0.7427
	loss_value_1: 0.03702
	loss_reward_1: 0.00577
	loss_policy_2: 0.02645
	accuracy_policy_2: 0.7218
	loss_value_2: 0.03735
	loss_reward_2: 0.00521
	loss_policy_3: 0.02886
	accuracy_policy_3: 0.7002
	loss_value_3: 0.03783
	loss_reward_3: 0.00534
	loss_policy_4: 0.03067
	accuracy_policy_4: 0.68916
	loss_value_4: 0.0382
	loss_reward_4: 0.00573
	loss_policy_5: 0.03221
	accuracy_policy_5: 0.67809
	loss_value_5: 0.03862
	loss_reward_5: 0.00642
	loss_policy: 0.23632
	loss_value: 0.37209
	loss_reward: 0.02848
[2024-05-12 02:25:11] nn step 7000, lr: 0.09.
	loss_policy_0: 0.08157
	accuracy_policy_0: 0.80033
	loss_value_0: 0.17428
	loss_policy_1: 0.0214
	accuracy_policy_1: 0.75305
	loss_value_1: 0.03526
	loss_reward_1: 0.00556
	loss_policy_2: 0.02327
	accuracy_policy_2: 0.73482
	loss_value_2: 0.03558
	loss_reward_2: 0.00481
	loss_policy_3: 0.02537
	accuracy_policy_3: 0.7182
	loss_value_3: 0.03593
	loss_reward_3: 0.00495
	loss_policy_4: 0.02666
	accuracy_policy_4: 0.70727
	loss_value_4: 0.03633
	loss_reward_4: 0.00559
	loss_policy_5: 0.02862
	accuracy_policy_5: 0.69287
	loss_value_5: 0.03671
	loss_reward_5: 0.00608
	loss_policy: 0.20689
	loss_value: 0.35409
	loss_reward: 0.027
Optimization_Done 7000
[2024-05-12 02:27:29] [command] train weight_iter_7000.pkl 34 36
[2024-05-12 02:28:33] nn step 7100, lr: 0.09.
	loss_policy_0: 0.07471
	accuracy_policy_0: 0.8168
	loss_value_0: 0.18132
	loss_policy_1: 0.01993
	accuracy_policy_1: 0.77615
	loss_value_1: 0.03682
	loss_reward_1: 0.00581
	loss_policy_2: 0.02226
	accuracy_policy_2: 0.75352
	loss_value_2: 0.03729
	loss_reward_2: 0.00559
	loss_policy_3: 0.02429
	accuracy_policy_3: 0.73383
	loss_value_3: 0.03769
	loss_reward_3: 0.00552
	loss_policy_4: 0.02599
	accuracy_policy_4: 0.71979
	loss_value_4: 0.03803
	loss_reward_4: 0.00604
	loss_policy_5: 0.02779
	accuracy_policy_5: 0.70551
	loss_value_5: 0.03837
	loss_reward_5: 0.00659
	loss_policy: 0.19497
	loss_value: 0.36953
	loss_reward: 0.02954
[2024-05-12 02:29:36] nn step 7200, lr: 0.09.
	loss_policy_0: 0.06316
	accuracy_policy_0: 0.83227
	loss_value_0: 0.17532
	loss_policy_1: 0.01745
	accuracy_policy_1: 0.78893
	loss_value_1: 0.03563
	loss_reward_1: 0.00564
	loss_policy_2: 0.01977
	accuracy_policy_2: 0.76852
	loss_value_2: 0.036
	loss_reward_2: 0.00516
	loss_policy_3: 0.02151
	accuracy_policy_3: 0.75129
	loss_value_3: 0.03641
	loss_reward_3: 0.0053
	loss_policy_4: 0.02324
	accuracy_policy_4: 0.7291
	loss_value_4: 0.03674
	loss_reward_4: 0.00553
	loss_policy_5: 0.02502
	accuracy_policy_5: 0.71789
	loss_value_5: 0.03708
	loss_reward_5: 0.00635
	loss_policy: 0.17017
	loss_value: 0.35717
	loss_reward: 0.02798
Optimization_Done 7200
[2024-05-12 02:31:56] [command] train weight_iter_7200.pkl 35 37
[2024-05-12 02:32:56] nn step 7300, lr: 0.09.
	loss_policy_0: 0.07874
	accuracy_policy_0: 0.80854
	loss_value_0: 0.16892
	loss_policy_1: 0.0208
	accuracy_policy_1: 0.76488
	loss_value_1: 0.03431
	loss_reward_1: 0.00491
	loss_policy_2: 0.02313
	accuracy_policy_2: 0.73828
	loss_value_2: 0.03476
	loss_reward_2: 0.00469
	loss_policy_3: 0.02532
	accuracy_policy_3: 0.72105
	loss_value_3: 0.03515
	loss_reward_3: 0.0047
	loss_policy_4: 0.02739
	accuracy_policy_4: 0.70039
	loss_value_4: 0.03546
	loss_reward_4: 0.00516
	loss_policy_5: 0.02941
	accuracy_policy_5: 0.68576
	loss_value_5: 0.03572
	loss_reward_5: 0.00576
	loss_policy: 0.20479
	loss_value: 0.34432
	loss_reward: 0.02522
[2024-05-12 02:33:56] nn step 7400, lr: 0.09.
	loss_policy_0: 0.0688
	accuracy_policy_0: 0.8274
	loss_value_0: 0.17121
	loss_policy_1: 0.01872
	accuracy_policy_1: 0.7877
	loss_value_1: 0.03481
	loss_reward_1: 0.00511
	loss_policy_2: 0.02116
	accuracy_policy_2: 0.76268
	loss_value_2: 0.03524
	loss_reward_2: 0.00464
	loss_policy_3: 0.02326
	accuracy_policy_3: 0.74416
	loss_value_3: 0.03562
	loss_reward_3: 0.00486
	loss_policy_4: 0.02509
	accuracy_policy_4: 0.72576
	loss_value_4: 0.03597
	loss_reward_4: 0.00511
	loss_policy_5: 0.0269
	accuracy_policy_5: 0.71221
	loss_value_5: 0.03627
	loss_reward_5: 0.00571
	loss_policy: 0.18392
	loss_value: 0.34913
	loss_reward: 0.02543
Optimization_Done 7400
[2024-05-12 02:36:17] [command] train weight_iter_7400.pkl 36 38
[2024-05-12 02:37:22] nn step 7500, lr: 0.09.
	loss_policy_0: 0.08119
	accuracy_policy_0: 0.79352
	loss_value_0: 0.16342
	loss_policy_1: 0.02157
	accuracy_policy_1: 0.74303
	loss_value_1: 0.03323
	loss_reward_1: 0.00608
	loss_policy_2: 0.02401
	accuracy_policy_2: 0.71838
	loss_value_2: 0.03365
	loss_reward_2: 0.00544
	loss_policy_3: 0.02625
	accuracy_policy_3: 0.69959
	loss_value_3: 0.03425
	loss_reward_3: 0.00547
	loss_policy_4: 0.02799
	accuracy_policy_4: 0.68133
	loss_value_4: 0.03481
	loss_reward_4: 0.006
	loss_policy_5: 0.02955
	accuracy_policy_5: 0.66906
	loss_value_5: 0.03519
	loss_reward_5: 0.00669
	loss_policy: 0.21058
	loss_value: 0.33455
	loss_reward: 0.02969
[2024-05-12 02:38:18] nn step 7600, lr: 0.09.
	loss_policy_0: 0.07256
	accuracy_policy_0: 0.81416
	loss_value_0: 0.17002
	loss_policy_1: 0.0199
	accuracy_policy_1: 0.76494
	loss_value_1: 0.03469
	loss_reward_1: 0.00582
	loss_policy_2: 0.0223
	accuracy_policy_2: 0.74271
	loss_value_2: 0.03533
	loss_reward_2: 0.00529
	loss_policy_3: 0.02422
	accuracy_policy_3: 0.72371
	loss_value_3: 0.03594
	loss_reward_3: 0.00557
	loss_policy_4: 0.02616
	accuracy_policy_4: 0.70879
	loss_value_4: 0.03642
	loss_reward_4: 0.00602
	loss_policy_5: 0.02797
	accuracy_policy_5: 0.69699
	loss_value_5: 0.03699
	loss_reward_5: 0.00651
	loss_policy: 0.19311
	loss_value: 0.34938
	loss_reward: 0.02922
Optimization_Done 7600
[2024-05-12 02:40:29] [command] train weight_iter_7600.pkl 37 39
[2024-05-12 02:41:28] nn step 7700, lr: 0.09.
	loss_policy_0: 0.11088
	accuracy_policy_0: 0.769
	loss_value_0: 0.18657
	loss_policy_1: 0.02852
	accuracy_policy_1: 0.71834
	loss_value_1: 0.03805
	loss_reward_1: 0.00622
	loss_policy_2: 0.03135
	accuracy_policy_2: 0.69369
	loss_value_2: 0.0388
	loss_reward_2: 0.00574
	loss_policy_3: 0.034
	accuracy_policy_3: 0.67064
	loss_value_3: 0.03949
	loss_reward_3: 0.00586
	loss_policy_4: 0.0362
	accuracy_policy_4: 0.6535
	loss_value_4: 0.04005
	loss_reward_4: 0.00649
	loss_policy_5: 0.03782
	accuracy_policy_5: 0.64078
	loss_value_5: 0.04047
	loss_reward_5: 0.00733
	loss_policy: 0.27876
	loss_value: 0.38342
	loss_reward: 0.03164
[2024-05-12 02:42:24] nn step 7800, lr: 0.09.
	loss_policy_0: 0.09847
	accuracy_policy_0: 0.78727
	loss_value_0: 0.18476
	loss_policy_1: 0.026
	accuracy_policy_1: 0.73992
	loss_value_1: 0.03773
	loss_reward_1: 0.0061
	loss_policy_2: 0.02916
	accuracy_policy_2: 0.70969
	loss_value_2: 0.03833
	loss_reward_2: 0.00567
	loss_policy_3: 0.03117
	accuracy_policy_3: 0.69273
	loss_value_3: 0.03889
	loss_reward_3: 0.00584
	loss_policy_4: 0.03347
	accuracy_policy_4: 0.67254
	loss_value_4: 0.03949
	loss_reward_4: 0.00641
	loss_policy_5: 0.03555
	accuracy_policy_5: 0.65898
	loss_value_5: 0.04008
	loss_reward_5: 0.00744
	loss_policy: 0.25382
	loss_value: 0.37927
	loss_reward: 0.03147
Optimization_Done 7800
[2024-05-12 02:44:45] [command] train weight_iter_7800.pkl 38 40
[2024-05-12 02:45:43] nn step 7900, lr: 0.09.
	loss_policy_0: 0.11446
	accuracy_policy_0: 0.7599
	loss_value_0: 0.17727
	loss_policy_1: 0.02906
	accuracy_policy_1: 0.70949
	loss_value_1: 0.03615
	loss_reward_1: 0.00589
	loss_policy_2: 0.03224
	accuracy_policy_2: 0.67832
	loss_value_2: 0.03684
	loss_reward_2: 0.00551
	loss_policy_3: 0.0349
	accuracy_policy_3: 0.65854
	loss_value_3: 0.03743
	loss_reward_3: 0.00559
	loss_policy_4: 0.03725
	accuracy_policy_4: 0.63508
	loss_value_4: 0.03795
	loss_reward_4: 0.00621
	loss_policy_5: 0.03915
	accuracy_policy_5: 0.62107
	loss_value_5: 0.0384
	loss_reward_5: 0.00689
	loss_policy: 0.28706
	loss_value: 0.36402
	loss_reward: 0.0301
[2024-05-12 02:46:40] nn step 8000, lr: 0.09.
	loss_policy_0: 0.10096
	accuracy_policy_0: 0.78188
	loss_value_0: 0.17505
	loss_policy_1: 0.02623
	accuracy_policy_1: 0.73553
	loss_value_1: 0.03569
	loss_reward_1: 0.00578
	loss_policy_2: 0.02887
	accuracy_policy_2: 0.71039
	loss_value_2: 0.03647
	loss_reward_2: 0.00554
	loss_policy_3: 0.03142
	accuracy_policy_3: 0.69125
	loss_value_3: 0.03715
	loss_reward_3: 0.00556
	loss_policy_4: 0.03345
	accuracy_policy_4: 0.67311
	loss_value_4: 0.03781
	loss_reward_4: 0.00621
	loss_policy_5: 0.03582
	accuracy_policy_5: 0.651
	loss_value_5: 0.0384
	loss_reward_5: 0.00711
	loss_policy: 0.25674
	loss_value: 0.36056
	loss_reward: 0.03019
Optimization_Done 8000
[2024-05-12 02:48:26] [command] train weight_iter_8000.pkl 39 41
[2024-05-12 02:49:16] nn step 8100, lr: 0.09.
	loss_policy_0: 0.13236
	accuracy_policy_0: 0.74074
	loss_value_0: 0.17842
	loss_policy_1: 0.03241
	accuracy_policy_1: 0.69805
	loss_value_1: 0.03648
	loss_reward_1: 0.00546
	loss_policy_2: 0.03547
	accuracy_policy_2: 0.67219
	loss_value_2: 0.03729
	loss_reward_2: 0.00511
	loss_policy_3: 0.03814
	accuracy_policy_3: 0.64949
	loss_value_3: 0.03785
	loss_reward_3: 0.00531
	loss_policy_4: 0.04072
	accuracy_policy_4: 0.63105
	loss_value_4: 0.03863
	loss_reward_4: 0.00587
	loss_policy_5: 0.04321
	accuracy_policy_5: 0.60955
	loss_value_5: 0.03926
	loss_reward_5: 0.00661
	loss_policy: 0.32231
	loss_value: 0.36794
	loss_reward: 0.02836
[2024-05-12 02:50:12] nn step 8200, lr: 0.09.
	loss_policy_0: 0.11494
	accuracy_policy_0: 0.76434
	loss_value_0: 0.17469
	loss_policy_1: 0.02911
	accuracy_policy_1: 0.71549
	loss_value_1: 0.03566
	loss_reward_1: 0.00529
	loss_policy_2: 0.03129
	accuracy_policy_2: 0.6991
	loss_value_2: 0.03635
	loss_reward_2: 0.00502
	loss_policy_3: 0.03405
	accuracy_policy_3: 0.67576
	loss_value_3: 0.03705
	loss_reward_3: 0.00521
	loss_policy_4: 0.03659
	accuracy_policy_4: 0.65734
	loss_value_4: 0.0377
	loss_reward_4: 0.00566
	loss_policy_5: 0.0391
	accuracy_policy_5: 0.6358
	loss_value_5: 0.03847
	loss_reward_5: 0.00635
	loss_policy: 0.28507
	loss_value: 0.35992
	loss_reward: 0.02753
Optimization_Done 8200
[2024-05-12 02:52:33] [command] train weight_iter_8200.pkl 40 42
[2024-05-12 02:53:33] nn step 8300, lr: 0.09.
	loss_policy_0: 0.12631
	accuracy_policy_0: 0.74424
	loss_value_0: 0.17915
	loss_policy_1: 0.0313
	accuracy_policy_1: 0.69037
	loss_value_1: 0.03695
	loss_reward_1: 0.00582
	loss_policy_2: 0.03374
	accuracy_policy_2: 0.67381
	loss_value_2: 0.03781
	loss_reward_2: 0.00538
	loss_policy_3: 0.0364
	accuracy_policy_3: 0.64828
	loss_value_3: 0.0387
	loss_reward_3: 0.00575
	loss_policy_4: 0.03885
	accuracy_policy_4: 0.62838
	loss_value_4: 0.03943
	loss_reward_4: 0.00621
	loss_policy_5: 0.04083
	accuracy_policy_5: 0.61354
	loss_value_5: 0.04008
	loss_reward_5: 0.00706
	loss_policy: 0.30744
	loss_value: 0.37211
	loss_reward: 0.03021
[2024-05-12 02:54:40] nn step 8400, lr: 0.09.
	loss_policy_0: 0.11165
	accuracy_policy_0: 0.76689
	loss_value_0: 0.17803
	loss_policy_1: 0.02857
	accuracy_policy_1: 0.71268
	loss_value_1: 0.0366
	loss_reward_1: 0.00573
	loss_policy_2: 0.03116
	accuracy_policy_2: 0.69305
	loss_value_2: 0.03744
	loss_reward_2: 0.00542
	loss_policy_3: 0.03353
	accuracy_policy_3: 0.67311
	loss_value_3: 0.03832
	loss_reward_3: 0.00559
	loss_policy_4: 0.03546
	accuracy_policy_4: 0.65748
	loss_value_4: 0.03905
	loss_reward_4: 0.00605
	loss_policy_5: 0.03793
	accuracy_policy_5: 0.63811
	loss_value_5: 0.03989
	loss_reward_5: 0.00685
	loss_policy: 0.27829
	loss_value: 0.36932
	loss_reward: 0.02964
Optimization_Done 8400
[2024-05-12 02:56:48] [command] train weight_iter_8400.pkl 41 43
[2024-05-12 02:57:51] nn step 8500, lr: 0.09.
	loss_policy_0: 0.14498
	accuracy_policy_0: 0.72182
	loss_value_0: 0.19382
	loss_policy_1: 0.0352
	accuracy_policy_1: 0.67096
	loss_value_1: 0.03986
	loss_reward_1: 0.00637
	loss_policy_2: 0.03779
	accuracy_policy_2: 0.64932
	loss_value_2: 0.04089
	loss_reward_2: 0.00598
	loss_policy_3: 0.04063
	accuracy_policy_3: 0.62576
	loss_value_3: 0.0417
	loss_reward_3: 0.0064
	loss_policy_4: 0.04341
	accuracy_policy_4: 0.60412
	loss_value_4: 0.04263
	loss_reward_4: 0.00686
	loss_policy_5: 0.04512
	accuracy_policy_5: 0.59055
	loss_value_5: 0.04334
	loss_reward_5: 0.00777
	loss_policy: 0.34713
	loss_value: 0.40225
	loss_reward: 0.03338
[2024-05-12 02:58:57] nn step 8600, lr: 0.09.
	loss_policy_0: 0.12957
	accuracy_policy_0: 0.74312
	loss_value_0: 0.18746
	loss_policy_1: 0.03217
	accuracy_policy_1: 0.68896
	loss_value_1: 0.03851
	loss_reward_1: 0.00629
	loss_policy_2: 0.03507
	accuracy_policy_2: 0.66531
	loss_value_2: 0.03935
	loss_reward_2: 0.00572
	loss_policy_3: 0.03762
	accuracy_policy_3: 0.64689
	loss_value_3: 0.04015
	loss_reward_3: 0.00602
	loss_policy_4: 0.03946
	accuracy_policy_4: 0.63
	loss_value_4: 0.04102
	loss_reward_4: 0.00658
	loss_policy_5: 0.04173
	accuracy_policy_5: 0.61162
	loss_value_5: 0.04182
	loss_reward_5: 0.0075
	loss_policy: 0.31562
	loss_value: 0.38831
	loss_reward: 0.03212
Optimization_Done 8600
[2024-05-12 03:01:22] [command] train weight_iter_8600.pkl 42 44
[2024-05-12 03:02:14] nn step 8700, lr: 0.09.
	loss_policy_0: 0.14334
	accuracy_policy_0: 0.73525
	loss_value_0: 0.199
	loss_policy_1: 0.03517
	accuracy_policy_1: 0.68848
	loss_value_1: 0.04101
	loss_reward_1: 0.00627
	loss_policy_2: 0.0382
	accuracy_policy_2: 0.65895
	loss_value_2: 0.04206
	loss_reward_2: 0.00578
	loss_policy_3: 0.0411
	accuracy_policy_3: 0.63414
	loss_value_3: 0.04279
	loss_reward_3: 0.00617
	loss_policy_4: 0.04372
	accuracy_policy_4: 0.61012
	loss_value_4: 0.04357
	loss_reward_4: 0.00653
	loss_policy_5: 0.04615
	accuracy_policy_5: 0.59297
	loss_value_5: 0.04419
	loss_reward_5: 0.00782
	loss_policy: 0.34767
	loss_value: 0.41262
	loss_reward: 0.03257
[2024-05-12 03:03:13] nn step 8800, lr: 0.09.
	loss_policy_0: 0.12256
	accuracy_policy_0: 0.75984
	loss_value_0: 0.18654
	loss_policy_1: 0.03087
	accuracy_policy_1: 0.70771
	loss_value_1: 0.03851
	loss_reward_1: 0.00594
	loss_policy_2: 0.03416
	accuracy_policy_2: 0.68043
	loss_value_2: 0.0394
	loss_reward_2: 0.00559
	loss_policy_3: 0.0367
	accuracy_policy_3: 0.65469
	loss_value_3: 0.0402
	loss_reward_3: 0.00594
	loss_policy_4: 0.03959
	accuracy_policy_4: 0.63422
	loss_value_4: 0.04095
	loss_reward_4: 0.00644
	loss_policy_5: 0.04195
	accuracy_policy_5: 0.60875
	loss_value_5: 0.04172
	loss_reward_5: 0.00744
	loss_policy: 0.30583
	loss_value: 0.38733
	loss_reward: 0.03134
Optimization_Done 8800
[2024-05-12 03:05:34] [command] train weight_iter_8800.pkl 43 45
[2024-05-12 03:06:39] nn step 8900, lr: 0.09.
	loss_policy_0: 0.12766
	accuracy_policy_0: 0.75141
	loss_value_0: 0.18533
	loss_policy_1: 0.03181
	accuracy_policy_1: 0.70174
	loss_value_1: 0.03808
	loss_reward_1: 0.00519
	loss_policy_2: 0.03507
	accuracy_policy_2: 0.67045
	loss_value_2: 0.039
	loss_reward_2: 0.00519
	loss_policy_3: 0.03826
	accuracy_policy_3: 0.64283
	loss_value_3: 0.03985
	loss_reward_3: 0.00534
	loss_policy_4: 0.04112
	accuracy_policy_4: 0.61812
	loss_value_4: 0.0405
	loss_reward_4: 0.00578
	loss_policy_5: 0.0438
	accuracy_policy_5: 0.59592
	loss_value_5: 0.04117
	loss_reward_5: 0.00655
	loss_policy: 0.31771
	loss_value: 0.38393
	loss_reward: 0.02804
[2024-05-12 03:07:44] nn step 9000, lr: 0.09.
	loss_policy_0: 0.11628
	accuracy_policy_0: 0.7726
	loss_value_0: 0.18604
	loss_policy_1: 0.02981
	accuracy_policy_1: 0.72107
	loss_value_1: 0.03844
	loss_reward_1: 0.00547
	loss_policy_2: 0.03337
	accuracy_policy_2: 0.68996
	loss_value_2: 0.03929
	loss_reward_2: 0.00512
	loss_policy_3: 0.03631
	accuracy_policy_3: 0.66752
	loss_value_3: 0.04009
	loss_reward_3: 0.00537
	loss_policy_4: 0.03942
	accuracy_policy_4: 0.64139
	loss_value_4: 0.04094
	loss_reward_4: 0.00588
	loss_policy_5: 0.04236
	accuracy_policy_5: 0.61895
	loss_value_5: 0.04163
	loss_reward_5: 0.00666
	loss_policy: 0.29754
	loss_value: 0.38643
	loss_reward: 0.0285
Optimization_Done 9000
[2024-05-12 03:10:04] [command] train weight_iter_9000.pkl 44 46
[2024-05-12 03:11:06] nn step 9100, lr: 0.09.
	loss_policy_0: 0.13415
	accuracy_policy_0: 0.73055
	loss_value_0: 0.16614
	loss_policy_1: 0.03283
	accuracy_policy_1: 0.6823
	loss_value_1: 0.03447
	loss_reward_1: 0.00525
	loss_policy_2: 0.03605
	accuracy_policy_2: 0.65854
	loss_value_2: 0.03551
	loss_reward_2: 0.0049
	loss_policy_3: 0.03915
	accuracy_policy_3: 0.63557
	loss_value_3: 0.03644
	loss_reward_3: 0.00516
	loss_policy_4: 0.04231
	accuracy_policy_4: 0.61123
	loss_value_4: 0.03746
	loss_reward_4: 0.00564
	loss_policy_5: 0.04466
	accuracy_policy_5: 0.59551
	loss_value_5: 0.03841
	loss_reward_5: 0.00653
	loss_policy: 0.32913
	loss_value: 0.34844
	loss_reward: 0.02747
[2024-05-12 03:12:08] nn step 9200, lr: 0.09.
	loss_policy_0: 0.12002
	accuracy_policy_0: 0.76293
	loss_value_0: 0.17387
	loss_policy_1: 0.03084
	accuracy_policy_1: 0.7126
	loss_value_1: 0.03608
	loss_reward_1: 0.0054
	loss_policy_2: 0.03403
	accuracy_policy_2: 0.68844
	loss_value_2: 0.03715
	loss_reward_2: 0.00503
	loss_policy_3: 0.03747
	accuracy_policy_3: 0.66291
	loss_value_3: 0.03796
	loss_reward_3: 0.00539
	loss_policy_4: 0.0407
	accuracy_policy_4: 0.63889
	loss_value_4: 0.03877
	loss_reward_4: 0.00576
	loss_policy_5: 0.04439
	accuracy_policy_5: 0.61281
	loss_value_5: 0.03972
	loss_reward_5: 0.00666
	loss_policy: 0.30746
	loss_value: 0.36356
	loss_reward: 0.02823
Optimization_Done 9200
[2024-05-12 03:14:29] [command] train weight_iter_9200.pkl 45 47
[2024-05-12 03:15:24] nn step 9300, lr: 0.09.
	loss_policy_0: 0.14283
	accuracy_policy_0: 0.7248
	loss_value_0: 0.17006
	loss_policy_1: 0.03479
	accuracy_policy_1: 0.67875
	loss_value_1: 0.03533
	loss_reward_1: 0.00618
	loss_policy_2: 0.03782
	accuracy_policy_2: 0.65359
	loss_value_2: 0.03646
	loss_reward_2: 0.0056
	loss_policy_3: 0.04079
	accuracy_policy_3: 0.63354
	loss_value_3: 0.03751
	loss_reward_3: 0.00596
	loss_policy_4: 0.04332
	accuracy_policy_4: 0.61316
	loss_value_4: 0.03851
	loss_reward_4: 0.00642
	loss_policy_5: 0.04592
	accuracy_policy_5: 0.59691
	loss_value_5: 0.03947
	loss_reward_5: 0.00716
	loss_policy: 0.34546
	loss_value: 0.35735
	loss_reward: 0.03131
[2024-05-12 03:16:22] nn step 9400, lr: 0.09.
	loss_policy_0: 0.12417
	accuracy_policy_0: 0.75338
	loss_value_0: 0.16706
	loss_policy_1: 0.0312
	accuracy_policy_1: 0.70273
	loss_value_1: 0.03476
	loss_reward_1: 0.00599
	loss_policy_2: 0.03424
	accuracy_policy_2: 0.68305
	loss_value_2: 0.03587
	loss_reward_2: 0.00551
	loss_policy_3: 0.03685
	accuracy_policy_3: 0.6618
	loss_value_3: 0.03691
	loss_reward_3: 0.00571
	loss_policy_4: 0.03948
	accuracy_policy_4: 0.64201
	loss_value_4: 0.03783
	loss_reward_4: 0.0062
	loss_policy_5: 0.04231
	accuracy_policy_5: 0.62299
	loss_value_5: 0.03893
	loss_reward_5: 0.00689
	loss_policy: 0.30825
	loss_value: 0.35135
	loss_reward: 0.03029
Optimization_Done 9400
[2024-05-12 03:18:33] [command] train weight_iter_9400.pkl 46 48
[2024-05-12 03:19:33] nn step 9500, lr: 0.09.
	loss_policy_0: 0.14766
	accuracy_policy_0: 0.72561
	loss_value_0: 0.17932
	loss_policy_1: 0.03598
	accuracy_policy_1: 0.67135
	loss_value_1: 0.03723
	loss_reward_1: 0.00609
	loss_policy_2: 0.03902
	accuracy_policy_2: 0.64721
	loss_value_2: 0.03818
	loss_reward_2: 0.00563
	loss_policy_3: 0.04155
	accuracy_policy_3: 0.62631
	loss_value_3: 0.03909
	loss_reward_3: 0.00591
	loss_policy_4: 0.04389
	accuracy_policy_4: 0.6059
	loss_value_4: 0.03998
	loss_reward_4: 0.00631
	loss_policy_5: 0.04636
	accuracy_policy_5: 0.58816
	loss_value_5: 0.04085
	loss_reward_5: 0.00727
	loss_policy: 0.35447
	loss_value: 0.37464
	loss_reward: 0.03121
[2024-05-12 03:20:32] nn step 9600, lr: 0.09.
	loss_policy_0: 0.13184
	accuracy_policy_0: 0.74822
	loss_value_0: 0.17782
	loss_policy_1: 0.0334
	accuracy_policy_1: 0.6957
	loss_value_1: 0.0369
	loss_reward_1: 0.00596
	loss_policy_2: 0.0362
	accuracy_policy_2: 0.67062
	loss_value_2: 0.03795
	loss_reward_2: 0.00541
	loss_policy_3: 0.03852
	accuracy_policy_3: 0.65178
	loss_value_3: 0.03892
	loss_reward_3: 0.00569
	loss_policy_4: 0.04133
	accuracy_policy_4: 0.63047
	loss_value_4: 0.03986
	loss_reward_4: 0.00637
	loss_policy_5: 0.04406
	accuracy_policy_5: 0.60994
	loss_value_5: 0.04081
	loss_reward_5: 0.00732
	loss_policy: 0.32536
	loss_value: 0.37225
	loss_reward: 0.03075
Optimization_Done 9600
[2024-05-12 03:22:53] [command] train weight_iter_9600.pkl 47 49
[2024-05-12 03:23:45] nn step 9700, lr: 0.09.
	loss_policy_0: 0.14871
	accuracy_policy_0: 0.72395
	loss_value_0: 0.18669
	loss_policy_1: 0.03581
	accuracy_policy_1: 0.67609
	loss_value_1: 0.03834
	loss_reward_1: 0.00528
	loss_policy_2: 0.0383
	accuracy_policy_2: 0.65668
	loss_value_2: 0.03915
	loss_reward_2: 0.00499
	loss_policy_3: 0.04094
	accuracy_policy_3: 0.63924
	loss_value_3: 0.03982
	loss_reward_3: 0.00515
	loss_policy_4: 0.04359
	accuracy_policy_4: 0.61658
	loss_value_4: 0.04054
	loss_reward_4: 0.00555
	loss_policy_5: 0.04634
	accuracy_policy_5: 0.5967
	loss_value_5: 0.04135
	loss_reward_5: 0.00636
	loss_policy: 0.35369
	loss_value: 0.3859
	loss_reward: 0.02733
[2024-05-12 03:24:42] nn step 9800, lr: 0.09.
	loss_policy_0: 0.12575
	accuracy_policy_0: 0.75439
	loss_value_0: 0.17842
	loss_policy_1: 0.03172
	accuracy_policy_1: 0.70311
	loss_value_1: 0.03665
	loss_reward_1: 0.00536
	loss_policy_2: 0.03416
	accuracy_policy_2: 0.68209
	loss_value_2: 0.03737
	loss_reward_2: 0.00503
	loss_policy_3: 0.03629
	accuracy_policy_3: 0.66189
	loss_value_3: 0.03798
	loss_reward_3: 0.005
	loss_policy_4: 0.03884
	accuracy_policy_4: 0.64195
	loss_value_4: 0.0387
	loss_reward_4: 0.00554
	loss_policy_5: 0.04131
	accuracy_policy_5: 0.62566
	loss_value_5: 0.03937
	loss_reward_5: 0.00631
	loss_policy: 0.30807
	loss_value: 0.3685
	loss_reward: 0.02723
Optimization_Done 9800
[2024-05-12 03:27:03] [command] train weight_iter_9800.pkl 48 50
[2024-05-12 03:28:02] nn step 9900, lr: 0.09.
	loss_policy_0: 0.15083
	accuracy_policy_0: 0.70254
	loss_value_0: 0.17783
	loss_policy_1: 0.03617
	accuracy_policy_1: 0.65346
	loss_value_1: 0.03678
	loss_reward_1: 0.00507
	loss_policy_2: 0.03866
	accuracy_policy_2: 0.63346
	loss_value_2: 0.03777
	loss_reward_2: 0.00487
	loss_policy_3: 0.04146
	accuracy_policy_3: 0.61395
	loss_value_3: 0.03862
	loss_reward_3: 0.00507
	loss_policy_4: 0.04434
	accuracy_policy_4: 0.59109
	loss_value_4: 0.03942
	loss_reward_4: 0.00546
	loss_policy_5: 0.04714
	accuracy_policy_5: 0.56832
	loss_value_5: 0.04022
	loss_reward_5: 0.00636
	loss_policy: 0.3586
	loss_value: 0.37063
	loss_reward: 0.02683
[2024-05-12 03:29:04] nn step 10000, lr: 0.09.
	loss_policy_0: 0.13498
	accuracy_policy_0: 0.73219
	loss_value_0: 0.17797
	loss_policy_1: 0.03333
	accuracy_policy_1: 0.68215
	loss_value_1: 0.03685
	loss_reward_1: 0.00512
	loss_policy_2: 0.03605
	accuracy_policy_2: 0.66008
	loss_value_2: 0.03761
	loss_reward_2: 0.00483
	loss_policy_3: 0.03904
	accuracy_policy_3: 0.63803
	loss_value_3: 0.03843
	loss_reward_3: 0.00504
	loss_policy_4: 0.0418
	accuracy_policy_4: 0.61553
	loss_value_4: 0.03919
	loss_reward_4: 0.00547
	loss_policy_5: 0.04488
	accuracy_policy_5: 0.59398
	loss_value_5: 0.04002
	loss_reward_5: 0.00635
	loss_policy: 0.33008
	loss_value: 0.37008
	loss_reward: 0.0268
Optimization_Done 10000
[2024-05-12 03:31:24] [command] train weight_iter_10000.pkl 49 51
[2024-05-12 03:32:27] nn step 10100, lr: 0.09.
	loss_policy_0: 0.15371
	accuracy_policy_0: 0.70703
	loss_value_0: 0.16695
	loss_policy_1: 0.03706
	accuracy_policy_1: 0.66025
	loss_value_1: 0.03463
	loss_reward_1: 0.00564
	loss_policy_2: 0.03977
	accuracy_policy_2: 0.63951
	loss_value_2: 0.03579
	loss_reward_2: 0.00537
	loss_policy_3: 0.04254
	accuracy_policy_3: 0.62195
	loss_value_3: 0.0368
	loss_reward_3: 0.00568
	loss_policy_4: 0.04501
	accuracy_policy_4: 0.60438
	loss_value_4: 0.03775
	loss_reward_4: 0.00612
	loss_policy_5: 0.04763
	accuracy_policy_5: 0.58941
	loss_value_5: 0.03873
	loss_reward_5: 0.00698
	loss_policy: 0.36571
	loss_value: 0.35066
	loss_reward: 0.0298
[2024-05-12 03:33:27] nn step 10200, lr: 0.09.
	loss_policy_0: 0.14179
	accuracy_policy_0: 0.73932
	loss_value_0: 0.17212
	loss_policy_1: 0.03607
	accuracy_policy_1: 0.68039
	loss_value_1: 0.03573
	loss_reward_1: 0.00593
	loss_policy_2: 0.0391
	accuracy_policy_2: 0.6609
	loss_value_2: 0.03694
	loss_reward_2: 0.00561
	loss_policy_3: 0.04185
	accuracy_policy_3: 0.64227
	loss_value_3: 0.03797
	loss_reward_3: 0.00588
	loss_policy_4: 0.04484
	accuracy_policy_4: 0.62898
	loss_value_4: 0.03886
	loss_reward_4: 0.00629
	loss_policy_5: 0.0475
	accuracy_policy_5: 0.60973
	loss_value_5: 0.03995
	loss_reward_5: 0.00734
	loss_policy: 0.35116
	loss_value: 0.36157
	loss_reward: 0.03105
Optimization_Done 10200
[2024-05-12 03:35:15] [command] train weight_iter_10200.pkl 50 52
[2024-05-12 03:36:08] nn step 10300, lr: 0.09.
	loss_policy_0: 0.17424
	accuracy_policy_0: 0.68768
	loss_value_0: 0.17251
	loss_policy_1: 0.04153
	accuracy_policy_1: 0.63492
	loss_value_1: 0.03609
	loss_reward_1: 0.00618
	loss_policy_2: 0.04414
	accuracy_policy_2: 0.61848
	loss_value_2: 0.03751
	loss_reward_2: 0.00584
	loss_policy_3: 0.04728
	accuracy_policy_3: 0.59518
	loss_value_3: 0.03861
	loss_reward_3: 0.00632
	loss_policy_4: 0.04988
	accuracy_policy_4: 0.58092
	loss_value_4: 0.03967
	loss_reward_4: 0.00676
	loss_policy_5: 0.05208
	accuracy_policy_5: 0.56502
	loss_value_5: 0.0409
	loss_reward_5: 0.0078
	loss_policy: 0.40915
	loss_value: 0.36529
	loss_reward: 0.0329
[2024-05-12 03:37:07] nn step 10400, lr: 0.09.
	loss_policy_0: 0.14717
	accuracy_policy_0: 0.71375
	loss_value_0: 0.16151
	loss_policy_1: 0.03656
	accuracy_policy_1: 0.6591
	loss_value_1: 0.03403
	loss_reward_1: 0.00568
	loss_policy_2: 0.03967
	accuracy_policy_2: 0.63893
	loss_value_2: 0.03527
	loss_reward_2: 0.0054
	loss_policy_3: 0.04219
	accuracy_policy_3: 0.61641
	loss_value_3: 0.03629
	loss_reward_3: 0.00587
	loss_policy_4: 0.0448
	accuracy_policy_4: 0.59889
	loss_value_4: 0.03745
	loss_reward_4: 0.00637
	loss_policy_5: 0.04762
	accuracy_policy_5: 0.58273
	loss_value_5: 0.03854
	loss_reward_5: 0.00721
	loss_policy: 0.358
	loss_value: 0.34309
	loss_reward: 0.03053
Optimization_Done 10400
[2024-05-12 03:39:28] [command] train weight_iter_10400.pkl 51 53
[2024-05-12 03:40:31] nn step 10500, lr: 0.09.
	loss_policy_0: 0.17641
	accuracy_policy_0: 0.69197
	loss_value_0: 0.17324
	loss_policy_1: 0.04253
	accuracy_policy_1: 0.63916
	loss_value_1: 0.03628
	loss_reward_1: 0.00639
	loss_policy_2: 0.04541
	accuracy_policy_2: 0.61867
	loss_value_2: 0.03726
	loss_reward_2: 0.00606
	loss_policy_3: 0.04821
	accuracy_policy_3: 0.60125
	loss_value_3: 0.03832
	loss_reward_3: 0.00643
	loss_policy_4: 0.05167
	accuracy_policy_4: 0.57748
	loss_value_4: 0.03951
	loss_reward_4: 0.00697
	loss_policy_5: 0.05451
	accuracy_policy_5: 0.55818
	loss_value_5: 0.04044
	loss_reward_5: 0.00805
	loss_policy: 0.41874
	loss_value: 0.36505
	loss_reward: 0.0339
[2024-05-12 03:41:37] nn step 10600, lr: 0.09.
	loss_policy_0: 0.14208
	accuracy_policy_0: 0.72684
	loss_value_0: 0.158
	loss_policy_1: 0.03598
	accuracy_policy_1: 0.66703
	loss_value_1: 0.033
	loss_reward_1: 0.0058
	loss_policy_2: 0.0393
	accuracy_policy_2: 0.64098
	loss_value_2: 0.03423
	loss_reward_2: 0.00556
	loss_policy_3: 0.04175
	accuracy_policy_3: 0.62281
	loss_value_3: 0.03518
	loss_reward_3: 0.0059
	loss_policy_4: 0.04493
	accuracy_policy_4: 0.60363
	loss_value_4: 0.03617
	loss_reward_4: 0.00646
	loss_policy_5: 0.04767
	accuracy_policy_5: 0.58561
	loss_value_5: 0.03721
	loss_reward_5: 0.00731
	loss_policy: 0.35171
	loss_value: 0.33379
	loss_reward: 0.03103
Optimization_Done 10600
[2024-05-12 03:44:02] [command] train weight_iter_10600.pkl 52 54
[2024-05-12 03:44:55] nn step 10700, lr: 0.09.
	loss_policy_0: 0.17915
	accuracy_policy_0: 0.66881
	loss_value_0: 0.16299
	loss_policy_1: 0.0421
	accuracy_policy_1: 0.62244
	loss_value_1: 0.03395
	loss_reward_1: 0.00578
	loss_policy_2: 0.04483
	accuracy_policy_2: 0.60045
	loss_value_2: 0.03514
	loss_reward_2: 0.00557
	loss_policy_3: 0.04839
	accuracy_policy_3: 0.57293
	loss_value_3: 0.03627
	loss_reward_3: 0.00579
	loss_policy_4: 0.05131
	accuracy_policy_4: 0.54791
	loss_value_4: 0.0374
	loss_reward_4: 0.00652
	loss_policy_5: 0.05425
	accuracy_policy_5: 0.52621
	loss_value_5: 0.03838
	loss_reward_5: 0.0073
	loss_policy: 0.42003
	loss_value: 0.34414
	loss_reward: 0.03095
[2024-05-12 03:45:58] nn step 10800, lr: 0.09.
	loss_policy_0: 0.15425
	accuracy_policy_0: 0.70119
	loss_value_0: 0.15697
	loss_policy_1: 0.03798
	accuracy_policy_1: 0.64189
	loss_value_1: 0.03292
	loss_reward_1: 0.00557
	loss_policy_2: 0.041
	accuracy_policy_2: 0.61934
	loss_value_2: 0.03389
	loss_reward_2: 0.00541
	loss_policy_3: 0.04405
	accuracy_policy_3: 0.59621
	loss_value_3: 0.03491
	loss_reward_3: 0.00574
	loss_policy_4: 0.0475
	accuracy_policy_4: 0.57004
	loss_value_4: 0.03609
	loss_reward_4: 0.00615
	loss_policy_5: 0.05009
	accuracy_policy_5: 0.54916
	loss_value_5: 0.03705
	loss_reward_5: 0.00724
	loss_policy: 0.37487
	loss_value: 0.33184
	loss_reward: 0.0301
Optimization_Done 10800
[2024-05-12 03:48:18] [command] train weight_iter_10800.pkl 53 55
[2024-05-12 03:49:23] nn step 10900, lr: 0.09.
	loss_policy_0: 0.19126
	accuracy_policy_0: 0.63891
	loss_value_0: 0.15734
	loss_policy_1: 0.04438
	accuracy_policy_1: 0.59219
	loss_value_1: 0.03265
	loss_reward_1: 0.00585
	loss_policy_2: 0.04752
	accuracy_policy_2: 0.56896
	loss_value_2: 0.03391
	loss_reward_2: 0.00555
	loss_policy_3: 0.05035
	accuracy_policy_3: 0.54428
	loss_value_3: 0.03502
	loss_reward_3: 0.00586
	loss_policy_4: 0.05343
	accuracy_policy_4: 0.52266
	loss_value_4: 0.03622
	loss_reward_4: 0.00631
	loss_policy_5: 0.0561
	accuracy_policy_5: 0.50275
	loss_value_5: 0.03715
	loss_reward_5: 0.00736
	loss_policy: 0.44305
	loss_value: 0.33229
	loss_reward: 0.03093
[2024-05-12 03:50:26] nn step 11000, lr: 0.09.
	loss_policy_0: 0.17012
	accuracy_policy_0: 0.68045
	loss_value_0: 0.16047
	loss_policy_1: 0.04189
	accuracy_policy_1: 0.61957
	loss_value_1: 0.03331
	loss_reward_1: 0.00589
	loss_policy_2: 0.04458
	accuracy_policy_2: 0.59572
	loss_value_2: 0.03448
	loss_reward_2: 0.00569
	loss_policy_3: 0.0477
	accuracy_policy_3: 0.57434
	loss_value_3: 0.03567
	loss_reward_3: 0.00574
	loss_policy_4: 0.05074
	accuracy_policy_4: 0.55152
	loss_value_4: 0.03679
	loss_reward_4: 0.00639
	loss_policy_5: 0.05386
	accuracy_policy_5: 0.52908
	loss_value_5: 0.03784
	loss_reward_5: 0.00724
	loss_policy: 0.40889
	loss_value: 0.33856
	loss_reward: 0.03096
Optimization_Done 11000
[2024-05-12 03:52:19] [command] train weight_iter_11000.pkl 54 56
[2024-05-12 03:53:20] nn step 11100, lr: 0.09.
	loss_policy_0: 0.20295
	accuracy_policy_0: 0.61514
	loss_value_0: 0.16313
	loss_policy_1: 0.04728
	accuracy_policy_1: 0.56377
	loss_value_1: 0.03412
	loss_reward_1: 0.00599
	loss_policy_2: 0.05054
	accuracy_policy_2: 0.53432
	loss_value_2: 0.03543
	loss_reward_2: 0.0053
	loss_policy_3: 0.05342
	accuracy_policy_3: 0.51121
	loss_value_3: 0.03651
	loss_reward_3: 0.00584
	loss_policy_4: 0.05573
	accuracy_policy_4: 0.49213
	loss_value_4: 0.03767
	loss_reward_4: 0.00631
	loss_policy_5: 0.05841
	accuracy_policy_5: 0.47402
	loss_value_5: 0.0386
	loss_reward_5: 0.00733
	loss_policy: 0.46832
	loss_value: 0.34546
	loss_reward: 0.03077
[2024-05-12 03:54:23] nn step 11200, lr: 0.09.
	loss_policy_0: 0.17928
	accuracy_policy_0: 0.65707
	loss_value_0: 0.16532
	loss_policy_1: 0.04371
	accuracy_policy_1: 0.59254
	loss_value_1: 0.03461
	loss_reward_1: 0.00583
	loss_policy_2: 0.04654
	accuracy_policy_2: 0.5726
	loss_value_2: 0.03592
	loss_reward_2: 0.00525
	loss_policy_3: 0.04959
	accuracy_policy_3: 0.54633
	loss_value_3: 0.03708
	loss_reward_3: 0.00551
	loss_policy_4: 0.05246
	accuracy_policy_4: 0.52352
	loss_value_4: 0.03809
	loss_reward_4: 0.00627
	loss_policy_5: 0.05533
	accuracy_policy_5: 0.50363
	loss_value_5: 0.03906
	loss_reward_5: 0.00727
	loss_policy: 0.42691
	loss_value: 0.35008
	loss_reward: 0.03012
Optimization_Done 11200
[2024-05-12 03:56:40] [command] train weight_iter_11200.pkl 55 57
[2024-05-12 03:57:39] nn step 11300, lr: 0.09.
	loss_policy_0: 0.18742
	accuracy_policy_0: 0.66492
	loss_value_0: 0.17286
	loss_policy_1: 0.04414
	accuracy_policy_1: 0.61072
	loss_value_1: 0.03613
	loss_reward_1: 0.00529
	loss_policy_2: 0.04736
	accuracy_policy_2: 0.58471
	loss_value_2: 0.0375
	loss_reward_2: 0.00483
	loss_policy_3: 0.05024
	accuracy_policy_3: 0.5624
	loss_value_3: 0.03853
	loss_reward_3: 0.00536
	loss_policy_4: 0.05282
	accuracy_policy_4: 0.54166
	loss_value_4: 0.03972
	loss_reward_4: 0.00592
	loss_policy_5: 0.05563
	accuracy_policy_5: 0.52102
	loss_value_5: 0.04075
	loss_reward_5: 0.00704
	loss_policy: 0.43762
	loss_value: 0.36549
	loss_reward: 0.02844
[2024-05-12 03:58:42] nn step 11400, lr: 0.09.
	loss_policy_0: 0.16561
	accuracy_policy_0: 0.69646
	loss_value_0: 0.17223
	loss_policy_1: 0.04106
	accuracy_policy_1: 0.63463
	loss_value_1: 0.03596
	loss_reward_1: 0.00533
	loss_policy_2: 0.04434
	accuracy_policy_2: 0.60701
	loss_value_2: 0.03723
	loss_reward_2: 0.00495
	loss_policy_3: 0.04748
	accuracy_policy_3: 0.58775
	loss_value_3: 0.03825
	loss_reward_3: 0.00542
	loss_policy_4: 0.04969
	accuracy_policy_4: 0.57115
	loss_value_4: 0.03921
	loss_reward_4: 0.00581
	loss_policy_5: 0.05289
	accuracy_policy_5: 0.5509
	loss_value_5: 0.0403
	loss_reward_5: 0.00679
	loss_policy: 0.40106
	loss_value: 0.36317
	loss_reward: 0.02829
Optimization_Done 11400
[2024-05-12 04:00:57] [command] train weight_iter_11400.pkl 56 58
[2024-05-12 04:02:00] nn step 11500, lr: 0.09.
	loss_policy_0: 0.15618
	accuracy_policy_0: 0.69975
	loss_value_0: 0.16993
	loss_policy_1: 0.03821
	accuracy_policy_1: 0.64584
	loss_value_1: 0.03529
	loss_reward_1: 0.00525
	loss_policy_2: 0.04122
	accuracy_policy_2: 0.62141
	loss_value_2: 0.03656
	loss_reward_2: 0.00498
	loss_policy_3: 0.04436
	accuracy_policy_3: 0.59723
	loss_value_3: 0.0376
	loss_reward_3: 0.0052
	loss_policy_4: 0.04687
	accuracy_policy_4: 0.57785
	loss_value_4: 0.03863
	loss_reward_4: 0.00562
	loss_policy_5: 0.04967
	accuracy_policy_5: 0.56104
	loss_value_5: 0.0396
	loss_reward_5: 0.00642
	loss_policy: 0.3765
	loss_value: 0.35761
	loss_reward: 0.02747
[2024-05-12 04:03:05] nn step 11600, lr: 0.09.
	loss_policy_0: 0.13267
	accuracy_policy_0: 0.72895
	loss_value_0: 0.1615
	loss_policy_1: 0.03422
	accuracy_policy_1: 0.66668
	loss_value_1: 0.03372
	loss_reward_1: 0.00493
	loss_policy_2: 0.03701
	accuracy_policy_2: 0.64527
	loss_value_2: 0.03464
	loss_reward_2: 0.0046
	loss_policy_3: 0.03922
	accuracy_policy_3: 0.62541
	loss_value_3: 0.03561
	loss_reward_3: 0.00492
	loss_policy_4: 0.04196
	accuracy_policy_4: 0.60637
	loss_value_4: 0.03645
	loss_reward_4: 0.00533
	loss_policy_5: 0.04492
	accuracy_policy_5: 0.58223
	loss_value_5: 0.03733
	loss_reward_5: 0.00617
	loss_policy: 0.33
	loss_value: 0.33926
	loss_reward: 0.02594
Optimization_Done 11600
[2024-05-12 04:05:20] [command] train weight_iter_11600.pkl 57 59
[2024-05-12 04:06:23] nn step 11700, lr: 0.09.
	loss_policy_0: 0.15477
	accuracy_policy_0: 0.7025
	loss_value_0: 0.1694
	loss_policy_1: 0.03798
	accuracy_policy_1: 0.65057
	loss_value_1: 0.03536
	loss_reward_1: 0.00533
	loss_policy_2: 0.0413
	accuracy_policy_2: 0.6227
	loss_value_2: 0.03657
	loss_reward_2: 0.00515
	loss_policy_3: 0.04434
	accuracy_policy_3: 0.60156
	loss_value_3: 0.03761
	loss_reward_3: 0.00558
	loss_policy_4: 0.04718
	accuracy_policy_4: 0.58189
	loss_value_4: 0.03878
	loss_reward_4: 0.00622
	loss_policy_5: 0.04997
	accuracy_policy_5: 0.56391
	loss_value_5: 0.03977
	loss_reward_5: 0.00711
	loss_policy: 0.37555
	loss_value: 0.35748
	loss_reward: 0.02939
[2024-05-12 04:07:16] nn step 11800, lr: 0.09.
	loss_policy_0: 0.13202
	accuracy_policy_0: 0.74488
	loss_value_0: 0.17313
	loss_policy_1: 0.03464
	accuracy_policy_1: 0.68307
	loss_value_1: 0.03616
	loss_reward_1: 0.0055
	loss_policy_2: 0.03794
	accuracy_policy_2: 0.65473
	loss_value_2: 0.03739
	loss_reward_2: 0.00514
	loss_policy_3: 0.04092
	accuracy_policy_3: 0.63221
	loss_value_3: 0.03833
	loss_reward_3: 0.00564
	loss_policy_4: 0.04427
	accuracy_policy_4: 0.60852
	loss_value_4: 0.03937
	loss_reward_4: 0.00602
	loss_policy_5: 0.04721
	accuracy_policy_5: 0.58871
	loss_value_5: 0.0405
	loss_reward_5: 0.00713
	loss_policy: 0.33701
	loss_value: 0.36489
	loss_reward: 0.02942
Optimization_Done 11800
[2024-05-12 04:09:29] [command] train weight_iter_11800.pkl 58 60
[2024-05-12 04:10:24] nn step 11900, lr: 0.09.
	loss_policy_0: 0.17498
	accuracy_policy_0: 0.65945
	loss_value_0: 0.1736
	loss_policy_1: 0.04306
	accuracy_policy_1: 0.59971
	loss_value_1: 0.03632
	loss_reward_1: 0.0057
	loss_policy_2: 0.04682
	accuracy_policy_2: 0.5733
	loss_value_2: 0.03757
	loss_reward_2: 0.00511
	loss_policy_3: 0.04984
	accuracy_policy_3: 0.55088
	loss_value_3: 0.03876
	loss_reward_3: 0.00563
	loss_policy_4: 0.05328
	accuracy_policy_4: 0.52201
	loss_value_4: 0.03982
	loss_reward_4: 0.00619
	loss_policy_5: 0.05577
	accuracy_policy_5: 0.50623
	loss_value_5: 0.0409
	loss_reward_5: 0.00715
	loss_policy: 0.42376
	loss_value: 0.36697
	loss_reward: 0.02978
[2024-05-12 04:11:16] nn step 12000, lr: 0.09.
	loss_policy_0: 0.15164
	accuracy_policy_0: 0.70219
	loss_value_0: 0.17339
	loss_policy_1: 0.03911
	accuracy_policy_1: 0.6326
	loss_value_1: 0.03622
	loss_reward_1: 0.00562
	loss_policy_2: 0.0426
	accuracy_policy_2: 0.6076
	loss_value_2: 0.03753
	loss_reward_2: 0.00509
	loss_policy_3: 0.04572
	accuracy_policy_3: 0.58467
	loss_value_3: 0.03869
	loss_reward_3: 0.00545
	loss_policy_4: 0.04861
	accuracy_policy_4: 0.56523
	loss_value_4: 0.03971
	loss_reward_4: 0.00596
	loss_policy_5: 0.05168
	accuracy_policy_5: 0.54279
	loss_value_5: 0.04077
	loss_reward_5: 0.00686
	loss_policy: 0.37935
	loss_value: 0.36631
	loss_reward: 0.02899
Optimization_Done 12000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-12 04:23:48] [command] train weight_iter_12000.pkl 59 61
[2024-05-12 04:24:50] nn step 12100, lr: 0.081.
	loss_policy_0: 0.17702
	accuracy_policy_0: 0.6882
	loss_value_0: 0.17865
	loss_policy_1: 0.04403
	accuracy_policy_1: 0.62498
	loss_value_1: 0.0373
	loss_reward_1: 0.00536
	loss_policy_2: 0.04747
	accuracy_policy_2: 0.60121
	loss_value_2: 0.0386
	loss_reward_2: 0.00488
	loss_policy_3: 0.05089
	accuracy_policy_3: 0.5725
	loss_value_3: 0.03979
	loss_reward_3: 0.00561
	loss_policy_4: 0.05415
	accuracy_policy_4: 0.54941
	loss_value_4: 0.04106
	loss_reward_4: 0.00629
	loss_policy_5: 0.05773
	accuracy_policy_5: 0.52525
	loss_value_5: 0.04219
	loss_reward_5: 0.00705
	loss_policy: 0.43128
	loss_value: 0.37759
	loss_reward: 0.02919
[2024-05-12 04:25:36] nn step 12200, lr: 0.081.
	loss_policy_0: 0.15938
	accuracy_policy_0: 0.71736
	loss_value_0: 0.18079
	loss_policy_1: 0.04095
	accuracy_policy_1: 0.6517
	loss_value_1: 0.03801
	loss_reward_1: 0.0054
	loss_policy_2: 0.04484
	accuracy_policy_2: 0.62199
	loss_value_2: 0.03935
	loss_reward_2: 0.00501
	loss_policy_3: 0.04826
	accuracy_policy_3: 0.60168
	loss_value_3: 0.04064
	loss_reward_3: 0.00565
	loss_policy_4: 0.05173
	accuracy_policy_4: 0.57857
	loss_value_4: 0.04182
	loss_reward_4: 0.00615
	loss_policy_5: 0.05587
	accuracy_policy_5: 0.54977
	loss_value_5: 0.04297
	loss_reward_5: 0.00711
	loss_policy: 0.40105
	loss_value: 0.38357
	loss_reward: 0.02932
Optimization_Done 12200
[2024-05-12 04:27:54] [command] train weight_iter_12200.pkl 60 62
[2024-05-12 04:28:46] nn step 12300, lr: 0.081.
	loss_policy_0: 0.14891
	accuracy_policy_0: 0.71873
	loss_value_0: 0.16966
	loss_policy_1: 0.03753
	accuracy_policy_1: 0.65865
	loss_value_1: 0.03548
	loss_reward_1: 0.00463
	loss_policy_2: 0.04057
	accuracy_policy_2: 0.63229
	loss_value_2: 0.03674
	loss_reward_2: 0.00421
	loss_policy_3: 0.04354
	accuracy_policy_3: 0.61322
	loss_value_3: 0.03778
	loss_reward_3: 0.00446
	loss_policy_4: 0.04641
	accuracy_policy_4: 0.58926
	loss_value_4: 0.03885
	loss_reward_4: 0.00502
	loss_policy_5: 0.0496
	accuracy_policy_5: 0.56684
	loss_value_5: 0.03996
	loss_reward_5: 0.00545
	loss_policy: 0.36656
	loss_value: 0.35846
	loss_reward: 0.02378
[2024-05-12 04:29:38] nn step 12400, lr: 0.081.
	loss_policy_0: 0.12791
	accuracy_policy_0: 0.74969
	loss_value_0: 0.1688
	loss_policy_1: 0.03391
	accuracy_policy_1: 0.68324
	loss_value_1: 0.03521
	loss_reward_1: 0.00452
	loss_policy_2: 0.03706
	accuracy_policy_2: 0.66023
	loss_value_2: 0.03652
	loss_reward_2: 0.00419
	loss_policy_3: 0.04027
	accuracy_policy_3: 0.63459
	loss_value_3: 0.0376
	loss_reward_3: 0.0044
	loss_policy_4: 0.04301
	accuracy_policy_4: 0.61787
	loss_value_4: 0.03863
	loss_reward_4: 0.00478
	loss_policy_5: 0.04632
	accuracy_policy_5: 0.59684
	loss_value_5: 0.03967
	loss_reward_5: 0.0055
	loss_policy: 0.32847
	loss_value: 0.35644
	loss_reward: 0.02339
Optimization_Done 12400
[2024-05-12 04:31:55] [command] train weight_iter_12400.pkl 61 63
[2024-05-12 04:32:49] nn step 12500, lr: 0.081.
	loss_policy_0: 0.14068
	accuracy_policy_0: 0.69412
	loss_value_0: 0.16577
	loss_policy_1: 0.03507
	accuracy_policy_1: 0.63469
	loss_value_1: 0.03462
	loss_reward_1: 0.00485
	loss_policy_2: 0.03768
	accuracy_policy_2: 0.6149
	loss_value_2: 0.03589
	loss_reward_2: 0.00456
	loss_policy_3: 0.04055
	accuracy_policy_3: 0.59188
	loss_value_3: 0.03706
	loss_reward_3: 0.00496
	loss_policy_4: 0.04324
	accuracy_policy_4: 0.57131
	loss_value_4: 0.03803
	loss_reward_4: 0.00554
	loss_policy_5: 0.04584
	accuracy_policy_5: 0.55486
	loss_value_5: 0.03916
	loss_reward_5: 0.00627
	loss_policy: 0.34305
	loss_value: 0.35052
	loss_reward: 0.02618
[2024-05-12 04:33:41] nn step 12600, lr: 0.081.
	loss_policy_0: 0.11689
	accuracy_policy_0: 0.74781
	loss_value_0: 0.16435
	loss_policy_1: 0.03085
	accuracy_policy_1: 0.67818
	loss_value_1: 0.03438
	loss_reward_1: 0.00469
	loss_policy_2: 0.03404
	accuracy_policy_2: 0.65363
	loss_value_2: 0.03558
	loss_reward_2: 0.0045
	loss_policy_3: 0.03673
	accuracy_policy_3: 0.62734
	loss_value_3: 0.03661
	loss_reward_3: 0.00489
	loss_policy_4: 0.03972
	accuracy_policy_4: 0.61016
	loss_value_4: 0.03761
	loss_reward_4: 0.00538
	loss_policy_5: 0.04242
	accuracy_policy_5: 0.59004
	loss_value_5: 0.03867
	loss_reward_5: 0.00621
	loss_policy: 0.30066
	loss_value: 0.3472
	loss_reward: 0.02568
Optimization_Done 12600
[2024-05-12 04:35:57] [command] train weight_iter_12600.pkl 62 64
[2024-05-12 04:36:51] nn step 12700, lr: 0.081.
	loss_policy_0: 0.14765
	accuracy_policy_0: 0.69512
	loss_value_0: 0.17085
	loss_policy_1: 0.03772
	accuracy_policy_1: 0.62592
	loss_value_1: 0.03589
	loss_reward_1: 0.0053
	loss_policy_2: 0.04095
	accuracy_policy_2: 0.59842
	loss_value_2: 0.03714
	loss_reward_2: 0.00491
	loss_policy_3: 0.04404
	accuracy_policy_3: 0.57395
	loss_value_3: 0.03822
	loss_reward_3: 0.00527
	loss_policy_4: 0.04681
	accuracy_policy_4: 0.5509
	loss_value_4: 0.03923
	loss_reward_4: 0.00579
	loss_policy_5: 0.04978
	accuracy_policy_5: 0.52844
	loss_value_5: 0.04033
	loss_reward_5: 0.00658
	loss_policy: 0.36694
	loss_value: 0.36167
	loss_reward: 0.02785
[2024-05-12 04:37:43] nn step 12800, lr: 0.081.
	loss_policy_0: 0.12835
	accuracy_policy_0: 0.73824
	loss_value_0: 0.17681
	loss_policy_1: 0.035
	accuracy_policy_1: 0.65918
	loss_value_1: 0.03703
	loss_reward_1: 0.00535
	loss_policy_2: 0.03841
	accuracy_policy_2: 0.63023
	loss_value_2: 0.0381
	loss_reward_2: 0.00483
	loss_policy_3: 0.04128
	accuracy_policy_3: 0.61064
	loss_value_3: 0.03925
	loss_reward_3: 0.00546
	loss_policy_4: 0.04447
	accuracy_policy_4: 0.58758
	loss_value_4: 0.04026
	loss_reward_4: 0.00577
	loss_policy_5: 0.04724
	accuracy_policy_5: 0.56416
	loss_value_5: 0.04136
	loss_reward_5: 0.00668
	loss_policy: 0.33474
	loss_value: 0.37281
	loss_reward: 0.02809
Optimization_Done 12800
[2024-05-12 04:40:03] [command] train weight_iter_12800.pkl 63 65
[2024-05-12 04:40:56] nn step 12900, lr: 0.081.
	loss_policy_0: 0.15668
	accuracy_policy_0: 0.69059
	loss_value_0: 0.1629
	loss_policy_1: 0.03972
	accuracy_policy_1: 0.61982
	loss_value_1: 0.03421
	loss_reward_1: 0.00514
	loss_policy_2: 0.04294
	accuracy_policy_2: 0.59439
	loss_value_2: 0.03553
	loss_reward_2: 0.00463
	loss_policy_3: 0.04568
	accuracy_policy_3: 0.57307
	loss_value_3: 0.03668
	loss_reward_3: 0.00527
	loss_policy_4: 0.0483
	accuracy_policy_4: 0.55432
	loss_value_4: 0.03783
	loss_reward_4: 0.00566
	loss_policy_5: 0.05159
	accuracy_policy_5: 0.52664
	loss_value_5: 0.03897
	loss_reward_5: 0.00666
	loss_policy: 0.38491
	loss_value: 0.3461
	loss_reward: 0.02737
[2024-05-12 04:41:47] nn step 13000, lr: 0.081.
	loss_policy_0: 0.14898
	accuracy_policy_0: 0.72271
	loss_value_0: 0.17573
	loss_policy_1: 0.03892
	accuracy_policy_1: 0.64596
	loss_value_1: 0.03713
	loss_reward_1: 0.00548
	loss_policy_2: 0.04283
	accuracy_policy_2: 0.62012
	loss_value_2: 0.03857
	loss_reward_2: 0.00509
	loss_policy_3: 0.04638
	accuracy_policy_3: 0.59668
	loss_value_3: 0.03984
	loss_reward_3: 0.0056
	loss_policy_4: 0.04939
	accuracy_policy_4: 0.57453
	loss_value_4: 0.04108
	loss_reward_4: 0.00621
	loss_policy_5: 0.05295
	accuracy_policy_5: 0.55094
	loss_value_5: 0.04246
	loss_reward_5: 0.00706
	loss_policy: 0.37946
	loss_value: 0.37481
	loss_reward: 0.02944
Optimization_Done 13000
[2024-05-12 04:44:03] [command] train weight_iter_13000.pkl 64 66
[2024-05-12 04:44:57] nn step 13100, lr: 0.081.
	loss_policy_0: 0.17132
	accuracy_policy_0: 0.671
	loss_value_0: 0.17074
	loss_policy_1: 0.04155
	accuracy_policy_1: 0.6141
	loss_value_1: 0.03552
	loss_reward_1: 0.00588
	loss_policy_2: 0.04487
	accuracy_policy_2: 0.58676
	loss_value_2: 0.03696
	loss_reward_2: 0.00549
	loss_policy_3: 0.04808
	accuracy_policy_3: 0.56654
	loss_value_3: 0.03825
	loss_reward_3: 0.00597
	loss_policy_4: 0.05077
	accuracy_policy_4: 0.54535
	loss_value_4: 0.03923
	loss_reward_4: 0.00642
	loss_policy_5: 0.05369
	accuracy_policy_5: 0.52127
	loss_value_5: 0.04029
	loss_reward_5: 0.00719
	loss_policy: 0.41027
	loss_value: 0.361
	loss_reward: 0.03095
[2024-05-12 04:45:48] nn step 13200, lr: 0.081.
	loss_policy_0: 0.14127
	accuracy_policy_0: 0.72
	loss_value_0: 0.16353
	loss_policy_1: 0.03683
	accuracy_policy_1: 0.6465
	loss_value_1: 0.03424
	loss_reward_1: 0.00565
	loss_policy_2: 0.04013
	accuracy_policy_2: 0.62307
	loss_value_2: 0.03547
	loss_reward_2: 0.00528
	loss_policy_3: 0.04339
	accuracy_policy_3: 0.59846
	loss_value_3: 0.0364
	loss_reward_3: 0.0055
	loss_policy_4: 0.04656
	accuracy_policy_4: 0.57266
	loss_value_4: 0.0377
	loss_reward_4: 0.00609
	loss_policy_5: 0.04946
	accuracy_policy_5: 0.55402
	loss_value_5: 0.03878
	loss_reward_5: 0.00696
	loss_policy: 0.35763
	loss_value: 0.34613
	loss_reward: 0.02949
Optimization_Done 13200
[2024-05-12 04:48:03] [command] train weight_iter_13200.pkl 65 67
[2024-05-12 04:48:55] nn step 13300, lr: 0.081.
	loss_policy_0: 0.17916
	accuracy_policy_0: 0.65371
	loss_value_0: 0.16292
	loss_policy_1: 0.04291
	accuracy_policy_1: 0.59682
	loss_value_1: 0.03425
	loss_reward_1: 0.00549
	loss_policy_2: 0.04551
	accuracy_policy_2: 0.57539
	loss_value_2: 0.03577
	loss_reward_2: 0.00514
	loss_policy_3: 0.04877
	accuracy_policy_3: 0.55422
	loss_value_3: 0.03702
	loss_reward_3: 0.00553
	loss_policy_4: 0.05162
	accuracy_policy_4: 0.53602
	loss_value_4: 0.03821
	loss_reward_4: 0.00623
	loss_policy_5: 0.05454
	accuracy_policy_5: 0.51867
	loss_value_5: 0.03956
	loss_reward_5: 0.00698
	loss_policy: 0.42251
	loss_value: 0.34772
	loss_reward: 0.02937
[2024-05-12 04:49:46] nn step 13400, lr: 0.081.
	loss_policy_0: 0.15228
	accuracy_policy_0: 0.70186
	loss_value_0: 0.15889
	loss_policy_1: 0.03867
	accuracy_policy_1: 0.6352
	loss_value_1: 0.03337
	loss_reward_1: 0.00555
	loss_policy_2: 0.0414
	accuracy_policy_2: 0.61352
	loss_value_2: 0.03473
	loss_reward_2: 0.00516
	loss_policy_3: 0.04474
	accuracy_policy_3: 0.58764
	loss_value_3: 0.03611
	loss_reward_3: 0.00548
	loss_policy_4: 0.04767
	accuracy_policy_4: 0.56641
	loss_value_4: 0.03728
	loss_reward_4: 0.00591
	loss_policy_5: 0.05084
	accuracy_policy_5: 0.54834
	loss_value_5: 0.03841
	loss_reward_5: 0.00678
	loss_policy: 0.3756
	loss_value: 0.33879
	loss_reward: 0.02888
Optimization_Done 13400
[2024-05-12 04:51:58] [command] train weight_iter_13400.pkl 66 68
[2024-05-12 04:52:53] nn step 13500, lr: 0.081.
	loss_policy_0: 0.18115
	accuracy_policy_0: 0.6665
	loss_value_0: 0.18529
	loss_policy_1: 0.04513
	accuracy_policy_1: 0.59762
	loss_value_1: 0.03877
	loss_reward_1: 0.0064
	loss_policy_2: 0.04876
	accuracy_policy_2: 0.57576
	loss_value_2: 0.04017
	loss_reward_2: 0.00589
	loss_policy_3: 0.05266
	accuracy_policy_3: 0.55127
	loss_value_3: 0.04152
	loss_reward_3: 0.00629
	loss_policy_4: 0.05574
	accuracy_policy_4: 0.52459
	loss_value_4: 0.04286
	loss_reward_4: 0.00705
	loss_policy_5: 0.05882
	accuracy_policy_5: 0.50572
	loss_value_5: 0.04414
	loss_reward_5: 0.00815
	loss_policy: 0.44226
	loss_value: 0.39275
	loss_reward: 0.03377
[2024-05-12 04:53:45] nn step 13600, lr: 0.081.
	loss_policy_0: 0.15657
	accuracy_policy_0: 0.70465
	loss_value_0: 0.18277
	loss_policy_1: 0.04126
	accuracy_policy_1: 0.62654
	loss_value_1: 0.03812
	loss_reward_1: 0.00632
	loss_policy_2: 0.04476
	accuracy_policy_2: 0.60621
	loss_value_2: 0.03963
	loss_reward_2: 0.00591
	loss_policy_3: 0.04855
	accuracy_policy_3: 0.57957
	loss_value_3: 0.04093
	loss_reward_3: 0.00625
	loss_policy_4: 0.05227
	accuracy_policy_4: 0.54836
	loss_value_4: 0.04207
	loss_reward_4: 0.00695
	loss_policy_5: 0.05586
	accuracy_policy_5: 0.52639
	loss_value_5: 0.04346
	loss_reward_5: 0.00786
	loss_policy: 0.39928
	loss_value: 0.38697
	loss_reward: 0.03329
Optimization_Done 13600
[2024-05-12 04:55:37] [command] train weight_iter_13600.pkl 67 69
[2024-05-12 04:56:32] nn step 13700, lr: 0.081.
	loss_policy_0: 0.18529
	accuracy_policy_0: 0.66285
	loss_value_0: 0.18336
	loss_policy_1: 0.04617
	accuracy_policy_1: 0.59643
	loss_value_1: 0.03852
	loss_reward_1: 0.00555
	loss_policy_2: 0.04934
	accuracy_policy_2: 0.5658
	loss_value_2: 0.03989
	loss_reward_2: 0.00526
	loss_policy_3: 0.05261
	accuracy_policy_3: 0.54377
	loss_value_3: 0.04099
	loss_reward_3: 0.00561
	loss_policy_4: 0.05585
	accuracy_policy_4: 0.52273
	loss_value_4: 0.04201
	loss_reward_4: 0.00624
	loss_policy_5: 0.05921
	accuracy_policy_5: 0.49729
	loss_value_5: 0.04304
	loss_reward_5: 0.0072
	loss_policy: 0.44846
	loss_value: 0.38782
	loss_reward: 0.02985
[2024-05-12 04:57:24] nn step 13800, lr: 0.081.
	loss_policy_0: 0.16212
	accuracy_policy_0: 0.70096
	loss_value_0: 0.1809
	loss_policy_1: 0.04228
	accuracy_policy_1: 0.62238
	loss_value_1: 0.03807
	loss_reward_1: 0.00563
	loss_policy_2: 0.04592
	accuracy_policy_2: 0.5973
	loss_value_2: 0.03934
	loss_reward_2: 0.00517
	loss_policy_3: 0.04895
	accuracy_policy_3: 0.57418
	loss_value_3: 0.04039
	loss_reward_3: 0.00543
	loss_policy_4: 0.05207
	accuracy_policy_4: 0.551
	loss_value_4: 0.04139
	loss_reward_4: 0.00607
	loss_policy_5: 0.05584
	accuracy_policy_5: 0.52426
	loss_value_5: 0.04246
	loss_reward_5: 0.00716
	loss_policy: 0.40719
	loss_value: 0.38254
	loss_reward: 0.02947
Optimization_Done 13800
[2024-05-12 04:59:40] [command] train weight_iter_13800.pkl 68 70
[2024-05-12 05:00:30] nn step 13900, lr: 0.081.
	loss_policy_0: 0.19997
	accuracy_policy_0: 0.63414
	loss_value_0: 0.17062
	loss_policy_1: 0.04781
	accuracy_policy_1: 0.57076
	loss_value_1: 0.03555
	loss_reward_1: 0.00501
	loss_policy_2: 0.05057
	accuracy_policy_2: 0.54793
	loss_value_2: 0.03685
	loss_reward_2: 0.00458
	loss_policy_3: 0.05361
	accuracy_policy_3: 0.52449
	loss_value_3: 0.03777
	loss_reward_3: 0.00503
	loss_policy_4: 0.05677
	accuracy_policy_4: 0.49877
	loss_value_4: 0.03881
	loss_reward_4: 0.00549
	loss_policy_5: 0.05959
	accuracy_policy_5: 0.47707
	loss_value_5: 0.03988
	loss_reward_5: 0.00638
	loss_policy: 0.46832
	loss_value: 0.35947
	loss_reward: 0.02649
[2024-05-12 05:01:21] nn step 14000, lr: 0.081.
	loss_policy_0: 0.18228
	accuracy_policy_0: 0.68316
	loss_value_0: 0.18034
	loss_policy_1: 0.04554
	accuracy_policy_1: 0.61508
	loss_value_1: 0.0377
	loss_reward_1: 0.00552
	loss_policy_2: 0.049
	accuracy_policy_2: 0.58891
	loss_value_2: 0.03901
	loss_reward_2: 0.00505
	loss_policy_3: 0.05273
	accuracy_policy_3: 0.56285
	loss_value_3: 0.03997
	loss_reward_3: 0.00538
	loss_policy_4: 0.0558
	accuracy_policy_4: 0.53914
	loss_value_4: 0.04099
	loss_reward_4: 0.00594
	loss_policy_5: 0.05951
	accuracy_policy_5: 0.51201
	loss_value_5: 0.04213
	loss_reward_5: 0.00711
	loss_policy: 0.44487
	loss_value: 0.38014
	loss_reward: 0.02899
Optimization_Done 14000
[2024-05-12 05:03:02] [command] train weight_iter_14000.pkl 69 71
[2024-05-12 05:03:53] nn step 14100, lr: 0.081.
	loss_policy_0: 0.22644
	accuracy_policy_0: 0.64121
	loss_value_0: 0.17402
	loss_policy_1: 0.05205
	accuracy_policy_1: 0.59268
	loss_value_1: 0.03612
	loss_reward_1: 0.00501
	loss_policy_2: 0.05499
	accuracy_policy_2: 0.57336
	loss_value_2: 0.03753
	loss_reward_2: 0.00478
	loss_policy_3: 0.05815
	accuracy_policy_3: 0.54773
	loss_value_3: 0.03874
	loss_reward_3: 0.00506
	loss_policy_4: 0.06133
	accuracy_policy_4: 0.53053
	loss_value_4: 0.03964
	loss_reward_4: 0.00549
	loss_policy_5: 0.06495
	accuracy_policy_5: 0.50146
	loss_value_5: 0.0406
	loss_reward_5: 0.00639
	loss_policy: 0.51791
	loss_value: 0.36664
	loss_reward: 0.02673
[2024-05-12 05:04:46] nn step 14200, lr: 0.081.
	loss_policy_0: 0.18594
	accuracy_policy_0: 0.67896
	loss_value_0: 0.16067
	loss_policy_1: 0.04437
	accuracy_policy_1: 0.62094
	loss_value_1: 0.0335
	loss_reward_1: 0.00473
	loss_policy_2: 0.04721
	accuracy_policy_2: 0.60113
	loss_value_2: 0.03453
	loss_reward_2: 0.00445
	loss_policy_3: 0.05039
	accuracy_policy_3: 0.57904
	loss_value_3: 0.03547
	loss_reward_3: 0.00486
	loss_policy_4: 0.0538
	accuracy_policy_4: 0.55346
	loss_value_4: 0.03624
	loss_reward_4: 0.00531
	loss_policy_5: 0.05689
	accuracy_policy_5: 0.53094
	loss_value_5: 0.03709
	loss_reward_5: 0.00595
	loss_policy: 0.43861
	loss_value: 0.3375
	loss_reward: 0.0253
Optimization_Done 14200
[2024-05-12 05:07:00] [command] train weight_iter_14200.pkl 70 72
[2024-05-12 05:07:53] nn step 14300, lr: 0.081.
	loss_policy_0: 0.20954
	accuracy_policy_0: 0.63709
	loss_value_0: 0.15988
	loss_policy_1: 0.04836
	accuracy_policy_1: 0.58494
	loss_value_1: 0.03333
	loss_reward_1: 0.00512
	loss_policy_2: 0.05106
	accuracy_policy_2: 0.56967
	loss_value_2: 0.03451
	loss_reward_2: 0.00481
	loss_policy_3: 0.05384
	accuracy_policy_3: 0.54818
	loss_value_3: 0.03581
	loss_reward_3: 0.00488
	loss_policy_4: 0.0569
	accuracy_policy_4: 0.52615
	loss_value_4: 0.03692
	loss_reward_4: 0.00553
	loss_policy_5: 0.05967
	accuracy_policy_5: 0.50656
	loss_value_5: 0.03805
	loss_reward_5: 0.00621
	loss_policy: 0.47936
	loss_value: 0.33849
	loss_reward: 0.02655
[2024-05-12 05:08:44] nn step 14400, lr: 0.081.
	loss_policy_0: 0.18444
	accuracy_policy_0: 0.68256
	loss_value_0: 0.16296
	loss_policy_1: 0.04495
	accuracy_policy_1: 0.62225
	loss_value_1: 0.03391
	loss_reward_1: 0.00503
	loss_policy_2: 0.04753
	accuracy_policy_2: 0.60252
	loss_value_2: 0.03513
	loss_reward_2: 0.00463
	loss_policy_3: 0.05097
	accuracy_policy_3: 0.58162
	loss_value_3: 0.03616
	loss_reward_3: 0.00492
	loss_policy_4: 0.0541
	accuracy_policy_4: 0.55512
	loss_value_4: 0.03715
	loss_reward_4: 0.00545
	loss_policy_5: 0.05783
	accuracy_policy_5: 0.53166
	loss_value_5: 0.0382
	loss_reward_5: 0.00626
	loss_policy: 0.43981
	loss_value: 0.3435
	loss_reward: 0.0263
Optimization_Done 14400
[2024-05-12 05:10:58] [command] train weight_iter_14400.pkl 71 73
[2024-05-12 05:11:50] nn step 14500, lr: 0.081.
	loss_policy_0: 0.19151
	accuracy_policy_0: 0.65791
	loss_value_0: 0.16556
	loss_policy_1: 0.04588
	accuracy_policy_1: 0.60443
	loss_value_1: 0.03471
	loss_reward_1: 0.00542
	loss_policy_2: 0.04871
	accuracy_policy_2: 0.58
	loss_value_2: 0.03594
	loss_reward_2: 0.00519
	loss_policy_3: 0.05209
	accuracy_policy_3: 0.55777
	loss_value_3: 0.03699
	loss_reward_3: 0.00553
	loss_policy_4: 0.05476
	accuracy_policy_4: 0.53424
	loss_value_4: 0.03806
	loss_reward_4: 0.00605
	loss_policy_5: 0.0577
	accuracy_policy_5: 0.50953
	loss_value_5: 0.03897
	loss_reward_5: 0.00708
	loss_policy: 0.45065
	loss_value: 0.35022
	loss_reward: 0.02927
[2024-05-12 05:12:42] nn step 14600, lr: 0.081.
	loss_policy_0: 0.16984
	accuracy_policy_0: 0.69307
	loss_value_0: 0.16474
	loss_policy_1: 0.04224
	accuracy_policy_1: 0.62611
	loss_value_1: 0.03453
	loss_reward_1: 0.00538
	loss_policy_2: 0.04524
	accuracy_policy_2: 0.60674
	loss_value_2: 0.03568
	loss_reward_2: 0.00504
	loss_policy_3: 0.04849
	accuracy_policy_3: 0.58133
	loss_value_3: 0.03666
	loss_reward_3: 0.00546
	loss_policy_4: 0.05136
	accuracy_policy_4: 0.55969
	loss_value_4: 0.03769
	loss_reward_4: 0.00595
	loss_policy_5: 0.05462
	accuracy_policy_5: 0.53621
	loss_value_5: 0.0387
	loss_reward_5: 0.00692
	loss_policy: 0.4118
	loss_value: 0.348
	loss_reward: 0.02876
Optimization_Done 14600
[2024-05-12 05:14:55] [command] train weight_iter_14600.pkl 72 74
[2024-05-12 05:15:47] nn step 14700, lr: 0.081.
	loss_policy_0: 0.18687
	accuracy_policy_0: 0.66443
	loss_value_0: 0.16635
	loss_policy_1: 0.04548
	accuracy_policy_1: 0.60133
	loss_value_1: 0.03471
	loss_reward_1: 0.00535
	loss_policy_2: 0.04873
	accuracy_policy_2: 0.57916
	loss_value_2: 0.03609
	loss_reward_2: 0.00498
	loss_policy_3: 0.0517
	accuracy_policy_3: 0.55773
	loss_value_3: 0.03759
	loss_reward_3: 0.00532
	loss_policy_4: 0.05479
	accuracy_policy_4: 0.53109
	loss_value_4: 0.03885
	loss_reward_4: 0.006
	loss_policy_5: 0.0587
	accuracy_policy_5: 0.50418
	loss_value_5: 0.04004
	loss_reward_5: 0.00703
	loss_policy: 0.44627
	loss_value: 0.35363
	loss_reward: 0.02868
[2024-05-12 05:16:40] nn step 14800, lr: 0.081.
	loss_policy_0: 0.15932
	accuracy_policy_0: 0.69975
	loss_value_0: 0.15795
	loss_policy_1: 0.04019
	accuracy_policy_1: 0.63018
	loss_value_1: 0.03322
	loss_reward_1: 0.00522
	loss_policy_2: 0.04369
	accuracy_policy_2: 0.60322
	loss_value_2: 0.03441
	loss_reward_2: 0.00478
	loss_policy_3: 0.04667
	accuracy_policy_3: 0.5841
	loss_value_3: 0.03562
	loss_reward_3: 0.00523
	loss_policy_4: 0.05003
	accuracy_policy_4: 0.55525
	loss_value_4: 0.03661
	loss_reward_4: 0.00575
	loss_policy_5: 0.05358
	accuracy_policy_5: 0.53102
	loss_value_5: 0.03769
	loss_reward_5: 0.00676
	loss_policy: 0.39348
	loss_value: 0.33549
	loss_reward: 0.02774
Optimization_Done 14800
[2024-05-12 05:18:42] [command] train weight_iter_14800.pkl 73 75
[2024-05-12 05:19:34] nn step 14900, lr: 0.081.
	loss_policy_0: 0.18978
	accuracy_policy_0: 0.65326
	loss_value_0: 0.16613
	loss_policy_1: 0.04464
	accuracy_policy_1: 0.59639
	loss_value_1: 0.03482
	loss_reward_1: 0.0058
	loss_policy_2: 0.04804
	accuracy_policy_2: 0.56949
	loss_value_2: 0.0361
	loss_reward_2: 0.00522
	loss_policy_3: 0.05101
	accuracy_policy_3: 0.5459
	loss_value_3: 0.03746
	loss_reward_3: 0.00566
	loss_policy_4: 0.05411
	accuracy_policy_4: 0.52314
	loss_value_4: 0.03885
	loss_reward_4: 0.00617
	loss_policy_5: 0.05711
	accuracy_policy_5: 0.49629
	loss_value_5: 0.04001
	loss_reward_5: 0.00735
	loss_policy: 0.44468
	loss_value: 0.35336
	loss_reward: 0.03019
[2024-05-12 05:20:26] nn step 15000, lr: 0.081.
	loss_policy_0: 0.17166
	accuracy_policy_0: 0.69086
	loss_value_0: 0.16946
	loss_policy_1: 0.0425
	accuracy_policy_1: 0.62635
	loss_value_1: 0.03555
	loss_reward_1: 0.00589
	loss_policy_2: 0.04602
	accuracy_policy_2: 0.5991
	loss_value_2: 0.03697
	loss_reward_2: 0.0054
	loss_policy_3: 0.04905
	accuracy_policy_3: 0.57842
	loss_value_3: 0.03831
	loss_reward_3: 0.00592
	loss_policy_4: 0.05238
	accuracy_policy_4: 0.55463
	loss_value_4: 0.03946
	loss_reward_4: 0.00629
	loss_policy_5: 0.05558
	accuracy_policy_5: 0.5282
	loss_value_5: 0.04079
	loss_reward_5: 0.0074
	loss_policy: 0.4172
	loss_value: 0.36054
	loss_reward: 0.0309
Optimization_Done 15000
[2024-05-12 05:22:40] [command] train weight_iter_15000.pkl 74 76
[2024-05-12 05:23:33] nn step 15100, lr: 0.081.
	loss_policy_0: 0.1878
	accuracy_policy_0: 0.65146
	loss_value_0: 0.16704
	loss_policy_1: 0.04443
	accuracy_policy_1: 0.58824
	loss_value_1: 0.03536
	loss_reward_1: 0.00595
	loss_policy_2: 0.04777
	accuracy_policy_2: 0.56553
	loss_value_2: 0.03694
	loss_reward_2: 0.00547
	loss_policy_3: 0.05092
	accuracy_policy_3: 0.5415
	loss_value_3: 0.03818
	loss_reward_3: 0.0059
	loss_policy_4: 0.05385
	accuracy_policy_4: 0.51965
	loss_value_4: 0.03946
	loss_reward_4: 0.00664
	loss_policy_5: 0.05709
	accuracy_policy_5: 0.49787
	loss_value_5: 0.04067
	loss_reward_5: 0.00762
	loss_policy: 0.44185
	loss_value: 0.35765
	loss_reward: 0.03158
[2024-05-12 05:24:25] nn step 15200, lr: 0.081.
	loss_policy_0: 0.16871
	accuracy_policy_0: 0.69074
	loss_value_0: 0.16879
	loss_policy_1: 0.04102
	accuracy_policy_1: 0.62744
	loss_value_1: 0.03551
	loss_reward_1: 0.00583
	loss_policy_2: 0.04502
	accuracy_policy_2: 0.59379
	loss_value_2: 0.03694
	loss_reward_2: 0.0056
	loss_policy_3: 0.0485
	accuracy_policy_3: 0.57246
	loss_value_3: 0.0382
	loss_reward_3: 0.00609
	loss_policy_4: 0.05179
	accuracy_policy_4: 0.54936
	loss_value_4: 0.03945
	loss_reward_4: 0.00669
	loss_policy_5: 0.05524
	accuracy_policy_5: 0.52045
	loss_value_5: 0.04069
	loss_reward_5: 0.00764
	loss_policy: 0.41029
	loss_value: 0.35957
	loss_reward: 0.03186
Optimization_Done 15200
[2024-05-12 05:26:38] [command] train weight_iter_15200.pkl 75 77
[2024-05-12 05:27:30] nn step 15300, lr: 0.081.
	loss_policy_0: 0.18778
	accuracy_policy_0: 0.66502
	loss_value_0: 0.18203
	loss_policy_1: 0.04629
	accuracy_policy_1: 0.5923
	loss_value_1: 0.03833
	loss_reward_1: 0.00585
	loss_policy_2: 0.05007
	accuracy_policy_2: 0.56064
	loss_value_2: 0.03978
	loss_reward_2: 0.00533
	loss_policy_3: 0.05344
	accuracy_policy_3: 0.53646
	loss_value_3: 0.04116
	loss_reward_3: 0.00598
	loss_policy_4: 0.05628
	accuracy_policy_4: 0.51871
	loss_value_4: 0.0423
	loss_reward_4: 0.00634
	loss_policy_5: 0.05932
	accuracy_policy_5: 0.4949
	loss_value_5: 0.04365
	loss_reward_5: 0.00736
	loss_policy: 0.45319
	loss_value: 0.38726
	loss_reward: 0.03086
[2024-05-12 05:28:20] nn step 15400, lr: 0.081.
	loss_policy_0: 0.16646
	accuracy_policy_0: 0.69775
	loss_value_0: 0.18114
	loss_policy_1: 0.04208
	accuracy_policy_1: 0.62225
	loss_value_1: 0.03815
	loss_reward_1: 0.00585
	loss_policy_2: 0.0463
	accuracy_policy_2: 0.59189
	loss_value_2: 0.03965
	loss_reward_2: 0.0053
	loss_policy_3: 0.04994
	accuracy_policy_3: 0.56277
	loss_value_3: 0.04094
	loss_reward_3: 0.0059
	loss_policy_4: 0.05328
	accuracy_policy_4: 0.53879
	loss_value_4: 0.04213
	loss_reward_4: 0.00648
	loss_policy_5: 0.05674
	accuracy_policy_5: 0.51654
	loss_value_5: 0.0434
	loss_reward_5: 0.00757
	loss_policy: 0.41481
	loss_value: 0.3854
	loss_reward: 0.03111
Optimization_Done 15400
[2024-05-12 05:30:23] [command] train weight_iter_15400.pkl 76 78
[2024-05-12 05:31:15] nn step 15500, lr: 0.081.
	loss_policy_0: 0.16942
	accuracy_policy_0: 0.67209
	loss_value_0: 0.16481
	loss_policy_1: 0.04182
	accuracy_policy_1: 0.60412
	loss_value_1: 0.03462
	loss_reward_1: 0.0051
	loss_policy_2: 0.04569
	accuracy_policy_2: 0.57018
	loss_value_2: 0.0359
	loss_reward_2: 0.00475
	loss_policy_3: 0.0487
	accuracy_policy_3: 0.54676
	loss_value_3: 0.03688
	loss_reward_3: 0.00514
	loss_policy_4: 0.05177
	accuracy_policy_4: 0.519
	loss_value_4: 0.03803
	loss_reward_4: 0.00562
	loss_policy_5: 0.05454
	accuracy_policy_5: 0.49744
	loss_value_5: 0.03916
	loss_reward_5: 0.00676
	loss_policy: 0.41195
	loss_value: 0.3494
	loss_reward: 0.02737
[2024-05-12 05:32:07] nn step 15600, lr: 0.081.
	loss_policy_0: 0.15142
	accuracy_policy_0: 0.70621
	loss_value_0: 0.16341
	loss_policy_1: 0.03896
	accuracy_policy_1: 0.62947
	loss_value_1: 0.03436
	loss_reward_1: 0.00527
	loss_policy_2: 0.04281
	accuracy_policy_2: 0.59723
	loss_value_2: 0.03579
	loss_reward_2: 0.00469
	loss_policy_3: 0.04599
	accuracy_policy_3: 0.57186
	loss_value_3: 0.03686
	loss_reward_3: 0.00513
	loss_policy_4: 0.04905
	accuracy_policy_4: 0.54807
	loss_value_4: 0.03811
	loss_reward_4: 0.00566
	loss_policy_5: 0.05196
	accuracy_policy_5: 0.525
	loss_value_5: 0.03919
	loss_reward_5: 0.00679
	loss_policy: 0.38019
	loss_value: 0.34772
	loss_reward: 0.02753
Optimization_Done 15600
[2024-05-12 05:34:22] [command] train weight_iter_15600.pkl 77 79
[2024-05-12 05:35:10] nn step 15700, lr: 0.081.
	loss_policy_0: 0.2059
	accuracy_policy_0: 0.63803
	loss_value_0: 0.1764
	loss_policy_1: 0.04877
	accuracy_policy_1: 0.58145
	loss_value_1: 0.03669
	loss_reward_1: 0.00501
	loss_policy_2: 0.05249
	accuracy_policy_2: 0.55355
	loss_value_2: 0.03797
	loss_reward_2: 0.00478
	loss_policy_3: 0.05595
	accuracy_policy_3: 0.52492
	loss_value_3: 0.03918
	loss_reward_3: 0.00495
	loss_policy_4: 0.05881
	accuracy_policy_4: 0.50246
	loss_value_4: 0.04028
	loss_reward_4: 0.00553
	loss_policy_5: 0.06187
	accuracy_policy_5: 0.48287
	loss_value_5: 0.0416
	loss_reward_5: 0.00647
	loss_policy: 0.48379
	loss_value: 0.37212
	loss_reward: 0.02674
[2024-05-12 05:35:56] nn step 15800, lr: 0.081.
	loss_policy_0: 0.17143
	accuracy_policy_0: 0.68934
	loss_value_0: 0.16843
	loss_policy_1: 0.04337
	accuracy_policy_1: 0.61662
	loss_value_1: 0.0352
	loss_reward_1: 0.005
	loss_policy_2: 0.04717
	accuracy_policy_2: 0.58912
	loss_value_2: 0.03639
	loss_reward_2: 0.00463
	loss_policy_3: 0.05054
	accuracy_policy_3: 0.56295
	loss_value_3: 0.03754
	loss_reward_3: 0.0049
	loss_policy_4: 0.05371
	accuracy_policy_4: 0.53977
	loss_value_4: 0.03856
	loss_reward_4: 0.00544
	loss_policy_5: 0.0567
	accuracy_policy_5: 0.51885
	loss_value_5: 0.03969
	loss_reward_5: 0.00618
	loss_policy: 0.42291
	loss_value: 0.35582
	loss_reward: 0.02614
Optimization_Done 15800
[2024-05-12 05:38:10] [command] train weight_iter_15800.pkl 78 80
[2024-05-12 05:39:01] nn step 15900, lr: 0.081.
	loss_policy_0: 0.19325
	accuracy_policy_0: 0.6493
	loss_value_0: 0.16516
	loss_policy_1: 0.04442
	accuracy_policy_1: 0.60281
	loss_value_1: 0.03448
	loss_reward_1: 0.00518
	loss_policy_2: 0.04758
	accuracy_policy_2: 0.57709
	loss_value_2: 0.03586
	loss_reward_2: 0.00487
	loss_policy_3: 0.05018
	accuracy_policy_3: 0.56064
	loss_value_3: 0.03721
	loss_reward_3: 0.00524
	loss_policy_4: 0.05311
	accuracy_policy_4: 0.53604
	loss_value_4: 0.0384
	loss_reward_4: 0.00571
	loss_policy_5: 0.05621
	accuracy_policy_5: 0.50896
	loss_value_5: 0.0396
	loss_reward_5: 0.00663
	loss_policy: 0.44475
	loss_value: 0.3507
	loss_reward: 0.02764
[2024-05-12 05:39:53] nn step 16000, lr: 0.081.
	loss_policy_0: 0.16826
	accuracy_policy_0: 0.6959
	loss_value_0: 0.16591
	loss_policy_1: 0.04073
	accuracy_policy_1: 0.63512
	loss_value_1: 0.03467
	loss_reward_1: 0.00515
	loss_policy_2: 0.0438
	accuracy_policy_2: 0.61166
	loss_value_2: 0.03589
	loss_reward_2: 0.0048
	loss_policy_3: 0.04698
	accuracy_policy_3: 0.58723
	loss_value_3: 0.03712
	loss_reward_3: 0.00512
	loss_policy_4: 0.05007
	accuracy_policy_4: 0.56551
	loss_value_4: 0.03837
	loss_reward_4: 0.00559
	loss_policy_5: 0.05333
	accuracy_policy_5: 0.54123
	loss_value_5: 0.0395
	loss_reward_5: 0.00671
	loss_policy: 0.40318
	loss_value: 0.35146
	loss_reward: 0.02737
Optimization_Done 16000
[2024-05-12 05:42:04] [command] train weight_iter_16000.pkl 79 81
[2024-05-12 05:42:56] nn step 16100, lr: 0.081.
	loss_policy_0: 0.19159
	accuracy_policy_0: 0.675
	loss_value_0: 0.18368
	loss_policy_1: 0.04584
	accuracy_policy_1: 0.61889
	loss_value_1: 0.03856
	loss_reward_1: 0.00595
	loss_policy_2: 0.04949
	accuracy_policy_2: 0.59525
	loss_value_2: 0.04009
	loss_reward_2: 0.00576
	loss_policy_3: 0.05311
	accuracy_policy_3: 0.5733
	loss_value_3: 0.04143
	loss_reward_3: 0.00624
	loss_policy_4: 0.05628
	accuracy_policy_4: 0.55137
	loss_value_4: 0.04284
	loss_reward_4: 0.00679
	loss_policy_5: 0.05936
	accuracy_policy_5: 0.53324
	loss_value_5: 0.04415
	loss_reward_5: 0.00781
	loss_policy: 0.45567
	loss_value: 0.39075
	loss_reward: 0.03254
[2024-05-12 05:43:48] nn step 16200, lr: 0.081.
	loss_policy_0: 0.16435
	accuracy_policy_0: 0.70975
	loss_value_0: 0.17783
	loss_policy_1: 0.04121
	accuracy_policy_1: 0.6452
	loss_value_1: 0.03729
	loss_reward_1: 0.00594
	loss_policy_2: 0.04472
	accuracy_policy_2: 0.62043
	loss_value_2: 0.03844
	loss_reward_2: 0.0055
	loss_policy_3: 0.04776
	accuracy_policy_3: 0.60275
	loss_value_3: 0.0398
	loss_reward_3: 0.00585
	loss_policy_4: 0.05075
	accuracy_policy_4: 0.58193
	loss_value_4: 0.04111
	loss_reward_4: 0.00643
	loss_policy_5: 0.05416
	accuracy_policy_5: 0.56041
	loss_value_5: 0.04249
	loss_reward_5: 0.00751
	loss_policy: 0.40295
	loss_value: 0.37696
	loss_reward: 0.03123
Optimization_Done 16200
[2024-05-12 05:45:31] [command] train weight_iter_16200.pkl 80 82
[2024-05-12 05:46:23] nn step 16300, lr: 0.081.
	loss_policy_0: 0.18426
	accuracy_policy_0: 0.6718
	loss_value_0: 0.18839
	loss_policy_1: 0.04417
	accuracy_policy_1: 0.61305
	loss_value_1: 0.03934
	loss_reward_1: 0.00574
	loss_policy_2: 0.04769
	accuracy_policy_2: 0.58674
	loss_value_2: 0.04087
	loss_reward_2: 0.00542
	loss_policy_3: 0.05051
	accuracy_policy_3: 0.56811
	loss_value_3: 0.04212
	loss_reward_3: 0.00596
	loss_policy_4: 0.05294
	accuracy_policy_4: 0.55102
	loss_value_4: 0.0434
	loss_reward_4: 0.00652
	loss_policy_5: 0.05562
	accuracy_policy_5: 0.53338
	loss_value_5: 0.0445
	loss_reward_5: 0.00754
	loss_policy: 0.43518
	loss_value: 0.39862
	loss_reward: 0.03119
[2024-05-12 05:47:15] nn step 16400, lr: 0.081.
	loss_policy_0: 0.15956
	accuracy_policy_0: 0.71857
	loss_value_0: 0.1878
	loss_policy_1: 0.0407
	accuracy_policy_1: 0.64615
	loss_value_1: 0.03928
	loss_reward_1: 0.00567
	loss_policy_2: 0.04439
	accuracy_policy_2: 0.62277
	loss_value_2: 0.04053
	loss_reward_2: 0.00538
	loss_policy_3: 0.0475
	accuracy_policy_3: 0.59902
	loss_value_3: 0.0417
	loss_reward_3: 0.00601
	loss_policy_4: 0.05028
	accuracy_policy_4: 0.58189
	loss_value_4: 0.04285
	loss_reward_4: 0.00635
	loss_policy_5: 0.05375
	accuracy_policy_5: 0.55715
	loss_value_5: 0.04412
	loss_reward_5: 0.00763
	loss_policy: 0.39617
	loss_value: 0.39629
	loss_reward: 0.03104
Optimization_Done 16400
[2024-05-12 05:49:28] [command] train weight_iter_16400.pkl 81 83
[2024-05-12 05:50:21] nn step 16500, lr: 0.081.
	loss_policy_0: 0.19188
	accuracy_policy_0: 0.64209
	loss_value_0: 0.17339
	loss_policy_1: 0.04505
	accuracy_policy_1: 0.58791
	loss_value_1: 0.03644
	loss_reward_1: 0.00485
	loss_policy_2: 0.04772
	accuracy_policy_2: 0.56885
	loss_value_2: 0.03754
	loss_reward_2: 0.00456
	loss_policy_3: 0.05041
	accuracy_policy_3: 0.54902
	loss_value_3: 0.03874
	loss_reward_3: 0.00489
	loss_policy_4: 0.05279
	accuracy_policy_4: 0.53291
	loss_value_4: 0.03988
	loss_reward_4: 0.00535
	loss_policy_5: 0.05558
	accuracy_policy_5: 0.5117
	loss_value_5: 0.04089
	loss_reward_5: 0.00637
	loss_policy: 0.44344
	loss_value: 0.36689
	loss_reward: 0.02601
[2024-05-12 05:51:14] nn step 16600, lr: 0.081.
	loss_policy_0: 0.16742
	accuracy_policy_0: 0.69434
	loss_value_0: 0.17108
	loss_policy_1: 0.04106
	accuracy_policy_1: 0.63176
	loss_value_1: 0.0359
	loss_reward_1: 0.00502
	loss_policy_2: 0.04434
	accuracy_policy_2: 0.60764
	loss_value_2: 0.03714
	loss_reward_2: 0.0047
	loss_policy_3: 0.04744
	accuracy_policy_3: 0.58578
	loss_value_3: 0.0382
	loss_reward_3: 0.00514
	loss_policy_4: 0.04999
	accuracy_policy_4: 0.569
	loss_value_4: 0.03925
	loss_reward_4: 0.00563
	loss_policy_5: 0.05313
	accuracy_policy_5: 0.54316
	loss_value_5: 0.04044
	loss_reward_5: 0.00668
	loss_policy: 0.40338
	loss_value: 0.36201
	loss_reward: 0.02717
Optimization_Done 16600
[2024-05-12 05:53:27] [command] train weight_iter_16600.pkl 82 84
[2024-05-12 05:54:15] nn step 16700, lr: 0.081.
	loss_policy_0: 0.21775
	accuracy_policy_0: 0.63371
	loss_value_0: 0.16444
	loss_policy_1: 0.04869
	accuracy_policy_1: 0.5909
	loss_value_1: 0.03441
	loss_reward_1: 0.00453
	loss_policy_2: 0.05196
	accuracy_policy_2: 0.56545
	loss_value_2: 0.03585
	loss_reward_2: 0.00448
	loss_policy_3: 0.05475
	accuracy_policy_3: 0.54148
	loss_value_3: 0.03702
	loss_reward_3: 0.00461
	loss_policy_4: 0.05719
	accuracy_policy_4: 0.52484
	loss_value_4: 0.03809
	loss_reward_4: 0.00511
	loss_policy_5: 0.06019
	accuracy_policy_5: 0.50219
	loss_value_5: 0.03924
	loss_reward_5: 0.0058
	loss_policy: 0.49052
	loss_value: 0.34903
	loss_reward: 0.02453
[2024-05-12 05:55:06] nn step 16800, lr: 0.081.
	loss_policy_0: 0.1697
	accuracy_policy_0: 0.68459
	loss_value_0: 0.14892
	loss_policy_1: 0.04047
	accuracy_policy_1: 0.62646
	loss_value_1: 0.03115
	loss_reward_1: 0.00424
	loss_policy_2: 0.04297
	accuracy_policy_2: 0.60607
	loss_value_2: 0.03219
	loss_reward_2: 0.00405
	loss_policy_3: 0.04585
	accuracy_policy_3: 0.58211
	loss_value_3: 0.03314
	loss_reward_3: 0.00418
	loss_policy_4: 0.04846
	accuracy_policy_4: 0.55631
	loss_value_4: 0.03418
	loss_reward_4: 0.00463
	loss_policy_5: 0.05128
	accuracy_policy_5: 0.53582
	loss_value_5: 0.03527
	loss_reward_5: 0.00528
	loss_policy: 0.39874
	loss_value: 0.31485
	loss_reward: 0.02238
Optimization_Done 16800
[2024-05-12 05:57:07] [command] train weight_iter_16800.pkl 83 85
[2024-05-12 05:57:59] nn step 16900, lr: 0.081.
	loss_policy_0: 0.20058
	accuracy_policy_0: 0.6477
	loss_value_0: 0.16609
	loss_policy_1: 0.04614
	accuracy_policy_1: 0.60232
	loss_value_1: 0.03476
	loss_reward_1: 0.00512
	loss_policy_2: 0.0489
	accuracy_policy_2: 0.58277
	loss_value_2: 0.03616
	loss_reward_2: 0.0048
	loss_policy_3: 0.05141
	accuracy_policy_3: 0.56697
	loss_value_3: 0.03733
	loss_reward_3: 0.0051
	loss_policy_4: 0.05436
	accuracy_policy_4: 0.55104
	loss_value_4: 0.03856
	loss_reward_4: 0.00561
	loss_policy_5: 0.0572
	accuracy_policy_5: 0.53123
	loss_value_5: 0.03974
	loss_reward_5: 0.00632
	loss_policy: 0.45858
	loss_value: 0.35263
	loss_reward: 0.02695
[2024-05-12 05:58:51] nn step 17000, lr: 0.081.
	loss_policy_0: 0.16519
	accuracy_policy_0: 0.69842
	loss_value_0: 0.15346
	loss_policy_1: 0.03922
	accuracy_policy_1: 0.64578
	loss_value_1: 0.03213
	loss_reward_1: 0.00466
	loss_policy_2: 0.04217
	accuracy_policy_2: 0.61938
	loss_value_2: 0.03313
	loss_reward_2: 0.00439
	loss_policy_3: 0.0449
	accuracy_policy_3: 0.60031
	loss_value_3: 0.0342
	loss_reward_3: 0.00488
	loss_policy_4: 0.04756
	accuracy_policy_4: 0.58049
	loss_value_4: 0.03532
	loss_reward_4: 0.00516
	loss_policy_5: 0.05031
	accuracy_policy_5: 0.55951
	loss_value_5: 0.03649
	loss_reward_5: 0.00601
	loss_policy: 0.38935
	loss_value: 0.32472
	loss_reward: 0.0251
Optimization_Done 17000
[2024-05-12 06:01:04] [command] train weight_iter_17000.pkl 84 86
[2024-05-12 06:01:54] nn step 17100, lr: 0.081.
	loss_policy_0: 0.1888
	accuracy_policy_0: 0.65096
	loss_value_0: 0.17266
	loss_policy_1: 0.04454
	accuracy_policy_1: 0.59277
	loss_value_1: 0.0361
	loss_reward_1: 0.00534
	loss_policy_2: 0.0478
	accuracy_policy_2: 0.57051
	loss_value_2: 0.03742
	loss_reward_2: 0.00498
	loss_policy_3: 0.05051
	accuracy_policy_3: 0.54865
	loss_value_3: 0.03845
	loss_reward_3: 0.00556
	loss_policy_4: 0.05349
	accuracy_policy_4: 0.52789
	loss_value_4: 0.03952
	loss_reward_4: 0.00617
	loss_policy_5: 0.05608
	accuracy_policy_5: 0.50813
	loss_value_5: 0.04074
	loss_reward_5: 0.00696
	loss_policy: 0.44123
	loss_value: 0.36489
	loss_reward: 0.02902
[2024-05-12 06:02:46] nn step 17200, lr: 0.081.
	loss_policy_0: 0.16831
	accuracy_policy_0: 0.69848
	loss_value_0: 0.17424
	loss_policy_1: 0.04127
	accuracy_policy_1: 0.63297
	loss_value_1: 0.0364
	loss_reward_1: 0.00555
	loss_policy_2: 0.04426
	accuracy_policy_2: 0.61387
	loss_value_2: 0.03763
	loss_reward_2: 0.00528
	loss_policy_3: 0.04747
	accuracy_policy_3: 0.5843
	loss_value_3: 0.0387
	loss_reward_3: 0.00555
	loss_policy_4: 0.05078
	accuracy_policy_4: 0.56342
	loss_value_4: 0.03985
	loss_reward_4: 0.00622
	loss_policy_5: 0.05388
	accuracy_policy_5: 0.54189
	loss_value_5: 0.041
	loss_reward_5: 0.00712
	loss_policy: 0.40596
	loss_value: 0.36782
	loss_reward: 0.02972
Optimization_Done 17200
[2024-05-12 06:05:00] [command] train weight_iter_17200.pkl 85 87
[2024-05-12 06:05:52] nn step 17300, lr: 0.081.
	loss_policy_0: 0.16966
	accuracy_policy_0: 0.66506
	loss_value_0: 0.16072
	loss_policy_1: 0.0409
	accuracy_policy_1: 0.59914
	loss_value_1: 0.0336
	loss_reward_1: 0.00539
	loss_policy_2: 0.04412
	accuracy_policy_2: 0.57553
	loss_value_2: 0.03501
	loss_reward_2: 0.00513
	loss_policy_3: 0.04701
	accuracy_policy_3: 0.55219
	loss_value_3: 0.03626
	loss_reward_3: 0.00553
	loss_policy_4: 0.04969
	accuracy_policy_4: 0.53064
	loss_value_4: 0.0375
	loss_reward_4: 0.00622
	loss_policy_5: 0.0525
	accuracy_policy_5: 0.51197
	loss_value_5: 0.03864
	loss_reward_5: 0.00718
	loss_policy: 0.40389
	loss_value: 0.34172
	loss_reward: 0.02945
[2024-05-12 06:06:45] nn step 17400, lr: 0.081.
	loss_policy_0: 0.15481
	accuracy_policy_0: 0.70219
	loss_value_0: 0.16511
	loss_policy_1: 0.03908
	accuracy_policy_1: 0.63445
	loss_value_1: 0.03458
	loss_reward_1: 0.00549
	loss_policy_2: 0.04283
	accuracy_policy_2: 0.60402
	loss_value_2: 0.03589
	loss_reward_2: 0.0054
	loss_policy_3: 0.04611
	accuracy_policy_3: 0.57676
	loss_value_3: 0.03704
	loss_reward_3: 0.0057
	loss_policy_4: 0.04917
	accuracy_policy_4: 0.55439
	loss_value_4: 0.0384
	loss_reward_4: 0.00648
	loss_policy_5: 0.05266
	accuracy_policy_5: 0.52916
	loss_value_5: 0.03965
	loss_reward_5: 0.00775
	loss_policy: 0.38466
	loss_value: 0.35066
	loss_reward: 0.03081
Optimization_Done 17400
[2024-05-12 06:08:45] [command] train weight_iter_17400.pkl 86 88
[2024-05-12 06:09:37] nn step 17500, lr: 0.081.
	loss_policy_0: 0.17255
	accuracy_policy_0: 0.66287
	loss_value_0: 0.15743
	loss_policy_1: 0.04102
	accuracy_policy_1: 0.6065
	loss_value_1: 0.03302
	loss_reward_1: 0.00533
	loss_policy_2: 0.04431
	accuracy_policy_2: 0.57848
	loss_value_2: 0.03445
	loss_reward_2: 0.0051
	loss_policy_3: 0.04775
	accuracy_policy_3: 0.55006
	loss_value_3: 0.03585
	loss_reward_3: 0.00551
	loss_policy_4: 0.05036
	accuracy_policy_4: 0.53178
	loss_value_4: 0.03715
	loss_reward_4: 0.00605
	loss_policy_5: 0.05329
	accuracy_policy_5: 0.50896
	loss_value_5: 0.03838
	loss_reward_5: 0.00702
	loss_policy: 0.40927
	loss_value: 0.33628
	loss_reward: 0.029
[2024-05-12 06:10:30] nn step 17600, lr: 0.081.
	loss_policy_0: 0.15322
	accuracy_policy_0: 0.70379
	loss_value_0: 0.15567
	loss_policy_1: 0.03817
	accuracy_policy_1: 0.63662
	loss_value_1: 0.03263
	loss_reward_1: 0.00543
	loss_policy_2: 0.04214
	accuracy_policy_2: 0.6058
	loss_value_2: 0.03397
	loss_reward_2: 0.00517
	loss_policy_3: 0.04537
	accuracy_policy_3: 0.57662
	loss_value_3: 0.03523
	loss_reward_3: 0.00555
	loss_policy_4: 0.04806
	accuracy_policy_4: 0.55701
	loss_value_4: 0.03648
	loss_reward_4: 0.00619
	loss_policy_5: 0.05118
	accuracy_policy_5: 0.53471
	loss_value_5: 0.03774
	loss_reward_5: 0.00704
	loss_policy: 0.37815
	loss_value: 0.33172
	loss_reward: 0.02938
Optimization_Done 17600
[2024-05-12 06:12:42] [command] train weight_iter_17600.pkl 87 89
[2024-05-12 06:13:35] nn step 17700, lr: 0.081.
	loss_policy_0: 0.17758
	accuracy_policy_0: 0.6623
	loss_value_0: 0.15935
	loss_policy_1: 0.04057
	accuracy_policy_1: 0.61967
	loss_value_1: 0.03333
	loss_reward_1: 0.00539
	loss_policy_2: 0.04386
	accuracy_policy_2: 0.59264
	loss_value_2: 0.0347
	loss_reward_2: 0.00517
	loss_policy_3: 0.04706
	accuracy_policy_3: 0.5692
	loss_value_3: 0.03599
	loss_reward_3: 0.00558
	loss_policy_4: 0.04984
	accuracy_policy_4: 0.54824
	loss_value_4: 0.03732
	loss_reward_4: 0.00611
	loss_policy_5: 0.05258
	accuracy_policy_5: 0.52707
	loss_value_5: 0.03879
	loss_reward_5: 0.00709
	loss_policy: 0.41149
	loss_value: 0.33949
	loss_reward: 0.02934
[2024-05-12 06:14:27] nn step 17800, lr: 0.081.
	loss_policy_0: 0.15037
	accuracy_policy_0: 0.71215
	loss_value_0: 0.15663
	loss_policy_1: 0.0367
	accuracy_policy_1: 0.65223
	loss_value_1: 0.03291
	loss_reward_1: 0.0053
	loss_policy_2: 0.04007
	accuracy_policy_2: 0.62635
	loss_value_2: 0.03442
	loss_reward_2: 0.0052
	loss_policy_3: 0.04362
	accuracy_policy_3: 0.59814
	loss_value_3: 0.03583
	loss_reward_3: 0.00565
	loss_policy_4: 0.0462
	accuracy_policy_4: 0.58053
	loss_value_4: 0.03692
	loss_reward_4: 0.00596
	loss_policy_5: 0.04926
	accuracy_policy_5: 0.55695
	loss_value_5: 0.03825
	loss_reward_5: 0.00705
	loss_policy: 0.36622
	loss_value: 0.33496
	loss_reward: 0.02916
Optimization_Done 17800
[2024-05-12 06:16:39] [command] train weight_iter_17800.pkl 88 90
[2024-05-12 06:17:31] nn step 17900, lr: 0.081.
	loss_policy_0: 0.18277
	accuracy_policy_0: 0.67377
	loss_value_0: 0.17499
	loss_policy_1: 0.04375
	accuracy_policy_1: 0.61252
	loss_value_1: 0.03669
	loss_reward_1: 0.00606
	loss_policy_2: 0.047
	accuracy_policy_2: 0.59209
	loss_value_2: 0.03826
	loss_reward_2: 0.00561
	loss_policy_3: 0.05026
	accuracy_policy_3: 0.56711
	loss_value_3: 0.03978
	loss_reward_3: 0.00602
	loss_policy_4: 0.05323
	accuracy_policy_4: 0.54939
	loss_value_4: 0.04111
	loss_reward_4: 0.00669
	loss_policy_5: 0.05586
	accuracy_policy_5: 0.529
	loss_value_5: 0.0425
	loss_reward_5: 0.00769
	loss_policy: 0.43287
	loss_value: 0.37334
	loss_reward: 0.03208
[2024-05-12 06:18:21] nn step 18000, lr: 0.081.
	loss_policy_0: 0.1519
	accuracy_policy_0: 0.71438
	loss_value_0: 0.16613
	loss_policy_1: 0.03744
	accuracy_policy_1: 0.65121
	loss_value_1: 0.03497
	loss_reward_1: 0.00562
	loss_policy_2: 0.04136
	accuracy_policy_2: 0.62379
	loss_value_2: 0.03642
	loss_reward_2: 0.00524
	loss_policy_3: 0.04475
	accuracy_policy_3: 0.59893
	loss_value_3: 0.03765
	loss_reward_3: 0.0057
	loss_policy_4: 0.04749
	accuracy_policy_4: 0.57846
	loss_value_4: 0.03887
	loss_reward_4: 0.0061
	loss_policy_5: 0.05038
	accuracy_policy_5: 0.55557
	loss_value_5: 0.04005
	loss_reward_5: 0.00713
	loss_policy: 0.37331
	loss_value: 0.35409
	loss_reward: 0.02979
Optimization_Done 18000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-12 10:29:41] [command] train weight_iter_18000.pkl 89 91
[2024-05-12 10:30:53] nn step 18100, lr: 0.0729.
	loss_policy_0: 0.1607
	accuracy_policy_0: 0.69621
	loss_value_0: 0.16883
	loss_policy_1: 0.03967
	accuracy_policy_1: 0.63621
	loss_value_1: 0.03546
	loss_reward_1: 0.00544
	loss_policy_2: 0.04343
	accuracy_policy_2: 0.60965
	loss_value_2: 0.03694
	loss_reward_2: 0.005
	loss_policy_3: 0.04647
	accuracy_policy_3: 0.58414
	loss_value_3: 0.0382
	loss_reward_3: 0.00556
	loss_policy_4: 0.04981
	accuracy_policy_4: 0.56045
	loss_value_4: 0.03946
	loss_reward_4: 0.00605
	loss_policy_5: 0.05306
	accuracy_policy_5: 0.5383
	loss_value_5: 0.04061
	loss_reward_5: 0.0072
	loss_policy: 0.39314
	loss_value: 0.3595
	loss_reward: 0.02924
[2024-05-12 10:31:42] nn step 18200, lr: 0.0729.
	loss_policy_0: 0.14079
	accuracy_policy_0: 0.73174
	loss_value_0: 0.16676
	loss_policy_1: 0.0367
	accuracy_policy_1: 0.66074
	loss_value_1: 0.03509
	loss_reward_1: 0.0053
	loss_policy_2: 0.04021
	accuracy_policy_2: 0.6376
	loss_value_2: 0.03657
	loss_reward_2: 0.00512
	loss_policy_3: 0.04365
	accuracy_policy_3: 0.61057
	loss_value_3: 0.03782
	loss_reward_3: 0.00562
	loss_policy_4: 0.04673
	accuracy_policy_4: 0.5858
	loss_value_4: 0.03904
	loss_reward_4: 0.00608
	loss_policy_5: 0.05024
	accuracy_policy_5: 0.56393
	loss_value_5: 0.04019
	loss_reward_5: 0.00703
	loss_policy: 0.35833
	loss_value: 0.35547
	loss_reward: 0.02915
Optimization_Done 18200
[2024-05-12 10:33:59] [command] train weight_iter_18200.pkl 90 92
[2024-05-12 10:35:01] nn step 18300, lr: 0.0729.
	loss_policy_0: 0.15965
	accuracy_policy_0: 0.69131
	loss_value_0: 0.16158
	loss_policy_1: 0.03948
	accuracy_policy_1: 0.62834
	loss_value_1: 0.03378
	loss_reward_1: 0.00511
	loss_policy_2: 0.04319
	accuracy_policy_2: 0.59967
	loss_value_2: 0.03514
	loss_reward_2: 0.00469
	loss_policy_3: 0.04642
	accuracy_policy_3: 0.57627
	loss_value_3: 0.0364
	loss_reward_3: 0.00514
	loss_policy_4: 0.04933
	accuracy_policy_4: 0.55389
	loss_value_4: 0.03745
	loss_reward_4: 0.0056
	loss_policy_5: 0.05251
	accuracy_policy_5: 0.52955
	loss_value_5: 0.03848
	loss_reward_5: 0.00635
	loss_policy: 0.39058
	loss_value: 0.34284
	loss_reward: 0.02689
[2024-05-12 10:36:03] nn step 18400, lr: 0.0729.
	loss_policy_0: 0.12806
	accuracy_policy_0: 0.73781
	loss_value_0: 0.15705
	loss_policy_1: 0.03391
	accuracy_policy_1: 0.66402
	loss_value_1: 0.03278
	loss_reward_1: 0.00484
	loss_policy_2: 0.03729
	accuracy_policy_2: 0.63951
	loss_value_2: 0.03392
	loss_reward_2: 0.00454
	loss_policy_3: 0.04058
	accuracy_policy_3: 0.61346
	loss_value_3: 0.03517
	loss_reward_3: 0.00483
	loss_policy_4: 0.04407
	accuracy_policy_4: 0.5859
	loss_value_4: 0.03619
	loss_reward_4: 0.00518
	loss_policy_5: 0.04733
	accuracy_policy_5: 0.5635
	loss_value_5: 0.03733
	loss_reward_5: 0.00616
	loss_policy: 0.33124
	loss_value: 0.33244
	loss_reward: 0.02556
Optimization_Done 18400
[2024-05-12 10:38:19] [command] train weight_iter_18400.pkl 91 93
[2024-05-12 10:39:22] nn step 18500, lr: 0.0729.
	loss_policy_0: 0.15801
	accuracy_policy_0: 0.67176
	loss_value_0: 0.1576
	loss_policy_1: 0.03791
	accuracy_policy_1: 0.61863
	loss_value_1: 0.03308
	loss_reward_1: 0.0048
	loss_policy_2: 0.04097
	accuracy_policy_2: 0.59646
	loss_value_2: 0.03443
	loss_reward_2: 0.00466
	loss_policy_3: 0.04449
	accuracy_policy_3: 0.57053
	loss_value_3: 0.03558
	loss_reward_3: 0.00488
	loss_policy_4: 0.04754
	accuracy_policy_4: 0.54943
	loss_value_4: 0.03674
	loss_reward_4: 0.00534
	loss_policy_5: 0.0514
	accuracy_policy_5: 0.51822
	loss_value_5: 0.03799
	loss_reward_5: 0.00647
	loss_policy: 0.38032
	loss_value: 0.33542
	loss_reward: 0.02614
[2024-05-12 10:40:26] nn step 18600, lr: 0.0729.
	loss_policy_0: 0.132
	accuracy_policy_0: 0.72762
	loss_value_0: 0.15746
	loss_policy_1: 0.03409
	accuracy_policy_1: 0.65904
	loss_value_1: 0.03292
	loss_reward_1: 0.00478
	loss_policy_2: 0.0377
	accuracy_policy_2: 0.6325
	loss_value_2: 0.03408
	loss_reward_2: 0.00469
	loss_policy_3: 0.04086
	accuracy_policy_3: 0.61047
	loss_value_3: 0.0352
	loss_reward_3: 0.00505
	loss_policy_4: 0.04486
	accuracy_policy_4: 0.58369
	loss_value_4: 0.03635
	loss_reward_4: 0.00537
	loss_policy_5: 0.04803
	accuracy_policy_5: 0.55629
	loss_value_5: 0.03759
	loss_reward_5: 0.00634
	loss_policy: 0.33754
	loss_value: 0.3336
	loss_reward: 0.02623
Optimization_Done 18600
[2024-05-12 10:42:44] [command] train weight_iter_18600.pkl 92 94
[2024-05-12 10:43:44] nn step 18700, lr: 0.0729.
	loss_policy_0: 0.14579
	accuracy_policy_0: 0.71367
	loss_value_0: 0.16315
	loss_policy_1: 0.03639
	accuracy_policy_1: 0.65334
	loss_value_1: 0.03416
	loss_reward_1: 0.00534
	loss_policy_2: 0.03998
	accuracy_policy_2: 0.62963
	loss_value_2: 0.03552
	loss_reward_2: 0.00516
	loss_policy_3: 0.04368
	accuracy_policy_3: 0.60088
	loss_value_3: 0.03667
	loss_reward_3: 0.00539
	loss_policy_4: 0.04675
	accuracy_policy_4: 0.57451
	loss_value_4: 0.03786
	loss_reward_4: 0.0059
	loss_policy_5: 0.04962
	accuracy_policy_5: 0.55779
	loss_value_5: 0.03896
	loss_reward_5: 0.00689
	loss_policy: 0.36221
	loss_value: 0.34634
	loss_reward: 0.02867
[2024-05-12 10:44:41] nn step 18800, lr: 0.0729.
	loss_policy_0: 0.12521
	accuracy_policy_0: 0.74809
	loss_value_0: 0.16299
	loss_policy_1: 0.03337
	accuracy_policy_1: 0.67693
	loss_value_1: 0.03414
	loss_reward_1: 0.00528
	loss_policy_2: 0.03684
	accuracy_policy_2: 0.6543
	loss_value_2: 0.03553
	loss_reward_2: 0.00514
	loss_policy_3: 0.03996
	accuracy_policy_3: 0.62791
	loss_value_3: 0.0366
	loss_reward_3: 0.00538
	loss_policy_4: 0.04343
	accuracy_policy_4: 0.59957
	loss_value_4: 0.03763
	loss_reward_4: 0.00597
	loss_policy_5: 0.04674
	accuracy_policy_5: 0.57934
	loss_value_5: 0.03864
	loss_reward_5: 0.00668
	loss_policy: 0.32555
	loss_value: 0.34553
	loss_reward: 0.02844
Optimization_Done 18800
[2024-05-12 10:47:02] [command] train weight_iter_18800.pkl 93 95
[2024-05-12 10:48:07] nn step 18900, lr: 0.0729.
	loss_policy_0: 0.15287
	accuracy_policy_0: 0.69654
	loss_value_0: 0.15458
	loss_policy_1: 0.03675
	accuracy_policy_1: 0.64812
	loss_value_1: 0.0324
	loss_reward_1: 0.00525
	loss_policy_2: 0.04024
	accuracy_policy_2: 0.6176
	loss_value_2: 0.03392
	loss_reward_2: 0.00502
	loss_policy_3: 0.04319
	accuracy_policy_3: 0.59855
	loss_value_3: 0.03519
	loss_reward_3: 0.00538
	loss_policy_4: 0.04633
	accuracy_policy_4: 0.57182
	loss_value_4: 0.03636
	loss_reward_4: 0.00575
	loss_policy_5: 0.04927
	accuracy_policy_5: 0.55125
	loss_value_5: 0.03737
	loss_reward_5: 0.00672
	loss_policy: 0.36865
	loss_value: 0.32982
	loss_reward: 0.02812
[2024-05-12 10:49:07] nn step 19000, lr: 0.0729.
	loss_policy_0: 0.13323
	accuracy_policy_0: 0.74016
	loss_value_0: 0.16061
	loss_policy_1: 0.03482
	accuracy_policy_1: 0.67246
	loss_value_1: 0.03387
	loss_reward_1: 0.00517
	loss_policy_2: 0.03858
	accuracy_policy_2: 0.6433
	loss_value_2: 0.03523
	loss_reward_2: 0.00503
	loss_policy_3: 0.04154
	accuracy_policy_3: 0.61824
	loss_value_3: 0.03642
	loss_reward_3: 0.00553
	loss_policy_4: 0.04466
	accuracy_policy_4: 0.59668
	loss_value_4: 0.03753
	loss_reward_4: 0.00595
	loss_policy_5: 0.04863
	accuracy_policy_5: 0.5707
	loss_value_5: 0.03893
	loss_reward_5: 0.00707
	loss_policy: 0.34145
	loss_value: 0.3426
	loss_reward: 0.02875
Optimization_Done 19000
[2024-05-12 10:51:27] [command] train weight_iter_19000.pkl 94 96
[2024-05-12 10:52:29] nn step 19100, lr: 0.0729.
	loss_policy_0: 0.18001
	accuracy_policy_0: 0.65705
	loss_value_0: 0.17313
	loss_policy_1: 0.04177
	accuracy_policy_1: 0.60932
	loss_value_1: 0.03578
	loss_reward_1: 0.00683
	loss_policy_2: 0.04475
	accuracy_policy_2: 0.581
	loss_value_2: 0.03712
	loss_reward_2: 0.0065
	loss_policy_3: 0.04736
	accuracy_policy_3: 0.56141
	loss_value_3: 0.03825
	loss_reward_3: 0.00686
	loss_policy_4: 0.04972
	accuracy_policy_4: 0.54363
	loss_value_4: 0.0392
	loss_reward_4: 0.00731
	loss_policy_5: 0.05195
	accuracy_policy_5: 0.53127
	loss_value_5: 0.04011
	loss_reward_5: 0.00807
	loss_policy: 0.41556
	loss_value: 0.3636
	loss_reward: 0.03557
[2024-05-12 10:53:35] nn step 19200, lr: 0.0729.
	loss_policy_0: 0.14936
	accuracy_policy_0: 0.71891
	loss_value_0: 0.16664
	loss_policy_1: 0.03718
	accuracy_policy_1: 0.65986
	loss_value_1: 0.03491
	loss_reward_1: 0.0069
	loss_policy_2: 0.04048
	accuracy_policy_2: 0.63369
	loss_value_2: 0.03612
	loss_reward_2: 0.00658
	loss_policy_3: 0.0435
	accuracy_policy_3: 0.61205
	loss_value_3: 0.03726
	loss_reward_3: 0.00688
	loss_policy_4: 0.04603
	accuracy_policy_4: 0.59424
	loss_value_4: 0.03833
	loss_reward_4: 0.00724
	loss_policy_5: 0.04865
	accuracy_policy_5: 0.57623
	loss_value_5: 0.03941
	loss_reward_5: 0.00825
	loss_policy: 0.36521
	loss_value: 0.35266
	loss_reward: 0.03584
Optimization_Done 19200
[2024-05-12 10:55:50] [command] train weight_iter_19200.pkl 95 97
[2024-05-12 10:56:58] nn step 19300, lr: 0.0729.
	loss_policy_0: 0.16653
	accuracy_policy_0: 0.66715
	loss_value_0: 0.15548
	loss_policy_1: 0.03885
	accuracy_policy_1: 0.61721
	loss_value_1: 0.03265
	loss_reward_1: 0.00617
	loss_policy_2: 0.04174
	accuracy_policy_2: 0.59592
	loss_value_2: 0.03393
	loss_reward_2: 0.00609
	loss_policy_3: 0.04467
	accuracy_policy_3: 0.57119
	loss_value_3: 0.03533
	loss_reward_3: 0.00645
	loss_policy_4: 0.04668
	accuracy_policy_4: 0.55799
	loss_value_4: 0.0365
	loss_reward_4: 0.00697
	loss_policy_5: 0.04916
	accuracy_policy_5: 0.53734
	loss_value_5: 0.03756
	loss_reward_5: 0.00768
	loss_policy: 0.38762
	loss_value: 0.33144
	loss_reward: 0.03336
[2024-05-12 10:58:03] nn step 19400, lr: 0.0729.
	loss_policy_0: 0.15153
	accuracy_policy_0: 0.71225
	loss_value_0: 0.16931
	loss_policy_1: 0.03735
	accuracy_policy_1: 0.65293
	loss_value_1: 0.03541
	loss_reward_1: 0.00638
	loss_policy_2: 0.04043
	accuracy_policy_2: 0.63094
	loss_value_2: 0.03654
	loss_reward_2: 0.00633
	loss_policy_3: 0.04339
	accuracy_policy_3: 0.60479
	loss_value_3: 0.03787
	loss_reward_3: 0.00669
	loss_policy_4: 0.0461
	accuracy_policy_4: 0.58877
	loss_value_4: 0.03898
	loss_reward_4: 0.00724
	loss_policy_5: 0.04894
	accuracy_policy_5: 0.56324
	loss_value_5: 0.04017
	loss_reward_5: 0.00811
	loss_policy: 0.36774
	loss_value: 0.35828
	loss_reward: 0.03475
Optimization_Done 19400
[2024-05-12 11:00:16] [command] train weight_iter_19400.pkl 96 98
[2024-05-12 11:01:22] nn step 19500, lr: 0.0729.
	loss_policy_0: 0.16248
	accuracy_policy_0: 0.70703
	loss_value_0: 0.18782
	loss_policy_1: 0.04076
	accuracy_policy_1: 0.63842
	loss_value_1: 0.03927
	loss_reward_1: 0.00714
	loss_policy_2: 0.04484
	accuracy_policy_2: 0.61045
	loss_value_2: 0.04063
	loss_reward_2: 0.00703
	loss_policy_3: 0.04793
	accuracy_policy_3: 0.58955
	loss_value_3: 0.04188
	loss_reward_3: 0.00759
	loss_policy_4: 0.05155
	accuracy_policy_4: 0.56447
	loss_value_4: 0.04315
	loss_reward_4: 0.00807
	loss_policy_5: 0.05485
	accuracy_policy_5: 0.54096
	loss_value_5: 0.04434
	loss_reward_5: 0.009
	loss_policy: 0.40242
	loss_value: 0.39709
	loss_reward: 0.03883
[2024-05-12 11:02:24] nn step 19600, lr: 0.0729.
	loss_policy_0: 0.1495
	accuracy_policy_0: 0.73137
	loss_value_0: 0.19344
	loss_policy_1: 0.03918
	accuracy_policy_1: 0.65684
	loss_value_1: 0.04058
	loss_reward_1: 0.00728
	loss_policy_2: 0.04322
	accuracy_policy_2: 0.62877
	loss_value_2: 0.04198
	loss_reward_2: 0.00695
	loss_policy_3: 0.04699
	accuracy_policy_3: 0.6023
	loss_value_3: 0.04324
	loss_reward_3: 0.00761
	loss_policy_4: 0.05029
	accuracy_policy_4: 0.58252
	loss_value_4: 0.04453
	loss_reward_4: 0.00818
	loss_policy_5: 0.05368
	accuracy_policy_5: 0.55693
	loss_value_5: 0.04601
	loss_reward_5: 0.00906
	loss_policy: 0.38286
	loss_value: 0.40977
	loss_reward: 0.03908
Optimization_Done 19600
[2024-05-12 11:04:16] [command] train weight_iter_19600.pkl 97 99
[2024-05-12 11:05:22] nn step 19700, lr: 0.0729.
	loss_policy_0: 0.15855
	accuracy_policy_0: 0.69934
	loss_value_0: 0.18532
	loss_policy_1: 0.03992
	accuracy_policy_1: 0.63449
	loss_value_1: 0.03893
	loss_reward_1: 0.00586
	loss_policy_2: 0.04393
	accuracy_policy_2: 0.60379
	loss_value_2: 0.04014
	loss_reward_2: 0.00571
	loss_policy_3: 0.0471
	accuracy_policy_3: 0.57768
	loss_value_3: 0.04124
	loss_reward_3: 0.00627
	loss_policy_4: 0.05023
	accuracy_policy_4: 0.55475
	loss_value_4: 0.04224
	loss_reward_4: 0.00678
	loss_policy_5: 0.0531
	accuracy_policy_5: 0.52838
	loss_value_5: 0.04319
	loss_reward_5: 0.00785
	loss_policy: 0.39283
	loss_value: 0.39107
	loss_reward: 0.03248
[2024-05-12 11:06:27] nn step 19800, lr: 0.0729.
	loss_policy_0: 0.12874
	accuracy_policy_0: 0.74156
	loss_value_0: 0.17143
	loss_policy_1: 0.03448
	accuracy_policy_1: 0.66881
	loss_value_1: 0.0362
	loss_reward_1: 0.00566
	loss_policy_2: 0.03826
	accuracy_policy_2: 0.63258
	loss_value_2: 0.03738
	loss_reward_2: 0.00547
	loss_policy_3: 0.04144
	accuracy_policy_3: 0.60727
	loss_value_3: 0.03857
	loss_reward_3: 0.00589
	loss_policy_4: 0.04494
	accuracy_policy_4: 0.57832
	loss_value_4: 0.03958
	loss_reward_4: 0.00653
	loss_policy_5: 0.04816
	accuracy_policy_5: 0.54912
	loss_value_5: 0.0406
	loss_reward_5: 0.00752
	loss_policy: 0.33602
	loss_value: 0.36376
	loss_reward: 0.03107
Optimization_Done 19800
[2024-05-12 11:08:43] [command] train weight_iter_19800.pkl 98 100
[2024-05-12 11:09:38] nn step 19900, lr: 0.0729.
	loss_policy_0: 0.1781
	accuracy_policy_0: 0.65869
	loss_value_0: 0.16156
	loss_policy_1: 0.04266
	accuracy_policy_1: 0.60057
	loss_value_1: 0.03386
	loss_reward_1: 0.00528
	loss_policy_2: 0.0461
	accuracy_policy_2: 0.57492
	loss_value_2: 0.03503
	loss_reward_2: 0.00516
	loss_policy_3: 0.04949
	accuracy_policy_3: 0.54461
	loss_value_3: 0.03599
	loss_reward_3: 0.00544
	loss_policy_4: 0.05265
	accuracy_policy_4: 0.5167
	loss_value_4: 0.03694
	loss_reward_4: 0.00593
	loss_policy_5: 0.05588
	accuracy_policy_5: 0.49217
	loss_value_5: 0.03793
	loss_reward_5: 0.00665
	loss_policy: 0.42488
	loss_value: 0.34131
	loss_reward: 0.02847
[2024-05-12 11:10:27] nn step 20000, lr: 0.0729.
	loss_policy_0: 0.14742
	accuracy_policy_0: 0.71809
	loss_value_0: 0.16131
	loss_policy_1: 0.03837
	accuracy_policy_1: 0.64506
	loss_value_1: 0.03385
	loss_reward_1: 0.00547
	loss_policy_2: 0.04213
	accuracy_policy_2: 0.61318
	loss_value_2: 0.03497
	loss_reward_2: 0.00525
	loss_policy_3: 0.04573
	accuracy_policy_3: 0.58438
	loss_value_3: 0.03595
	loss_reward_3: 0.00558
	loss_policy_4: 0.04937
	accuracy_policy_4: 0.55564
	loss_value_4: 0.03684
	loss_reward_4: 0.00601
	loss_policy_5: 0.05281
	accuracy_policy_5: 0.5258
	loss_value_5: 0.03786
	loss_reward_5: 0.00684
	loss_policy: 0.37583
	loss_value: 0.34078
	loss_reward: 0.02916
Optimization_Done 20000
[2024-05-12 11:12:10] [command] train weight_iter_20000.pkl 99 101
[2024-05-12 11:13:01] nn step 20100, lr: 0.0729.
	loss_policy_0: 0.19751
	accuracy_policy_0: 0.65518
	loss_value_0: 0.17021
	loss_policy_1: 0.0453
	accuracy_policy_1: 0.60861
	loss_value_1: 0.03526
	loss_reward_1: 0.00508
	loss_policy_2: 0.04946
	accuracy_policy_2: 0.57346
	loss_value_2: 0.03629
	loss_reward_2: 0.00497
	loss_policy_3: 0.0529
	accuracy_policy_3: 0.54441
	loss_value_3: 0.0372
	loss_reward_3: 0.00524
	loss_policy_4: 0.05628
	accuracy_policy_4: 0.51307
	loss_value_4: 0.03822
	loss_reward_4: 0.00577
	loss_policy_5: 0.05938
	accuracy_policy_5: 0.49217
	loss_value_5: 0.03902
	loss_reward_5: 0.00625
	loss_policy: 0.46082
	loss_value: 0.3562
	loss_reward: 0.0273
[2024-05-12 11:13:51] nn step 20200, lr: 0.0729.
	loss_policy_0: 0.16979
	accuracy_policy_0: 0.70344
	loss_value_0: 0.1654
	loss_policy_1: 0.04132
	accuracy_policy_1: 0.64406
	loss_value_1: 0.03409
	loss_reward_1: 0.00513
	loss_policy_2: 0.04498
	accuracy_policy_2: 0.6135
	loss_value_2: 0.03493
	loss_reward_2: 0.00503
	loss_policy_3: 0.04855
	accuracy_policy_3: 0.58557
	loss_value_3: 0.03562
	loss_reward_3: 0.00549
	loss_policy_4: 0.05196
	accuracy_policy_4: 0.56045
	loss_value_4: 0.03647
	loss_reward_4: 0.00565
	loss_policy_5: 0.05577
	accuracy_policy_5: 0.52973
	loss_value_5: 0.03733
	loss_reward_5: 0.0064
	loss_policy: 0.41237
	loss_value: 0.34384
	loss_reward: 0.0277
Optimization_Done 20200
[2024-05-12 11:16:03] [command] train weight_iter_20200.pkl 100 102
[2024-05-12 11:16:55] nn step 20300, lr: 0.0729.
	loss_policy_0: 0.21901
	accuracy_policy_0: 0.6333
	loss_value_0: 0.17386
	loss_policy_1: 0.04934
	accuracy_policy_1: 0.59027
	loss_value_1: 0.036
	loss_reward_1: 0.00554
	loss_policy_2: 0.05296
	accuracy_policy_2: 0.56631
	loss_value_2: 0.03713
	loss_reward_2: 0.00537
	loss_policy_3: 0.05602
	accuracy_policy_3: 0.54828
	loss_value_3: 0.03807
	loss_reward_3: 0.00557
	loss_policy_4: 0.0593
	accuracy_policy_4: 0.51893
	loss_value_4: 0.039
	loss_reward_4: 0.00619
	loss_policy_5: 0.06205
	accuracy_policy_5: 0.5043
	loss_value_5: 0.03983
	loss_reward_5: 0.00677
	loss_policy: 0.49869
	loss_value: 0.36389
	loss_reward: 0.02943
[2024-05-12 11:17:44] nn step 20400, lr: 0.0729.
	loss_policy_0: 0.17754
	accuracy_policy_0: 0.68947
	loss_value_0: 0.16662
	loss_policy_1: 0.04259
	accuracy_policy_1: 0.62861
	loss_value_1: 0.03466
	loss_reward_1: 0.00526
	loss_policy_2: 0.04593
	accuracy_policy_2: 0.60504
	loss_value_2: 0.03553
	loss_reward_2: 0.00517
	loss_policy_3: 0.0498
	accuracy_policy_3: 0.57619
	loss_value_3: 0.03647
	loss_reward_3: 0.0054
	loss_policy_4: 0.05335
	accuracy_policy_4: 0.54937
	loss_value_4: 0.03724
	loss_reward_4: 0.0058
	loss_policy_5: 0.05666
	accuracy_policy_5: 0.52434
	loss_value_5: 0.03826
	loss_reward_5: 0.00652
	loss_policy: 0.42588
	loss_value: 0.34877
	loss_reward: 0.02814
Optimization_Done 20400
[2024-05-12 11:19:58] [command] train weight_iter_20400.pkl 101 103
[2024-05-12 11:20:50] nn step 20500, lr: 0.0729.
	loss_policy_0: 0.18789
	accuracy_policy_0: 0.6683
	loss_value_0: 0.18501
	loss_policy_1: 0.04419
	accuracy_policy_1: 0.61947
	loss_value_1: 0.03842
	loss_reward_1: 0.006
	loss_policy_2: 0.04738
	accuracy_policy_2: 0.5909
	loss_value_2: 0.03957
	loss_reward_2: 0.00576
	loss_policy_3: 0.05052
	accuracy_policy_3: 0.57279
	loss_value_3: 0.04063
	loss_reward_3: 0.0064
	loss_policy_4: 0.05329
	accuracy_policy_4: 0.54922
	loss_value_4: 0.04151
	loss_reward_4: 0.00693
	loss_policy_5: 0.05592
	accuracy_policy_5: 0.5317
	loss_value_5: 0.04261
	loss_reward_5: 0.00772
	loss_policy: 0.43919
	loss_value: 0.38774
	loss_reward: 0.03282
[2024-05-12 11:21:42] nn step 20600, lr: 0.0729.
	loss_policy_0: 0.1636
	accuracy_policy_0: 0.71389
	loss_value_0: 0.18154
	loss_policy_1: 0.03949
	accuracy_policy_1: 0.65982
	loss_value_1: 0.03775
	loss_reward_1: 0.00591
	loss_policy_2: 0.04357
	accuracy_policy_2: 0.62365
	loss_value_2: 0.03886
	loss_reward_2: 0.00578
	loss_policy_3: 0.04706
	accuracy_policy_3: 0.5993
	loss_value_3: 0.03975
	loss_reward_3: 0.00624
	loss_policy_4: 0.04947
	accuracy_policy_4: 0.5808
	loss_value_4: 0.04074
	loss_reward_4: 0.00656
	loss_policy_5: 0.05257
	accuracy_policy_5: 0.5591
	loss_value_5: 0.04191
	loss_reward_5: 0.00767
	loss_policy: 0.39575
	loss_value: 0.38056
	loss_reward: 0.03216
Optimization_Done 20600
[2024-05-12 11:23:55] [command] train weight_iter_20600.pkl 102 104
[2024-05-12 11:24:48] nn step 20700, lr: 0.0729.
	loss_policy_0: 0.16609
	accuracy_policy_0: 0.69465
	loss_value_0: 0.1686
	loss_policy_1: 0.03937
	accuracy_policy_1: 0.6417
	loss_value_1: 0.03525
	loss_reward_1: 0.00548
	loss_policy_2: 0.04284
	accuracy_policy_2: 0.61236
	loss_value_2: 0.0365
	loss_reward_2: 0.00528
	loss_policy_3: 0.04579
	accuracy_policy_3: 0.58797
	loss_value_3: 0.03773
	loss_reward_3: 0.00588
	loss_policy_4: 0.04811
	accuracy_policy_4: 0.5723
	loss_value_4: 0.03889
	loss_reward_4: 0.00633
	loss_policy_5: 0.05069
	accuracy_policy_5: 0.55596
	loss_value_5: 0.04002
	loss_reward_5: 0.00712
	loss_policy: 0.39289
	loss_value: 0.35698
	loss_reward: 0.03008
[2024-05-12 11:25:39] nn step 20800, lr: 0.0729.
	loss_policy_0: 0.14941
	accuracy_policy_0: 0.73305
	loss_value_0: 0.17436
	loss_policy_1: 0.03758
	accuracy_policy_1: 0.66895
	loss_value_1: 0.03656
	loss_reward_1: 0.00581
	loss_policy_2: 0.04158
	accuracy_policy_2: 0.6402
	loss_value_2: 0.03785
	loss_reward_2: 0.00556
	loss_policy_3: 0.0451
	accuracy_policy_3: 0.61516
	loss_value_3: 0.03902
	loss_reward_3: 0.00614
	loss_policy_4: 0.04797
	accuracy_policy_4: 0.59574
	loss_value_4: 0.04034
	loss_reward_4: 0.0066
	loss_policy_5: 0.05076
	accuracy_policy_5: 0.57428
	loss_value_5: 0.04164
	loss_reward_5: 0.00769
	loss_policy: 0.37241
	loss_value: 0.36978
	loss_reward: 0.0318
Optimization_Done 20800
[2024-05-12 11:27:40] [command] train weight_iter_20800.pkl 103 105
[2024-05-12 11:28:37] nn step 20900, lr: 0.0729.
	loss_policy_0: 0.18872
	accuracy_policy_0: 0.67418
	loss_value_0: 0.18235
	loss_policy_1: 0.04357
	accuracy_policy_1: 0.63025
	loss_value_1: 0.03813
	loss_reward_1: 0.00588
	loss_policy_2: 0.04753
	accuracy_policy_2: 0.5951
	loss_value_2: 0.0396
	loss_reward_2: 0.00563
	loss_policy_3: 0.0511
	accuracy_policy_3: 0.56867
	loss_value_3: 0.04107
	loss_reward_3: 0.00609
	loss_policy_4: 0.05448
	accuracy_policy_4: 0.54572
	loss_value_4: 0.04251
	loss_reward_4: 0.00656
	loss_policy_5: 0.0572
	accuracy_policy_5: 0.52279
	loss_value_5: 0.04386
	loss_reward_5: 0.00763
	loss_policy: 0.44259
	loss_value: 0.38753
	loss_reward: 0.03178
[2024-05-12 11:29:34] nn step 21000, lr: 0.0729.
	loss_policy_0: 0.15158
	accuracy_policy_0: 0.72504
	loss_value_0: 0.17045
	loss_policy_1: 0.03769
	accuracy_policy_1: 0.66045
	loss_value_1: 0.0358
	loss_reward_1: 0.00558
	loss_policy_2: 0.04169
	accuracy_policy_2: 0.62986
	loss_value_2: 0.03723
	loss_reward_2: 0.00538
	loss_policy_3: 0.04522
	accuracy_policy_3: 0.60373
	loss_value_3: 0.03858
	loss_reward_3: 0.00593
	loss_policy_4: 0.04831
	accuracy_policy_4: 0.58051
	loss_value_4: 0.03992
	loss_reward_4: 0.00644
	loss_policy_5: 0.05103
	accuracy_policy_5: 0.56086
	loss_value_5: 0.04119
	loss_reward_5: 0.00736
	loss_policy: 0.37552
	loss_value: 0.36316
	loss_reward: 0.0307
Optimization_Done 21000
[2024-05-12 11:31:49] [command] train weight_iter_21000.pkl 104 106
[2024-05-12 11:32:48] nn step 21100, lr: 0.0729.
	loss_policy_0: 0.19948
	accuracy_policy_0: 0.65031
	loss_value_0: 0.17735
	loss_policy_1: 0.04598
	accuracy_policy_1: 0.6024
	loss_value_1: 0.03712
	loss_reward_1: 0.0055
	loss_policy_2: 0.0498
	accuracy_policy_2: 0.57457
	loss_value_2: 0.03873
	loss_reward_2: 0.00537
	loss_policy_3: 0.05295
	accuracy_policy_3: 0.55016
	loss_value_3: 0.04021
	loss_reward_3: 0.00572
	loss_policy_4: 0.05659
	accuracy_policy_4: 0.5265
	loss_value_4: 0.04156
	loss_reward_4: 0.00647
	loss_policy_5: 0.05931
	accuracy_policy_5: 0.5068
	loss_value_5: 0.04293
	loss_reward_5: 0.00735
	loss_policy: 0.46411
	loss_value: 0.37792
	loss_reward: 0.0304
[2024-05-12 11:33:47] nn step 21200, lr: 0.0729.
	loss_policy_0: 0.16406
	accuracy_policy_0: 0.69852
	loss_value_0: 0.16635
	loss_policy_1: 0.03975
	accuracy_policy_1: 0.63541
	loss_value_1: 0.03505
	loss_reward_1: 0.00536
	loss_policy_2: 0.0435
	accuracy_policy_2: 0.6115
	loss_value_2: 0.03657
	loss_reward_2: 0.00506
	loss_policy_3: 0.04721
	accuracy_policy_3: 0.58203
	loss_value_3: 0.03799
	loss_reward_3: 0.00541
	loss_policy_4: 0.05098
	accuracy_policy_4: 0.55699
	loss_value_4: 0.03935
	loss_reward_4: 0.00597
	loss_policy_5: 0.05376
	accuracy_policy_5: 0.53812
	loss_value_5: 0.04068
	loss_reward_5: 0.00705
	loss_policy: 0.39926
	loss_value: 0.35599
	loss_reward: 0.02885
Optimization_Done 21200
[2024-05-12 11:36:01] [command] train weight_iter_21200.pkl 105 107
[2024-05-12 11:37:05] nn step 21300, lr: 0.0729.
	loss_policy_0: 0.18498
	accuracy_policy_0: 0.65996
	loss_value_0: 0.17297
	loss_policy_1: 0.04342
	accuracy_policy_1: 0.60289
	loss_value_1: 0.03622
	loss_reward_1: 0.00517
	loss_policy_2: 0.04701
	accuracy_policy_2: 0.57443
	loss_value_2: 0.03763
	loss_reward_2: 0.00479
	loss_policy_3: 0.0502
	accuracy_policy_3: 0.54875
	loss_value_3: 0.03909
	loss_reward_3: 0.00522
	loss_policy_4: 0.05359
	accuracy_policy_4: 0.52725
	loss_value_4: 0.04055
	loss_reward_4: 0.00577
	loss_policy_5: 0.0562
	accuracy_policy_5: 0.50395
	loss_value_5: 0.04187
	loss_reward_5: 0.00655
	loss_policy: 0.4354
	loss_value: 0.36833
	loss_reward: 0.0275
[2024-05-12 11:38:03] nn step 21400, lr: 0.0729.
	loss_policy_0: 0.15905
	accuracy_policy_0: 0.70486
	loss_value_0: 0.17127
	loss_policy_1: 0.03954
	accuracy_policy_1: 0.63461
	loss_value_1: 0.03604
	loss_reward_1: 0.00522
	loss_policy_2: 0.04363
	accuracy_policy_2: 0.60475
	loss_value_2: 0.03745
	loss_reward_2: 0.00479
	loss_policy_3: 0.047
	accuracy_policy_3: 0.5791
	loss_value_3: 0.03875
	loss_reward_3: 0.00513
	loss_policy_4: 0.05022
	accuracy_policy_4: 0.55641
	loss_value_4: 0.03994
	loss_reward_4: 0.00562
	loss_policy_5: 0.05332
	accuracy_policy_5: 0.53256
	loss_value_5: 0.04128
	loss_reward_5: 0.00673
	loss_policy: 0.39277
	loss_value: 0.36473
	loss_reward: 0.02749
Optimization_Done 21400
[2024-05-12 11:40:14] [command] train weight_iter_21400.pkl 106 108
[2024-05-12 11:41:16] nn step 21500, lr: 0.0729.
	loss_policy_0: 0.17365
	accuracy_policy_0: 0.67012
	loss_value_0: 0.16672
	loss_policy_1: 0.04065
	accuracy_policy_1: 0.6215
	loss_value_1: 0.03495
	loss_reward_1: 0.0047
	loss_policy_2: 0.04434
	accuracy_policy_2: 0.58883
	loss_value_2: 0.03636
	loss_reward_2: 0.00445
	loss_policy_3: 0.04744
	accuracy_policy_3: 0.566
	loss_value_3: 0.03779
	loss_reward_3: 0.00469
	loss_policy_4: 0.05027
	accuracy_policy_4: 0.53773
	loss_value_4: 0.03905
	loss_reward_4: 0.00526
	loss_policy_5: 0.05294
	accuracy_policy_5: 0.51895
	loss_value_5: 0.04032
	loss_reward_5: 0.00637
	loss_policy: 0.40928
	loss_value: 0.35519
	loss_reward: 0.02546
[2024-05-12 11:42:20] nn step 21600, lr: 0.0729.
	loss_policy_0: 0.1391
	accuracy_policy_0: 0.71656
	loss_value_0: 0.15641
	loss_policy_1: 0.03503
	accuracy_policy_1: 0.64998
	loss_value_1: 0.03278
	loss_reward_1: 0.00452
	loss_policy_2: 0.03858
	accuracy_policy_2: 0.62059
	loss_value_2: 0.03421
	loss_reward_2: 0.00424
	loss_policy_3: 0.04189
	accuracy_policy_3: 0.5942
	loss_value_3: 0.03551
	loss_reward_3: 0.00455
	loss_policy_4: 0.04493
	accuracy_policy_4: 0.57023
	loss_value_4: 0.03681
	loss_reward_4: 0.00505
	loss_policy_5: 0.04784
	accuracy_policy_5: 0.55137
	loss_value_5: 0.03807
	loss_reward_5: 0.00613
	loss_policy: 0.34736
	loss_value: 0.3338
	loss_reward: 0.02449
Optimization_Done 21600
[2024-05-12 11:44:39] [command] train weight_iter_21600.pkl 107 109
[2024-05-12 11:45:46] nn step 21700, lr: 0.0729.
	loss_policy_0: 0.16711
	accuracy_policy_0: 0.66188
	loss_value_0: 0.1515
	loss_policy_1: 0.03828
	accuracy_policy_1: 0.62098
	loss_value_1: 0.0314
	loss_reward_1: 0.00465
	loss_policy_2: 0.04113
	accuracy_policy_2: 0.59588
	loss_value_2: 0.03245
	loss_reward_2: 0.00451
	loss_policy_3: 0.04398
	accuracy_policy_3: 0.57357
	loss_value_3: 0.03363
	loss_reward_3: 0.00483
	loss_policy_4: 0.04646
	accuracy_policy_4: 0.55396
	loss_value_4: 0.0348
	loss_reward_4: 0.00521
	loss_policy_5: 0.0487
	accuracy_policy_5: 0.53525
	loss_value_5: 0.03576
	loss_reward_5: 0.00597
	loss_policy: 0.38566
	loss_value: 0.31954
	loss_reward: 0.02518
[2024-05-12 11:46:52] nn step 21800, lr: 0.0729.
	loss_policy_0: 0.13968
	accuracy_policy_0: 0.71574
	loss_value_0: 0.15031
	loss_policy_1: 0.03422
	accuracy_policy_1: 0.66188
	loss_value_1: 0.03152
	loss_reward_1: 0.00473
	loss_policy_2: 0.03732
	accuracy_policy_2: 0.63498
	loss_value_2: 0.03261
	loss_reward_2: 0.00446
	loss_policy_3: 0.04001
	accuracy_policy_3: 0.61529
	loss_value_3: 0.03359
	loss_reward_3: 0.00467
	loss_policy_4: 0.04351
	accuracy_policy_4: 0.58977
	loss_value_4: 0.03468
	loss_reward_4: 0.00501
	loss_policy_5: 0.04609
	accuracy_policy_5: 0.56445
	loss_value_5: 0.03574
	loss_reward_5: 0.00598
	loss_policy: 0.34083
	loss_value: 0.31846
	loss_reward: 0.02485
Optimization_Done 21800
[2024-05-12 11:49:12] [command] train weight_iter_21800.pkl 108 110
[2024-05-12 11:50:14] nn step 21900, lr: 0.0729.
	loss_policy_0: 0.17792
	accuracy_policy_0: 0.65295
	loss_value_0: 0.16106
	loss_policy_1: 0.04079
	accuracy_policy_1: 0.60344
	loss_value_1: 0.03344
	loss_reward_1: 0.00472
	loss_policy_2: 0.04396
	accuracy_policy_2: 0.57689
	loss_value_2: 0.03482
	loss_reward_2: 0.0047
	loss_policy_3: 0.0469
	accuracy_policy_3: 0.5534
	loss_value_3: 0.03613
	loss_reward_3: 0.00512
	loss_policy_4: 0.0497
	accuracy_policy_4: 0.5317
	loss_value_4: 0.03735
	loss_reward_4: 0.00565
	loss_policy_5: 0.05211
	accuracy_policy_5: 0.51232
	loss_value_5: 0.03854
	loss_reward_5: 0.0065
	loss_policy: 0.41139
	loss_value: 0.34133
	loss_reward: 0.02669
[2024-05-12 11:51:17] nn step 22000, lr: 0.0729.
	loss_policy_0: 0.15173
	accuracy_policy_0: 0.70424
	loss_value_0: 0.1639
	loss_policy_1: 0.03691
	accuracy_policy_1: 0.64621
	loss_value_1: 0.03416
	loss_reward_1: 0.00469
	loss_policy_2: 0.04058
	accuracy_policy_2: 0.61684
	loss_value_2: 0.03548
	loss_reward_2: 0.0048
	loss_policy_3: 0.04355
	accuracy_policy_3: 0.59162
	loss_value_3: 0.03655
	loss_reward_3: 0.00513
	loss_policy_4: 0.04646
	accuracy_policy_4: 0.57133
	loss_value_4: 0.03782
	loss_reward_4: 0.00562
	loss_policy_5: 0.04936
	accuracy_policy_5: 0.54797
	loss_value_5: 0.0389
	loss_reward_5: 0.00678
	loss_policy: 0.36858
	loss_value: 0.34682
	loss_reward: 0.02703
Optimization_Done 22000
[2024-05-12 11:53:31] [command] train weight_iter_22000.pkl 109 111
[2024-05-12 11:54:36] nn step 22100, lr: 0.0729.
	loss_policy_0: 0.1702
	accuracy_policy_0: 0.67527
	loss_value_0: 0.17987
	loss_policy_1: 0.04167
	accuracy_policy_1: 0.61162
	loss_value_1: 0.0374
	loss_reward_1: 0.00574
	loss_policy_2: 0.04631
	accuracy_policy_2: 0.57514
	loss_value_2: 0.03868
	loss_reward_2: 0.0054
	loss_policy_3: 0.0496
	accuracy_policy_3: 0.54975
	loss_value_3: 0.03983
	loss_reward_3: 0.00591
	loss_policy_4: 0.05238
	accuracy_policy_4: 0.52828
	loss_value_4: 0.04092
	loss_reward_4: 0.00657
	loss_policy_5: 0.05534
	accuracy_policy_5: 0.50693
	loss_value_5: 0.04189
	loss_reward_5: 0.00762
	loss_policy: 0.4155
	loss_value: 0.37858
	loss_reward: 0.03125
[2024-05-12 11:55:40] nn step 22200, lr: 0.0729.
	loss_policy_0: 0.14523
	accuracy_policy_0: 0.71445
	loss_value_0: 0.17593
	loss_policy_1: 0.03751
	accuracy_policy_1: 0.63873
	loss_value_1: 0.03671
	loss_reward_1: 0.00551
	loss_policy_2: 0.04194
	accuracy_policy_2: 0.6074
	loss_value_2: 0.03791
	loss_reward_2: 0.00516
	loss_policy_3: 0.04537
	accuracy_policy_3: 0.58082
	loss_value_3: 0.03894
	loss_reward_3: 0.00572
	loss_policy_4: 0.04851
	accuracy_policy_4: 0.56086
	loss_value_4: 0.04
	loss_reward_4: 0.00623
	loss_policy_5: 0.05178
	accuracy_policy_5: 0.53879
	loss_value_5: 0.04102
	loss_reward_5: 0.0074
	loss_policy: 0.37033
	loss_value: 0.37052
	loss_reward: 0.03002
Optimization_Done 22200
[2024-05-12 11:57:27] [command] train weight_iter_22200.pkl 110 112
[2024-05-12 11:58:20] nn step 22300, lr: 0.0729.
	loss_policy_0: 0.1765
	accuracy_policy_0: 0.65785
	loss_value_0: 0.18194
	loss_policy_1: 0.04357
	accuracy_policy_1: 0.59209
	loss_value_1: 0.03823
	loss_reward_1: 0.00526
	loss_policy_2: 0.04735
	accuracy_policy_2: 0.55998
	loss_value_2: 0.03953
	loss_reward_2: 0.00512
	loss_policy_3: 0.05062
	accuracy_policy_3: 0.53391
	loss_value_3: 0.04091
	loss_reward_3: 0.00555
	loss_policy_4: 0.05376
	accuracy_policy_4: 0.50771
	loss_value_4: 0.0419
	loss_reward_4: 0.00613
	loss_policy_5: 0.05655
	accuracy_policy_5: 0.48236
	loss_value_5: 0.04279
	loss_reward_5: 0.00756
	loss_policy: 0.42835
	loss_value: 0.3853
	loss_reward: 0.02962
[2024-05-12 11:59:26] nn step 22400, lr: 0.0729.
	loss_policy_0: 0.14508
	accuracy_policy_0: 0.7068
	loss_value_0: 0.17264
	loss_policy_1: 0.03794
	accuracy_policy_1: 0.63088
	loss_value_1: 0.03631
	loss_reward_1: 0.00518
	loss_policy_2: 0.04202
	accuracy_policy_2: 0.59332
	loss_value_2: 0.03762
	loss_reward_2: 0.0049
	loss_policy_3: 0.04587
	accuracy_policy_3: 0.56285
	loss_value_3: 0.03864
	loss_reward_3: 0.00537
	loss_policy_4: 0.04908
	accuracy_policy_4: 0.53777
	loss_value_4: 0.03969
	loss_reward_4: 0.00587
	loss_policy_5: 0.05199
	accuracy_policy_5: 0.51223
	loss_value_5: 0.04072
	loss_reward_5: 0.00722
	loss_policy: 0.37199
	loss_value: 0.36562
	loss_reward: 0.02854
Optimization_Done 22400
[2024-05-12 12:01:42] [command] train weight_iter_22400.pkl 111 113
[2024-05-12 12:02:41] nn step 22500, lr: 0.0729.
	loss_policy_0: 0.19558
	accuracy_policy_0: 0.64225
	loss_value_0: 0.17245
	loss_policy_1: 0.04648
	accuracy_policy_1: 0.58494
	loss_value_1: 0.03628
	loss_reward_1: 0.00468
	loss_policy_2: 0.05006
	accuracy_policy_2: 0.55619
	loss_value_2: 0.03776
	loss_reward_2: 0.0045
	loss_policy_3: 0.05355
	accuracy_policy_3: 0.53074
	loss_value_3: 0.03896
	loss_reward_3: 0.00483
	loss_policy_4: 0.05644
	accuracy_policy_4: 0.50555
	loss_value_4: 0.0402
	loss_reward_4: 0.00539
	loss_policy_5: 0.05907
	accuracy_policy_5: 0.48699
	loss_value_5: 0.04136
	loss_reward_5: 0.00639
	loss_policy: 0.46118
	loss_value: 0.36701
	loss_reward: 0.02578
[2024-05-12 12:03:40] nn step 22600, lr: 0.0729.
	loss_policy_0: 0.16788
	accuracy_policy_0: 0.6935
	loss_value_0: 0.17109
	loss_policy_1: 0.04228
	accuracy_policy_1: 0.62166
	loss_value_1: 0.03589
	loss_reward_1: 0.0049
	loss_policy_2: 0.04664
	accuracy_policy_2: 0.5882
	loss_value_2: 0.03725
	loss_reward_2: 0.00465
	loss_policy_3: 0.05002
	accuracy_policy_3: 0.56121
	loss_value_3: 0.03844
	loss_reward_3: 0.00481
	loss_policy_4: 0.05343
	accuracy_policy_4: 0.53707
	loss_value_4: 0.03963
	loss_reward_4: 0.00542
	loss_policy_5: 0.05701
	accuracy_policy_5: 0.5093
	loss_value_5: 0.0408
	loss_reward_5: 0.00665
	loss_policy: 0.41726
	loss_value: 0.36311
	loss_reward: 0.02643
Optimization_Done 22600
[2024-05-12 12:05:56] [command] train weight_iter_22600.pkl 112 114
[2024-05-12 12:06:58] nn step 22700, lr: 0.0729.
	loss_policy_0: 0.20278
	accuracy_policy_0: 0.64668
	loss_value_0: 0.16003
	loss_policy_1: 0.04632
	accuracy_policy_1: 0.59629
	loss_value_1: 0.03355
	loss_reward_1: 0.00444
	loss_policy_2: 0.04942
	accuracy_policy_2: 0.57123
	loss_value_2: 0.03497
	loss_reward_2: 0.00416
	loss_policy_3: 0.05259
	accuracy_policy_3: 0.54973
	loss_value_3: 0.03629
	loss_reward_3: 0.00458
	loss_policy_4: 0.05561
	accuracy_policy_4: 0.52445
	loss_value_4: 0.0375
	loss_reward_4: 0.005
	loss_policy_5: 0.05782
	accuracy_policy_5: 0.50818
	loss_value_5: 0.0388
	loss_reward_5: 0.00574
	loss_policy: 0.46454
	loss_value: 0.34114
	loss_reward: 0.02391
[2024-05-12 12:08:06] nn step 22800, lr: 0.0729.
	loss_policy_0: 0.16331
	accuracy_policy_0: 0.6933
	loss_value_0: 0.14598
	loss_policy_1: 0.039
	accuracy_policy_1: 0.63266
	loss_value_1: 0.03068
	loss_reward_1: 0.00401
	loss_policy_2: 0.04263
	accuracy_policy_2: 0.59996
	loss_value_2: 0.03193
	loss_reward_2: 0.00388
	loss_policy_3: 0.04563
	accuracy_policy_3: 0.5758
	loss_value_3: 0.03315
	loss_reward_3: 0.00401
	loss_policy_4: 0.04861
	accuracy_policy_4: 0.55078
	loss_value_4: 0.03423
	loss_reward_4: 0.0046
	loss_policy_5: 0.05115
	accuracy_policy_5: 0.53322
	loss_value_5: 0.03544
	loss_reward_5: 0.00544
	loss_policy: 0.39033
	loss_value: 0.31141
	loss_reward: 0.02194
Optimization_Done 22800
[2024-05-12 12:10:06] [command] train weight_iter_22800.pkl 113 115
[2024-05-12 12:11:06] nn step 22900, lr: 0.0729.
	loss_policy_0: 0.18779
	accuracy_policy_0: 0.64254
	loss_value_0: 0.14601
	loss_policy_1: 0.04284
	accuracy_policy_1: 0.59479
	loss_value_1: 0.03062
	loss_reward_1: 0.00442
	loss_policy_2: 0.04605
	accuracy_policy_2: 0.56787
	loss_value_2: 0.03188
	loss_reward_2: 0.00412
	loss_policy_3: 0.04896
	accuracy_policy_3: 0.54373
	loss_value_3: 0.03311
	loss_reward_3: 0.00453
	loss_policy_4: 0.05165
	accuracy_policy_4: 0.51979
	loss_value_4: 0.03432
	loss_reward_4: 0.00495
	loss_policy_5: 0.05427
	accuracy_policy_5: 0.50168
	loss_value_5: 0.03551
	loss_reward_5: 0.0056
	loss_policy: 0.43155
	loss_value: 0.31145
	loss_reward: 0.02362
[2024-05-12 12:12:04] nn step 23000, lr: 0.0729.
	loss_policy_0: 0.17843
	accuracy_policy_0: 0.68336
	loss_value_0: 0.15698
	loss_policy_1: 0.04197
	accuracy_policy_1: 0.63381
	loss_value_1: 0.03287
	loss_reward_1: 0.00476
	loss_policy_2: 0.04589
	accuracy_policy_2: 0.60482
	loss_value_2: 0.03421
	loss_reward_2: 0.00439
	loss_policy_3: 0.04968
	accuracy_policy_3: 0.57443
	loss_value_3: 0.03542
	loss_reward_3: 0.00466
	loss_policy_4: 0.05264
	accuracy_policy_4: 0.55512
	loss_value_4: 0.03674
	loss_reward_4: 0.0051
	loss_policy_5: 0.05547
	accuracy_policy_5: 0.53455
	loss_value_5: 0.03796
	loss_reward_5: 0.00614
	loss_policy: 0.42408
	loss_value: 0.33417
	loss_reward: 0.02505
Optimization_Done 23000
[2024-05-12 12:14:18] [command] train weight_iter_23000.pkl 114 116
[2024-05-12 12:15:23] nn step 23100, lr: 0.0729.
	loss_policy_0: 0.17663
	accuracy_policy_0: 0.67572
	loss_value_0: 0.16346
	loss_policy_1: 0.04163
	accuracy_policy_1: 0.62473
	loss_value_1: 0.03426
	loss_reward_1: 0.00527
	loss_policy_2: 0.04542
	accuracy_policy_2: 0.59359
	loss_value_2: 0.03555
	loss_reward_2: 0.00494
	loss_policy_3: 0.04873
	accuracy_policy_3: 0.56914
	loss_value_3: 0.03676
	loss_reward_3: 0.0053
	loss_policy_4: 0.05141
	accuracy_policy_4: 0.55346
	loss_value_4: 0.038
	loss_reward_4: 0.00592
	loss_policy_5: 0.05376
	accuracy_policy_5: 0.52979
	loss_value_5: 0.03914
	loss_reward_5: 0.007
	loss_policy: 0.41759
	loss_value: 0.34716
	loss_reward: 0.02843
[2024-05-12 12:16:26] nn step 23200, lr: 0.0729.
	loss_policy_0: 0.14542
	accuracy_policy_0: 0.72189
	loss_value_0: 0.15678
	loss_policy_1: 0.03645
	accuracy_policy_1: 0.65449
	loss_value_1: 0.03277
	loss_reward_1: 0.00496
	loss_policy_2: 0.04014
	accuracy_policy_2: 0.62186
	loss_value_2: 0.03406
	loss_reward_2: 0.00469
	loss_policy_3: 0.04331
	accuracy_policy_3: 0.5965
	loss_value_3: 0.03538
	loss_reward_3: 0.00497
	loss_policy_4: 0.04561
	accuracy_policy_4: 0.57816
	loss_value_4: 0.03655
	loss_reward_4: 0.00544
	loss_policy_5: 0.04817
	accuracy_policy_5: 0.55574
	loss_value_5: 0.03757
	loss_reward_5: 0.00653
	loss_policy: 0.35909
	loss_value: 0.33311
	loss_reward: 0.02658
Optimization_Done 23200
[2024-05-12 12:18:41] [command] train weight_iter_23200.pkl 115 117
[2024-05-12 12:19:42] nn step 23300, lr: 0.0729.
	loss_policy_0: 0.1741
	accuracy_policy_0: 0.66471
	loss_value_0: 0.15278
	loss_policy_1: 0.04175
	accuracy_policy_1: 0.60891
	loss_value_1: 0.0322
	loss_reward_1: 0.00515
	loss_policy_2: 0.0447
	accuracy_policy_2: 0.57832
	loss_value_2: 0.03368
	loss_reward_2: 0.00492
	loss_policy_3: 0.04723
	accuracy_policy_3: 0.55762
	loss_value_3: 0.03512
	loss_reward_3: 0.00521
	loss_policy_4: 0.04993
	accuracy_policy_4: 0.5384
	loss_value_4: 0.0364
	loss_reward_4: 0.00573
	loss_policy_5: 0.0525
	accuracy_policy_5: 0.5159
	loss_value_5: 0.03768
	loss_reward_5: 0.00682
	loss_policy: 0.41022
	loss_value: 0.32787
	loss_reward: 0.02783
[2024-05-12 12:20:40] nn step 23400, lr: 0.0729.
	loss_policy_0: 0.15002
	accuracy_policy_0: 0.71494
	loss_value_0: 0.15634
	loss_policy_1: 0.0375
	accuracy_policy_1: 0.64883
	loss_value_1: 0.03292
	loss_reward_1: 0.00529
	loss_policy_2: 0.04133
	accuracy_policy_2: 0.61871
	loss_value_2: 0.03418
	loss_reward_2: 0.00492
	loss_policy_3: 0.04471
	accuracy_policy_3: 0.59107
	loss_value_3: 0.03555
	loss_reward_3: 0.00512
	loss_policy_4: 0.04753
	accuracy_policy_4: 0.57027
	loss_value_4: 0.03678
	loss_reward_4: 0.00584
	loss_policy_5: 0.05035
	accuracy_policy_5: 0.54771
	loss_value_5: 0.03799
	loss_reward_5: 0.00715
	loss_policy: 0.37144
	loss_value: 0.33377
	loss_reward: 0.02832
Optimization_Done 23400
[2024-05-12 12:22:41] [command] train weight_iter_23400.pkl 116 118
[2024-05-12 12:23:49] nn step 23500, lr: 0.0729.
	loss_policy_0: 0.19929
	accuracy_policy_0: 0.62291
	loss_value_0: 0.15918
	loss_policy_1: 0.04506
	accuracy_policy_1: 0.58078
	loss_value_1: 0.03337
	loss_reward_1: 0.00498
	loss_policy_2: 0.04833
	accuracy_policy_2: 0.55154
	loss_value_2: 0.03486
	loss_reward_2: 0.00482
	loss_policy_3: 0.05118
	accuracy_policy_3: 0.52652
	loss_value_3: 0.03618
	loss_reward_3: 0.00527
	loss_policy_4: 0.05392
	accuracy_policy_4: 0.50525
	loss_value_4: 0.03736
	loss_reward_4: 0.00573
	loss_policy_5: 0.05615
	accuracy_policy_5: 0.49152
	loss_value_5: 0.03857
	loss_reward_5: 0.00663
	loss_policy: 0.45392
	loss_value: 0.33953
	loss_reward: 0.02742
[2024-05-12 12:24:57] nn step 23600, lr: 0.0729.
	loss_policy_0: 0.16923
	accuracy_policy_0: 0.68076
	loss_value_0: 0.15802
	loss_policy_1: 0.03991
	accuracy_policy_1: 0.62627
	loss_value_1: 0.03319
	loss_reward_1: 0.0051
	loss_policy_2: 0.04366
	accuracy_policy_2: 0.59705
	loss_value_2: 0.03459
	loss_reward_2: 0.00501
	loss_policy_3: 0.04728
	accuracy_policy_3: 0.56672
	loss_value_3: 0.03606
	loss_reward_3: 0.00524
	loss_policy_4: 0.0504
	accuracy_policy_4: 0.54643
	loss_value_4: 0.03746
	loss_reward_4: 0.00564
	loss_policy_5: 0.05326
	accuracy_policy_5: 0.5267
	loss_value_5: 0.03872
	loss_reward_5: 0.00667
	loss_policy: 0.40373
	loss_value: 0.33804
	loss_reward: 0.02766
Optimization_Done 23600
[2024-05-12 12:27:11] [command] train weight_iter_23600.pkl 117 119
[2024-05-12 12:28:15] nn step 23700, lr: 0.0729.
	loss_policy_0: 0.18816
	accuracy_policy_0: 0.64367
	loss_value_0: 0.16258
	loss_policy_1: 0.04243
	accuracy_policy_1: 0.60102
	loss_value_1: 0.03411
	loss_reward_1: 0.00543
	loss_policy_2: 0.04574
	accuracy_policy_2: 0.57459
	loss_value_2: 0.03552
	loss_reward_2: 0.0052
	loss_policy_3: 0.04852
	accuracy_policy_3: 0.5527
	loss_value_3: 0.03692
	loss_reward_3: 0.00578
	loss_policy_4: 0.05082
	accuracy_policy_4: 0.53484
	loss_value_4: 0.03814
	loss_reward_4: 0.00621
	loss_policy_5: 0.05339
	accuracy_policy_5: 0.51809
	loss_value_5: 0.0394
	loss_reward_5: 0.00718
	loss_policy: 0.42906
	loss_value: 0.34666
	loss_reward: 0.02979
[2024-05-12 12:29:18] nn step 23800, lr: 0.0729.
	loss_policy_0: 0.16575
	accuracy_policy_0: 0.69469
	loss_value_0: 0.16347
	loss_policy_1: 0.0396
	accuracy_policy_1: 0.64066
	loss_value_1: 0.03432
	loss_reward_1: 0.00533
	loss_policy_2: 0.04388
	accuracy_policy_2: 0.60166
	loss_value_2: 0.03573
	loss_reward_2: 0.00522
	loss_policy_3: 0.04693
	accuracy_policy_3: 0.57643
	loss_value_3: 0.03725
	loss_reward_3: 0.00563
	loss_policy_4: 0.04953
	accuracy_policy_4: 0.56123
	loss_value_4: 0.0387
	loss_reward_4: 0.00625
	loss_policy_5: 0.05219
	accuracy_policy_5: 0.54059
	loss_value_5: 0.04007
	loss_reward_5: 0.00723
	loss_policy: 0.39788
	loss_value: 0.34954
	loss_reward: 0.02966
Optimization_Done 23800
[2024-05-12 12:31:32] [command] train weight_iter_23800.pkl 118 120
[2024-05-12 12:32:38] nn step 23900, lr: 0.0729.
	loss_policy_0: 0.17011
	accuracy_policy_0: 0.67064
	loss_value_0: 0.17011
	loss_policy_1: 0.0407
	accuracy_policy_1: 0.61098
	loss_value_1: 0.03561
	loss_reward_1: 0.0058
	loss_policy_2: 0.04421
	accuracy_policy_2: 0.57941
	loss_value_2: 0.03709
	loss_reward_2: 0.00549
	loss_policy_3: 0.04756
	accuracy_policy_3: 0.55437
	loss_value_3: 0.03839
	loss_reward_3: 0.00587
	loss_policy_4: 0.05025
	accuracy_policy_4: 0.53455
	loss_value_4: 0.03964
	loss_reward_4: 0.00645
	loss_policy_5: 0.0526
	accuracy_policy_5: 0.51627
	loss_value_5: 0.04092
	loss_reward_5: 0.00729
	loss_policy: 0.40543
	loss_value: 0.36176
	loss_reward: 0.0309
[2024-05-12 12:33:44] nn step 24000, lr: 0.0729.
	loss_policy_0: 0.16229
	accuracy_policy_0: 0.70736
	loss_value_0: 0.18101
	loss_policy_1: 0.04047
	accuracy_policy_1: 0.639
	loss_value_1: 0.03812
	loss_reward_1: 0.00608
	loss_policy_2: 0.04505
	accuracy_policy_2: 0.6025
	loss_value_2: 0.03948
	loss_reward_2: 0.00586
	loss_policy_3: 0.04894
	accuracy_policy_3: 0.57639
	loss_value_3: 0.04099
	loss_reward_3: 0.00634
	loss_policy_4: 0.0512
	accuracy_policy_4: 0.55973
	loss_value_4: 0.04228
	loss_reward_4: 0.00677
	loss_policy_5: 0.05426
	accuracy_policy_5: 0.53775
	loss_value_5: 0.0437
	loss_reward_5: 0.00778
	loss_policy: 0.40222
	loss_value: 0.38559
	loss_reward: 0.03282
Optimization_Done 24000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-12 12:59:37] [command] train weight_iter_24000.pkl 119 121
[2024-05-12 13:00:41] nn step 24100, lr: 0.06561.
	loss_policy_0: 0.15574
	accuracy_policy_0: 0.70865
	loss_value_0: 0.1724
	loss_policy_1: 0.03838
	accuracy_policy_1: 0.64801
	loss_value_1: 0.03627
	loss_reward_1: 0.0055
	loss_policy_2: 0.04258
	accuracy_policy_2: 0.61295
	loss_value_2: 0.03769
	loss_reward_2: 0.00511
	loss_policy_3: 0.04604
	accuracy_policy_3: 0.58379
	loss_value_3: 0.0391
	loss_reward_3: 0.00566
	loss_policy_4: 0.04883
	accuracy_policy_4: 0.56193
	loss_value_4: 0.04043
	loss_reward_4: 0.0062
	loss_policy_5: 0.05149
	accuracy_policy_5: 0.5432
	loss_value_5: 0.04165
	loss_reward_5: 0.00696
	loss_policy: 0.38305
	loss_value: 0.36753
	loss_reward: 0.02943
[2024-05-12 13:01:30] nn step 24200, lr: 0.06561.
	loss_policy_0: 0.12789
	accuracy_policy_0: 0.7493
	loss_value_0: 0.16289
	loss_policy_1: 0.03325
	accuracy_policy_1: 0.68178
	loss_value_1: 0.0343
	loss_reward_1: 0.00535
	loss_policy_2: 0.0377
	accuracy_policy_2: 0.64727
	loss_value_2: 0.0358
	loss_reward_2: 0.00502
	loss_policy_3: 0.04139
	accuracy_policy_3: 0.62053
	loss_value_3: 0.03716
	loss_reward_3: 0.00546
	loss_policy_4: 0.04416
	accuracy_policy_4: 0.59703
	loss_value_4: 0.0384
	loss_reward_4: 0.00601
	loss_policy_5: 0.04688
	accuracy_policy_5: 0.57941
	loss_value_5: 0.03964
	loss_reward_5: 0.00673
	loss_policy: 0.33127
	loss_value: 0.34818
	loss_reward: 0.02857
Optimization_Done 24200
[2024-05-12 13:03:48] [command] train weight_iter_24200.pkl 120 122
[2024-05-12 13:04:47] nn step 24300, lr: 0.06561.
	loss_policy_0: 0.15349
	accuracy_policy_0: 0.71178
	loss_value_0: 0.15777
	loss_policy_1: 0.03749
	accuracy_policy_1: 0.6517
	loss_value_1: 0.03339
	loss_reward_1: 0.00449
	loss_policy_2: 0.04202
	accuracy_policy_2: 0.6165
	loss_value_2: 0.03482
	loss_reward_2: 0.00421
	loss_policy_3: 0.04535
	accuracy_policy_3: 0.59297
	loss_value_3: 0.03611
	loss_reward_3: 0.00467
	loss_policy_4: 0.04824
	accuracy_policy_4: 0.57359
	loss_value_4: 0.0371
	loss_reward_4: 0.00485
	loss_policy_5: 0.05112
	accuracy_policy_5: 0.55432
	loss_value_5: 0.03827
	loss_reward_5: 0.00568
	loss_policy: 0.3777
	loss_value: 0.33747
	loss_reward: 0.02389
[2024-05-12 13:05:45] nn step 24400, lr: 0.06561.
	loss_policy_0: 0.12458
	accuracy_policy_0: 0.75627
	loss_value_0: 0.15564
	loss_policy_1: 0.03348
	accuracy_policy_1: 0.6817
	loss_value_1: 0.03271
	loss_reward_1: 0.00434
	loss_policy_2: 0.03779
	accuracy_policy_2: 0.65035
	loss_value_2: 0.03422
	loss_reward_2: 0.00421
	loss_policy_3: 0.0416
	accuracy_policy_3: 0.62146
	loss_value_3: 0.03552
	loss_reward_3: 0.00446
	loss_policy_4: 0.04468
	accuracy_policy_4: 0.60287
	loss_value_4: 0.03677
	loss_reward_4: 0.0048
	loss_policy_5: 0.04786
	accuracy_policy_5: 0.5793
	loss_value_5: 0.03803
	loss_reward_5: 0.00553
	loss_policy: 0.32999
	loss_value: 0.33289
	loss_reward: 0.02335
Optimization_Done 24400
[2024-05-12 13:08:03] [command] train weight_iter_24400.pkl 121 123
[2024-05-12 13:09:05] nn step 24500, lr: 0.06561.
	loss_policy_0: 0.16842
	accuracy_policy_0: 0.66674
	loss_value_0: 0.14871
	loss_policy_1: 0.03952
	accuracy_policy_1: 0.6117
	loss_value_1: 0.03107
	loss_reward_1: 0.0045
	loss_policy_2: 0.04313
	accuracy_policy_2: 0.58229
	loss_value_2: 0.03247
	loss_reward_2: 0.00431
	loss_policy_3: 0.04641
	accuracy_policy_3: 0.5599
	loss_value_3: 0.03368
	loss_reward_3: 0.00472
	loss_policy_4: 0.04927
	accuracy_policy_4: 0.53701
	loss_value_4: 0.03503
	loss_reward_4: 0.00529
	loss_policy_5: 0.05184
	accuracy_policy_5: 0.51488
	loss_value_5: 0.03622
	loss_reward_5: 0.00596
	loss_policy: 0.3986
	loss_value: 0.31719
	loss_reward: 0.02478
[2024-05-12 13:10:01] nn step 24600, lr: 0.06561.
	loss_policy_0: 0.13294
	accuracy_policy_0: 0.72857
	loss_value_0: 0.14486
	loss_policy_1: 0.03359
	accuracy_policy_1: 0.66695
	loss_value_1: 0.03052
	loss_reward_1: 0.0043
	loss_policy_2: 0.03772
	accuracy_policy_2: 0.63227
	loss_value_2: 0.03183
	loss_reward_2: 0.0041
	loss_policy_3: 0.04116
	accuracy_policy_3: 0.60643
	loss_value_3: 0.03305
	loss_reward_3: 0.00449
	loss_policy_4: 0.0443
	accuracy_policy_4: 0.57658
	loss_value_4: 0.03403
	loss_reward_4: 0.00486
	loss_policy_5: 0.04721
	accuracy_policy_5: 0.55771
	loss_value_5: 0.03518
	loss_reward_5: 0.00587
	loss_policy: 0.33692
	loss_value: 0.30945
	loss_reward: 0.02362
Optimization_Done 24600
[2024-05-12 13:12:18] [command] train weight_iter_24600.pkl 122 124
[2024-05-12 13:13:07] nn step 24700, lr: 0.06561.
	loss_policy_0: 0.16338
	accuracy_policy_0: 0.67715
	loss_value_0: 0.15392
	loss_policy_1: 0.03978
	accuracy_policy_1: 0.6102
	loss_value_1: 0.03239
	loss_reward_1: 0.00508
	loss_policy_2: 0.04386
	accuracy_policy_2: 0.57551
	loss_value_2: 0.03363
	loss_reward_2: 0.00502
	loss_policy_3: 0.0472
	accuracy_policy_3: 0.54926
	loss_value_3: 0.03483
	loss_reward_3: 0.00556
	loss_policy_4: 0.04992
	accuracy_policy_4: 0.52781
	loss_value_4: 0.03595
	loss_reward_4: 0.00612
	loss_policy_5: 0.05286
	accuracy_policy_5: 0.50256
	loss_value_5: 0.03711
	loss_reward_5: 0.00699
	loss_policy: 0.397
	loss_value: 0.32783
	loss_reward: 0.02878
[2024-05-12 13:14:01] nn step 24800, lr: 0.06561.
	loss_policy_0: 0.1422
	accuracy_policy_0: 0.71889
	loss_value_0: 0.15678
	loss_policy_1: 0.03704
	accuracy_policy_1: 0.64238
	loss_value_1: 0.03302
	loss_reward_1: 0.0053
	loss_policy_2: 0.0414
	accuracy_policy_2: 0.6074
	loss_value_2: 0.0342
	loss_reward_2: 0.00499
	loss_policy_3: 0.04503
	accuracy_policy_3: 0.57561
	loss_value_3: 0.03531
	loss_reward_3: 0.00551
	loss_policy_4: 0.04792
	accuracy_policy_4: 0.55381
	loss_value_4: 0.03651
	loss_reward_4: 0.00586
	loss_policy_5: 0.051
	accuracy_policy_5: 0.53215
	loss_value_5: 0.03768
	loss_reward_5: 0.00695
	loss_policy: 0.36459
	loss_value: 0.33351
	loss_reward: 0.02862
Optimization_Done 24800
[2024-05-12 13:16:21] [command] train weight_iter_24800.pkl 123 125
[2024-05-12 13:17:17] nn step 24900, lr: 0.06561.
	loss_policy_0: 0.15846
	accuracy_policy_0: 0.70051
	loss_value_0: 0.15379
	loss_policy_1: 0.03869
	accuracy_policy_1: 0.63971
	loss_value_1: 0.03237
	loss_reward_1: 0.00544
	loss_policy_2: 0.04265
	accuracy_policy_2: 0.60496
	loss_value_2: 0.03357
	loss_reward_2: 0.00527
	loss_policy_3: 0.04619
	accuracy_policy_3: 0.57268
	loss_value_3: 0.03485
	loss_reward_3: 0.00579
	loss_policy_4: 0.04891
	accuracy_policy_4: 0.5475
	loss_value_4: 0.03598
	loss_reward_4: 0.00605
	loss_policy_5: 0.05141
	accuracy_policy_5: 0.52834
	loss_value_5: 0.03718
	loss_reward_5: 0.00714
	loss_policy: 0.38632
	loss_value: 0.32773
	loss_reward: 0.02968
[2024-05-12 13:18:10] nn step 25000, lr: 0.06561.
	loss_policy_0: 0.13737
	accuracy_policy_0: 0.73379
	loss_value_0: 0.15505
	loss_policy_1: 0.03644
	accuracy_policy_1: 0.65582
	loss_value_1: 0.03266
	loss_reward_1: 0.00544
	loss_policy_2: 0.04042
	accuracy_policy_2: 0.62221
	loss_value_2: 0.034
	loss_reward_2: 0.0052
	loss_policy_3: 0.0437
	accuracy_policy_3: 0.59311
	loss_value_3: 0.0351
	loss_reward_3: 0.00579
	loss_policy_4: 0.04673
	accuracy_policy_4: 0.5668
	loss_value_4: 0.03629
	loss_reward_4: 0.00605
	loss_policy_5: 0.04945
	accuracy_policy_5: 0.549
	loss_value_5: 0.03752
	loss_reward_5: 0.00744
	loss_policy: 0.35412
	loss_value: 0.33061
	loss_reward: 0.02992
Optimization_Done 25000
[2024-05-12 13:20:39] [command] train weight_iter_25000.pkl 124 126
[2024-05-12 13:21:35] nn step 25100, lr: 0.06561.
	loss_policy_0: 0.15819
	accuracy_policy_0: 0.69494
	loss_value_0: 0.15379
	loss_policy_1: 0.03746
	accuracy_policy_1: 0.63355
	loss_value_1: 0.0323
	loss_reward_1: 0.00554
	loss_policy_2: 0.04119
	accuracy_policy_2: 0.60268
	loss_value_2: 0.03354
	loss_reward_2: 0.0054
	loss_policy_3: 0.04451
	accuracy_policy_3: 0.57648
	loss_value_3: 0.03468
	loss_reward_3: 0.00568
	loss_policy_4: 0.04696
	accuracy_policy_4: 0.55758
	loss_value_4: 0.03581
	loss_reward_4: 0.00613
	loss_policy_5: 0.04927
	accuracy_policy_5: 0.53979
	loss_value_5: 0.03691
	loss_reward_5: 0.00712
	loss_policy: 0.37758
	loss_value: 0.32704
	loss_reward: 0.02986
[2024-05-12 13:22:32] nn step 25200, lr: 0.06561.
	loss_policy_0: 0.1297
	accuracy_policy_0: 0.7333
	loss_value_0: 0.14878
	loss_policy_1: 0.03293
	accuracy_policy_1: 0.66764
	loss_value_1: 0.03132
	loss_reward_1: 0.00535
	loss_policy_2: 0.03697
	accuracy_policy_2: 0.63475
	loss_value_2: 0.03255
	loss_reward_2: 0.00509
	loss_policy_3: 0.04012
	accuracy_policy_3: 0.60639
	loss_value_3: 0.03367
	loss_reward_3: 0.00541
	loss_policy_4: 0.04259
	accuracy_policy_4: 0.58477
	loss_value_4: 0.03485
	loss_reward_4: 0.00582
	loss_policy_5: 0.04554
	accuracy_policy_5: 0.56746
	loss_value_5: 0.03599
	loss_reward_5: 0.0068
	loss_policy: 0.32786
	loss_value: 0.31716
	loss_reward: 0.02846
Optimization_Done 25200
[2024-05-12 13:24:38] [command] train weight_iter_25200.pkl 125 127
[2024-05-12 13:25:33] nn step 25300, lr: 0.06561.
	loss_policy_0: 0.16682
	accuracy_policy_0: 0.67016
	loss_value_0: 0.15318
	loss_policy_1: 0.03865
	accuracy_policy_1: 0.6216
	loss_value_1: 0.0322
	loss_reward_1: 0.00523
	loss_policy_2: 0.04183
	accuracy_policy_2: 0.59391
	loss_value_2: 0.03355
	loss_reward_2: 0.00494
	loss_policy_3: 0.04526
	accuracy_policy_3: 0.56559
	loss_value_3: 0.03471
	loss_reward_3: 0.00535
	loss_policy_4: 0.04805
	accuracy_policy_4: 0.54566
	loss_value_4: 0.03605
	loss_reward_4: 0.0058
	loss_policy_5: 0.05077
	accuracy_policy_5: 0.52309
	loss_value_5: 0.03727
	loss_reward_5: 0.00689
	loss_policy: 0.39138
	loss_value: 0.32695
	loss_reward: 0.02822
[2024-05-12 13:26:31] nn step 25400, lr: 0.06561.
	loss_policy_0: 0.14659
	accuracy_policy_0: 0.71719
	loss_value_0: 0.15644
	loss_policy_1: 0.03562
	accuracy_policy_1: 0.66014
	loss_value_1: 0.0327
	loss_reward_1: 0.00525
	loss_policy_2: 0.03961
	accuracy_policy_2: 0.62365
	loss_value_2: 0.03393
	loss_reward_2: 0.00516
	loss_policy_3: 0.04308
	accuracy_policy_3: 0.59711
	loss_value_3: 0.03514
	loss_reward_3: 0.00544
	loss_policy_4: 0.04603
	accuracy_policy_4: 0.57234
	loss_value_4: 0.03649
	loss_reward_4: 0.00596
	loss_policy_5: 0.04902
	accuracy_policy_5: 0.55197
	loss_value_5: 0.03782
	loss_reward_5: 0.00694
	loss_policy: 0.35996
	loss_value: 0.33252
	loss_reward: 0.02875
Optimization_Done 25400
[2024-05-12 13:28:47] [command] train weight_iter_25400.pkl 126 128
[2024-05-12 13:29:45] nn step 25500, lr: 0.06561.
	loss_policy_0: 0.17086
	accuracy_policy_0: 0.6723
	loss_value_0: 0.16932
	loss_policy_1: 0.04077
	accuracy_policy_1: 0.6108
	loss_value_1: 0.03546
	loss_reward_1: 0.00602
	loss_policy_2: 0.04479
	accuracy_policy_2: 0.5765
	loss_value_2: 0.03683
	loss_reward_2: 0.00578
	loss_policy_3: 0.04774
	accuracy_policy_3: 0.5534
	loss_value_3: 0.03818
	loss_reward_3: 0.00619
	loss_policy_4: 0.05051
	accuracy_policy_4: 0.53119
	loss_value_4: 0.03935
	loss_reward_4: 0.00667
	loss_policy_5: 0.05314
	accuracy_policy_5: 0.51344
	loss_value_5: 0.04069
	loss_reward_5: 0.00782
	loss_policy: 0.40781
	loss_value: 0.35983
	loss_reward: 0.03248
[2024-05-12 13:30:43] nn step 25600, lr: 0.06561.
	loss_policy_0: 0.14423
	accuracy_policy_0: 0.71363
	loss_value_0: 0.16091
	loss_policy_1: 0.0362
	accuracy_policy_1: 0.64271
	loss_value_1: 0.0337
	loss_reward_1: 0.00558
	loss_policy_2: 0.04039
	accuracy_policy_2: 0.60318
	loss_value_2: 0.03506
	loss_reward_2: 0.00551
	loss_policy_3: 0.04355
	accuracy_policy_3: 0.57918
	loss_value_3: 0.03628
	loss_reward_3: 0.00576
	loss_policy_4: 0.04625
	accuracy_policy_4: 0.55762
	loss_value_4: 0.03746
	loss_reward_4: 0.00636
	loss_policy_5: 0.04918
	accuracy_policy_5: 0.5341
	loss_value_5: 0.03871
	loss_reward_5: 0.00733
	loss_policy: 0.35979
	loss_value: 0.34213
	loss_reward: 0.03054
Optimization_Done 25600
[2024-05-12 13:32:48] [command] train weight_iter_25600.pkl 127 129
[2024-05-12 13:33:47] nn step 25700, lr: 0.06561.
	loss_policy_0: 0.15626
	accuracy_policy_0: 0.6866
	loss_value_0: 0.15612
	loss_policy_1: 0.03767
	accuracy_policy_1: 0.62789
	loss_value_1: 0.03281
	loss_reward_1: 0.00521
	loss_policy_2: 0.04089
	accuracy_policy_2: 0.59705
	loss_value_2: 0.03402
	loss_reward_2: 0.0051
	loss_policy_3: 0.04377
	accuracy_policy_3: 0.57438
	loss_value_3: 0.03514
	loss_reward_3: 0.00566
	loss_policy_4: 0.04604
	accuracy_policy_4: 0.55148
	loss_value_4: 0.03622
	loss_reward_4: 0.00597
	loss_policy_5: 0.04854
	accuracy_policy_5: 0.52963
	loss_value_5: 0.03737
	loss_reward_5: 0.00694
	loss_policy: 0.37317
	loss_value: 0.33167
	loss_reward: 0.02887
[2024-05-12 13:34:44] nn step 25800, lr: 0.06561.
	loss_policy_0: 0.13902
	accuracy_policy_0: 0.72727
	loss_value_0: 0.15657
	loss_policy_1: 0.03526
	accuracy_policy_1: 0.65643
	loss_value_1: 0.03306
	loss_reward_1: 0.00536
	loss_policy_2: 0.03891
	accuracy_policy_2: 0.61902
	loss_value_2: 0.03429
	loss_reward_2: 0.00513
	loss_policy_3: 0.042
	accuracy_policy_3: 0.59627
	loss_value_3: 0.03551
	loss_reward_3: 0.00546
	loss_policy_4: 0.04445
	accuracy_policy_4: 0.57355
	loss_value_4: 0.03653
	loss_reward_4: 0.00613
	loss_policy_5: 0.04741
	accuracy_policy_5: 0.55299
	loss_value_5: 0.03761
	loss_reward_5: 0.00709
	loss_policy: 0.34705
	loss_value: 0.33357
	loss_reward: 0.02917
Optimization_Done 25800
[2024-05-12 13:36:59] [command] train weight_iter_25800.pkl 128 130
[2024-05-12 13:37:59] nn step 25900, lr: 0.06561.
	loss_policy_0: 0.16167
	accuracy_policy_0: 0.67838
	loss_value_0: 0.15684
	loss_policy_1: 0.03825
	accuracy_policy_1: 0.62568
	loss_value_1: 0.03291
	loss_reward_1: 0.00523
	loss_policy_2: 0.04154
	accuracy_policy_2: 0.59971
	loss_value_2: 0.03411
	loss_reward_2: 0.0049
	loss_policy_3: 0.04454
	accuracy_policy_3: 0.57322
	loss_value_3: 0.0352
	loss_reward_3: 0.00533
	loss_policy_4: 0.04703
	accuracy_policy_4: 0.54936
	loss_value_4: 0.03634
	loss_reward_4: 0.00578
	loss_policy_5: 0.04918
	accuracy_policy_5: 0.53566
	loss_value_5: 0.03743
	loss_reward_5: 0.0066
	loss_policy: 0.38222
	loss_value: 0.33283
	loss_reward: 0.02784
[2024-05-12 13:38:56] nn step 26000, lr: 0.06561.
	loss_policy_0: 0.14012
	accuracy_policy_0: 0.73162
	loss_value_0: 0.16248
	loss_policy_1: 0.035
	accuracy_policy_1: 0.66336
	loss_value_1: 0.03402
	loss_reward_1: 0.00532
	loss_policy_2: 0.03948
	accuracy_policy_2: 0.63064
	loss_value_2: 0.03527
	loss_reward_2: 0.00502
	loss_policy_3: 0.0431
	accuracy_policy_3: 0.60236
	loss_value_3: 0.03662
	loss_reward_3: 0.00552
	loss_policy_4: 0.0456
	accuracy_policy_4: 0.57939
	loss_value_4: 0.03789
	loss_reward_4: 0.00603
	loss_policy_5: 0.04823
	accuracy_policy_5: 0.5616
	loss_value_5: 0.03898
	loss_reward_5: 0.00689
	loss_policy: 0.35153
	loss_value: 0.34526
	loss_reward: 0.02878
Optimization_Done 26000
[2024-05-12 13:40:52] [command] train weight_iter_26000.pkl 129 131
[2024-05-12 13:41:50] nn step 26100, lr: 0.06561.
	loss_policy_0: 0.17419
	accuracy_policy_0: 0.66262
	loss_value_0: 0.15449
	loss_policy_1: 0.03978
	accuracy_policy_1: 0.61734
	loss_value_1: 0.03234
	loss_reward_1: 0.00456
	loss_policy_2: 0.04319
	accuracy_policy_2: 0.58611
	loss_value_2: 0.03361
	loss_reward_2: 0.00438
	loss_policy_3: 0.04622
	accuracy_policy_3: 0.56311
	loss_value_3: 0.03477
	loss_reward_3: 0.00455
	loss_policy_4: 0.04861
	accuracy_policy_4: 0.54562
	loss_value_4: 0.03595
	loss_reward_4: 0.00506
	loss_policy_5: 0.05139
	accuracy_policy_5: 0.52846
	loss_value_5: 0.03707
	loss_reward_5: 0.00568
	loss_policy: 0.40338
	loss_value: 0.32823
	loss_reward: 0.02422
[2024-05-12 13:42:46] nn step 26200, lr: 0.06561.
	loss_policy_0: 0.14916
	accuracy_policy_0: 0.71131
	loss_value_0: 0.15725
	loss_policy_1: 0.03568
	accuracy_policy_1: 0.65672
	loss_value_1: 0.03269
	loss_reward_1: 0.00427
	loss_policy_2: 0.03966
	accuracy_policy_2: 0.62576
	loss_value_2: 0.03391
	loss_reward_2: 0.00436
	loss_policy_3: 0.04304
	accuracy_policy_3: 0.59645
	loss_value_3: 0.03507
	loss_reward_3: 0.00458
	loss_policy_4: 0.04598
	accuracy_policy_4: 0.57385
	loss_value_4: 0.03624
	loss_reward_4: 0.00502
	loss_policy_5: 0.04879
	accuracy_policy_5: 0.5533
	loss_value_5: 0.0374
	loss_reward_5: 0.00577
	loss_policy: 0.36231
	loss_value: 0.33256
	loss_reward: 0.024
Optimization_Done 26200
[2024-05-12 13:45:03] [command] train weight_iter_26200.pkl 130 132
[2024-05-12 13:46:02] nn step 26300, lr: 0.06561.
	loss_policy_0: 0.18999
	accuracy_policy_0: 0.62389
	loss_value_0: 0.16398
	loss_policy_1: 0.04332
	accuracy_policy_1: 0.58393
	loss_value_1: 0.03402
	loss_reward_1: 0.00534
	loss_policy_2: 0.0471
	accuracy_policy_2: 0.55025
	loss_value_2: 0.03544
	loss_reward_2: 0.0049
	loss_policy_3: 0.04995
	accuracy_policy_3: 0.52121
	loss_value_3: 0.03664
	loss_reward_3: 0.00535
	loss_policy_4: 0.05254
	accuracy_policy_4: 0.50352
	loss_value_4: 0.0379
	loss_reward_4: 0.00592
	loss_policy_5: 0.05518
	accuracy_policy_5: 0.48533
	loss_value_5: 0.03919
	loss_reward_5: 0.00657
	loss_policy: 0.43808
	loss_value: 0.34716
	loss_reward: 0.02808
[2024-05-12 13:47:01] nn step 26400, lr: 0.06561.
	loss_policy_0: 0.17475
	accuracy_policy_0: 0.67988
	loss_value_0: 0.17419
	loss_policy_1: 0.04223
	accuracy_policy_1: 0.61916
	loss_value_1: 0.03635
	loss_reward_1: 0.00558
	loss_policy_2: 0.04666
	accuracy_policy_2: 0.58447
	loss_value_2: 0.0376
	loss_reward_2: 0.00525
	loss_policy_3: 0.0502
	accuracy_policy_3: 0.55836
	loss_value_3: 0.03883
	loss_reward_3: 0.00576
	loss_policy_4: 0.05286
	accuracy_policy_4: 0.53893
	loss_value_4: 0.04005
	loss_reward_4: 0.00615
	loss_policy_5: 0.05635
	accuracy_policy_5: 0.51432
	loss_value_5: 0.0413
	loss_reward_5: 0.00705
	loss_policy: 0.42305
	loss_value: 0.36832
	loss_reward: 0.02979
Optimization_Done 26400
[2024-05-12 13:49:08] [command] train weight_iter_26400.pkl 131 133
[2024-05-12 13:49:57] nn step 26500, lr: 0.06561.
	loss_policy_0: 0.20185
	accuracy_policy_0: 0.62518
	loss_value_0: 0.17552
	loss_policy_1: 0.04699
	accuracy_policy_1: 0.57217
	loss_value_1: 0.03679
	loss_reward_1: 0.00588
	loss_policy_2: 0.0512
	accuracy_policy_2: 0.53342
	loss_value_2: 0.03793
	loss_reward_2: 0.00542
	loss_policy_3: 0.05384
	accuracy_policy_3: 0.50803
	loss_value_3: 0.03921
	loss_reward_3: 0.00587
	loss_policy_4: 0.05687
	accuracy_policy_4: 0.48617
	loss_value_4: 0.04035
	loss_reward_4: 0.00641
	loss_policy_5: 0.05963
	accuracy_policy_5: 0.46809
	loss_value_5: 0.04157
	loss_reward_5: 0.00737
	loss_policy: 0.47038
	loss_value: 0.37137
	loss_reward: 0.03095
[2024-05-12 13:50:50] nn step 26600, lr: 0.06561.
	loss_policy_0: 0.16998
	accuracy_policy_0: 0.67082
	loss_value_0: 0.16668
	loss_policy_1: 0.04176
	accuracy_policy_1: 0.60025
	loss_value_1: 0.03476
	loss_reward_1: 0.00537
	loss_policy_2: 0.04559
	accuracy_policy_2: 0.57217
	loss_value_2: 0.03596
	loss_reward_2: 0.00515
	loss_policy_3: 0.04875
	accuracy_policy_3: 0.54385
	loss_value_3: 0.03715
	loss_reward_3: 0.00554
	loss_policy_4: 0.05214
	accuracy_policy_4: 0.51582
	loss_value_4: 0.03833
	loss_reward_4: 0.00607
	loss_policy_5: 0.05465
	accuracy_policy_5: 0.4909
	loss_value_5: 0.03942
	loss_reward_5: 0.00703
	loss_policy: 0.41286
	loss_value: 0.35229
	loss_reward: 0.02915
Optimization_Done 26600
[2024-05-12 13:53:07] [command] train weight_iter_26600.pkl 132 134
[2024-05-12 13:54:01] nn step 26700, lr: 0.06561.
	loss_policy_0: 0.19011
	accuracy_policy_0: 0.65779
	loss_value_0: 0.16828
	loss_policy_1: 0.04509
	accuracy_policy_1: 0.59928
	loss_value_1: 0.03531
	loss_reward_1: 0.00583
	loss_policy_2: 0.04905
	accuracy_policy_2: 0.56531
	loss_value_2: 0.03679
	loss_reward_2: 0.00553
	loss_policy_3: 0.0524
	accuracy_policy_3: 0.54148
	loss_value_3: 0.03799
	loss_reward_3: 0.00612
	loss_policy_4: 0.05543
	accuracy_policy_4: 0.51617
	loss_value_4: 0.03927
	loss_reward_4: 0.00653
	loss_policy_5: 0.05815
	accuracy_policy_5: 0.49627
	loss_value_5: 0.04053
	loss_reward_5: 0.00777
	loss_policy: 0.45023
	loss_value: 0.35817
	loss_reward: 0.03178
[2024-05-12 13:54:59] nn step 26800, lr: 0.06561.
	loss_policy_0: 0.15914
	accuracy_policy_0: 0.6943
	loss_value_0: 0.16132
	loss_policy_1: 0.03989
	accuracy_policy_1: 0.62717
	loss_value_1: 0.03379
	loss_reward_1: 0.00563
	loss_policy_2: 0.04379
	accuracy_policy_2: 0.5951
	loss_value_2: 0.03514
	loss_reward_2: 0.00535
	loss_policy_3: 0.0475
	accuracy_policy_3: 0.56297
	loss_value_3: 0.03636
	loss_reward_3: 0.00579
	loss_policy_4: 0.05043
	accuracy_policy_4: 0.54121
	loss_value_4: 0.03756
	loss_reward_4: 0.00622
	loss_policy_5: 0.05346
	accuracy_policy_5: 0.51906
	loss_value_5: 0.03886
	loss_reward_5: 0.00749
	loss_policy: 0.39421
	loss_value: 0.34303
	loss_reward: 0.03049
Optimization_Done 26800
[2024-05-12 13:57:19] [command] train weight_iter_26800.pkl 133 135
[2024-05-12 13:58:19] nn step 26900, lr: 0.06561.
	loss_policy_0: 0.18543
	accuracy_policy_0: 0.64463
	loss_value_0: 0.16136
	loss_policy_1: 0.04227
	accuracy_policy_1: 0.60381
	loss_value_1: 0.0338
	loss_reward_1: 0.00504
	loss_policy_2: 0.04562
	accuracy_policy_2: 0.57393
	loss_value_2: 0.03523
	loss_reward_2: 0.00499
	loss_policy_3: 0.04917
	accuracy_policy_3: 0.54551
	loss_value_3: 0.03647
	loss_reward_3: 0.00531
	loss_policy_4: 0.05167
	accuracy_policy_4: 0.53033
	loss_value_4: 0.03785
	loss_reward_4: 0.0058
	loss_policy_5: 0.05374
	accuracy_policy_5: 0.51334
	loss_value_5: 0.039
	loss_reward_5: 0.0068
	loss_policy: 0.42789
	loss_value: 0.34371
	loss_reward: 0.02794
[2024-05-12 13:59:17] nn step 27000, lr: 0.06561.
	loss_policy_0: 0.16103
	accuracy_policy_0: 0.69646
	loss_value_0: 0.16021
	loss_policy_1: 0.03853
	accuracy_policy_1: 0.63539
	loss_value_1: 0.03364
	loss_reward_1: 0.00513
	loss_policy_2: 0.04255
	accuracy_policy_2: 0.6084
	loss_value_2: 0.03511
	loss_reward_2: 0.00501
	loss_policy_3: 0.04579
	accuracy_policy_3: 0.58074
	loss_value_3: 0.03642
	loss_reward_3: 0.00547
	loss_policy_4: 0.04931
	accuracy_policy_4: 0.55738
	loss_value_4: 0.03778
	loss_reward_4: 0.00579
	loss_policy_5: 0.0523
	accuracy_policy_5: 0.52975
	loss_value_5: 0.03888
	loss_reward_5: 0.00686
	loss_policy: 0.38952
	loss_value: 0.34204
	loss_reward: 0.02825
Optimization_Done 27000
[2024-05-12 14:01:33] [command] train weight_iter_27000.pkl 134 136
[2024-05-12 14:02:33] nn step 27100, lr: 0.06561.
	loss_policy_0: 0.18195
	accuracy_policy_0: 0.65984
	loss_value_0: 0.16076
	loss_policy_1: 0.04163
	accuracy_policy_1: 0.6133
	loss_value_1: 0.03375
	loss_reward_1: 0.00551
	loss_policy_2: 0.04525
	accuracy_policy_2: 0.58311
	loss_value_2: 0.03522
	loss_reward_2: 0.00513
	loss_policy_3: 0.0488
	accuracy_policy_3: 0.5548
	loss_value_3: 0.03668
	loss_reward_3: 0.00569
	loss_policy_4: 0.0516
	accuracy_policy_4: 0.53309
	loss_value_4: 0.03793
	loss_reward_4: 0.00624
	loss_policy_5: 0.0542
	accuracy_policy_5: 0.51322
	loss_value_5: 0.03925
	loss_reward_5: 0.00737
	loss_policy: 0.42343
	loss_value: 0.34359
	loss_reward: 0.02994
[2024-05-12 14:03:31] nn step 27200, lr: 0.06561.
	loss_policy_0: 0.15662
	accuracy_policy_0: 0.70422
	loss_value_0: 0.16197
	loss_policy_1: 0.03847
	accuracy_policy_1: 0.64494
	loss_value_1: 0.03396
	loss_reward_1: 0.0054
	loss_policy_2: 0.0426
	accuracy_policy_2: 0.61066
	loss_value_2: 0.03531
	loss_reward_2: 0.00519
	loss_policy_3: 0.04615
	accuracy_policy_3: 0.58398
	loss_value_3: 0.03662
	loss_reward_3: 0.00568
	loss_policy_4: 0.04939
	accuracy_policy_4: 0.5642
	loss_value_4: 0.03807
	loss_reward_4: 0.0062
	loss_policy_5: 0.05278
	accuracy_policy_5: 0.53732
	loss_value_5: 0.03939
	loss_reward_5: 0.00741
	loss_policy: 0.38602
	loss_value: 0.34531
	loss_reward: 0.02987
Optimization_Done 27200
[2024-05-12 14:05:35] [command] train weight_iter_27200.pkl 135 137
[2024-05-12 14:06:33] nn step 27300, lr: 0.06561.
	loss_policy_0: 0.16567
	accuracy_policy_0: 0.67039
	loss_value_0: 0.16128
	loss_policy_1: 0.03943
	accuracy_policy_1: 0.61686
	loss_value_1: 0.03377
	loss_reward_1: 0.00534
	loss_policy_2: 0.04336
	accuracy_policy_2: 0.58271
	loss_value_2: 0.0351
	loss_reward_2: 0.00505
	loss_policy_3: 0.04605
	accuracy_policy_3: 0.55514
	loss_value_3: 0.03637
	loss_reward_3: 0.00553
	loss_policy_4: 0.04899
	accuracy_policy_4: 0.5351
	loss_value_4: 0.03751
	loss_reward_4: 0.00604
	loss_policy_5: 0.05162
	accuracy_policy_5: 0.51186
	loss_value_5: 0.03866
	loss_reward_5: 0.00697
	loss_policy: 0.39513
	loss_value: 0.34269
	loss_reward: 0.02893
[2024-05-12 14:07:31] nn step 27400, lr: 0.06561.
	loss_policy_0: 0.14017
	accuracy_policy_0: 0.70639
	loss_value_0: 0.15653
	loss_policy_1: 0.03506
	accuracy_policy_1: 0.64408
	loss_value_1: 0.03277
	loss_reward_1: 0.00523
	loss_policy_2: 0.03927
	accuracy_policy_2: 0.60965
	loss_value_2: 0.03399
	loss_reward_2: 0.00492
	loss_policy_3: 0.04233
	accuracy_policy_3: 0.57887
	loss_value_3: 0.03517
	loss_reward_3: 0.00539
	loss_policy_4: 0.0452
	accuracy_policy_4: 0.55846
	loss_value_4: 0.03619
	loss_reward_4: 0.0058
	loss_policy_5: 0.04792
	accuracy_policy_5: 0.53363
	loss_value_5: 0.03727
	loss_reward_5: 0.00667
	loss_policy: 0.34996
	loss_value: 0.33192
	loss_reward: 0.028
Optimization_Done 27400
[2024-05-12 14:09:39] [command] train weight_iter_27400.pkl 136 138
[2024-05-12 14:10:37] nn step 27500, lr: 0.06561.
	loss_policy_0: 0.1701
	accuracy_policy_0: 0.64424
	loss_value_0: 0.1565
	loss_policy_1: 0.04055
	accuracy_policy_1: 0.58516
	loss_value_1: 0.0327
	loss_reward_1: 0.00484
	loss_policy_2: 0.04431
	accuracy_policy_2: 0.54949
	loss_value_2: 0.03371
	loss_reward_2: 0.00459
	loss_policy_3: 0.04688
	accuracy_policy_3: 0.52359
	loss_value_3: 0.03485
	loss_reward_3: 0.00488
	loss_policy_4: 0.04964
	accuracy_policy_4: 0.50012
	loss_value_4: 0.03584
	loss_reward_4: 0.00537
	loss_policy_5: 0.05191
	accuracy_policy_5: 0.48137
	loss_value_5: 0.0368
	loss_reward_5: 0.00635
	loss_policy: 0.40339
	loss_value: 0.33039
	loss_reward: 0.02604
[2024-05-12 14:11:35] nn step 27600, lr: 0.06561.
	loss_policy_0: 0.15473
	accuracy_policy_0: 0.69859
	loss_value_0: 0.16581
	loss_policy_1: 0.03905
	accuracy_policy_1: 0.62824
	loss_value_1: 0.03483
	loss_reward_1: 0.00521
	loss_policy_2: 0.04349
	accuracy_policy_2: 0.59443
	loss_value_2: 0.03631
	loss_reward_2: 0.00501
	loss_policy_3: 0.04706
	accuracy_policy_3: 0.56182
	loss_value_3: 0.03743
	loss_reward_3: 0.00531
	loss_policy_4: 0.04997
	accuracy_policy_4: 0.53877
	loss_value_4: 0.03853
	loss_reward_4: 0.0058
	loss_policy_5: 0.05334
	accuracy_policy_5: 0.51328
	loss_value_5: 0.03971
	loss_reward_5: 0.00708
	loss_policy: 0.38764
	loss_value: 0.35262
	loss_reward: 0.02841
Optimization_Done 27600
[2024-05-12 14:13:53] [command] train weight_iter_27600.pkl 137 139
[2024-05-12 14:14:51] nn step 27700, lr: 0.06561.
	loss_policy_0: 0.17508
	accuracy_policy_0: 0.64645
	loss_value_0: 0.15534
	loss_policy_1: 0.04106
	accuracy_policy_1: 0.59379
	loss_value_1: 0.03256
	loss_reward_1: 0.00467
	loss_policy_2: 0.0446
	accuracy_policy_2: 0.56437
	loss_value_2: 0.03395
	loss_reward_2: 0.00452
	loss_policy_3: 0.04807
	accuracy_policy_3: 0.53252
	loss_value_3: 0.03519
	loss_reward_3: 0.00478
	loss_policy_4: 0.05114
	accuracy_policy_4: 0.50678
	loss_value_4: 0.03636
	loss_reward_4: 0.00522
	loss_policy_5: 0.05341
	accuracy_policy_5: 0.48951
	loss_value_5: 0.0375
	loss_reward_5: 0.00612
	loss_policy: 0.41337
	loss_value: 0.33089
	loss_reward: 0.02531
[2024-05-12 14:15:47] nn step 27800, lr: 0.06561.
	loss_policy_0: 0.15345
	accuracy_policy_0: 0.69713
	loss_value_0: 0.15886
	loss_policy_1: 0.03783
	accuracy_policy_1: 0.63094
	loss_value_1: 0.0333
	loss_reward_1: 0.00465
	loss_policy_2: 0.04221
	accuracy_policy_2: 0.59307
	loss_value_2: 0.03463
	loss_reward_2: 0.00456
	loss_policy_3: 0.04577
	accuracy_policy_3: 0.56148
	loss_value_3: 0.03589
	loss_reward_3: 0.0049
	loss_policy_4: 0.04859
	accuracy_policy_4: 0.54268
	loss_value_4: 0.03703
	loss_reward_4: 0.00523
	loss_policy_5: 0.05126
	accuracy_policy_5: 0.52076
	loss_value_5: 0.03815
	loss_reward_5: 0.0063
	loss_policy: 0.37912
	loss_value: 0.33788
	loss_reward: 0.02564
Optimization_Done 27800
[2024-05-12 14:18:06] [command] train weight_iter_27800.pkl 138 140
[2024-05-12 14:19:05] nn step 27900, lr: 0.06561.
	loss_policy_0: 0.18182
	accuracy_policy_0: 0.6602
	loss_value_0: 0.16496
	loss_policy_1: 0.04145
	accuracy_policy_1: 0.61609
	loss_value_1: 0.03469
	loss_reward_1: 0.00479
	loss_policy_2: 0.04518
	accuracy_policy_2: 0.58928
	loss_value_2: 0.036
	loss_reward_2: 0.00467
	loss_policy_3: 0.04831
	accuracy_policy_3: 0.56705
	loss_value_3: 0.03744
	loss_reward_3: 0.00501
	loss_policy_4: 0.05106
	accuracy_policy_4: 0.54514
	loss_value_4: 0.03873
	loss_reward_4: 0.00552
	loss_policy_5: 0.05376
	accuracy_policy_5: 0.52387
	loss_value_5: 0.03998
	loss_reward_5: 0.00641
	loss_policy: 0.42158
	loss_value: 0.3518
	loss_reward: 0.0264
[2024-05-12 14:20:02] nn step 28000, lr: 0.06561.
	loss_policy_0: 0.15494
	accuracy_policy_0: 0.71242
	loss_value_0: 0.16435
	loss_policy_1: 0.03738
	accuracy_policy_1: 0.65604
	loss_value_1: 0.03447
	loss_reward_1: 0.00464
	loss_policy_2: 0.04163
	accuracy_policy_2: 0.62254
	loss_value_2: 0.03594
	loss_reward_2: 0.00463
	loss_policy_3: 0.04505
	accuracy_policy_3: 0.59316
	loss_value_3: 0.03742
	loss_reward_3: 0.00503
	loss_policy_4: 0.04807
	accuracy_policy_4: 0.57355
	loss_value_4: 0.03875
	loss_reward_4: 0.00549
	loss_policy_5: 0.05123
	accuracy_policy_5: 0.54797
	loss_value_5: 0.04001
	loss_reward_5: 0.00653
	loss_policy: 0.3783
	loss_value: 0.35093
	loss_reward: 0.02633
Optimization_Done 28000
[2024-05-12 14:22:19] [command] train weight_iter_28000.pkl 139 141
[2024-05-12 14:23:17] nn step 28100, lr: 0.06561.
	loss_policy_0: 0.15963
	accuracy_policy_0: 0.69748
	loss_value_0: 0.17266
	loss_policy_1: 0.03776
	accuracy_policy_1: 0.64914
	loss_value_1: 0.03638
	loss_reward_1: 0.00562
	loss_policy_2: 0.04157
	accuracy_policy_2: 0.6159
	loss_value_2: 0.03772
	loss_reward_2: 0.00542
	loss_policy_3: 0.0448
	accuracy_policy_3: 0.58973
	loss_value_3: 0.03902
	loss_reward_3: 0.00584
	loss_policy_4: 0.0472
	accuracy_policy_4: 0.57559
	loss_value_4: 0.04041
	loss_reward_4: 0.00642
	loss_policy_5: 0.0501
	accuracy_policy_5: 0.55764
	loss_value_5: 0.04172
	loss_reward_5: 0.00742
	loss_policy: 0.38107
	loss_value: 0.36791
	loss_reward: 0.03072
[2024-05-12 14:24:14] nn step 28200, lr: 0.06561.
	loss_policy_0: 0.13434
	accuracy_policy_0: 0.73771
	loss_value_0: 0.16685
	loss_policy_1: 0.03333
	accuracy_policy_1: 0.67783
	loss_value_1: 0.03516
	loss_reward_1: 0.00532
	loss_policy_2: 0.0376
	accuracy_policy_2: 0.64207
	loss_value_2: 0.03665
	loss_reward_2: 0.00509
	loss_policy_3: 0.04091
	accuracy_policy_3: 0.61799
	loss_value_3: 0.03784
	loss_reward_3: 0.00558
	loss_policy_4: 0.04356
	accuracy_policy_4: 0.59695
	loss_value_4: 0.03916
	loss_reward_4: 0.00607
	loss_policy_5: 0.04648
	accuracy_policy_5: 0.57764
	loss_value_5: 0.04046
	loss_reward_5: 0.0071
	loss_policy: 0.33623
	loss_value: 0.35612
	loss_reward: 0.02916
Optimization_Done 28200
[2024-05-12 14:26:01] [command] train weight_iter_28200.pkl 140 142
[2024-05-12 14:26:58] nn step 28300, lr: 0.06561.
	loss_policy_0: 0.15257
	accuracy_policy_0: 0.69357
	loss_value_0: 0.17122
	loss_policy_1: 0.03657
	accuracy_policy_1: 0.63828
	loss_value_1: 0.03597
	loss_reward_1: 0.00531
	loss_policy_2: 0.0399
	accuracy_policy_2: 0.60723
	loss_value_2: 0.0373
	loss_reward_2: 0.00504
	loss_policy_3: 0.04261
	accuracy_policy_3: 0.58338
	loss_value_3: 0.03866
	loss_reward_3: 0.00555
	loss_policy_4: 0.04496
	accuracy_policy_4: 0.56586
	loss_value_4: 0.03991
	loss_reward_4: 0.00604
	loss_policy_5: 0.0472
	accuracy_policy_5: 0.54633
	loss_value_5: 0.04104
	loss_reward_5: 0.00703
	loss_policy: 0.3638
	loss_value: 0.36411
	loss_reward: 0.02898
[2024-05-12 14:27:55] nn step 28400, lr: 0.06561.
	loss_policy_0: 0.13076
	accuracy_policy_0: 0.74176
	loss_value_0: 0.16916
	loss_policy_1: 0.03296
	accuracy_policy_1: 0.67557
	loss_value_1: 0.03562
	loss_reward_1: 0.00518
	loss_policy_2: 0.03618
	accuracy_policy_2: 0.64309
	loss_value_2: 0.03706
	loss_reward_2: 0.00516
	loss_policy_3: 0.03943
	accuracy_policy_3: 0.61719
	loss_value_3: 0.03823
	loss_reward_3: 0.00549
	loss_policy_4: 0.04192
	accuracy_policy_4: 0.59811
	loss_value_4: 0.03936
	loss_reward_4: 0.00606
	loss_policy_5: 0.04448
	accuracy_policy_5: 0.57783
	loss_value_5: 0.04051
	loss_reward_5: 0.00703
	loss_policy: 0.32572
	loss_value: 0.35994
	loss_reward: 0.02893
Optimization_Done 28400
[2024-05-12 14:30:14] [command] train weight_iter_28400.pkl 141 143
[2024-05-12 14:31:11] nn step 28500, lr: 0.06561.
	loss_policy_0: 0.17015
	accuracy_policy_0: 0.65654
	loss_value_0: 0.16348
	loss_policy_1: 0.03913
	accuracy_policy_1: 0.60998
	loss_value_1: 0.03431
	loss_reward_1: 0.00501
	loss_policy_2: 0.04267
	accuracy_policy_2: 0.58365
	loss_value_2: 0.03583
	loss_reward_2: 0.00496
	loss_policy_3: 0.04564
	accuracy_policy_3: 0.55434
	loss_value_3: 0.0371
	loss_reward_3: 0.0055
	loss_policy_4: 0.04808
	accuracy_policy_4: 0.53771
	loss_value_4: 0.03832
	loss_reward_4: 0.00581
	loss_policy_5: 0.05058
	accuracy_policy_5: 0.5183
	loss_value_5: 0.03955
	loss_reward_5: 0.00678
	loss_policy: 0.39625
	loss_value: 0.34859
	loss_reward: 0.02806
[2024-05-12 14:32:09] nn step 28600, lr: 0.06561.
	loss_policy_0: 0.1331
	accuracy_policy_0: 0.72027
	loss_value_0: 0.15585
	loss_policy_1: 0.03308
	accuracy_policy_1: 0.65967
	loss_value_1: 0.03272
	loss_reward_1: 0.00494
	loss_policy_2: 0.03733
	accuracy_policy_2: 0.6168
	loss_value_2: 0.03413
	loss_reward_2: 0.00468
	loss_policy_3: 0.04006
	accuracy_policy_3: 0.59203
	loss_value_3: 0.03523
	loss_reward_3: 0.00511
	loss_policy_4: 0.04257
	accuracy_policy_4: 0.57414
	loss_value_4: 0.03646
	loss_reward_4: 0.00559
	loss_policy_5: 0.04513
	accuracy_policy_5: 0.55131
	loss_value_5: 0.0376
	loss_reward_5: 0.00657
	loss_policy: 0.33127
	loss_value: 0.33199
	loss_reward: 0.02688
Optimization_Done 28600
[2024-05-12 14:34:29] [command] train weight_iter_28600.pkl 142 144
[2024-05-12 14:35:23] nn step 28700, lr: 0.06561.
	loss_policy_0: 0.17087
	accuracy_policy_0: 0.66039
	loss_value_0: 0.14764
	loss_policy_1: 0.03863
	accuracy_policy_1: 0.62379
	loss_value_1: 0.03114
	loss_reward_1: 0.00443
	loss_policy_2: 0.04199
	accuracy_policy_2: 0.59277
	loss_value_2: 0.03251
	loss_reward_2: 0.00443
	loss_policy_3: 0.0447
	accuracy_policy_3: 0.569
	loss_value_3: 0.03369
	loss_reward_3: 0.0048
	loss_policy_4: 0.04713
	accuracy_policy_4: 0.55168
	loss_value_4: 0.03496
	loss_reward_4: 0.00519
	loss_policy_5: 0.04922
	accuracy_policy_5: 0.53637
	loss_value_5: 0.03625
	loss_reward_5: 0.00604
	loss_policy: 0.39254
	loss_value: 0.31619
	loss_reward: 0.0249
[2024-05-12 14:36:14] nn step 28800, lr: 0.06561.
	loss_policy_0: 0.1487
	accuracy_policy_0: 0.70938
	loss_value_0: 0.15072
	loss_policy_1: 0.03543
	accuracy_policy_1: 0.65916
	loss_value_1: 0.0317
	loss_reward_1: 0.00461
	loss_policy_2: 0.03922
	accuracy_policy_2: 0.62543
	loss_value_2: 0.03304
	loss_reward_2: 0.00449
	loss_policy_3: 0.04222
	accuracy_policy_3: 0.601
	loss_value_3: 0.03441
	loss_reward_3: 0.00477
	loss_policy_4: 0.04478
	accuracy_policy_4: 0.58367
	loss_value_4: 0.03555
	loss_reward_4: 0.00527
	loss_policy_5: 0.04752
	accuracy_policy_5: 0.5652
	loss_value_5: 0.0368
	loss_reward_5: 0.00606
	loss_policy: 0.35787
	loss_value: 0.32223
	loss_reward: 0.0252
Optimization_Done 28800
[2024-05-12 14:38:31] [command] train weight_iter_28800.pkl 143 145
[2024-05-12 14:39:23] nn step 28900, lr: 0.06561.
	loss_policy_0: 0.17309
	accuracy_policy_0: 0.67002
	loss_value_0: 0.15943
	loss_policy_1: 0.03893
	accuracy_policy_1: 0.63418
	loss_value_1: 0.03341
	loss_reward_1: 0.00515
	loss_policy_2: 0.04221
	accuracy_policy_2: 0.6083
	loss_value_2: 0.03496
	loss_reward_2: 0.005
	loss_policy_3: 0.04493
	accuracy_policy_3: 0.59121
	loss_value_3: 0.0362
	loss_reward_3: 0.00544
	loss_policy_4: 0.04684
	accuracy_policy_4: 0.57357
	loss_value_4: 0.03736
	loss_reward_4: 0.00581
	loss_policy_5: 0.04949
	accuracy_policy_5: 0.55764
	loss_value_5: 0.03852
	loss_reward_5: 0.00673
	loss_policy: 0.39549
	loss_value: 0.33989
	loss_reward: 0.02813
[2024-05-12 14:40:13] nn step 29000, lr: 0.06561.
	loss_policy_0: 0.15993
	accuracy_policy_0: 0.71738
	loss_value_0: 0.17014
	loss_policy_1: 0.03793
	accuracy_policy_1: 0.66729
	loss_value_1: 0.03562
	loss_reward_1: 0.00544
	loss_policy_2: 0.04157
	accuracy_policy_2: 0.63752
	loss_value_2: 0.03711
	loss_reward_2: 0.00536
	loss_policy_3: 0.04477
	accuracy_policy_3: 0.61707
	loss_value_3: 0.03856
	loss_reward_3: 0.00561
	loss_policy_4: 0.04793
	accuracy_policy_4: 0.59596
	loss_value_4: 0.03987
	loss_reward_4: 0.00612
	loss_policy_5: 0.05051
	accuracy_policy_5: 0.57881
	loss_value_5: 0.04122
	loss_reward_5: 0.00721
	loss_policy: 0.38265
	loss_value: 0.36251
	loss_reward: 0.02974
Optimization_Done 29000
[2024-05-12 14:42:17] [command] train weight_iter_29000.pkl 144 146
[2024-05-12 14:43:10] nn step 29100, lr: 0.06561.
	loss_policy_0: 0.17083
	accuracy_policy_0: 0.68641
	loss_value_0: 0.17016
	loss_policy_1: 0.03915
	accuracy_policy_1: 0.6443
	loss_value_1: 0.03567
	loss_reward_1: 0.00531
	loss_policy_2: 0.04198
	accuracy_policy_2: 0.61834
	loss_value_2: 0.03699
	loss_reward_2: 0.00511
	loss_policy_3: 0.04467
	accuracy_policy_3: 0.5959
	loss_value_3: 0.03817
	loss_reward_3: 0.00545
	loss_policy_4: 0.0471
	accuracy_policy_4: 0.58039
	loss_value_4: 0.0393
	loss_reward_4: 0.00587
	loss_policy_5: 0.04915
	accuracy_policy_5: 0.56141
	loss_value_5: 0.04059
	loss_reward_5: 0.00696
	loss_policy: 0.39288
	loss_value: 0.36088
	loss_reward: 0.02869
[2024-05-12 14:44:01] nn step 29200, lr: 0.06561.
	loss_policy_0: 0.13938
	accuracy_policy_0: 0.72998
	loss_value_0: 0.16408
	loss_policy_1: 0.03392
	accuracy_policy_1: 0.67232
	loss_value_1: 0.03426
	loss_reward_1: 0.00503
	loss_policy_2: 0.03735
	accuracy_policy_2: 0.64166
	loss_value_2: 0.03549
	loss_reward_2: 0.00487
	loss_policy_3: 0.03965
	accuracy_policy_3: 0.62055
	loss_value_3: 0.03654
	loss_reward_3: 0.0053
	loss_policy_4: 0.04243
	accuracy_policy_4: 0.6027
	loss_value_4: 0.0377
	loss_reward_4: 0.00564
	loss_policy_5: 0.04536
	accuracy_policy_5: 0.58297
	loss_value_5: 0.03888
	loss_reward_5: 0.00681
	loss_policy: 0.33809
	loss_value: 0.34695
	loss_reward: 0.02765
Optimization_Done 29200
[2024-05-12 14:46:19] [command] train weight_iter_29200.pkl 145 147
[2024-05-12 14:47:11] nn step 29300, lr: 0.06561.
	loss_policy_0: 0.18282
	accuracy_policy_0: 0.64793
	loss_value_0: 0.17022
	loss_policy_1: 0.04201
	accuracy_policy_1: 0.60414
	loss_value_1: 0.03556
	loss_reward_1: 0.00525
	loss_policy_2: 0.04516
	accuracy_policy_2: 0.57762
	loss_value_2: 0.03685
	loss_reward_2: 0.00501
	loss_policy_3: 0.04823
	accuracy_policy_3: 0.55418
	loss_value_3: 0.03804
	loss_reward_3: 0.00548
	loss_policy_4: 0.05089
	accuracy_policy_4: 0.53299
	loss_value_4: 0.03913
	loss_reward_4: 0.00597
	loss_policy_5: 0.05348
	accuracy_policy_5: 0.51313
	loss_value_5: 0.04027
	loss_reward_5: 0.00709
	loss_policy: 0.42258
	loss_value: 0.36008
	loss_reward: 0.02881
[2024-05-12 14:48:03] nn step 29400, lr: 0.06561.
	loss_policy_0: 0.14994
	accuracy_policy_0: 0.70814
	loss_value_0: 0.17013
	loss_policy_1: 0.03703
	accuracy_policy_1: 0.64479
	loss_value_1: 0.03555
	loss_reward_1: 0.00534
	loss_policy_2: 0.04103
	accuracy_policy_2: 0.61277
	loss_value_2: 0.03683
	loss_reward_2: 0.00507
	loss_policy_3: 0.04409
	accuracy_policy_3: 0.59512
	loss_value_3: 0.03815
	loss_reward_3: 0.00536
	loss_policy_4: 0.04721
	accuracy_policy_4: 0.56773
	loss_value_4: 0.03933
	loss_reward_4: 0.00583
	loss_policy_5: 0.05058
	accuracy_policy_5: 0.54244
	loss_value_5: 0.04051
	loss_reward_5: 0.00707
	loss_policy: 0.36988
	loss_value: 0.3605
	loss_reward: 0.02867
Optimization_Done 29400
[2024-05-12 14:50:10] [command] train weight_iter_29400.pkl 146 148
[2024-05-12 14:51:03] nn step 29500, lr: 0.06561.
	loss_policy_0: 0.16598
	accuracy_policy_0: 0.66613
	loss_value_0: 0.15916
	loss_policy_1: 0.03814
	accuracy_policy_1: 0.62045
	loss_value_1: 0.03336
	loss_reward_1: 0.0045
	loss_policy_2: 0.04136
	accuracy_policy_2: 0.59449
	loss_value_2: 0.0348
	loss_reward_2: 0.00447
	loss_policy_3: 0.04423
	accuracy_policy_3: 0.57273
	loss_value_3: 0.03608
	loss_reward_3: 0.00481
	loss_policy_4: 0.04708
	accuracy_policy_4: 0.55199
	loss_value_4: 0.03721
	loss_reward_4: 0.0052
	loss_policy_5: 0.04923
	accuracy_policy_5: 0.53264
	loss_value_5: 0.03829
	loss_reward_5: 0.0061
	loss_policy: 0.38603
	loss_value: 0.33891
	loss_reward: 0.02508
[2024-05-12 14:51:55] nn step 29600, lr: 0.06561.
	loss_policy_0: 0.14506
	accuracy_policy_0: 0.71557
	loss_value_0: 0.16473
	loss_policy_1: 0.03573
	accuracy_policy_1: 0.65682
	loss_value_1: 0.03451
	loss_reward_1: 0.00489
	loss_policy_2: 0.03986
	accuracy_policy_2: 0.62137
	loss_value_2: 0.03575
	loss_reward_2: 0.00465
	loss_policy_3: 0.0433
	accuracy_policy_3: 0.59691
	loss_value_3: 0.03687
	loss_reward_3: 0.00501
	loss_policy_4: 0.0459
	accuracy_policy_4: 0.57639
	loss_value_4: 0.03816
	loss_reward_4: 0.00547
	loss_policy_5: 0.04877
	accuracy_policy_5: 0.55715
	loss_value_5: 0.03928
	loss_reward_5: 0.00655
	loss_policy: 0.35863
	loss_value: 0.34931
	loss_reward: 0.02657
Optimization_Done 29600
[2024-05-12 14:54:16] [command] train weight_iter_29600.pkl 147 149
[2024-05-12 14:55:09] nn step 29700, lr: 0.06561.
	loss_policy_0: 0.17417
	accuracy_policy_0: 0.66506
	loss_value_0: 0.17008
	loss_policy_1: 0.03996
	accuracy_policy_1: 0.62139
	loss_value_1: 0.03563
	loss_reward_1: 0.00533
	loss_policy_2: 0.04402
	accuracy_policy_2: 0.58879
	loss_value_2: 0.03695
	loss_reward_2: 0.0053
	loss_policy_3: 0.04685
	accuracy_policy_3: 0.56361
	loss_value_3: 0.03818
	loss_reward_3: 0.00566
	loss_policy_4: 0.05008
	accuracy_policy_4: 0.53775
	loss_value_4: 0.03953
	loss_reward_4: 0.00615
	loss_policy_5: 0.05261
	accuracy_policy_5: 0.51703
	loss_value_5: 0.04063
	loss_reward_5: 0.00695
	loss_policy: 0.40768
	loss_value: 0.361
	loss_reward: 0.02938
[2024-05-12 14:55:59] nn step 29800, lr: 0.06561.
	loss_policy_0: 0.1478
	accuracy_policy_0: 0.72137
	loss_value_0: 0.17293
	loss_policy_1: 0.03656
	accuracy_policy_1: 0.66312
	loss_value_1: 0.03635
	loss_reward_1: 0.00529
	loss_policy_2: 0.04117
	accuracy_policy_2: 0.62627
	loss_value_2: 0.03773
	loss_reward_2: 0.0052
	loss_policy_3: 0.04457
	accuracy_policy_3: 0.60057
	loss_value_3: 0.03903
	loss_reward_3: 0.00562
	loss_policy_4: 0.0476
	accuracy_policy_4: 0.57557
	loss_value_4: 0.04025
	loss_reward_4: 0.00603
	loss_policy_5: 0.05109
	accuracy_policy_5: 0.55104
	loss_value_5: 0.04147
	loss_reward_5: 0.00717
	loss_policy: 0.36878
	loss_value: 0.36774
	loss_reward: 0.02932
Optimization_Done 29800
[2024-05-12 14:58:21] [command] train weight_iter_29800.pkl 148 150
[2024-05-12 14:59:13] nn step 29900, lr: 0.06561.
	loss_policy_0: 0.14474
	accuracy_policy_0: 0.70104
	loss_value_0: 0.16302
	loss_policy_1: 0.03519
	accuracy_policy_1: 0.65193
	loss_value_1: 0.03425
	loss_reward_1: 0.00566
	loss_policy_2: 0.03912
	accuracy_policy_2: 0.61535
	loss_value_2: 0.03559
	loss_reward_2: 0.00524
	loss_policy_3: 0.04214
	accuracy_policy_3: 0.59318
	loss_value_3: 0.03676
	loss_reward_3: 0.00562
	loss_policy_4: 0.04493
	accuracy_policy_4: 0.56861
	loss_value_4: 0.03786
	loss_reward_4: 0.00624
	loss_policy_5: 0.04769
	accuracy_policy_5: 0.54879
	loss_value_5: 0.03899
	loss_reward_5: 0.00713
	loss_policy: 0.35381
	loss_value: 0.34646
	loss_reward: 0.02989
[2024-05-12 15:00:03] nn step 30000, lr: 0.06561.
	loss_policy_0: 0.12948
	accuracy_policy_0: 0.74156
	loss_value_0: 0.16689
	loss_policy_1: 0.03345
	accuracy_policy_1: 0.67447
	loss_value_1: 0.03498
	loss_reward_1: 0.00566
	loss_policy_2: 0.03764
	accuracy_policy_2: 0.63949
	loss_value_2: 0.03637
	loss_reward_2: 0.0054
	loss_policy_3: 0.04083
	accuracy_policy_3: 0.6177
	loss_value_3: 0.03757
	loss_reward_3: 0.00578
	loss_policy_4: 0.0436
	accuracy_policy_4: 0.5968
	loss_value_4: 0.03868
	loss_reward_4: 0.00623
	loss_policy_5: 0.04616
	accuracy_policy_5: 0.57729
	loss_value_5: 0.03986
	loss_reward_5: 0.00733
	loss_policy: 0.33116
	loss_value: 0.35435
	loss_reward: 0.03041
Optimization_Done 30000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-12 21:38:35] [command] train weight_iter_30000.pkl 149 151
[2024-05-12 21:39:38] nn step 30100, lr: 0.059049.
	loss_policy_0: 0.15112
	accuracy_policy_0: 0.70215
	loss_value_0: 0.1739
	loss_policy_1: 0.03781
	accuracy_policy_1: 0.64033
	loss_value_1: 0.03654
	loss_reward_1: 0.00539
	loss_policy_2: 0.04199
	accuracy_policy_2: 0.60506
	loss_value_2: 0.03785
	loss_reward_2: 0.00527
	loss_policy_3: 0.04535
	accuracy_policy_3: 0.57918
	loss_value_3: 0.03917
	loss_reward_3: 0.00559
	loss_policy_4: 0.04871
	accuracy_policy_4: 0.55363
	loss_value_4: 0.04025
	loss_reward_4: 0.00605
	loss_policy_5: 0.05151
	accuracy_policy_5: 0.53068
	loss_value_5: 0.04131
	loss_reward_5: 0.00689
	loss_policy: 0.3765
	loss_value: 0.36902
	loss_reward: 0.02919
[2024-05-12 21:40:27] nn step 30200, lr: 0.059049.
	loss_policy_0: 0.12019
	accuracy_policy_0: 0.75186
	loss_value_0: 0.16612
	loss_policy_1: 0.03244
	accuracy_policy_1: 0.676
	loss_value_1: 0.03479
	loss_reward_1: 0.00525
	loss_policy_2: 0.03661
	accuracy_policy_2: 0.64006
	loss_value_2: 0.03616
	loss_reward_2: 0.00498
	loss_policy_3: 0.0404
	accuracy_policy_3: 0.61268
	loss_value_3: 0.03728
	loss_reward_3: 0.00533
	loss_policy_4: 0.04363
	accuracy_policy_4: 0.58844
	loss_value_4: 0.03855
	loss_reward_4: 0.00587
	loss_policy_5: 0.04662
	accuracy_policy_5: 0.56238
	loss_value_5: 0.03968
	loss_reward_5: 0.00682
	loss_policy: 0.31988
	loss_value: 0.35258
	loss_reward: 0.02825
Optimization_Done 30200
[2024-05-12 21:42:48] [command] train weight_iter_30200.pkl 150 152
[2024-05-12 21:43:53] nn step 30300, lr: 0.059049.
	loss_policy_0: 0.15643
	accuracy_policy_0: 0.70096
	loss_value_0: 0.16504
	loss_policy_1: 0.0375
	accuracy_policy_1: 0.64305
	loss_value_1: 0.03479
	loss_reward_1: 0.0047
	loss_policy_2: 0.04132
	accuracy_policy_2: 0.6142
	loss_value_2: 0.03622
	loss_reward_2: 0.00457
	loss_policy_3: 0.04468
	accuracy_policy_3: 0.58715
	loss_value_3: 0.03742
	loss_reward_3: 0.00501
	loss_policy_4: 0.04764
	accuracy_policy_4: 0.56754
	loss_value_4: 0.03868
	loss_reward_4: 0.00531
	loss_policy_5: 0.0502
	accuracy_policy_5: 0.55014
	loss_value_5: 0.03981
	loss_reward_5: 0.00635
	loss_policy: 0.37777
	loss_value: 0.35196
	loss_reward: 0.02594
[2024-05-12 21:44:59] nn step 30400, lr: 0.059049.
	loss_policy_0: 0.12701
	accuracy_policy_0: 0.75074
	loss_value_0: 0.16912
	loss_policy_1: 0.03339
	accuracy_policy_1: 0.68781
	loss_value_1: 0.03554
	loss_reward_1: 0.00507
	loss_policy_2: 0.03785
	accuracy_policy_2: 0.65262
	loss_value_2: 0.03705
	loss_reward_2: 0.00473
	loss_policy_3: 0.04128
	accuracy_policy_3: 0.62867
	loss_value_3: 0.03827
	loss_reward_3: 0.00519
	loss_policy_4: 0.04434
	accuracy_policy_4: 0.60924
	loss_value_4: 0.03936
	loss_reward_4: 0.00555
	loss_policy_5: 0.04746
	accuracy_policy_5: 0.58861
	loss_value_5: 0.04051
	loss_reward_5: 0.00657
	loss_policy: 0.33132
	loss_value: 0.35985
	loss_reward: 0.02711
Optimization_Done 30400
[2024-05-12 21:47:20] [command] train weight_iter_30400.pkl 151 153
[2024-05-12 21:48:26] nn step 30500, lr: 0.059049.
	loss_policy_0: 0.15897
	accuracy_policy_0: 0.68932
	loss_value_0: 0.16152
	loss_policy_1: 0.03742
	accuracy_policy_1: 0.64223
	loss_value_1: 0.03414
	loss_reward_1: 0.00452
	loss_policy_2: 0.04152
	accuracy_policy_2: 0.6091
	loss_value_2: 0.03537
	loss_reward_2: 0.00446
	loss_policy_3: 0.04478
	accuracy_policy_3: 0.58818
	loss_value_3: 0.03665
	loss_reward_3: 0.00487
	loss_policy_4: 0.04754
	accuracy_policy_4: 0.56922
	loss_value_4: 0.03772
	loss_reward_4: 0.00505
	loss_policy_5: 0.05043
	accuracy_policy_5: 0.54963
	loss_value_5: 0.03885
	loss_reward_5: 0.0061
	loss_policy: 0.38066
	loss_value: 0.34424
	loss_reward: 0.02499
[2024-05-12 21:49:31] nn step 30600, lr: 0.059049.
	loss_policy_0: 0.13447
	accuracy_policy_0: 0.74221
	loss_value_0: 0.16602
	loss_policy_1: 0.03394
	accuracy_policy_1: 0.6835
	loss_value_1: 0.03491
	loss_reward_1: 0.00481
	loss_policy_2: 0.03832
	accuracy_policy_2: 0.64902
	loss_value_2: 0.03616
	loss_reward_2: 0.00467
	loss_policy_3: 0.04197
	accuracy_policy_3: 0.62615
	loss_value_3: 0.03744
	loss_reward_3: 0.00496
	loss_policy_4: 0.04523
	accuracy_policy_4: 0.60207
	loss_value_4: 0.0386
	loss_reward_4: 0.0054
	loss_policy_5: 0.04882
	accuracy_policy_5: 0.58096
	loss_value_5: 0.0398
	loss_reward_5: 0.00656
	loss_policy: 0.34274
	loss_value: 0.35294
	loss_reward: 0.0264
Optimization_Done 30600
[2024-05-12 21:51:58] [command] train weight_iter_30600.pkl 152 154
[2024-05-12 21:53:03] nn step 30700, lr: 0.059049.
	loss_policy_0: 0.12139
	accuracy_policy_0: 0.72324
	loss_value_0: 0.14149
	loss_policy_1: 0.0303
	accuracy_policy_1: 0.66299
	loss_value_1: 0.02958
	loss_reward_1: 0.00424
	loss_policy_2: 0.03349
	accuracy_policy_2: 0.63568
	loss_value_2: 0.03072
	loss_reward_2: 0.00403
	loss_policy_3: 0.03625
	accuracy_policy_3: 0.61145
	loss_value_3: 0.03184
	loss_reward_3: 0.00439
	loss_policy_4: 0.03903
	accuracy_policy_4: 0.58846
	loss_value_4: 0.03283
	loss_reward_4: 0.00477
	loss_policy_5: 0.04106
	accuracy_policy_5: 0.57053
	loss_value_5: 0.03385
	loss_reward_5: 0.00566
	loss_policy: 0.30151
	loss_value: 0.3003
	loss_reward: 0.0231
[2024-05-12 21:54:01] nn step 30800, lr: 0.059049.
	loss_policy_0: 0.11287
	accuracy_policy_0: 0.75801
	loss_value_0: 0.1497
	loss_policy_1: 0.02945
	accuracy_policy_1: 0.69102
	loss_value_1: 0.03121
	loss_reward_1: 0.00435
	loss_policy_2: 0.03297
	accuracy_policy_2: 0.65885
	loss_value_2: 0.0323
	loss_reward_2: 0.00422
	loss_policy_3: 0.03592
	accuracy_policy_3: 0.63549
	loss_value_3: 0.03343
	loss_reward_3: 0.00459
	loss_policy_4: 0.03867
	accuracy_policy_4: 0.61576
	loss_value_4: 0.03448
	loss_reward_4: 0.00502
	loss_policy_5: 0.04164
	accuracy_policy_5: 0.59549
	loss_value_5: 0.03557
	loss_reward_5: 0.00589
	loss_policy: 0.29152
	loss_value: 0.31669
	loss_reward: 0.02407
Optimization_Done 30800
[2024-05-12 21:56:28] [command] train weight_iter_30800.pkl 153 155
[2024-05-12 21:57:32] nn step 30900, lr: 0.059049.
	loss_policy_0: 0.13746
	accuracy_policy_0: 0.68883
	loss_value_0: 0.14994
	loss_policy_1: 0.0331
	accuracy_policy_1: 0.63445
	loss_value_1: 0.0315
	loss_reward_1: 0.00425
	loss_policy_2: 0.0364
	accuracy_policy_2: 0.60814
	loss_value_2: 0.03266
	loss_reward_2: 0.00409
	loss_policy_3: 0.0391
	accuracy_policy_3: 0.58045
	loss_value_3: 0.03377
	loss_reward_3: 0.0043
	loss_policy_4: 0.04154
	accuracy_policy_4: 0.55885
	loss_value_4: 0.03478
	loss_reward_4: 0.00481
	loss_policy_5: 0.04393
	accuracy_policy_5: 0.53975
	loss_value_5: 0.03581
	loss_reward_5: 0.00573
	loss_policy: 0.33153
	loss_value: 0.31845
	loss_reward: 0.02317
[2024-05-12 21:58:38] nn step 31000, lr: 0.059049.
	loss_policy_0: 0.11792
	accuracy_policy_0: 0.74107
	loss_value_0: 0.15459
	loss_policy_1: 0.03076
	accuracy_policy_1: 0.66684
	loss_value_1: 0.03238
	loss_reward_1: 0.00431
	loss_policy_2: 0.03439
	accuracy_policy_2: 0.63428
	loss_value_2: 0.03357
	loss_reward_2: 0.00398
	loss_policy_3: 0.03722
	accuracy_policy_3: 0.60895
	loss_value_3: 0.03446
	loss_reward_3: 0.0044
	loss_policy_4: 0.03967
	accuracy_policy_4: 0.59207
	loss_value_4: 0.03537
	loss_reward_4: 0.00482
	loss_policy_5: 0.0423
	accuracy_policy_5: 0.57305
	loss_value_5: 0.03634
	loss_reward_5: 0.00576
	loss_policy: 0.30226
	loss_value: 0.32671
	loss_reward: 0.02328
Optimization_Done 31000
[2024-05-12 22:00:58] [command] train weight_iter_31000.pkl 154 156
[2024-05-12 22:02:01] nn step 31100, lr: 0.059049.
	loss_policy_0: 0.14854
	accuracy_policy_0: 0.67047
	loss_value_0: 0.15014
	loss_policy_1: 0.03427
	accuracy_policy_1: 0.6235
	loss_value_1: 0.03124
	loss_reward_1: 0.00452
	loss_policy_2: 0.03671
	accuracy_policy_2: 0.601
	loss_value_2: 0.0323
	loss_reward_2: 0.00445
	loss_policy_3: 0.03889
	accuracy_policy_3: 0.58121
	loss_value_3: 0.03318
	loss_reward_3: 0.00469
	loss_policy_4: 0.04092
	accuracy_policy_4: 0.56156
	loss_value_4: 0.03411
	loss_reward_4: 0.00514
	loss_policy_5: 0.04292
	accuracy_policy_5: 0.54666
	loss_value_5: 0.03492
	loss_reward_5: 0.00575
	loss_policy: 0.34225
	loss_value: 0.31588
	loss_reward: 0.02455
[2024-05-12 22:03:05] nn step 31200, lr: 0.059049.
	loss_policy_0: 0.11775
	accuracy_policy_0: 0.73025
	loss_value_0: 0.14786
	loss_policy_1: 0.02958
	accuracy_policy_1: 0.66682
	loss_value_1: 0.03104
	loss_reward_1: 0.00447
	loss_policy_2: 0.03279
	accuracy_policy_2: 0.63633
	loss_value_2: 0.03208
	loss_reward_2: 0.00427
	loss_policy_3: 0.03491
	accuracy_policy_3: 0.61928
	loss_value_3: 0.03298
	loss_reward_3: 0.00462
	loss_policy_4: 0.03718
	accuracy_policy_4: 0.59896
	loss_value_4: 0.03392
	loss_reward_4: 0.00499
	loss_policy_5: 0.03944
	accuracy_policy_5: 0.57826
	loss_value_5: 0.03486
	loss_reward_5: 0.00586
	loss_policy: 0.29165
	loss_value: 0.31273
	loss_reward: 0.02421
Optimization_Done 31200
[2024-05-12 22:05:22] [command] train weight_iter_31200.pkl 155 157
[2024-05-12 22:06:26] nn step 31300, lr: 0.059049.
	loss_policy_0: 0.16571
	accuracy_policy_0: 0.65557
	loss_value_0: 0.16805
	loss_policy_1: 0.03808
	accuracy_policy_1: 0.6082
	loss_value_1: 0.03504
	loss_reward_1: 0.00523
	loss_policy_2: 0.04145
	accuracy_policy_2: 0.58045
	loss_value_2: 0.03628
	loss_reward_2: 0.00497
	loss_policy_3: 0.04383
	accuracy_policy_3: 0.56252
	loss_value_3: 0.03742
	loss_reward_3: 0.00544
	loss_policy_4: 0.04603
	accuracy_policy_4: 0.54215
	loss_value_4: 0.03853
	loss_reward_4: 0.00591
	loss_policy_5: 0.04817
	accuracy_policy_5: 0.52709
	loss_value_5: 0.03962
	loss_reward_5: 0.00662
	loss_policy: 0.38327
	loss_value: 0.35495
	loss_reward: 0.02817
[2024-05-12 22:07:32] nn step 31400, lr: 0.059049.
	loss_policy_0: 0.14803
	accuracy_policy_0: 0.70457
	loss_value_0: 0.17186
	loss_policy_1: 0.03635
	accuracy_policy_1: 0.64215
	loss_value_1: 0.036
	loss_reward_1: 0.0053
	loss_policy_2: 0.03992
	accuracy_policy_2: 0.61213
	loss_value_2: 0.03729
	loss_reward_2: 0.00522
	loss_policy_3: 0.04253
	accuracy_policy_3: 0.59195
	loss_value_3: 0.0384
	loss_reward_3: 0.00551
	loss_policy_4: 0.04536
	accuracy_policy_4: 0.57268
	loss_value_4: 0.03969
	loss_reward_4: 0.00595
	loss_policy_5: 0.04749
	accuracy_policy_5: 0.54824
	loss_value_5: 0.04078
	loss_reward_5: 0.00691
	loss_policy: 0.35968
	loss_value: 0.36403
	loss_reward: 0.02889
Optimization_Done 31400
[2024-05-12 22:09:45] [command] train weight_iter_31400.pkl 156 158
[2024-05-12 22:10:52] nn step 31500, lr: 0.059049.
	loss_policy_0: 0.18609
	accuracy_policy_0: 0.66648
	loss_value_0: 0.1848
	loss_policy_1: 0.04486
	accuracy_policy_1: 0.60262
	loss_value_1: 0.03859
	loss_reward_1: 0.00619
	loss_policy_2: 0.04872
	accuracy_policy_2: 0.57215
	loss_value_2: 0.04006
	loss_reward_2: 0.00574
	loss_policy_3: 0.05198
	accuracy_policy_3: 0.54744
	loss_value_3: 0.04158
	loss_reward_3: 0.00592
	loss_policy_4: 0.05505
	accuracy_policy_4: 0.52316
	loss_value_4: 0.04296
	loss_reward_4: 0.00657
	loss_policy_5: 0.05832
	accuracy_policy_5: 0.50299
	loss_value_5: 0.04426
	loss_reward_5: 0.00723
	loss_policy: 0.44502
	loss_value: 0.39225
	loss_reward: 0.03165
[2024-05-12 22:11:42] nn step 31600, lr: 0.059049.
	loss_policy_0: 0.15864
	accuracy_policy_0: 0.70758
	loss_value_0: 0.17812
	loss_policy_1: 0.04013
	accuracy_policy_1: 0.6401
	loss_value_1: 0.03736
	loss_reward_1: 0.00595
	loss_policy_2: 0.04469
	accuracy_policy_2: 0.60154
	loss_value_2: 0.039
	loss_reward_2: 0.00569
	loss_policy_3: 0.04805
	accuracy_policy_3: 0.57756
	loss_value_3: 0.04037
	loss_reward_3: 0.00582
	loss_policy_4: 0.05141
	accuracy_policy_4: 0.5526
	loss_value_4: 0.04168
	loss_reward_4: 0.00626
	loss_policy_5: 0.05444
	accuracy_policy_5: 0.52689
	loss_value_5: 0.04299
	loss_reward_5: 0.00723
	loss_policy: 0.39736
	loss_value: 0.37951
	loss_reward: 0.03095
Optimization_Done 31600
[2024-05-12 22:13:35] [command] train weight_iter_31600.pkl 157 159
[2024-05-12 22:14:30] nn step 31700, lr: 0.059049.
	loss_policy_0: 0.17852
	accuracy_policy_0: 0.66697
	loss_value_0: 0.18829
	loss_policy_1: 0.04373
	accuracy_policy_1: 0.60062
	loss_value_1: 0.03946
	loss_reward_1: 0.00545
	loss_policy_2: 0.04822
	accuracy_policy_2: 0.5693
	loss_value_2: 0.04062
	loss_reward_2: 0.0053
	loss_policy_3: 0.05141
	accuracy_policy_3: 0.53854
	loss_value_3: 0.04179
	loss_reward_3: 0.00564
	loss_policy_4: 0.05409
	accuracy_policy_4: 0.52148
	loss_value_4: 0.04285
	loss_reward_4: 0.0061
	loss_policy_5: 0.05704
	accuracy_policy_5: 0.49686
	loss_value_5: 0.04385
	loss_reward_5: 0.00699
	loss_policy: 0.43302
	loss_value: 0.39686
	loss_reward: 0.02948
[2024-05-12 22:15:22] nn step 31800, lr: 0.059049.
	loss_policy_0: 0.15094
	accuracy_policy_0: 0.71172
	loss_value_0: 0.17538
	loss_policy_1: 0.03897
	accuracy_policy_1: 0.63342
	loss_value_1: 0.03694
	loss_reward_1: 0.0054
	loss_policy_2: 0.0434
	accuracy_policy_2: 0.60031
	loss_value_2: 0.03836
	loss_reward_2: 0.00501
	loss_policy_3: 0.04712
	accuracy_policy_3: 0.57664
	loss_value_3: 0.0396
	loss_reward_3: 0.00546
	loss_policy_4: 0.04975
	accuracy_policy_4: 0.55393
	loss_value_4: 0.04076
	loss_reward_4: 0.00587
	loss_policy_5: 0.05281
	accuracy_policy_5: 0.53018
	loss_value_5: 0.04191
	loss_reward_5: 0.00672
	loss_policy: 0.38298
	loss_value: 0.37296
	loss_reward: 0.02846
Optimization_Done 31800
[2024-05-12 22:17:37] [command] train weight_iter_31800.pkl 158 160
[2024-05-12 22:18:26] nn step 31900, lr: 0.059049.
	loss_policy_0: 0.17988
	accuracy_policy_0: 0.67621
	loss_value_0: 0.16926
	loss_policy_1: 0.0428
	accuracy_policy_1: 0.61908
	loss_value_1: 0.03535
	loss_reward_1: 0.00503
	loss_policy_2: 0.04722
	accuracy_policy_2: 0.58654
	loss_value_2: 0.03669
	loss_reward_2: 0.00468
	loss_policy_3: 0.05024
	accuracy_policy_3: 0.5609
	loss_value_3: 0.03809
	loss_reward_3: 0.00512
	loss_policy_4: 0.05357
	accuracy_policy_4: 0.53748
	loss_value_4: 0.03924
	loss_reward_4: 0.0055
	loss_policy_5: 0.05642
	accuracy_policy_5: 0.5175
	loss_value_5: 0.04036
	loss_reward_5: 0.00646
	loss_policy: 0.43013
	loss_value: 0.359
	loss_reward: 0.02679
[2024-05-12 22:19:17] nn step 32000, lr: 0.059049.
	loss_policy_0: 0.15
	accuracy_policy_0: 0.72037
	loss_value_0: 0.1639
	loss_policy_1: 0.03832
	accuracy_policy_1: 0.65463
	loss_value_1: 0.03438
	loss_reward_1: 0.00506
	loss_policy_2: 0.04232
	accuracy_policy_2: 0.62164
	loss_value_2: 0.03569
	loss_reward_2: 0.00483
	loss_policy_3: 0.04622
	accuracy_policy_3: 0.59521
	loss_value_3: 0.03685
	loss_reward_3: 0.00504
	loss_policy_4: 0.04978
	accuracy_policy_4: 0.56688
	loss_value_4: 0.03813
	loss_reward_4: 0.00568
	loss_policy_5: 0.05285
	accuracy_policy_5: 0.54162
	loss_value_5: 0.03944
	loss_reward_5: 0.00629
	loss_policy: 0.37949
	loss_value: 0.34838
	loss_reward: 0.0269
Optimization_Done 32000
[2024-05-12 22:20:59] [command] train weight_iter_32000.pkl 159 161
[2024-05-12 22:21:51] nn step 32100, lr: 0.059049.
	loss_policy_0: 0.18279
	accuracy_policy_0: 0.68484
	loss_value_0: 0.17033
	loss_policy_1: 0.04251
	accuracy_policy_1: 0.64002
	loss_value_1: 0.03587
	loss_reward_1: 0.00518
	loss_policy_2: 0.04633
	accuracy_policy_2: 0.61055
	loss_value_2: 0.03732
	loss_reward_2: 0.00511
	loss_policy_3: 0.04985
	accuracy_policy_3: 0.58572
	loss_value_3: 0.03872
	loss_reward_3: 0.00545
	loss_policy_4: 0.05321
	accuracy_policy_4: 0.56443
	loss_value_4: 0.04001
	loss_reward_4: 0.00592
	loss_policy_5: 0.05643
	accuracy_policy_5: 0.54596
	loss_value_5: 0.04139
	loss_reward_5: 0.00688
	loss_policy: 0.43111
	loss_value: 0.36365
	loss_reward: 0.02854
[2024-05-12 22:22:43] nn step 32200, lr: 0.059049.
	loss_policy_0: 0.14843
	accuracy_policy_0: 0.72627
	loss_value_0: 0.15994
	loss_policy_1: 0.03644
	accuracy_policy_1: 0.67025
	loss_value_1: 0.03355
	loss_reward_1: 0.00476
	loss_policy_2: 0.04071
	accuracy_policy_2: 0.63592
	loss_value_2: 0.03497
	loss_reward_2: 0.00473
	loss_policy_3: 0.04416
	accuracy_policy_3: 0.61289
	loss_value_3: 0.03631
	loss_reward_3: 0.00502
	loss_policy_4: 0.04756
	accuracy_policy_4: 0.58723
	loss_value_4: 0.03753
	loss_reward_4: 0.0056
	loss_policy_5: 0.0505
	accuracy_policy_5: 0.57008
	loss_value_5: 0.03874
	loss_reward_5: 0.00653
	loss_policy: 0.3678
	loss_value: 0.34104
	loss_reward: 0.02665
Optimization_Done 32200
[2024-05-12 22:24:57] [command] train weight_iter_32200.pkl 160 162
[2024-05-12 22:25:49] nn step 32300, lr: 0.059049.
	loss_policy_0: 0.1983
	accuracy_policy_0: 0.64824
	loss_value_0: 0.15354
	loss_policy_1: 0.04504
	accuracy_policy_1: 0.60428
	loss_value_1: 0.03248
	loss_reward_1: 0.00494
	loss_policy_2: 0.04897
	accuracy_policy_2: 0.57451
	loss_value_2: 0.03411
	loss_reward_2: 0.00464
	loss_policy_3: 0.05208
	accuracy_policy_3: 0.55109
	loss_value_3: 0.03542
	loss_reward_3: 0.00515
	loss_policy_4: 0.05459
	accuracy_policy_4: 0.53275
	loss_value_4: 0.03678
	loss_reward_4: 0.00563
	loss_policy_5: 0.05752
	accuracy_policy_5: 0.51307
	loss_value_5: 0.03801
	loss_reward_5: 0.00652
	loss_policy: 0.4565
	loss_value: 0.33034
	loss_reward: 0.02688
[2024-05-12 22:26:41] nn step 32400, lr: 0.059049.
	loss_policy_0: 0.1695
	accuracy_policy_0: 0.70572
	loss_value_0: 0.1554
	loss_policy_1: 0.04101
	accuracy_policy_1: 0.64869
	loss_value_1: 0.03286
	loss_reward_1: 0.00505
	loss_policy_2: 0.04557
	accuracy_policy_2: 0.61514
	loss_value_2: 0.03448
	loss_reward_2: 0.00482
	loss_policy_3: 0.04886
	accuracy_policy_3: 0.59176
	loss_value_3: 0.03597
	loss_reward_3: 0.00527
	loss_policy_4: 0.05275
	accuracy_policy_4: 0.56623
	loss_value_4: 0.03725
	loss_reward_4: 0.0058
	loss_policy_5: 0.05565
	accuracy_policy_5: 0.54795
	loss_value_5: 0.03852
	loss_reward_5: 0.00661
	loss_policy: 0.41334
	loss_value: 0.33447
	loss_reward: 0.02755
Optimization_Done 32400
[2024-05-12 22:28:55] [command] train weight_iter_32400.pkl 161 163
[2024-05-12 22:29:48] nn step 32500, lr: 0.059049.
	loss_policy_0: 0.1674
	accuracy_policy_0: 0.67652
	loss_value_0: 0.14921
	loss_policy_1: 0.03938
	accuracy_policy_1: 0.62576
	loss_value_1: 0.03152
	loss_reward_1: 0.00444
	loss_policy_2: 0.04322
	accuracy_policy_2: 0.59488
	loss_value_2: 0.03292
	loss_reward_2: 0.00436
	loss_policy_3: 0.04599
	accuracy_policy_3: 0.57738
	loss_value_3: 0.03436
	loss_reward_3: 0.00464
	loss_policy_4: 0.04871
	accuracy_policy_4: 0.55469
	loss_value_4: 0.03562
	loss_reward_4: 0.00513
	loss_policy_5: 0.05103
	accuracy_policy_5: 0.53877
	loss_value_5: 0.03682
	loss_reward_5: 0.00584
	loss_policy: 0.39574
	loss_value: 0.32045
	loss_reward: 0.02442
[2024-05-12 22:30:38] nn step 32600, lr: 0.059049.
	loss_policy_0: 0.13975
	accuracy_policy_0: 0.72193
	loss_value_0: 0.14542
	loss_policy_1: 0.03503
	accuracy_policy_1: 0.65588
	loss_value_1: 0.03087
	loss_reward_1: 0.00444
	loss_policy_2: 0.0392
	accuracy_policy_2: 0.6199
	loss_value_2: 0.03219
	loss_reward_2: 0.0042
	loss_policy_3: 0.04229
	accuracy_policy_3: 0.59957
	loss_value_3: 0.03325
	loss_reward_3: 0.0044
	loss_policy_4: 0.0451
	accuracy_policy_4: 0.57625
	loss_value_4: 0.03435
	loss_reward_4: 0.00489
	loss_policy_5: 0.04811
	accuracy_policy_5: 0.55311
	loss_value_5: 0.03548
	loss_reward_5: 0.00573
	loss_policy: 0.34948
	loss_value: 0.31156
	loss_reward: 0.02365
Optimization_Done 32600
[2024-05-12 22:32:51] [command] train weight_iter_32600.pkl 162 164
[2024-05-12 22:33:43] nn step 32700, lr: 0.059049.
	loss_policy_0: 0.17855
	accuracy_policy_0: 0.6349
	loss_value_0: 0.15021
	loss_policy_1: 0.04101
	accuracy_policy_1: 0.58768
	loss_value_1: 0.03146
	loss_reward_1: 0.00474
	loss_policy_2: 0.04408
	accuracy_policy_2: 0.56662
	loss_value_2: 0.03291
	loss_reward_2: 0.00457
	loss_policy_3: 0.04649
	accuracy_policy_3: 0.5476
	loss_value_3: 0.03421
	loss_reward_3: 0.005
	loss_policy_4: 0.04883
	accuracy_policy_4: 0.52623
	loss_value_4: 0.03523
	loss_reward_4: 0.00524
	loss_policy_5: 0.05074
	accuracy_policy_5: 0.51287
	loss_value_5: 0.03648
	loss_reward_5: 0.00623
	loss_policy: 0.4097
	loss_value: 0.32051
	loss_reward: 0.02578
[2024-05-12 22:34:34] nn step 32800, lr: 0.059049.
	loss_policy_0: 0.14209
	accuracy_policy_0: 0.6935
	loss_value_0: 0.14351
	loss_policy_1: 0.03466
	accuracy_policy_1: 0.63244
	loss_value_1: 0.03017
	loss_reward_1: 0.0044
	loss_policy_2: 0.03856
	accuracy_policy_2: 0.59617
	loss_value_2: 0.03148
	loss_reward_2: 0.00438
	loss_policy_3: 0.04136
	accuracy_policy_3: 0.57357
	loss_value_3: 0.03257
	loss_reward_3: 0.00467
	loss_policy_4: 0.04364
	accuracy_policy_4: 0.55404
	loss_value_4: 0.03369
	loss_reward_4: 0.00498
	loss_policy_5: 0.04609
	accuracy_policy_5: 0.53398
	loss_value_5: 0.03492
	loss_reward_5: 0.00605
	loss_policy: 0.3464
	loss_value: 0.30633
	loss_reward: 0.02449
Optimization_Done 32800
[2024-05-12 22:36:47] [command] train weight_iter_32800.pkl 163 165
[2024-05-12 22:37:39] nn step 32900, lr: 0.059049.
	loss_policy_0: 0.19427
	accuracy_policy_0: 0.60771
	loss_value_0: 0.16651
	loss_policy_1: 0.04358
	accuracy_policy_1: 0.56689
	loss_value_1: 0.03477
	loss_reward_1: 0.00521
	loss_policy_2: 0.04715
	accuracy_policy_2: 0.5351
	loss_value_2: 0.03616
	loss_reward_2: 0.00517
	loss_policy_3: 0.04999
	accuracy_policy_3: 0.50869
	loss_value_3: 0.03749
	loss_reward_3: 0.00548
	loss_policy_4: 0.0524
	accuracy_policy_4: 0.48723
	loss_value_4: 0.03869
	loss_reward_4: 0.00584
	loss_policy_5: 0.05499
	accuracy_policy_5: 0.4676
	loss_value_5: 0.03992
	loss_reward_5: 0.0067
	loss_policy: 0.44238
	loss_value: 0.35354
	loss_reward: 0.0284
[2024-05-12 22:38:30] nn step 33000, lr: 0.059049.
	loss_policy_0: 0.15859
	accuracy_policy_0: 0.67496
	loss_value_0: 0.16342
	loss_policy_1: 0.03784
	accuracy_policy_1: 0.62014
	loss_value_1: 0.03411
	loss_reward_1: 0.00512
	loss_policy_2: 0.042
	accuracy_policy_2: 0.58168
	loss_value_2: 0.03535
	loss_reward_2: 0.0051
	loss_policy_3: 0.04533
	accuracy_policy_3: 0.54986
	loss_value_3: 0.03659
	loss_reward_3: 0.00548
	loss_policy_4: 0.0481
	accuracy_policy_4: 0.52666
	loss_value_4: 0.0378
	loss_reward_4: 0.00605
	loss_policy_5: 0.05062
	accuracy_policy_5: 0.50689
	loss_value_5: 0.03896
	loss_reward_5: 0.0067
	loss_policy: 0.38248
	loss_value: 0.34622
	loss_reward: 0.02845
Optimization_Done 33000
[2024-05-12 22:40:43] [command] train weight_iter_33000.pkl 164 166
[2024-05-12 22:41:33] nn step 33100, lr: 0.059049.
	loss_policy_0: 0.18737
	accuracy_policy_0: 0.65789
	loss_value_0: 0.1877
	loss_policy_1: 0.04351
	accuracy_policy_1: 0.60289
	loss_value_1: 0.03914
	loss_reward_1: 0.00687
	loss_policy_2: 0.04811
	accuracy_policy_2: 0.56676
	loss_value_2: 0.04069
	loss_reward_2: 0.0066
	loss_policy_3: 0.05147
	accuracy_policy_3: 0.54176
	loss_value_3: 0.04211
	loss_reward_3: 0.00728
	loss_policy_4: 0.05422
	accuracy_policy_4: 0.52051
	loss_value_4: 0.04338
	loss_reward_4: 0.00782
	loss_policy_5: 0.05718
	accuracy_policy_5: 0.49912
	loss_value_5: 0.0447
	loss_reward_5: 0.00909
	loss_policy: 0.44186
	loss_value: 0.39771
	loss_reward: 0.03767
[2024-05-12 22:42:24] nn step 33200, lr: 0.059049.
	loss_policy_0: 0.15344
	accuracy_policy_0: 0.69762
	loss_value_0: 0.17489
	loss_policy_1: 0.03801
	accuracy_policy_1: 0.631
	loss_value_1: 0.03652
	loss_reward_1: 0.00623
	loss_policy_2: 0.04257
	accuracy_policy_2: 0.59316
	loss_value_2: 0.03775
	loss_reward_2: 0.00614
	loss_policy_3: 0.04538
	accuracy_policy_3: 0.5702
	loss_value_3: 0.03906
	loss_reward_3: 0.00662
	loss_policy_4: 0.0485
	accuracy_policy_4: 0.54689
	loss_value_4: 0.04036
	loss_reward_4: 0.00705
	loss_policy_5: 0.05115
	accuracy_policy_5: 0.52471
	loss_value_5: 0.04161
	loss_reward_5: 0.00837
	loss_policy: 0.37904
	loss_value: 0.37018
	loss_reward: 0.03441
Optimization_Done 33200
[2024-05-12 22:44:35] [command] train weight_iter_33200.pkl 165 167
[2024-05-12 22:45:27] nn step 33300, lr: 0.059049.
	loss_policy_0: 0.16805
	accuracy_policy_0: 0.68301
	loss_value_0: 0.18743
	loss_policy_1: 0.04114
	accuracy_policy_1: 0.619
	loss_value_1: 0.03935
	loss_reward_1: 0.00726
	loss_policy_2: 0.04528
	accuracy_policy_2: 0.58355
	loss_value_2: 0.04081
	loss_reward_2: 0.00682
	loss_policy_3: 0.04835
	accuracy_policy_3: 0.56258
	loss_value_3: 0.04219
	loss_reward_3: 0.0072
	loss_policy_4: 0.0509
	accuracy_policy_4: 0.54336
	loss_value_4: 0.04344
	loss_reward_4: 0.00774
	loss_policy_5: 0.0541
	accuracy_policy_5: 0.51727
	loss_value_5: 0.04467
	loss_reward_5: 0.00907
	loss_policy: 0.40782
	loss_value: 0.39788
	loss_reward: 0.03809
[2024-05-12 22:46:18] nn step 33400, lr: 0.059049.
	loss_policy_0: 0.15015
	accuracy_policy_0: 0.71195
	loss_value_0: 0.18725
	loss_policy_1: 0.03819
	accuracy_policy_1: 0.64189
	loss_value_1: 0.03923
	loss_reward_1: 0.00702
	loss_policy_2: 0.04313
	accuracy_policy_2: 0.60373
	loss_value_2: 0.04062
	loss_reward_2: 0.00662
	loss_policy_3: 0.04661
	accuracy_policy_3: 0.57547
	loss_value_3: 0.04188
	loss_reward_3: 0.00715
	loss_policy_4: 0.04938
	accuracy_policy_4: 0.55627
	loss_value_4: 0.04308
	loss_reward_4: 0.00768
	loss_policy_5: 0.05229
	accuracy_policy_5: 0.53201
	loss_value_5: 0.04431
	loss_reward_5: 0.00901
	loss_policy: 0.37975
	loss_value: 0.39637
	loss_reward: 0.03748
Optimization_Done 33400
[2024-05-12 22:48:10] [command] train weight_iter_33400.pkl 166 168
[2024-05-12 22:48:57] nn step 33500, lr: 0.059049.
	loss_policy_0: 0.16554
	accuracy_policy_0: 0.67945
	loss_value_0: 0.1873
	loss_policy_1: 0.04106
	accuracy_policy_1: 0.61809
	loss_value_1: 0.0391
	loss_reward_1: 0.0066
	loss_policy_2: 0.04478
	accuracy_policy_2: 0.58383
	loss_value_2: 0.04034
	loss_reward_2: 0.00629
	loss_policy_3: 0.04786
	accuracy_policy_3: 0.561
	loss_value_3: 0.04147
	loss_reward_3: 0.00692
	loss_policy_4: 0.05016
	accuracy_policy_4: 0.54428
	loss_value_4: 0.04256
	loss_reward_4: 0.00761
	loss_policy_5: 0.05275
	accuracy_policy_5: 0.5273
	loss_value_5: 0.04371
	loss_reward_5: 0.00867
	loss_policy: 0.40216
	loss_value: 0.39448
	loss_reward: 0.03609
[2024-05-12 22:49:49] nn step 33600, lr: 0.059049.
	loss_policy_0: 0.13844
	accuracy_policy_0: 0.72598
	loss_value_0: 0.18129
	loss_policy_1: 0.0363
	accuracy_policy_1: 0.65391
	loss_value_1: 0.0379
	loss_reward_1: 0.00658
	loss_policy_2: 0.04051
	accuracy_policy_2: 0.61912
	loss_value_2: 0.03922
	loss_reward_2: 0.00624
	loss_policy_3: 0.0437
	accuracy_policy_3: 0.5917
	loss_value_3: 0.0404
	loss_reward_3: 0.00696
	loss_policy_4: 0.0462
	accuracy_policy_4: 0.57662
	loss_value_4: 0.04162
	loss_reward_4: 0.00747
	loss_policy_5: 0.04939
	accuracy_policy_5: 0.55391
	loss_value_5: 0.04288
	loss_reward_5: 0.0087
	loss_policy: 0.35455
	loss_value: 0.3833
	loss_reward: 0.03595
Optimization_Done 33600
[2024-05-12 22:52:01] [command] train weight_iter_33600.pkl 167 169
[2024-05-12 22:52:53] nn step 33700, lr: 0.059049.
	loss_policy_0: 0.16291
	accuracy_policy_0: 0.67092
	loss_value_0: 0.16521
	loss_policy_1: 0.03829
	accuracy_policy_1: 0.6234
	loss_value_1: 0.03457
	loss_reward_1: 0.00565
	loss_policy_2: 0.04174
	accuracy_policy_2: 0.59268
	loss_value_2: 0.03588
	loss_reward_2: 0.00539
	loss_policy_3: 0.04474
	accuracy_policy_3: 0.56996
	loss_value_3: 0.03719
	loss_reward_3: 0.00582
	loss_policy_4: 0.04769
	accuracy_policy_4: 0.54771
	loss_value_4: 0.03843
	loss_reward_4: 0.00625
	loss_policy_5: 0.05036
	accuracy_policy_5: 0.52785
	loss_value_5: 0.03967
	loss_reward_5: 0.00736
	loss_policy: 0.38573
	loss_value: 0.35095
	loss_reward: 0.03047
[2024-05-12 22:53:44] nn step 33800, lr: 0.059049.
	loss_policy_0: 0.14002
	accuracy_policy_0: 0.72273
	loss_value_0: 0.16918
	loss_policy_1: 0.03523
	accuracy_policy_1: 0.66125
	loss_value_1: 0.03537
	loss_reward_1: 0.00572
	loss_policy_2: 0.03959
	accuracy_policy_2: 0.62568
	loss_value_2: 0.03693
	loss_reward_2: 0.00516
	loss_policy_3: 0.04293
	accuracy_policy_3: 0.60104
	loss_value_3: 0.03832
	loss_reward_3: 0.0058
	loss_policy_4: 0.04589
	accuracy_policy_4: 0.58088
	loss_value_4: 0.03946
	loss_reward_4: 0.00642
	loss_policy_5: 0.04867
	accuracy_policy_5: 0.55932
	loss_value_5: 0.04078
	loss_reward_5: 0.00747
	loss_policy: 0.35232
	loss_value: 0.36004
	loss_reward: 0.03057
Optimization_Done 33800
[2024-05-12 22:56:02] [command] train weight_iter_33800.pkl 168 170
[2024-05-12 22:56:52] nn step 33900, lr: 0.059049.
	loss_policy_0: 0.16892
	accuracy_policy_0: 0.68781
	loss_value_0: 0.17475
	loss_policy_1: 0.03977
	accuracy_policy_1: 0.64064
	loss_value_1: 0.03672
	loss_reward_1: 0.00567
	loss_policy_2: 0.04366
	accuracy_policy_2: 0.60646
	loss_value_2: 0.03831
	loss_reward_2: 0.00544
	loss_policy_3: 0.0466
	accuracy_policy_3: 0.58613
	loss_value_3: 0.03966
	loss_reward_3: 0.00589
	loss_policy_4: 0.04952
	accuracy_policy_4: 0.56678
	loss_value_4: 0.04114
	loss_reward_4: 0.00657
	loss_policy_5: 0.05254
	accuracy_policy_5: 0.54152
	loss_value_5: 0.04251
	loss_reward_5: 0.00767
	loss_policy: 0.40101
	loss_value: 0.37309
	loss_reward: 0.03124
[2024-05-12 22:57:43] nn step 34000, lr: 0.059049.
	loss_policy_0: 0.15009
	accuracy_policy_0: 0.73027
	loss_value_0: 0.17924
	loss_policy_1: 0.03688
	accuracy_policy_1: 0.66998
	loss_value_1: 0.03775
	loss_reward_1: 0.00579
	loss_policy_2: 0.0414
	accuracy_policy_2: 0.63758
	loss_value_2: 0.03931
	loss_reward_2: 0.00557
	loss_policy_3: 0.04464
	accuracy_policy_3: 0.61457
	loss_value_3: 0.04077
	loss_reward_3: 0.00627
	loss_policy_4: 0.04802
	accuracy_policy_4: 0.59287
	loss_value_4: 0.04223
	loss_reward_4: 0.00679
	loss_policy_5: 0.05171
	accuracy_policy_5: 0.5682
	loss_value_5: 0.04361
	loss_reward_5: 0.00803
	loss_policy: 0.37274
	loss_value: 0.38292
	loss_reward: 0.03245
Optimization_Done 34000
[2024-05-12 22:59:55] [command] train weight_iter_34000.pkl 169 171
[2024-05-12 23:00:47] nn step 34100, lr: 0.059049.
	loss_policy_0: 0.16042
	accuracy_policy_0: 0.69541
	loss_value_0: 0.16018
	loss_policy_1: 0.0375
	accuracy_policy_1: 0.64748
	loss_value_1: 0.03375
	loss_reward_1: 0.00516
	loss_policy_2: 0.04111
	accuracy_policy_2: 0.61811
	loss_value_2: 0.03512
	loss_reward_2: 0.00498
	loss_policy_3: 0.04428
	accuracy_policy_3: 0.59332
	loss_value_3: 0.03649
	loss_reward_3: 0.00542
	loss_policy_4: 0.04671
	accuracy_policy_4: 0.57924
	loss_value_4: 0.03775
	loss_reward_4: 0.00598
	loss_policy_5: 0.04983
	accuracy_policy_5: 0.55326
	loss_value_5: 0.03894
	loss_reward_5: 0.00692
	loss_policy: 0.37986
	loss_value: 0.34223
	loss_reward: 0.02847
[2024-05-12 23:01:37] nn step 34200, lr: 0.059049.
	loss_policy_0: 0.13729
	accuracy_policy_0: 0.73934
	loss_value_0: 0.15934
	loss_policy_1: 0.0345
	accuracy_policy_1: 0.67754
	loss_value_1: 0.03344
	loss_reward_1: 0.00527
	loss_policy_2: 0.03791
	accuracy_policy_2: 0.64979
	loss_value_2: 0.03467
	loss_reward_2: 0.00494
	loss_policy_3: 0.04168
	accuracy_policy_3: 0.62213
	loss_value_3: 0.03592
	loss_reward_3: 0.00546
	loss_policy_4: 0.04479
	accuracy_policy_4: 0.60426
	loss_value_4: 0.03706
	loss_reward_4: 0.00594
	loss_policy_5: 0.04732
	accuracy_policy_5: 0.58326
	loss_value_5: 0.03833
	loss_reward_5: 0.00711
	loss_policy: 0.34349
	loss_value: 0.33875
	loss_reward: 0.02871
Optimization_Done 34200
[2024-05-12 23:03:39] [command] train weight_iter_34200.pkl 170 172
[2024-05-12 23:04:29] nn step 34300, lr: 0.059049.
	loss_policy_0: 0.14124
	accuracy_policy_0: 0.70637
	loss_value_0: 0.1512
	loss_policy_1: 0.03377
	accuracy_policy_1: 0.65283
	loss_value_1: 0.03175
	loss_reward_1: 0.00454
	loss_policy_2: 0.0369
	accuracy_policy_2: 0.62475
	loss_value_2: 0.03305
	loss_reward_2: 0.00434
	loss_policy_3: 0.03997
	accuracy_policy_3: 0.59842
	loss_value_3: 0.0343
	loss_reward_3: 0.00476
	loss_policy_4: 0.04265
	accuracy_policy_4: 0.58051
	loss_value_4: 0.03549
	loss_reward_4: 0.00527
	loss_policy_5: 0.04493
	accuracy_policy_5: 0.55834
	loss_value_5: 0.03658
	loss_reward_5: 0.00603
	loss_policy: 0.33947
	loss_value: 0.32237
	loss_reward: 0.02494
[2024-05-12 23:05:15] nn step 34400, lr: 0.059049.
	loss_policy_0: 0.11927
	accuracy_policy_0: 0.75113
	loss_value_0: 0.15115
	loss_policy_1: 0.03035
	accuracy_policy_1: 0.68551
	loss_value_1: 0.03192
	loss_reward_1: 0.00447
	loss_policy_2: 0.0343
	accuracy_policy_2: 0.65438
	loss_value_2: 0.03292
	loss_reward_2: 0.00443
	loss_policy_3: 0.03706
	accuracy_policy_3: 0.63504
	loss_value_3: 0.03412
	loss_reward_3: 0.00488
	loss_policy_4: 0.03979
	accuracy_policy_4: 0.60812
	loss_value_4: 0.03526
	loss_reward_4: 0.00531
	loss_policy_5: 0.04291
	accuracy_policy_5: 0.57863
	loss_value_5: 0.0365
	loss_reward_5: 0.00636
	loss_policy: 0.30368
	loss_value: 0.32186
	loss_reward: 0.02545
Optimization_Done 34400
[2024-05-12 23:07:09] [command] train weight_iter_34400.pkl 171 173
[2024-05-12 23:08:02] nn step 34500, lr: 0.059049.
	loss_policy_0: 0.16878
	accuracy_policy_0: 0.64885
	loss_value_0: 0.1467
	loss_policy_1: 0.03801
	accuracy_policy_1: 0.60709
	loss_value_1: 0.03065
	loss_reward_1: 0.00441
	loss_policy_2: 0.04086
	accuracy_policy_2: 0.5793
	loss_value_2: 0.03183
	loss_reward_2: 0.00442
	loss_policy_3: 0.04337
	accuracy_policy_3: 0.56033
	loss_value_3: 0.03286
	loss_reward_3: 0.00481
	loss_policy_4: 0.04543
	accuracy_policy_4: 0.53809
	loss_value_4: 0.03387
	loss_reward_4: 0.00508
	loss_policy_5: 0.04772
	accuracy_policy_5: 0.51969
	loss_value_5: 0.03494
	loss_reward_5: 0.00592
	loss_policy: 0.38417
	loss_value: 0.31085
	loss_reward: 0.02463
[2024-05-12 23:08:54] nn step 34600, lr: 0.059049.
	loss_policy_0: 0.14553
	accuracy_policy_0: 0.70643
	loss_value_0: 0.14968
	loss_policy_1: 0.03463
	accuracy_policy_1: 0.65336
	loss_value_1: 0.03124
	loss_reward_1: 0.00468
	loss_policy_2: 0.038
	accuracy_policy_2: 0.62482
	loss_value_2: 0.03245
	loss_reward_2: 0.00445
	loss_policy_3: 0.04095
	accuracy_policy_3: 0.60072
	loss_value_3: 0.03361
	loss_reward_3: 0.00498
	loss_policy_4: 0.04354
	accuracy_policy_4: 0.58105
	loss_value_4: 0.03481
	loss_reward_4: 0.00532
	loss_policy_5: 0.04586
	accuracy_policy_5: 0.56057
	loss_value_5: 0.03599
	loss_reward_5: 0.00622
	loss_policy: 0.34851
	loss_value: 0.31777
	loss_reward: 0.02566
Optimization_Done 34600
[2024-05-12 23:11:07] [command] train weight_iter_34600.pkl 172 174
[2024-05-12 23:12:00] nn step 34700, lr: 0.059049.
	loss_policy_0: 0.19978
	accuracy_policy_0: 0.61484
	loss_value_0: 0.17365
	loss_policy_1: 0.0445
	accuracy_policy_1: 0.57127
	loss_value_1: 0.03614
	loss_reward_1: 0.00516
	loss_policy_2: 0.04759
	accuracy_policy_2: 0.54371
	loss_value_2: 0.03762
	loss_reward_2: 0.00517
	loss_policy_3: 0.05001
	accuracy_policy_3: 0.52736
	loss_value_3: 0.039
	loss_reward_3: 0.00568
	loss_policy_4: 0.0524
	accuracy_policy_4: 0.50984
	loss_value_4: 0.04018
	loss_reward_4: 0.00599
	loss_policy_5: 0.05483
	accuracy_policy_5: 0.49215
	loss_value_5: 0.04147
	loss_reward_5: 0.00691
	loss_policy: 0.44911
	loss_value: 0.36805
	loss_reward: 0.02891
[2024-05-12 23:12:52] nn step 34800, lr: 0.059049.
	loss_policy_0: 0.1517
	accuracy_policy_0: 0.6743
	loss_value_0: 0.15531
	loss_policy_1: 0.03553
	accuracy_policy_1: 0.62307
	loss_value_1: 0.03236
	loss_reward_1: 0.00481
	loss_policy_2: 0.03909
	accuracy_policy_2: 0.59084
	loss_value_2: 0.03372
	loss_reward_2: 0.0048
	loss_policy_3: 0.04228
	accuracy_policy_3: 0.56354
	loss_value_3: 0.03486
	loss_reward_3: 0.00518
	loss_policy_4: 0.04426
	accuracy_policy_4: 0.54908
	loss_value_4: 0.03594
	loss_reward_4: 0.00539
	loss_policy_5: 0.04674
	accuracy_policy_5: 0.52904
	loss_value_5: 0.03714
	loss_reward_5: 0.00626
	loss_policy: 0.3596
	loss_value: 0.32932
	loss_reward: 0.02644
Optimization_Done 34800
[2024-05-12 23:15:09] [command] train weight_iter_34800.pkl 173 175
[2024-05-12 23:16:01] nn step 34900, lr: 0.059049.
	loss_policy_0: 0.19958
	accuracy_policy_0: 0.63762
	loss_value_0: 0.18582
	loss_policy_1: 0.04525
	accuracy_policy_1: 0.59037
	loss_value_1: 0.03863
	loss_reward_1: 0.00621
	loss_policy_2: 0.04886
	accuracy_policy_2: 0.56035
	loss_value_2: 0.04019
	loss_reward_2: 0.00617
	loss_policy_3: 0.05234
	accuracy_policy_3: 0.53002
	loss_value_3: 0.04158
	loss_reward_3: 0.00655
	loss_policy_4: 0.0551
	accuracy_policy_4: 0.51283
	loss_value_4: 0.04294
	loss_reward_4: 0.00712
	loss_policy_5: 0.05776
	accuracy_policy_5: 0.49205
	loss_value_5: 0.04432
	loss_reward_5: 0.0081
	loss_policy: 0.45888
	loss_value: 0.39348
	loss_reward: 0.03415
[2024-05-12 23:16:53] nn step 35000, lr: 0.059049.
	loss_policy_0: 0.17644
	accuracy_policy_0: 0.68893
	loss_value_0: 0.18694
	loss_policy_1: 0.04183
	accuracy_policy_1: 0.62787
	loss_value_1: 0.03905
	loss_reward_1: 0.00625
	loss_policy_2: 0.04645
	accuracy_policy_2: 0.59107
	loss_value_2: 0.04048
	loss_reward_2: 0.00626
	loss_policy_3: 0.05004
	accuracy_policy_3: 0.56557
	loss_value_3: 0.04198
	loss_reward_3: 0.00666
	loss_policy_4: 0.05283
	accuracy_policy_4: 0.54697
	loss_value_4: 0.04329
	loss_reward_4: 0.00702
	loss_policy_5: 0.05572
	accuracy_policy_5: 0.52668
	loss_value_5: 0.04463
	loss_reward_5: 0.00801
	loss_policy: 0.42331
	loss_value: 0.39637
	loss_reward: 0.0342
Optimization_Done 35000
[2024-05-12 23:18:46] [command] train weight_iter_35000.pkl 174 176
[2024-05-12 23:19:39] nn step 35100, lr: 0.059049.
	loss_policy_0: 0.17875
	accuracy_policy_0: 0.65529
	loss_value_0: 0.19158
	loss_policy_1: 0.0415
	accuracy_policy_1: 0.60416
	loss_value_1: 0.03988
	loss_reward_1: 0.00602
	loss_policy_2: 0.04541
	accuracy_policy_2: 0.57146
	loss_value_2: 0.04115
	loss_reward_2: 0.00592
	loss_policy_3: 0.04831
	accuracy_policy_3: 0.54449
	loss_value_3: 0.04254
	loss_reward_3: 0.00642
	loss_policy_4: 0.05085
	accuracy_policy_4: 0.52178
	loss_value_4: 0.04388
	loss_reward_4: 0.00679
	loss_policy_5: 0.05391
	accuracy_policy_5: 0.49906
	loss_value_5: 0.045
	loss_reward_5: 0.00797
	loss_policy: 0.41874
	loss_value: 0.40403
	loss_reward: 0.03313
[2024-05-12 23:20:32] nn step 35200, lr: 0.059049.
	loss_policy_0: 0.15647
	accuracy_policy_0: 0.70107
	loss_value_0: 0.18757
	loss_policy_1: 0.03823
	accuracy_policy_1: 0.63623
	loss_value_1: 0.03926
	loss_reward_1: 0.00615
	loss_policy_2: 0.04211
	accuracy_policy_2: 0.6065
	loss_value_2: 0.04054
	loss_reward_2: 0.00605
	loss_policy_3: 0.04576
	accuracy_policy_3: 0.575
	loss_value_3: 0.04182
	loss_reward_3: 0.00644
	loss_policy_4: 0.04899
	accuracy_policy_4: 0.55346
	loss_value_4: 0.04302
	loss_reward_4: 0.00687
	loss_policy_5: 0.05177
	accuracy_policy_5: 0.52541
	loss_value_5: 0.04434
	loss_reward_5: 0.00799
	loss_policy: 0.38333
	loss_value: 0.39655
	loss_reward: 0.0335
Optimization_Done 35200
[2024-05-12 23:22:52] [command] train weight_iter_35200.pkl 175 177
[2024-05-12 23:23:44] nn step 35300, lr: 0.059049.
	loss_policy_0: 0.17345
	accuracy_policy_0: 0.67945
	loss_value_0: 0.17885
	loss_policy_1: 0.04083
	accuracy_policy_1: 0.63123
	loss_value_1: 0.03742
	loss_reward_1: 0.00559
	loss_policy_2: 0.04483
	accuracy_policy_2: 0.59752
	loss_value_2: 0.03897
	loss_reward_2: 0.00541
	loss_policy_3: 0.04786
	accuracy_policy_3: 0.57414
	loss_value_3: 0.04038
	loss_reward_3: 0.00602
	loss_policy_4: 0.05067
	accuracy_policy_4: 0.55352
	loss_value_4: 0.04172
	loss_reward_4: 0.00648
	loss_policy_5: 0.05327
	accuracy_policy_5: 0.53094
	loss_value_5: 0.04296
	loss_reward_5: 0.00747
	loss_policy: 0.41091
	loss_value: 0.3803
	loss_reward: 0.03097
[2024-05-12 23:24:37] nn step 35400, lr: 0.059049.
	loss_policy_0: 0.14216
	accuracy_policy_0: 0.72154
	loss_value_0: 0.17018
	loss_policy_1: 0.03553
	accuracy_policy_1: 0.66029
	loss_value_1: 0.03564
	loss_reward_1: 0.00556
	loss_policy_2: 0.03951
	accuracy_policy_2: 0.63213
	loss_value_2: 0.0371
	loss_reward_2: 0.00521
	loss_policy_3: 0.04298
	accuracy_policy_3: 0.60125
	loss_value_3: 0.03831
	loss_reward_3: 0.00576
	loss_policy_4: 0.04582
	accuracy_policy_4: 0.58252
	loss_value_4: 0.03951
	loss_reward_4: 0.00641
	loss_policy_5: 0.04884
	accuracy_policy_5: 0.55592
	loss_value_5: 0.04076
	loss_reward_5: 0.00736
	loss_policy: 0.35484
	loss_value: 0.36149
	loss_reward: 0.03029
Optimization_Done 35400
[2024-05-12 23:26:52] [command] train weight_iter_35400.pkl 176 178
[2024-05-12 23:27:43] nn step 35500, lr: 0.059049.
	loss_policy_0: 0.14534
	accuracy_policy_0: 0.7067
	loss_value_0: 0.16272
	loss_policy_1: 0.03478
	accuracy_policy_1: 0.6609
	loss_value_1: 0.03421
	loss_reward_1: 0.00478
	loss_policy_2: 0.03834
	accuracy_policy_2: 0.63002
	loss_value_2: 0.03554
	loss_reward_2: 0.00474
	loss_policy_3: 0.04124
	accuracy_policy_3: 0.60877
	loss_value_3: 0.03686
	loss_reward_3: 0.00512
	loss_policy_4: 0.04407
	accuracy_policy_4: 0.58695
	loss_value_4: 0.03822
	loss_reward_4: 0.00562
	loss_policy_5: 0.04676
	accuracy_policy_5: 0.56598
	loss_value_5: 0.03931
	loss_reward_5: 0.00631
	loss_policy: 0.35054
	loss_value: 0.34686
	loss_reward: 0.02656
[2024-05-12 23:28:36] nn step 35600, lr: 0.059049.
	loss_policy_0: 0.12592
	accuracy_policy_0: 0.74688
	loss_value_0: 0.1642
	loss_policy_1: 0.03196
	accuracy_policy_1: 0.69018
	loss_value_1: 0.03426
	loss_reward_1: 0.00486
	loss_policy_2: 0.03578
	accuracy_policy_2: 0.6573
	loss_value_2: 0.03579
	loss_reward_2: 0.0047
	loss_policy_3: 0.03911
	accuracy_policy_3: 0.63584
	loss_value_3: 0.03715
	loss_reward_3: 0.0051
	loss_policy_4: 0.04161
	accuracy_policy_4: 0.61689
	loss_value_4: 0.0384
	loss_reward_4: 0.00566
	loss_policy_5: 0.04494
	accuracy_policy_5: 0.59197
	loss_value_5: 0.03967
	loss_reward_5: 0.00671
	loss_policy: 0.31932
	loss_value: 0.34946
	loss_reward: 0.02704
Optimization_Done 35600
[2024-05-12 23:30:51] [command] train weight_iter_35600.pkl 177 179
[2024-05-12 23:31:53] nn step 35700, lr: 0.059049.
	loss_policy_0: 0.15963
	accuracy_policy_0: 0.6899
	loss_value_0: 0.16983
	loss_policy_1: 0.03721
	accuracy_policy_1: 0.64533
	loss_value_1: 0.03581
	loss_reward_1: 0.0052
	loss_policy_2: 0.04151
	accuracy_policy_2: 0.60869
	loss_value_2: 0.03716
	loss_reward_2: 0.00494
	loss_policy_3: 0.04487
	accuracy_policy_3: 0.5865
	loss_value_3: 0.03858
	loss_reward_3: 0.00528
	loss_policy_4: 0.04794
	accuracy_policy_4: 0.56563
	loss_value_4: 0.03988
	loss_reward_4: 0.00594
	loss_policy_5: 0.05077
	accuracy_policy_5: 0.54254
	loss_value_5: 0.04113
	loss_reward_5: 0.00673
	loss_policy: 0.38194
	loss_value: 0.36239
	loss_reward: 0.02809
[2024-05-12 23:32:49] nn step 35800, lr: 0.059049.
	loss_policy_0: 0.13546
	accuracy_policy_0: 0.74205
	loss_value_0: 0.1727
	loss_policy_1: 0.03403
	accuracy_policy_1: 0.68279
	loss_value_1: 0.03628
	loss_reward_1: 0.00511
	loss_policy_2: 0.03864
	accuracy_policy_2: 0.64736
	loss_value_2: 0.03788
	loss_reward_2: 0.00504
	loss_policy_3: 0.04185
	accuracy_policy_3: 0.62721
	loss_value_3: 0.0394
	loss_reward_3: 0.00537
	loss_policy_4: 0.04539
	accuracy_policy_4: 0.60227
	loss_value_4: 0.0406
	loss_reward_4: 0.00597
	loss_policy_5: 0.04912
	accuracy_policy_5: 0.57623
	loss_value_5: 0.04172
	loss_reward_5: 0.0071
	loss_policy: 0.34448
	loss_value: 0.36858
	loss_reward: 0.0286
Optimization_Done 35800
[2024-05-12 23:35:02] [command] train weight_iter_35800.pkl 178 180
[2024-05-12 23:35:54] nn step 35900, lr: 0.059049.
	loss_policy_0: 0.13466
	accuracy_policy_0: 0.70146
	loss_value_0: 0.15814
	loss_policy_1: 0.03219
	accuracy_policy_1: 0.64758
	loss_value_1: 0.03327
	loss_reward_1: 0.00469
	loss_policy_2: 0.036
	accuracy_policy_2: 0.61719
	loss_value_2: 0.0346
	loss_reward_2: 0.00465
	loss_policy_3: 0.03902
	accuracy_policy_3: 0.58961
	loss_value_3: 0.03576
	loss_reward_3: 0.00486
	loss_policy_4: 0.04202
	accuracy_policy_4: 0.56684
	loss_value_4: 0.03684
	loss_reward_4: 0.00517
	loss_policy_5: 0.04466
	accuracy_policy_5: 0.54367
	loss_value_5: 0.03791
	loss_reward_5: 0.00613
	loss_policy: 0.32855
	loss_value: 0.3365
	loss_reward: 0.02551
[2024-05-12 23:36:55] nn step 36000, lr: 0.059049.
	loss_policy_0: 0.11147
	accuracy_policy_0: 0.7498
	loss_value_0: 0.15508
	loss_policy_1: 0.0288
	accuracy_policy_1: 0.68158
	loss_value_1: 0.03246
	loss_reward_1: 0.00453
	loss_policy_2: 0.03254
	accuracy_policy_2: 0.64871
	loss_value_2: 0.03361
	loss_reward_2: 0.00442
	loss_policy_3: 0.0356
	accuracy_policy_3: 0.62236
	loss_value_3: 0.03479
	loss_reward_3: 0.00483
	loss_policy_4: 0.03856
	accuracy_policy_4: 0.59668
	loss_value_4: 0.03588
	loss_reward_4: 0.00516
	loss_policy_5: 0.04196
	accuracy_policy_5: 0.56961
	loss_value_5: 0.03711
	loss_reward_5: 0.00597
	loss_policy: 0.28893
	loss_value: 0.32893
	loss_reward: 0.02491
Optimization_Done 36000
[2024-05-12 23:39:11] [command] train weight_iter_36000.pkl 179 181
[2024-05-12 23:40:04] nn step 36100, lr: 0.059049.
	loss_policy_0: 0.15262
	accuracy_policy_0: 0.66178
	loss_value_0: 0.15563
	loss_policy_1: 0.03486
	accuracy_policy_1: 0.6201
	loss_value_1: 0.03266
	loss_reward_1: 0.00428
	loss_policy_2: 0.0381
	accuracy_policy_2: 0.58805
	loss_value_2: 0.03393
	loss_reward_2: 0.00438
	loss_policy_3: 0.04099
	accuracy_policy_3: 0.56125
	loss_value_3: 0.03503
	loss_reward_3: 0.00463
	loss_policy_4: 0.04377
	accuracy_policy_4: 0.54076
	loss_value_4: 0.03605
	loss_reward_4: 0.00497
	loss_policy_5: 0.04628
	accuracy_policy_5: 0.51408
	loss_value_5: 0.03705
	loss_reward_5: 0.00576
	loss_policy: 0.35662
	loss_value: 0.33035
	loss_reward: 0.02402
[2024-05-12 23:41:06] nn step 36200, lr: 0.059049.
	loss_policy_0: 0.11801
	accuracy_policy_0: 0.72727
	loss_value_0: 0.15001
	loss_policy_1: 0.03019
	accuracy_policy_1: 0.66223
	loss_value_1: 0.03153
	loss_reward_1: 0.0043
	loss_policy_2: 0.03344
	accuracy_policy_2: 0.62994
	loss_value_2: 0.03275
	loss_reward_2: 0.00423
	loss_policy_3: 0.03658
	accuracy_policy_3: 0.59936
	loss_value_3: 0.03379
	loss_reward_3: 0.00455
	loss_policy_4: 0.03933
	accuracy_policy_4: 0.57178
	loss_value_4: 0.03472
	loss_reward_4: 0.00487
	loss_policy_5: 0.04225
	accuracy_policy_5: 0.54723
	loss_value_5: 0.03581
	loss_reward_5: 0.00581
	loss_policy: 0.2998
	loss_value: 0.31862
	loss_reward: 0.02376
Optimization_Done 36200
[2024-05-12 23:43:10] [command] train weight_iter_36200.pkl 180 182
[2024-05-12 23:44:11] nn step 36300, lr: 0.059049.
	loss_policy_0: 0.14828
	accuracy_policy_0: 0.65004
	loss_value_0: 0.14246
	loss_policy_1: 0.03392
	accuracy_policy_1: 0.6043
	loss_value_1: 0.02983
	loss_reward_1: 0.00392
	loss_policy_2: 0.03679
	accuracy_policy_2: 0.57762
	loss_value_2: 0.03112
	loss_reward_2: 0.00401
	loss_policy_3: 0.03949
	accuracy_policy_3: 0.55348
	loss_value_3: 0.03214
	loss_reward_3: 0.0044
	loss_policy_4: 0.04136
	accuracy_policy_4: 0.534
	loss_value_4: 0.03328
	loss_reward_4: 0.00478
	loss_policy_5: 0.04359
	accuracy_policy_5: 0.51656
	loss_value_5: 0.03426
	loss_reward_5: 0.0053
	loss_policy: 0.34343
	loss_value: 0.30309
	loss_reward: 0.02241
[2024-05-12 23:45:16] nn step 36400, lr: 0.059049.
	loss_policy_0: 0.12121
	accuracy_policy_0: 0.71619
	loss_value_0: 0.14171
	loss_policy_1: 0.03018
	accuracy_policy_1: 0.65369
	loss_value_1: 0.02964
	loss_reward_1: 0.004
	loss_policy_2: 0.03333
	accuracy_policy_2: 0.6225
	loss_value_2: 0.03083
	loss_reward_2: 0.00411
	loss_policy_3: 0.03622
	accuracy_policy_3: 0.59527
	loss_value_3: 0.03188
	loss_reward_3: 0.00426
	loss_policy_4: 0.03835
	accuracy_policy_4: 0.57705
	loss_value_4: 0.03307
	loss_reward_4: 0.0047
	loss_policy_5: 0.04109
	accuracy_policy_5: 0.55088
	loss_value_5: 0.03416
	loss_reward_5: 0.0054
	loss_policy: 0.30037
	loss_value: 0.3013
	loss_reward: 0.02248
Optimization_Done 36400
[2024-05-12 23:47:34] [command] train weight_iter_36400.pkl 181 183
[2024-05-12 23:48:35] nn step 36500, lr: 0.059049.
	loss_policy_0: 0.15693
	accuracy_policy_0: 0.67527
	loss_value_0: 0.16269
	loss_policy_1: 0.03621
	accuracy_policy_1: 0.62984
	loss_value_1: 0.03416
	loss_reward_1: 0.00491
	loss_policy_2: 0.03958
	accuracy_policy_2: 0.5982
	loss_value_2: 0.03561
	loss_reward_2: 0.00491
	loss_policy_3: 0.04201
	accuracy_policy_3: 0.58104
	loss_value_3: 0.03704
	loss_reward_3: 0.00525
	loss_policy_4: 0.04462
	accuracy_policy_4: 0.56025
	loss_value_4: 0.03833
	loss_reward_4: 0.00586
	loss_policy_5: 0.04774
	accuracy_policy_5: 0.53773
	loss_value_5: 0.03955
	loss_reward_5: 0.00689
	loss_policy: 0.3671
	loss_value: 0.34738
	loss_reward: 0.02782
[2024-05-12 23:49:36] nn step 36600, lr: 0.059049.
	loss_policy_0: 0.13748
	accuracy_policy_0: 0.72311
	loss_value_0: 0.16487
	loss_policy_1: 0.0338
	accuracy_policy_1: 0.66283
	loss_value_1: 0.03447
	loss_reward_1: 0.00499
	loss_policy_2: 0.03754
	accuracy_policy_2: 0.6318
	loss_value_2: 0.03597
	loss_reward_2: 0.00487
	loss_policy_3: 0.04049
	accuracy_policy_3: 0.60996
	loss_value_3: 0.03718
	loss_reward_3: 0.0053
	loss_policy_4: 0.04331
	accuracy_policy_4: 0.58795
	loss_value_4: 0.0387
	loss_reward_4: 0.00579
	loss_policy_5: 0.04624
	accuracy_policy_5: 0.56344
	loss_value_5: 0.0399
	loss_reward_5: 0.0069
	loss_policy: 0.33886
	loss_value: 0.3511
	loss_reward: 0.02785
Optimization_Done 36600
[2024-05-12 23:51:42] [command] train weight_iter_36600.pkl 182 184
[2024-05-12 23:52:34] nn step 36700, lr: 0.059049.
	loss_policy_0: 0.15121
	accuracy_policy_0: 0.69365
	loss_value_0: 0.17312
	loss_policy_1: 0.036
	accuracy_policy_1: 0.64281
	loss_value_1: 0.03621
	loss_reward_1: 0.00566
	loss_policy_2: 0.03982
	accuracy_policy_2: 0.6109
	loss_value_2: 0.03768
	loss_reward_2: 0.0054
	loss_policy_3: 0.04312
	accuracy_policy_3: 0.59041
	loss_value_3: 0.03889
	loss_reward_3: 0.00583
	loss_policy_4: 0.04602
	accuracy_policy_4: 0.56789
	loss_value_4: 0.04014
	loss_reward_4: 0.00641
	loss_policy_5: 0.04838
	accuracy_policy_5: 0.55152
	loss_value_5: 0.04142
	loss_reward_5: 0.00754
	loss_policy: 0.36455
	loss_value: 0.36746
	loss_reward: 0.03084
[2024-05-12 23:53:32] nn step 36800, lr: 0.059049.
	loss_policy_0: 0.1334
	accuracy_policy_0: 0.73678
	loss_value_0: 0.17636
	loss_policy_1: 0.03417
	accuracy_policy_1: 0.67002
	loss_value_1: 0.03684
	loss_reward_1: 0.00579
	loss_policy_2: 0.03812
	accuracy_policy_2: 0.63813
	loss_value_2: 0.03826
	loss_reward_2: 0.00551
	loss_policy_3: 0.04173
	accuracy_policy_3: 0.60887
	loss_value_3: 0.03954
	loss_reward_3: 0.00592
	loss_policy_4: 0.04489
	accuracy_policy_4: 0.5893
	loss_value_4: 0.04067
	loss_reward_4: 0.00646
	loss_policy_5: 0.04769
	accuracy_policy_5: 0.57088
	loss_value_5: 0.04217
	loss_reward_5: 0.00768
	loss_policy: 0.34
	loss_value: 0.37386
	loss_reward: 0.03135
Optimization_Done 36800
[2024-05-12 23:55:37] [command] train weight_iter_36800.pkl 183 185
[2024-05-12 23:56:28] nn step 36900, lr: 0.059049.
	loss_policy_0: 0.15817
	accuracy_policy_0: 0.68434
	loss_value_0: 0.17503
	loss_policy_1: 0.03701
	accuracy_policy_1: 0.64332
	loss_value_1: 0.03648
	loss_reward_1: 0.00499
	loss_policy_2: 0.04055
	accuracy_policy_2: 0.61053
	loss_value_2: 0.03776
	loss_reward_2: 0.00485
	loss_policy_3: 0.04322
	accuracy_policy_3: 0.59086
	loss_value_3: 0.03886
	loss_reward_3: 0.00529
	loss_policy_4: 0.04643
	accuracy_policy_4: 0.5709
	loss_value_4: 0.04002
	loss_reward_4: 0.0057
	loss_policy_5: 0.04901
	accuracy_policy_5: 0.54785
	loss_value_5: 0.04113
	loss_reward_5: 0.00666
	loss_policy: 0.37439
	loss_value: 0.36928
	loss_reward: 0.02748
[2024-05-12 23:57:14] nn step 37000, lr: 0.059049.
	loss_policy_0: 0.13896
	accuracy_policy_0: 0.73912
	loss_value_0: 0.18498
	loss_policy_1: 0.03546
	accuracy_policy_1: 0.67299
	loss_value_1: 0.0386
	loss_reward_1: 0.00531
	loss_policy_2: 0.0393
	accuracy_policy_2: 0.64438
	loss_value_2: 0.03988
	loss_reward_2: 0.00509
	loss_policy_3: 0.04273
	accuracy_policy_3: 0.61889
	loss_value_3: 0.04109
	loss_reward_3: 0.00568
	loss_policy_4: 0.04586
	accuracy_policy_4: 0.59943
	loss_value_4: 0.04217
	loss_reward_4: 0.00619
	loss_policy_5: 0.04893
	accuracy_policy_5: 0.57414
	loss_value_5: 0.04339
	loss_reward_5: 0.00713
	loss_policy: 0.35123
	loss_value: 0.3901
	loss_reward: 0.02941
Optimization_Done 37000
[2024-05-12 23:59:19] [command] train weight_iter_37000.pkl 184 186
[2024-05-13 00:00:22] nn step 37100, lr: 0.059049.
	loss_policy_0: 0.16446
	accuracy_policy_0: 0.68266
	loss_value_0: 0.16494
	loss_policy_1: 0.0382
	accuracy_policy_1: 0.63527
	loss_value_1: 0.0344
	loss_reward_1: 0.0046
	loss_policy_2: 0.04186
	accuracy_policy_2: 0.60934
	loss_value_2: 0.03577
	loss_reward_2: 0.00434
	loss_policy_3: 0.04486
	accuracy_policy_3: 0.58674
	loss_value_3: 0.03709
	loss_reward_3: 0.00469
	loss_policy_4: 0.04758
	accuracy_policy_4: 0.56453
	loss_value_4: 0.03816
	loss_reward_4: 0.00506
	loss_policy_5: 0.0502
	accuracy_policy_5: 0.54742
	loss_value_5: 0.03919
	loss_reward_5: 0.00607
	loss_policy: 0.38716
	loss_value: 0.34955
	loss_reward: 0.02476
[2024-05-13 00:01:27] nn step 37200, lr: 0.059049.
	loss_policy_0: 0.1366
	accuracy_policy_0: 0.73197
	loss_value_0: 0.16391
	loss_policy_1: 0.03376
	accuracy_policy_1: 0.67352
	loss_value_1: 0.0342
	loss_reward_1: 0.00449
	loss_policy_2: 0.03766
	accuracy_policy_2: 0.64289
	loss_value_2: 0.03549
	loss_reward_2: 0.00435
	loss_policy_3: 0.04072
	accuracy_policy_3: 0.61965
	loss_value_3: 0.03671
	loss_reward_3: 0.00468
	loss_policy_4: 0.04373
	accuracy_policy_4: 0.60016
	loss_value_4: 0.03782
	loss_reward_4: 0.00502
	loss_policy_5: 0.04686
	accuracy_policy_5: 0.57426
	loss_value_5: 0.03893
	loss_reward_5: 0.00603
	loss_policy: 0.33932
	loss_value: 0.34706
	loss_reward: 0.02457
Optimization_Done 37200
[2024-05-13 00:03:43] [command] train weight_iter_37200.pkl 185 187
[2024-05-13 00:04:49] nn step 37300, lr: 0.059049.
	loss_policy_0: 0.16481
	accuracy_policy_0: 0.68297
	loss_value_0: 0.16155
	loss_policy_1: 0.03812
	accuracy_policy_1: 0.64357
	loss_value_1: 0.03407
	loss_reward_1: 0.0042
	loss_policy_2: 0.04204
	accuracy_policy_2: 0.61279
	loss_value_2: 0.03539
	loss_reward_2: 0.00399
	loss_policy_3: 0.04462
	accuracy_policy_3: 0.5892
	loss_value_3: 0.03671
	loss_reward_3: 0.00442
	loss_policy_4: 0.04761
	accuracy_policy_4: 0.56936
	loss_value_4: 0.03808
	loss_reward_4: 0.00478
	loss_policy_5: 0.05038
	accuracy_policy_5: 0.54748
	loss_value_5: 0.03921
	loss_reward_5: 0.00559
	loss_policy: 0.38758
	loss_value: 0.34502
	loss_reward: 0.02299
[2024-05-13 00:05:52] nn step 37400, lr: 0.059049.
	loss_policy_0: 0.1405
	accuracy_policy_0: 0.73428
	loss_value_0: 0.16386
	loss_policy_1: 0.03414
	accuracy_policy_1: 0.68098
	loss_value_1: 0.03438
	loss_reward_1: 0.0043
	loss_policy_2: 0.03802
	accuracy_policy_2: 0.64857
	loss_value_2: 0.03566
	loss_reward_2: 0.0042
	loss_policy_3: 0.04159
	accuracy_policy_3: 0.62543
	loss_value_3: 0.03683
	loss_reward_3: 0.00447
	loss_policy_4: 0.04467
	accuracy_policy_4: 0.60109
	loss_value_4: 0.03807
	loss_reward_4: 0.00477
	loss_policy_5: 0.04756
	accuracy_policy_5: 0.58059
	loss_value_5: 0.0392
	loss_reward_5: 0.00563
	loss_policy: 0.34648
	loss_value: 0.34799
	loss_reward: 0.02337
Optimization_Done 37400
[2024-05-13 00:07:55] [command] train weight_iter_37400.pkl 186 188
[2024-05-13 00:08:47] nn step 37500, lr: 0.059049.
	loss_policy_0: 0.15368
	accuracy_policy_0: 0.71057
	loss_value_0: 0.16604
	loss_policy_1: 0.03608
	accuracy_policy_1: 0.67006
	loss_value_1: 0.03501
	loss_reward_1: 0.00476
	loss_policy_2: 0.03954
	accuracy_policy_2: 0.63871
	loss_value_2: 0.03626
	loss_reward_2: 0.00446
	loss_policy_3: 0.04292
	accuracy_policy_3: 0.61467
	loss_value_3: 0.03775
	loss_reward_3: 0.00472
	loss_policy_4: 0.04605
	accuracy_policy_4: 0.59307
	loss_value_4: 0.03903
	loss_reward_4: 0.00529
	loss_policy_5: 0.04896
	accuracy_policy_5: 0.57197
	loss_value_5: 0.04039
	loss_reward_5: 0.0061
	loss_policy: 0.36723
	loss_value: 0.35448
	loss_reward: 0.02533
[2024-05-13 00:09:38] nn step 37600, lr: 0.059049.
	loss_policy_0: 0.12929
	accuracy_policy_0: 0.75414
	loss_value_0: 0.16136
	loss_policy_1: 0.03197
	accuracy_policy_1: 0.70057
	loss_value_1: 0.03405
	loss_reward_1: 0.00476
	loss_policy_2: 0.03612
	accuracy_policy_2: 0.66902
	loss_value_2: 0.03551
	loss_reward_2: 0.00438
	loss_policy_3: 0.03917
	accuracy_policy_3: 0.64557
	loss_value_3: 0.0368
	loss_reward_3: 0.0047
	loss_policy_4: 0.04252
	accuracy_policy_4: 0.6216
	loss_value_4: 0.03804
	loss_reward_4: 0.00516
	loss_policy_5: 0.04517
	accuracy_policy_5: 0.60312
	loss_value_5: 0.03924
	loss_reward_5: 0.00609
	loss_policy: 0.32425
	loss_value: 0.34501
	loss_reward: 0.02509
Optimization_Done 37600
[2024-05-13 00:11:54] [command] train weight_iter_37600.pkl 187 189
[2024-05-13 00:12:45] nn step 37700, lr: 0.059049.
	loss_policy_0: 0.15849
	accuracy_policy_0: 0.69957
	loss_value_0: 0.17621
	loss_policy_1: 0.03747
	accuracy_policy_1: 0.65482
	loss_value_1: 0.03684
	loss_reward_1: 0.00558
	loss_policy_2: 0.04123
	accuracy_policy_2: 0.62369
	loss_value_2: 0.0384
	loss_reward_2: 0.00538
	loss_policy_3: 0.04376
	accuracy_policy_3: 0.60209
	loss_value_3: 0.03982
	loss_reward_3: 0.00565
	loss_policy_4: 0.04652
	accuracy_policy_4: 0.58096
	loss_value_4: 0.0411
	loss_reward_4: 0.00594
	loss_policy_5: 0.04937
	accuracy_policy_5: 0.56002
	loss_value_5: 0.04256
	loss_reward_5: 0.00705
	loss_policy: 0.37684
	loss_value: 0.37493
	loss_reward: 0.0296
[2024-05-13 00:13:33] nn step 37800, lr: 0.059049.
	loss_policy_0: 0.12864
	accuracy_policy_0: 0.74865
	loss_value_0: 0.17052
	loss_policy_1: 0.03235
	accuracy_policy_1: 0.6934
	loss_value_1: 0.03603
	loss_reward_1: 0.00528
	loss_policy_2: 0.03613
	accuracy_policy_2: 0.66148
	loss_value_2: 0.0375
	loss_reward_2: 0.00502
	loss_policy_3: 0.03934
	accuracy_policy_3: 0.64008
	loss_value_3: 0.03876
	loss_reward_3: 0.00531
	loss_policy_4: 0.04267
	accuracy_policy_4: 0.61211
	loss_value_4: 0.03998
	loss_reward_4: 0.00594
	loss_policy_5: 0.04585
	accuracy_policy_5: 0.58766
	loss_value_5: 0.04114
	loss_reward_5: 0.00697
	loss_policy: 0.32498
	loss_value: 0.36394
	loss_reward: 0.02851
Optimization_Done 37800
[2024-05-13 00:15:48] [command] train weight_iter_37800.pkl 188 190
[2024-05-13 00:16:40] nn step 37900, lr: 0.059049.
	loss_policy_0: 0.1507
	accuracy_policy_0: 0.70859
	loss_value_0: 0.16917
	loss_policy_1: 0.03636
	accuracy_policy_1: 0.6567
	loss_value_1: 0.03565
	loss_reward_1: 0.00516
	loss_policy_2: 0.03991
	accuracy_policy_2: 0.63021
	loss_value_2: 0.03719
	loss_reward_2: 0.00502
	loss_policy_3: 0.04287
	accuracy_policy_3: 0.60629
	loss_value_3: 0.03863
	loss_reward_3: 0.00539
	loss_policy_4: 0.04576
	accuracy_policy_4: 0.58027
	loss_value_4: 0.04002
	loss_reward_4: 0.00587
	loss_policy_5: 0.04895
	accuracy_policy_5: 0.55654
	loss_value_5: 0.04141
	loss_reward_5: 0.00705
	loss_policy: 0.36455
	loss_value: 0.36207
	loss_reward: 0.02849
[2024-05-13 00:17:30] nn step 38000, lr: 0.059049.
	loss_policy_0: 0.12097
	accuracy_policy_0: 0.75238
	loss_value_0: 0.16007
	loss_policy_1: 0.03087
	accuracy_policy_1: 0.69264
	loss_value_1: 0.03376
	loss_reward_1: 0.00511
	loss_policy_2: 0.03442
	accuracy_policy_2: 0.66035
	loss_value_2: 0.03505
	loss_reward_2: 0.00488
	loss_policy_3: 0.03747
	accuracy_policy_3: 0.63727
	loss_value_3: 0.03627
	loss_reward_3: 0.0052
	loss_policy_4: 0.04068
	accuracy_policy_4: 0.60992
	loss_value_4: 0.03746
	loss_reward_4: 0.00549
	loss_policy_5: 0.04377
	accuracy_policy_5: 0.58697
	loss_value_5: 0.03865
	loss_reward_5: 0.00664
	loss_policy: 0.30818
	loss_value: 0.34125
	loss_reward: 0.02731
Optimization_Done 38000
[2024-05-13 00:19:33] [command] train weight_iter_38000.pkl 189 191
[2024-05-13 00:20:25] nn step 38100, lr: 0.059049.
	loss_policy_0: 0.15614
	accuracy_policy_0: 0.70102
	loss_value_0: 0.17223
	loss_policy_1: 0.03653
	accuracy_policy_1: 0.65678
	loss_value_1: 0.03636
	loss_reward_1: 0.00547
	loss_policy_2: 0.04075
	accuracy_policy_2: 0.62209
	loss_value_2: 0.03801
	loss_reward_2: 0.00527
	loss_policy_3: 0.04373
	accuracy_policy_3: 0.59854
	loss_value_3: 0.03963
	loss_reward_3: 0.00564
	loss_policy_4: 0.04687
	accuracy_policy_4: 0.58029
	loss_value_4: 0.04108
	loss_reward_4: 0.00614
	loss_policy_5: 0.04963
	accuracy_policy_5: 0.56123
	loss_value_5: 0.04238
	loss_reward_5: 0.00728
	loss_policy: 0.37365
	loss_value: 0.3697
	loss_reward: 0.02981
[2024-05-13 00:21:15] nn step 38200, lr: 0.059049.
	loss_policy_0: 0.11765
	accuracy_policy_0: 0.74918
	loss_value_0: 0.15638
	loss_policy_1: 0.02964
	accuracy_policy_1: 0.68777
	loss_value_1: 0.03308
	loss_reward_1: 0.00492
	loss_policy_2: 0.03342
	accuracy_policy_2: 0.65311
	loss_value_2: 0.03457
	loss_reward_2: 0.00489
	loss_policy_3: 0.03661
	accuracy_policy_3: 0.62867
	loss_value_3: 0.03585
	loss_reward_3: 0.00531
	loss_policy_4: 0.03974
	accuracy_policy_4: 0.60734
	loss_value_4: 0.03721
	loss_reward_4: 0.00559
	loss_policy_5: 0.04217
	accuracy_policy_5: 0.58787
	loss_value_5: 0.03852
	loss_reward_5: 0.00669
	loss_policy: 0.29923
	loss_value: 0.33561
	loss_reward: 0.02739
Optimization_Done 38200
[2024-05-13 00:23:30] [command] train weight_iter_38200.pkl 190 192
[2024-05-13 00:24:21] nn step 38300, lr: 0.059049.
	loss_policy_0: 0.13793
	accuracy_policy_0: 0.71576
	loss_value_0: 0.16293
	loss_policy_1: 0.03303
	accuracy_policy_1: 0.66711
	loss_value_1: 0.03434
	loss_reward_1: 0.0058
	loss_policy_2: 0.03651
	accuracy_policy_2: 0.64107
	loss_value_2: 0.03577
	loss_reward_2: 0.00544
	loss_policy_3: 0.03991
	accuracy_policy_3: 0.61473
	loss_value_3: 0.03727
	loss_reward_3: 0.0059
	loss_policy_4: 0.0427
	accuracy_policy_4: 0.59236
	loss_value_4: 0.03852
	loss_reward_4: 0.00636
	loss_policy_5: 0.04562
	accuracy_policy_5: 0.57125
	loss_value_5: 0.03968
	loss_reward_5: 0.00724
	loss_policy: 0.33569
	loss_value: 0.34851
	loss_reward: 0.03073
[2024-05-13 00:25:10] nn step 38400, lr: 0.059049.
	loss_policy_0: 0.11887
	accuracy_policy_0: 0.75918
	loss_value_0: 0.16445
	loss_policy_1: 0.03057
	accuracy_policy_1: 0.69344
	loss_value_1: 0.03464
	loss_reward_1: 0.00572
	loss_policy_2: 0.03468
	accuracy_policy_2: 0.65955
	loss_value_2: 0.03615
	loss_reward_2: 0.00547
	loss_policy_3: 0.03764
	accuracy_policy_3: 0.64037
	loss_value_3: 0.03743
	loss_reward_3: 0.00591
	loss_policy_4: 0.04028
	accuracy_policy_4: 0.62223
	loss_value_4: 0.03874
	loss_reward_4: 0.00643
	loss_policy_5: 0.04345
	accuracy_policy_5: 0.60047
	loss_value_5: 0.03994
	loss_reward_5: 0.00761
	loss_policy: 0.30549
	loss_value: 0.35134
	loss_reward: 0.03114
Optimization_Done 38400
[2024-05-13 00:27:24] [command] train weight_iter_38400.pkl 191 193
[2024-05-13 00:28:16] nn step 38500, lr: 0.059049.
	loss_policy_0: 0.12723
	accuracy_policy_0: 0.71176
	loss_value_0: 0.15852
	loss_policy_1: 0.03142
	accuracy_policy_1: 0.65268
	loss_value_1: 0.03313
	loss_reward_1: 0.00549
	loss_policy_2: 0.03497
	accuracy_policy_2: 0.61914
	loss_value_2: 0.03444
	loss_reward_2: 0.00536
	loss_policy_3: 0.03738
	accuracy_policy_3: 0.60193
	loss_value_3: 0.03564
	loss_reward_3: 0.00582
	loss_policy_4: 0.04025
	accuracy_policy_4: 0.58008
	loss_value_4: 0.03672
	loss_reward_4: 0.00608
	loss_policy_5: 0.04289
	accuracy_policy_5: 0.55854
	loss_value_5: 0.03786
	loss_reward_5: 0.00718
	loss_policy: 0.31413
	loss_value: 0.3363
	loss_reward: 0.02993
[2024-05-13 00:29:08] nn step 38600, lr: 0.059049.
	loss_policy_0: 0.10398
	accuracy_policy_0: 0.75992
	loss_value_0: 0.15453
	loss_policy_1: 0.02719
	accuracy_policy_1: 0.68793
	loss_value_1: 0.03233
	loss_reward_1: 0.00525
	loss_policy_2: 0.03085
	accuracy_policy_2: 0.66035
	loss_value_2: 0.03365
	loss_reward_2: 0.00504
	loss_policy_3: 0.0338
	accuracy_policy_3: 0.63389
	loss_value_3: 0.03465
	loss_reward_3: 0.00542
	loss_policy_4: 0.03638
	accuracy_policy_4: 0.61383
	loss_value_4: 0.03575
	loss_reward_4: 0.00591
	loss_policy_5: 0.03938
	accuracy_policy_5: 0.59123
	loss_value_5: 0.03686
	loss_reward_5: 0.00696
	loss_policy: 0.27159
	loss_value: 0.32776
	loss_reward: 0.02858
Optimization_Done 38600
[2024-05-13 00:31:13] [command] train weight_iter_38600.pkl 192 194
[2024-05-13 00:32:03] nn step 38700, lr: 0.059049.
	loss_policy_0: 0.14039
	accuracy_policy_0: 0.6952
	loss_value_0: 0.16066
	loss_policy_1: 0.03383
	accuracy_policy_1: 0.63205
	loss_value_1: 0.03344
	loss_reward_1: 0.00505
	loss_policy_2: 0.03694
	accuracy_policy_2: 0.60646
	loss_value_2: 0.03478
	loss_reward_2: 0.00486
	loss_policy_3: 0.0395
	accuracy_policy_3: 0.58561
	loss_value_3: 0.03581
	loss_reward_3: 0.00533
	loss_policy_4: 0.04249
	accuracy_policy_4: 0.55857
	loss_value_4: 0.03693
	loss_reward_4: 0.00573
	loss_policy_5: 0.04494
	accuracy_policy_5: 0.53711
	loss_value_5: 0.03812
	loss_reward_5: 0.00675
	loss_policy: 0.3381
	loss_value: 0.33974
	loss_reward: 0.02772
[2024-05-13 00:32:54] nn step 38800, lr: 0.059049.
	loss_policy_0: 0.1175
	accuracy_policy_0: 0.74537
	loss_value_0: 0.16027
	loss_policy_1: 0.03078
	accuracy_policy_1: 0.67609
	loss_value_1: 0.03342
	loss_reward_1: 0.00526
	loss_policy_2: 0.03456
	accuracy_policy_2: 0.63713
	loss_value_2: 0.03465
	loss_reward_2: 0.00505
	loss_policy_3: 0.03718
	accuracy_policy_3: 0.61816
	loss_value_3: 0.03574
	loss_reward_3: 0.00566
	loss_policy_4: 0.03965
	accuracy_policy_4: 0.59793
	loss_value_4: 0.03674
	loss_reward_4: 0.00586
	loss_policy_5: 0.04254
	accuracy_policy_5: 0.56904
	loss_value_5: 0.03796
	loss_reward_5: 0.00687
	loss_policy: 0.30222
	loss_value: 0.33877
	loss_reward: 0.0287
Optimization_Done 38800
[2024-05-13 00:35:09] [command] train weight_iter_38800.pkl 193 195
[2024-05-13 00:35:57] nn step 38900, lr: 0.059049.
	loss_policy_0: 0.14899
	accuracy_policy_0: 0.68164
	loss_value_0: 0.15598
	loss_policy_1: 0.03459
	accuracy_policy_1: 0.63029
	loss_value_1: 0.03241
	loss_reward_1: 0.00452
	loss_policy_2: 0.0381
	accuracy_policy_2: 0.59564
	loss_value_2: 0.03375
	loss_reward_2: 0.00454
	loss_policy_3: 0.04059
	accuracy_policy_3: 0.58105
	loss_value_3: 0.03502
	loss_reward_3: 0.00488
	loss_policy_4: 0.04316
	accuracy_policy_4: 0.5608
	loss_value_4: 0.03611
	loss_reward_4: 0.00539
	loss_policy_5: 0.04566
	accuracy_policy_5: 0.53844
	loss_value_5: 0.03726
	loss_reward_5: 0.00623
	loss_policy: 0.35108
	loss_value: 0.33053
	loss_reward: 0.02556
[2024-05-13 00:36:48] nn step 39000, lr: 0.059049.
	loss_policy_0: 0.11886
	accuracy_policy_0: 0.73082
	loss_value_0: 0.14712
	loss_policy_1: 0.02975
	accuracy_policy_1: 0.66992
	loss_value_1: 0.03086
	loss_reward_1: 0.00443
	loss_policy_2: 0.03343
	accuracy_policy_2: 0.63623
	loss_value_2: 0.03211
	loss_reward_2: 0.00434
	loss_policy_3: 0.03627
	accuracy_policy_3: 0.61191
	loss_value_3: 0.03322
	loss_reward_3: 0.00469
	loss_policy_4: 0.03898
	accuracy_policy_4: 0.58775
	loss_value_4: 0.03445
	loss_reward_4: 0.00512
	loss_policy_5: 0.04167
	accuracy_policy_5: 0.55916
	loss_value_5: 0.03556
	loss_reward_5: 0.00605
	loss_policy: 0.29895
	loss_value: 0.31332
	loss_reward: 0.02464
Optimization_Done 39000
[2024-05-13 00:39:03] [command] train weight_iter_39000.pkl 194 196
[2024-05-13 00:39:55] nn step 39100, lr: 0.059049.
	loss_policy_0: 0.15245
	accuracy_policy_0: 0.69898
	loss_value_0: 0.16519
	loss_policy_1: 0.03598
	accuracy_policy_1: 0.65193
	loss_value_1: 0.03484
	loss_reward_1: 0.00531
	loss_policy_2: 0.03967
	accuracy_policy_2: 0.6202
	loss_value_2: 0.03655
	loss_reward_2: 0.00526
	loss_policy_3: 0.04272
	accuracy_policy_3: 0.5967
	loss_value_3: 0.03798
	loss_reward_3: 0.00554
	loss_policy_4: 0.04532
	accuracy_policy_4: 0.57545
	loss_value_4: 0.03921
	loss_reward_4: 0.00611
	loss_policy_5: 0.04819
	accuracy_policy_5: 0.5502
	loss_value_5: 0.04058
	loss_reward_5: 0.00706
	loss_policy: 0.36433
	loss_value: 0.35436
	loss_reward: 0.02928
[2024-05-13 00:40:45] nn step 39200, lr: 0.059049.
	loss_policy_0: 0.12563
	accuracy_policy_0: 0.74236
	loss_value_0: 0.15992
	loss_policy_1: 0.0316
	accuracy_policy_1: 0.67928
	loss_value_1: 0.03359
	loss_reward_1: 0.00509
	loss_policy_2: 0.0353
	accuracy_policy_2: 0.64371
	loss_value_2: 0.03489
	loss_reward_2: 0.00505
	loss_policy_3: 0.03838
	accuracy_policy_3: 0.61871
	loss_value_3: 0.03625
	loss_reward_3: 0.00538
	loss_policy_4: 0.04136
	accuracy_policy_4: 0.59857
	loss_value_4: 0.03748
	loss_reward_4: 0.00577
	loss_policy_5: 0.04437
	accuracy_policy_5: 0.57766
	loss_value_5: 0.03881
	loss_reward_5: 0.00661
	loss_policy: 0.31663
	loss_value: 0.34094
	loss_reward: 0.02789
Optimization_Done 39200
[2024-05-13 00:42:39] [command] train weight_iter_39200.pkl 195 197
[2024-05-13 00:43:31] nn step 39300, lr: 0.059049.
	loss_policy_0: 0.15328
	accuracy_policy_0: 0.68998
	loss_value_0: 0.17105
	loss_policy_1: 0.03722
	accuracy_policy_1: 0.63271
	loss_value_1: 0.0357
	loss_reward_1: 0.00618
	loss_policy_2: 0.04117
	accuracy_policy_2: 0.60227
	loss_value_2: 0.03722
	loss_reward_2: 0.00574
	loss_policy_3: 0.04473
	accuracy_policy_3: 0.57818
	loss_value_3: 0.03873
	loss_reward_3: 0.0064
	loss_policy_4: 0.04767
	accuracy_policy_4: 0.55725
	loss_value_4: 0.04004
	loss_reward_4: 0.00694
	loss_policy_5: 0.05076
	accuracy_policy_5: 0.53389
	loss_value_5: 0.04121
	loss_reward_5: 0.00798
	loss_policy: 0.37482
	loss_value: 0.36396
	loss_reward: 0.03323
[2024-05-13 00:44:21] nn step 39400, lr: 0.059049.
	loss_policy_0: 0.13853
	accuracy_policy_0: 0.73564
	loss_value_0: 0.17868
	loss_policy_1: 0.03601
	accuracy_policy_1: 0.66822
	loss_value_1: 0.03737
	loss_reward_1: 0.00644
	loss_policy_2: 0.04055
	accuracy_policy_2: 0.63125
	loss_value_2: 0.0389
	loss_reward_2: 0.00605
	loss_policy_3: 0.04371
	accuracy_policy_3: 0.61355
	loss_value_3: 0.04034
	loss_reward_3: 0.00667
	loss_policy_4: 0.04732
	accuracy_policy_4: 0.59008
	loss_value_4: 0.04183
	loss_reward_4: 0.0072
	loss_policy_5: 0.0508
	accuracy_policy_5: 0.56115
	loss_value_5: 0.04325
	loss_reward_5: 0.00841
	loss_policy: 0.35691
	loss_value: 0.38037
	loss_reward: 0.03477
Optimization_Done 39400
[2024-05-13 00:46:26] [command] train weight_iter_39400.pkl 196 198
[2024-05-13 00:47:18] nn step 39500, lr: 0.059049.
	loss_policy_0: 0.13626
	accuracy_policy_0: 0.71572
	loss_value_0: 0.16336
	loss_policy_1: 0.03339
	accuracy_policy_1: 0.6616
	loss_value_1: 0.03428
	loss_reward_1: 0.00539
	loss_policy_2: 0.03663
	accuracy_policy_2: 0.63625
	loss_value_2: 0.03572
	loss_reward_2: 0.00515
	loss_policy_3: 0.03966
	accuracy_policy_3: 0.61381
	loss_value_3: 0.03684
	loss_reward_3: 0.0056
	loss_policy_4: 0.04201
	accuracy_policy_4: 0.59826
	loss_value_4: 0.03799
	loss_reward_4: 0.00584
	loss_policy_5: 0.04468
	accuracy_policy_5: 0.5765
	loss_value_5: 0.03903
	loss_reward_5: 0.0069
	loss_policy: 0.33262
	loss_value: 0.34722
	loss_reward: 0.02888
[2024-05-13 00:48:08] nn step 39600, lr: 0.059049.
	loss_policy_0: 0.11122
	accuracy_policy_0: 0.76295
	loss_value_0: 0.15792
	loss_policy_1: 0.02943
	accuracy_policy_1: 0.6952
	loss_value_1: 0.033
	loss_reward_1: 0.00527
	loss_policy_2: 0.03291
	accuracy_policy_2: 0.66402
	loss_value_2: 0.03441
	loss_reward_2: 0.00494
	loss_policy_3: 0.03587
	accuracy_policy_3: 0.64191
	loss_value_3: 0.03558
	loss_reward_3: 0.00551
	loss_policy_4: 0.03859
	accuracy_policy_4: 0.62305
	loss_value_4: 0.03672
	loss_reward_4: 0.00593
	loss_policy_5: 0.04148
	accuracy_policy_5: 0.60131
	loss_value_5: 0.038
	loss_reward_5: 0.00696
	loss_policy: 0.2895
	loss_value: 0.33563
	loss_reward: 0.02861
Optimization_Done 39600
[2024-05-13 00:50:23] [command] train weight_iter_39600.pkl 197 199
[2024-05-13 00:51:12] nn step 39700, lr: 0.059049.
	loss_policy_0: 0.1548
	accuracy_policy_0: 0.69062
	loss_value_0: 0.1504
	loss_policy_1: 0.03509
	accuracy_policy_1: 0.65152
	loss_value_1: 0.03146
	loss_reward_1: 0.00435
	loss_policy_2: 0.03767
	accuracy_policy_2: 0.63061
	loss_value_2: 0.03275
	loss_reward_2: 0.00419
	loss_policy_3: 0.04038
	accuracy_policy_3: 0.61059
	loss_value_3: 0.03386
	loss_reward_3: 0.00465
	loss_policy_4: 0.0424
	accuracy_policy_4: 0.59611
	loss_value_4: 0.03494
	loss_reward_4: 0.00494
	loss_policy_5: 0.04459
	accuracy_policy_5: 0.57781
	loss_value_5: 0.03603
	loss_reward_5: 0.00579
	loss_policy: 0.35493
	loss_value: 0.31944
	loss_reward: 0.02392
[2024-05-13 00:51:58] nn step 39800, lr: 0.059049.
	loss_policy_0: 0.13
	accuracy_policy_0: 0.7509
	loss_value_0: 0.15769
	loss_policy_1: 0.03219
	accuracy_policy_1: 0.69996
	loss_value_1: 0.03309
	loss_reward_1: 0.00486
	loss_policy_2: 0.03561
	accuracy_policy_2: 0.66568
	loss_value_2: 0.03434
	loss_reward_2: 0.00465
	loss_policy_3: 0.03852
	accuracy_policy_3: 0.64727
	loss_value_3: 0.03552
	loss_reward_3: 0.00502
	loss_policy_4: 0.04155
	accuracy_policy_4: 0.62564
	loss_value_4: 0.03669
	loss_reward_4: 0.00538
	loss_policy_5: 0.04443
	accuracy_policy_5: 0.60613
	loss_value_5: 0.03797
	loss_reward_5: 0.00634
	loss_policy: 0.3223
	loss_value: 0.33529
	loss_reward: 0.02626
Optimization_Done 39800
[2024-05-13 00:54:12] [command] train weight_iter_39800.pkl 198 200
[2024-05-13 00:55:06] nn step 39900, lr: 0.059049.
	loss_policy_0: 0.16489
	accuracy_policy_0: 0.69889
	loss_value_0: 0.15869
	loss_policy_1: 0.03609
	accuracy_policy_1: 0.66684
	loss_value_1: 0.03343
	loss_reward_1: 0.0044
	loss_policy_2: 0.03899
	accuracy_policy_2: 0.64693
	loss_value_2: 0.03472
	loss_reward_2: 0.00435
	loss_policy_3: 0.04104
	accuracy_policy_3: 0.63117
	loss_value_3: 0.03596
	loss_reward_3: 0.00469
	loss_policy_4: 0.04358
	accuracy_policy_4: 0.6115
	loss_value_4: 0.03719
	loss_reward_4: 0.00516
	loss_policy_5: 0.04557
	accuracy_policy_5: 0.60016
	loss_value_5: 0.03855
	loss_reward_5: 0.00564
	loss_policy: 0.37016
	loss_value: 0.33854
	loss_reward: 0.02424
[2024-05-13 00:55:57] nn step 40000, lr: 0.059049.
	loss_policy_0: 0.13068
	accuracy_policy_0: 0.75219
	loss_value_0: 0.15474
	loss_policy_1: 0.03066
	accuracy_policy_1: 0.70828
	loss_value_1: 0.0324
	loss_reward_1: 0.00434
	loss_policy_2: 0.03358
	accuracy_policy_2: 0.68506
	loss_value_2: 0.03366
	loss_reward_2: 0.0042
	loss_policy_3: 0.03633
	accuracy_policy_3: 0.66547
	loss_value_3: 0.0349
	loss_reward_3: 0.00458
	loss_policy_4: 0.03875
	accuracy_policy_4: 0.64877
	loss_value_4: 0.03609
	loss_reward_4: 0.005
	loss_policy_5: 0.04152
	accuracy_policy_5: 0.62775
	loss_value_5: 0.03743
	loss_reward_5: 0.00567
	loss_policy: 0.31152
	loss_value: 0.32921
	loss_reward: 0.02379
Optimization_Done 40000
[2024-05-13 00:58:11] [command] train weight_iter_40000.pkl 199 201
[2024-05-13 00:59:04] nn step 40100, lr: 0.059049.
	loss_policy_0: 0.17436
	accuracy_policy_0: 0.68463
	loss_value_0: 0.16927
	loss_policy_1: 0.03937
	accuracy_policy_1: 0.6458
	loss_value_1: 0.03553
	loss_reward_1: 0.00527
	loss_policy_2: 0.04257
	accuracy_policy_2: 0.62115
	loss_value_2: 0.03704
	loss_reward_2: 0.00506
	loss_policy_3: 0.04531
	accuracy_policy_3: 0.60209
	loss_value_3: 0.03841
	loss_reward_3: 0.00546
	loss_policy_4: 0.04831
	accuracy_policy_4: 0.57545
	loss_value_4: 0.03973
	loss_reward_4: 0.00581
	loss_policy_5: 0.05135
	accuracy_policy_5: 0.55682
	loss_value_5: 0.04111
	loss_reward_5: 0.00659
	loss_policy: 0.40126
	loss_value: 0.3611
	loss_reward: 0.02819
[2024-05-13 00:59:56] nn step 40200, lr: 0.059049.
	loss_policy_0: 0.15279
	accuracy_policy_0: 0.73285
	loss_value_0: 0.17
	loss_policy_1: 0.03593
	accuracy_policy_1: 0.68691
	loss_value_1: 0.03588
	loss_reward_1: 0.00537
	loss_policy_2: 0.03965
	accuracy_policy_2: 0.65906
	loss_value_2: 0.03736
	loss_reward_2: 0.0051
	loss_policy_3: 0.04261
	accuracy_policy_3: 0.64016
	loss_value_3: 0.03886
	loss_reward_3: 0.00546
	loss_policy_4: 0.0459
	accuracy_policy_4: 0.61648
	loss_value_4: 0.04022
	loss_reward_4: 0.00599
	loss_policy_5: 0.04875
	accuracy_policy_5: 0.59221
	loss_value_5: 0.04159
	loss_reward_5: 0.00685
	loss_policy: 0.36563
	loss_value: 0.3639
	loss_reward: 0.02877
Optimization_Done 40200
[2024-05-13 01:01:45] [command] train weight_iter_40200.pkl 200 202
[2024-05-13 01:02:38] nn step 40300, lr: 0.059049.
	loss_policy_0: 0.15012
	accuracy_policy_0: 0.70396
	loss_value_0: 0.1654
	loss_policy_1: 0.0353
	accuracy_policy_1: 0.6518
	loss_value_1: 0.03457
	loss_reward_1: 0.00529
	loss_policy_2: 0.0384
	accuracy_policy_2: 0.62477
	loss_value_2: 0.03583
	loss_reward_2: 0.00502
	loss_policy_3: 0.04103
	accuracy_policy_3: 0.60213
	loss_value_3: 0.03698
	loss_reward_3: 0.00559
	loss_policy_4: 0.04372
	accuracy_policy_4: 0.58119
	loss_value_4: 0.03807
	loss_reward_4: 0.006
	loss_policy_5: 0.04625
	accuracy_policy_5: 0.56094
	loss_value_5: 0.03915
	loss_reward_5: 0.00682
	loss_policy: 0.3548
	loss_value: 0.35
	loss_reward: 0.02873
[2024-05-13 01:03:31] nn step 40400, lr: 0.059049.
	loss_policy_0: 0.13068
	accuracy_policy_0: 0.74799
	loss_value_0: 0.16866
	loss_policy_1: 0.03295
	accuracy_policy_1: 0.68689
	loss_value_1: 0.03532
	loss_reward_1: 0.00533
	loss_policy_2: 0.03693
	accuracy_policy_2: 0.65209
	loss_value_2: 0.03651
	loss_reward_2: 0.00536
	loss_policy_3: 0.03981
	accuracy_policy_3: 0.62813
	loss_value_3: 0.03766
	loss_reward_3: 0.00573
	loss_policy_4: 0.04284
	accuracy_policy_4: 0.60252
	loss_value_4: 0.03889
	loss_reward_4: 0.00605
	loss_policy_5: 0.04556
	accuracy_policy_5: 0.584
	loss_value_5: 0.04002
	loss_reward_5: 0.00705
	loss_policy: 0.32878
	loss_value: 0.35706
	loss_reward: 0.02951
Optimization_Done 40400
[2024-05-13 01:05:47] [command] train weight_iter_40400.pkl 201 203
[2024-05-13 01:06:39] nn step 40500, lr: 0.059049.
	loss_policy_0: 0.17012
	accuracy_policy_0: 0.69621
	loss_value_0: 0.17504
	loss_policy_1: 0.03952
	accuracy_policy_1: 0.65094
	loss_value_1: 0.03654
	loss_reward_1: 0.00514
	loss_policy_2: 0.04317
	accuracy_policy_2: 0.62395
	loss_value_2: 0.03804
	loss_reward_2: 0.00504
	loss_policy_3: 0.04607
	accuracy_policy_3: 0.5993
	loss_value_3: 0.03944
	loss_reward_3: 0.00545
	loss_policy_4: 0.04877
	accuracy_policy_4: 0.57904
	loss_value_4: 0.04085
	loss_reward_4: 0.00576
	loss_policy_5: 0.05186
	accuracy_policy_5: 0.55482
	loss_value_5: 0.04197
	loss_reward_5: 0.00678
	loss_policy: 0.3995
	loss_value: 0.37188
	loss_reward: 0.02818
[2024-05-13 01:07:31] nn step 40600, lr: 0.059049.
	loss_policy_0: 0.12686
	accuracy_policy_0: 0.75
	loss_value_0: 0.15914
	loss_policy_1: 0.0318
	accuracy_policy_1: 0.68836
	loss_value_1: 0.03332
	loss_reward_1: 0.0047
	loss_policy_2: 0.03533
	accuracy_policy_2: 0.65648
	loss_value_2: 0.03453
	loss_reward_2: 0.00474
	loss_policy_3: 0.03838
	accuracy_policy_3: 0.63338
	loss_value_3: 0.0358
	loss_reward_3: 0.00505
	loss_policy_4: 0.04135
	accuracy_policy_4: 0.60975
	loss_value_4: 0.037
	loss_reward_4: 0.00538
	loss_policy_5: 0.04418
	accuracy_policy_5: 0.58803
	loss_value_5: 0.03814
	loss_reward_5: 0.00633
	loss_policy: 0.3179
	loss_value: 0.33792
	loss_reward: 0.0262
Optimization_Done 40600
[2024-05-13 01:09:45] [command] train weight_iter_40600.pkl 202 204
[2024-05-13 01:10:38] nn step 40700, lr: 0.059049.
	loss_policy_0: 0.15784
	accuracy_policy_0: 0.70492
	loss_value_0: 0.17186
	loss_policy_1: 0.03578
	accuracy_policy_1: 0.67213
	loss_value_1: 0.03595
	loss_reward_1: 0.00504
	loss_policy_2: 0.03899
	accuracy_policy_2: 0.64504
	loss_value_2: 0.03736
	loss_reward_2: 0.00477
	loss_policy_3: 0.04209
	accuracy_policy_3: 0.62086
	loss_value_3: 0.03879
	loss_reward_3: 0.0051
	loss_policy_4: 0.04469
	accuracy_policy_4: 0.60355
	loss_value_4: 0.04021
	loss_reward_4: 0.0057
	loss_policy_5: 0.04714
	accuracy_policy_5: 0.58375
	loss_value_5: 0.04139
	loss_reward_5: 0.00649
	loss_policy: 0.36653
	loss_value: 0.36555
	loss_reward: 0.0271
[2024-05-13 01:11:30] nn step 40800, lr: 0.059049.
	loss_policy_0: 0.12031
	accuracy_policy_0: 0.76076
	loss_value_0: 0.15585
	loss_policy_1: 0.02963
	accuracy_policy_1: 0.70896
	loss_value_1: 0.03258
	loss_reward_1: 0.00466
	loss_policy_2: 0.03313
	accuracy_policy_2: 0.68328
	loss_value_2: 0.03389
	loss_reward_2: 0.00452
	loss_policy_3: 0.03584
	accuracy_policy_3: 0.66227
	loss_value_3: 0.03519
	loss_reward_3: 0.00486
	loss_policy_4: 0.03854
	accuracy_policy_4: 0.6427
	loss_value_4: 0.03642
	loss_reward_4: 0.00539
	loss_policy_5: 0.04133
	accuracy_policy_5: 0.61668
	loss_value_5: 0.03757
	loss_reward_5: 0.00629
	loss_policy: 0.29879
	loss_value: 0.33149
	loss_reward: 0.02572
Optimization_Done 40800
[2024-05-13 01:13:43] [command] train weight_iter_40800.pkl 203 205
[2024-05-13 01:14:36] nn step 40900, lr: 0.059049.
	loss_policy_0: 0.15297
	accuracy_policy_0: 0.70557
	loss_value_0: 0.16048
	loss_policy_1: 0.03567
	accuracy_policy_1: 0.65865
	loss_value_1: 0.03368
	loss_reward_1: 0.00503
	loss_policy_2: 0.03937
	accuracy_policy_2: 0.6293
	loss_value_2: 0.03525
	loss_reward_2: 0.00492
	loss_policy_3: 0.04221
	accuracy_policy_3: 0.61215
	loss_value_3: 0.03664
	loss_reward_3: 0.00549
	loss_policy_4: 0.04543
	accuracy_policy_4: 0.58748
	loss_value_4: 0.03806
	loss_reward_4: 0.00603
	loss_policy_5: 0.04839
	accuracy_policy_5: 0.56635
	loss_value_5: 0.03942
	loss_reward_5: 0.00669
	loss_policy: 0.36403
	loss_value: 0.34353
	loss_reward: 0.02817
[2024-05-13 01:15:28] nn step 41000, lr: 0.059049.
	loss_policy_0: 0.13451
	accuracy_policy_0: 0.75121
	loss_value_0: 0.16421
	loss_policy_1: 0.03311
	accuracy_policy_1: 0.70127
	loss_value_1: 0.03452
	loss_reward_1: 0.00515
	loss_policy_2: 0.03732
	accuracy_policy_2: 0.66996
	loss_value_2: 0.03597
	loss_reward_2: 0.00511
	loss_policy_3: 0.04097
	accuracy_policy_3: 0.64436
	loss_value_3: 0.03755
	loss_reward_3: 0.00556
	loss_policy_4: 0.04421
	accuracy_policy_4: 0.62008
	loss_value_4: 0.03887
	loss_reward_4: 0.00594
	loss_policy_5: 0.04735
	accuracy_policy_5: 0.60004
	loss_value_5: 0.04027
	loss_reward_5: 0.00705
	loss_policy: 0.33749
	loss_value: 0.35139
	loss_reward: 0.02881
Optimization_Done 41000
[2024-05-13 01:17:43] [command] train weight_iter_41000.pkl 204 206
[2024-05-13 01:18:46] nn step 41100, lr: 0.059049.
	loss_policy_0: 0.1456
	accuracy_policy_0: 0.70195
	loss_value_0: 0.15848
	loss_policy_1: 0.035
	accuracy_policy_1: 0.65201
	loss_value_1: 0.03338
	loss_reward_1: 0.00545
	loss_policy_2: 0.03905
	accuracy_policy_2: 0.61861
	loss_value_2: 0.03496
	loss_reward_2: 0.00528
	loss_policy_3: 0.04193
	accuracy_policy_3: 0.59816
	loss_value_3: 0.03642
	loss_reward_3: 0.00584
	loss_policy_4: 0.04476
	accuracy_policy_4: 0.57609
	loss_value_4: 0.03751
	loss_reward_4: 0.00625
	loss_policy_5: 0.04725
	accuracy_policy_5: 0.55922
	loss_value_5: 0.03872
	loss_reward_5: 0.00719
	loss_policy: 0.35358
	loss_value: 0.33947
	loss_reward: 0.03001
[2024-05-13 01:19:49] nn step 41200, lr: 0.059049.
	loss_policy_0: 0.12241
	accuracy_policy_0: 0.74756
	loss_value_0: 0.1559
	loss_policy_1: 0.03113
	accuracy_policy_1: 0.68656
	loss_value_1: 0.03294
	loss_reward_1: 0.00547
	loss_policy_2: 0.03504
	accuracy_policy_2: 0.6577
	loss_value_2: 0.03434
	loss_reward_2: 0.0051
	loss_policy_3: 0.03864
	accuracy_policy_3: 0.63191
	loss_value_3: 0.03562
	loss_reward_3: 0.00556
	loss_policy_4: 0.04157
	accuracy_policy_4: 0.60621
	loss_value_4: 0.03678
	loss_reward_4: 0.00587
	loss_policy_5: 0.04432
	accuracy_policy_5: 0.58971
	loss_value_5: 0.03794
	loss_reward_5: 0.00714
	loss_policy: 0.31311
	loss_value: 0.33352
	loss_reward: 0.02915
Optimization_Done 41200
[2024-05-13 01:21:53] [command] train weight_iter_41200.pkl 205 207
[2024-05-13 01:22:54] nn step 41300, lr: 0.059049.
	loss_policy_0: 0.15343
	accuracy_policy_0: 0.68971
	loss_value_0: 0.15848
	loss_policy_1: 0.0367
	accuracy_policy_1: 0.63699
	loss_value_1: 0.03337
	loss_reward_1: 0.00519
	loss_policy_2: 0.04009
	accuracy_policy_2: 0.60693
	loss_value_2: 0.0348
	loss_reward_2: 0.00495
	loss_policy_3: 0.04338
	accuracy_policy_3: 0.58139
	loss_value_3: 0.03581
	loss_reward_3: 0.00539
	loss_policy_4: 0.04571
	accuracy_policy_4: 0.56344
	loss_value_4: 0.03703
	loss_reward_4: 0.00587
	loss_policy_5: 0.04826
	accuracy_policy_5: 0.5473
	loss_value_5: 0.03815
	loss_reward_5: 0.00681
	loss_policy: 0.36757
	loss_value: 0.33764
	loss_reward: 0.02821
[2024-05-13 01:23:50] nn step 41400, lr: 0.059049.
	loss_policy_0: 0.12902
	accuracy_policy_0: 0.7407
	loss_value_0: 0.15445
	loss_policy_1: 0.0334
	accuracy_policy_1: 0.66824
	loss_value_1: 0.03258
	loss_reward_1: 0.00525
	loss_policy_2: 0.03711
	accuracy_policy_2: 0.63416
	loss_value_2: 0.03402
	loss_reward_2: 0.00505
	loss_policy_3: 0.04019
	accuracy_policy_3: 0.6125
	loss_value_3: 0.03523
	loss_reward_3: 0.00546
	loss_policy_4: 0.04327
	accuracy_policy_4: 0.58781
	loss_value_4: 0.03625
	loss_reward_4: 0.00587
	loss_policy_5: 0.0458
	accuracy_policy_5: 0.57424
	loss_value_5: 0.03739
	loss_reward_5: 0.00711
	loss_policy: 0.3288
	loss_value: 0.32992
	loss_reward: 0.02874
Optimization_Done 41400
[2024-05-13 01:26:05] [command] train weight_iter_41400.pkl 206 208
[2024-05-13 01:27:09] nn step 41500, lr: 0.059049.
	loss_policy_0: 0.16271
	accuracy_policy_0: 0.66926
	loss_value_0: 0.15966
	loss_policy_1: 0.03733
	accuracy_policy_1: 0.62748
	loss_value_1: 0.03346
	loss_reward_1: 0.00541
	loss_policy_2: 0.04068
	accuracy_policy_2: 0.59162
	loss_value_2: 0.03484
	loss_reward_2: 0.00524
	loss_policy_3: 0.04322
	accuracy_policy_3: 0.57365
	loss_value_3: 0.03619
	loss_reward_3: 0.00567
	loss_policy_4: 0.04555
	accuracy_policy_4: 0.55254
	loss_value_4: 0.03733
	loss_reward_4: 0.00609
	loss_policy_5: 0.04793
	accuracy_policy_5: 0.53639
	loss_value_5: 0.03846
	loss_reward_5: 0.00719
	loss_policy: 0.37743
	loss_value: 0.33992
	loss_reward: 0.02961
[2024-05-13 01:28:12] nn step 41600, lr: 0.059049.
	loss_policy_0: 0.13408
	accuracy_policy_0: 0.7266
	loss_value_0: 0.15699
	loss_policy_1: 0.03315
	accuracy_policy_1: 0.6677
	loss_value_1: 0.03294
	loss_reward_1: 0.00539
	loss_policy_2: 0.03666
	accuracy_policy_2: 0.63729
	loss_value_2: 0.03436
	loss_reward_2: 0.0052
	loss_policy_3: 0.03971
	accuracy_policy_3: 0.61182
	loss_value_3: 0.03575
	loss_reward_3: 0.00563
	loss_policy_4: 0.04272
	accuracy_policy_4: 0.59422
	loss_value_4: 0.03694
	loss_reward_4: 0.00606
	loss_policy_5: 0.04532
	accuracy_policy_5: 0.57117
	loss_value_5: 0.03805
	loss_reward_5: 0.00719
	loss_policy: 0.33164
	loss_value: 0.33503
	loss_reward: 0.02948
Optimization_Done 41600
[2024-05-13 01:30:08] [command] train weight_iter_41600.pkl 207 209
[2024-05-13 01:31:07] nn step 41700, lr: 0.059049.
	loss_policy_0: 0.161
	accuracy_policy_0: 0.67914
	loss_value_0: 0.16461
	loss_policy_1: 0.03707
	accuracy_policy_1: 0.63377
	loss_value_1: 0.03449
	loss_reward_1: 0.00543
	loss_policy_2: 0.04073
	accuracy_policy_2: 0.6042
	loss_value_2: 0.03608
	loss_reward_2: 0.00526
	loss_policy_3: 0.044
	accuracy_policy_3: 0.57602
	loss_value_3: 0.03754
	loss_reward_3: 0.0057
	loss_policy_4: 0.04693
	accuracy_policy_4: 0.55533
	loss_value_4: 0.0389
	loss_reward_4: 0.00613
	loss_policy_5: 0.04944
	accuracy_policy_5: 0.53604
	loss_value_5: 0.0403
	loss_reward_5: 0.0073
	loss_policy: 0.37917
	loss_value: 0.35192
	loss_reward: 0.02982
[2024-05-13 01:32:09] nn step 41800, lr: 0.059049.
	loss_policy_0: 0.13798
	accuracy_policy_0: 0.7267
	loss_value_0: 0.16514
	loss_policy_1: 0.0331
	accuracy_policy_1: 0.6765
	loss_value_1: 0.03463
	loss_reward_1: 0.00539
	loss_policy_2: 0.03716
	accuracy_policy_2: 0.64074
	loss_value_2: 0.03612
	loss_reward_2: 0.00547
	loss_policy_3: 0.04062
	accuracy_policy_3: 0.61289
	loss_value_3: 0.03741
	loss_reward_3: 0.00573
	loss_policy_4: 0.04349
	accuracy_policy_4: 0.59338
	loss_value_4: 0.03881
	loss_reward_4: 0.0063
	loss_policy_5: 0.04689
	accuracy_policy_5: 0.56643
	loss_value_5: 0.04016
	loss_reward_5: 0.00714
	loss_policy: 0.33925
	loss_value: 0.35227
	loss_reward: 0.03003
Optimization_Done 41800
[2024-05-13 01:34:24] [command] train weight_iter_41800.pkl 208 210
[2024-05-13 01:35:25] nn step 41900, lr: 0.059049.
	loss_policy_0: 0.1605
	accuracy_policy_0: 0.6809
	loss_value_0: 0.17377
	loss_policy_1: 0.0376
	accuracy_policy_1: 0.63521
	loss_value_1: 0.03664
	loss_reward_1: 0.00581
	loss_policy_2: 0.04143
	accuracy_policy_2: 0.60188
	loss_value_2: 0.03807
	loss_reward_2: 0.0058
	loss_policy_3: 0.04514
	accuracy_policy_3: 0.57582
	loss_value_3: 0.03964
	loss_reward_3: 0.00629
	loss_policy_4: 0.04808
	accuracy_policy_4: 0.55387
	loss_value_4: 0.04106
	loss_reward_4: 0.0068
	loss_policy_5: 0.05126
	accuracy_policy_5: 0.53311
	loss_value_5: 0.04238
	loss_reward_5: 0.00758
	loss_policy: 0.38402
	loss_value: 0.37156
	loss_reward: 0.03228
[2024-05-13 01:36:22] nn step 42000, lr: 0.059049.
	loss_policy_0: 0.13447
	accuracy_policy_0: 0.72584
	loss_value_0: 0.16886
	loss_policy_1: 0.03395
	accuracy_policy_1: 0.66367
	loss_value_1: 0.03553
	loss_reward_1: 0.00583
	loss_policy_2: 0.03747
	accuracy_policy_2: 0.63521
	loss_value_2: 0.03704
	loss_reward_2: 0.00555
	loss_policy_3: 0.04122
	accuracy_policy_3: 0.60643
	loss_value_3: 0.03847
	loss_reward_3: 0.00604
	loss_policy_4: 0.04439
	accuracy_policy_4: 0.58201
	loss_value_4: 0.03972
	loss_reward_4: 0.00652
	loss_policy_5: 0.04766
	accuracy_policy_5: 0.56279
	loss_value_5: 0.04093
	loss_reward_5: 0.0075
	loss_policy: 0.33918
	loss_value: 0.36054
	loss_reward: 0.03144
Optimization_Done 42000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-13 01:51:12] [command] train weight_iter_42000.pkl 209 211
[2024-05-13 01:52:17] nn step 42100, lr: 0.053144.
	loss_policy_0: 0.15557
	accuracy_policy_0: 0.70102
	loss_value_0: 0.17713
	loss_policy_1: 0.0375
	accuracy_policy_1: 0.64676
	loss_value_1: 0.03733
	loss_reward_1: 0.00543
	loss_policy_2: 0.04134
	accuracy_policy_2: 0.61451
	loss_value_2: 0.03889
	loss_reward_2: 0.00516
	loss_policy_3: 0.04506
	accuracy_policy_3: 0.58742
	loss_value_3: 0.04032
	loss_reward_3: 0.00583
	loss_policy_4: 0.04869
	accuracy_policy_4: 0.56557
	loss_value_4: 0.04155
	loss_reward_4: 0.00627
	loss_policy_5: 0.05226
	accuracy_policy_5: 0.53617
	loss_value_5: 0.04295
	loss_reward_5: 0.00709
	loss_policy: 0.38042
	loss_value: 0.37818
	loss_reward: 0.02979
[2024-05-13 01:53:05] nn step 42200, lr: 0.053144.
	loss_policy_0: 0.13393
	accuracy_policy_0: 0.74129
	loss_value_0: 0.17901
	loss_policy_1: 0.03431
	accuracy_policy_1: 0.68021
	loss_value_1: 0.03772
	loss_reward_1: 0.00564
	loss_policy_2: 0.0388
	accuracy_policy_2: 0.64682
	loss_value_2: 0.03915
	loss_reward_2: 0.00542
	loss_policy_3: 0.04259
	accuracy_policy_3: 0.61984
	loss_value_3: 0.0407
	loss_reward_3: 0.00591
	loss_policy_4: 0.04647
	accuracy_policy_4: 0.59389
	loss_value_4: 0.04216
	loss_reward_4: 0.00643
	loss_policy_5: 0.05013
	accuracy_policy_5: 0.56957
	loss_value_5: 0.04366
	loss_reward_5: 0.0071
	loss_policy: 0.34623
	loss_value: 0.3824
	loss_reward: 0.0305
Optimization_Done 42200
[2024-05-13 01:55:22] [command] train weight_iter_42200.pkl 210 212
[2024-05-13 01:56:23] nn step 42300, lr: 0.053144.
	loss_policy_0: 0.14805
	accuracy_policy_0: 0.70887
	loss_value_0: 0.16793
	loss_policy_1: 0.03597
	accuracy_policy_1: 0.6568
	loss_value_1: 0.03537
	loss_reward_1: 0.0053
	loss_policy_2: 0.0406
	accuracy_policy_2: 0.62191
	loss_value_2: 0.03709
	loss_reward_2: 0.0051
	loss_policy_3: 0.04428
	accuracy_policy_3: 0.59066
	loss_value_3: 0.03868
	loss_reward_3: 0.00548
	loss_policy_4: 0.04775
	accuracy_policy_4: 0.56883
	loss_value_4: 0.04008
	loss_reward_4: 0.00592
	loss_policy_5: 0.0511
	accuracy_policy_5: 0.55141
	loss_value_5: 0.04131
	loss_reward_5: 0.0068
	loss_policy: 0.36775
	loss_value: 0.36047
	loss_reward: 0.0286
[2024-05-13 01:57:24] nn step 42400, lr: 0.053144.
	loss_policy_0: 0.11321
	accuracy_policy_0: 0.75523
	loss_value_0: 0.15718
	loss_policy_1: 0.03008
	accuracy_policy_1: 0.68861
	loss_value_1: 0.03305
	loss_reward_1: 0.00489
	loss_policy_2: 0.03408
	accuracy_policy_2: 0.65184
	loss_value_2: 0.03443
	loss_reward_2: 0.0046
	loss_policy_3: 0.03796
	accuracy_policy_3: 0.62256
	loss_value_3: 0.03575
	loss_reward_3: 0.005
	loss_policy_4: 0.04111
	accuracy_policy_4: 0.60314
	loss_value_4: 0.03698
	loss_reward_4: 0.00552
	loss_policy_5: 0.04455
	accuracy_policy_5: 0.58066
	loss_value_5: 0.03821
	loss_reward_5: 0.00625
	loss_policy: 0.301
	loss_value: 0.3356
	loss_reward: 0.02627
Optimization_Done 42400
[2024-05-13 01:59:41] [command] train weight_iter_42400.pkl 211 213
[2024-05-13 02:00:43] nn step 42500, lr: 0.053144.
	loss_policy_0: 0.14544
	accuracy_policy_0: 0.69459
	loss_value_0: 0.16007
	loss_policy_1: 0.03509
	accuracy_policy_1: 0.63777
	loss_value_1: 0.03379
	loss_reward_1: 0.00513
	loss_policy_2: 0.03918
	accuracy_policy_2: 0.60307
	loss_value_2: 0.03542
	loss_reward_2: 0.00487
	loss_policy_3: 0.04329
	accuracy_policy_3: 0.57156
	loss_value_3: 0.03678
	loss_reward_3: 0.0054
	loss_policy_4: 0.04652
	accuracy_policy_4: 0.54326
	loss_value_4: 0.03805
	loss_reward_4: 0.00594
	loss_policy_5: 0.04967
	accuracy_policy_5: 0.52566
	loss_value_5: 0.03919
	loss_reward_5: 0.00701
	loss_policy: 0.3592
	loss_value: 0.3433
	loss_reward: 0.02834
[2024-05-13 02:01:45] nn step 42600, lr: 0.053144.
	loss_policy_0: 0.11945
	accuracy_policy_0: 0.741
	loss_value_0: 0.16109
	loss_policy_1: 0.03127
	accuracy_policy_1: 0.67656
	loss_value_1: 0.03415
	loss_reward_1: 0.00522
	loss_policy_2: 0.03572
	accuracy_policy_2: 0.64037
	loss_value_2: 0.03563
	loss_reward_2: 0.00486
	loss_policy_3: 0.0398
	accuracy_policy_3: 0.61006
	loss_value_3: 0.03702
	loss_reward_3: 0.00521
	loss_policy_4: 0.04308
	accuracy_policy_4: 0.58146
	loss_value_4: 0.0381
	loss_reward_4: 0.00563
	loss_policy_5: 0.04662
	accuracy_policy_5: 0.55725
	loss_value_5: 0.03954
	loss_reward_5: 0.0068
	loss_policy: 0.31594
	loss_value: 0.34553
	loss_reward: 0.02772
Optimization_Done 42600
[2024-05-13 02:04:05] [command] train weight_iter_42600.pkl 212 214
[2024-05-13 02:05:09] nn step 42700, lr: 0.053144.
	loss_policy_0: 0.11737
	accuracy_policy_0: 0.71193
	loss_value_0: 0.14775
	loss_policy_1: 0.02985
	accuracy_policy_1: 0.64455
	loss_value_1: 0.03108
	loss_reward_1: 0.00495
	loss_policy_2: 0.03409
	accuracy_policy_2: 0.60344
	loss_value_2: 0.0324
	loss_reward_2: 0.00477
	loss_policy_3: 0.03757
	accuracy_policy_3: 0.57195
	loss_value_3: 0.03364
	loss_reward_3: 0.00532
	loss_policy_4: 0.04052
	accuracy_policy_4: 0.54348
	loss_value_4: 0.03482
	loss_reward_4: 0.00557
	loss_policy_5: 0.04317
	accuracy_policy_5: 0.52148
	loss_value_5: 0.03592
	loss_reward_5: 0.0065
	loss_policy: 0.30256
	loss_value: 0.31561
	loss_reward: 0.02711
[2024-05-13 02:06:09] nn step 42800, lr: 0.053144.
	loss_policy_0: 0.09913
	accuracy_policy_0: 0.75285
	loss_value_0: 0.14796
	loss_policy_1: 0.02693
	accuracy_policy_1: 0.676
	loss_value_1: 0.03119
	loss_reward_1: 0.00498
	loss_policy_2: 0.03093
	accuracy_policy_2: 0.63758
	loss_value_2: 0.03253
	loss_reward_2: 0.00474
	loss_policy_3: 0.03414
	accuracy_policy_3: 0.61074
	loss_value_3: 0.03374
	loss_reward_3: 0.00514
	loss_policy_4: 0.03759
	accuracy_policy_4: 0.58068
	loss_value_4: 0.03481
	loss_reward_4: 0.00538
	loss_policy_5: 0.0406
	accuracy_policy_5: 0.55715
	loss_value_5: 0.03586
	loss_reward_5: 0.00626
	loss_policy: 0.26933
	loss_value: 0.31609
	loss_reward: 0.0265
Optimization_Done 42800
[2024-05-13 02:08:29] [command] train weight_iter_42800.pkl 213 215
[2024-05-13 02:09:27] nn step 42900, lr: 0.053144.
	loss_policy_0: 0.1307
	accuracy_policy_0: 0.66615
	loss_value_0: 0.14459
	loss_policy_1: 0.03186
	accuracy_policy_1: 0.5998
	loss_value_1: 0.03029
	loss_reward_1: 0.00448
	loss_policy_2: 0.03525
	accuracy_policy_2: 0.56396
	loss_value_2: 0.03151
	loss_reward_2: 0.00438
	loss_policy_3: 0.038
	accuracy_policy_3: 0.54465
	loss_value_3: 0.03255
	loss_reward_3: 0.00481
	loss_policy_4: 0.04063
	accuracy_policy_4: 0.51625
	loss_value_4: 0.03367
	loss_reward_4: 0.00527
	loss_policy_5: 0.04314
	accuracy_policy_5: 0.48848
	loss_value_5: 0.03471
	loss_reward_5: 0.00616
	loss_policy: 0.31957
	loss_value: 0.30732
	loss_reward: 0.0251
[2024-05-13 02:10:31] nn step 43000, lr: 0.053144.
	loss_policy_0: 0.11229
	accuracy_policy_0: 0.72439
	loss_value_0: 0.15046
	loss_policy_1: 0.02972
	accuracy_policy_1: 0.64414
	loss_value_1: 0.03161
	loss_reward_1: 0.00477
	loss_policy_2: 0.03319
	accuracy_policy_2: 0.60926
	loss_value_2: 0.03286
	loss_reward_2: 0.00459
	loss_policy_3: 0.03625
	accuracy_policy_3: 0.58697
	loss_value_3: 0.03401
	loss_reward_3: 0.00515
	loss_policy_4: 0.03954
	accuracy_policy_4: 0.55316
	loss_value_4: 0.035
	loss_reward_4: 0.00526
	loss_policy_5: 0.04234
	accuracy_policy_5: 0.53139
	loss_value_5: 0.03611
	loss_reward_5: 0.00629
	loss_policy: 0.29334
	loss_value: 0.32005
	loss_reward: 0.02607
Optimization_Done 43000
[2024-05-13 02:12:50] [command] train weight_iter_43000.pkl 214 216
[2024-05-13 02:13:53] nn step 43100, lr: 0.053144.
	loss_policy_0: 0.14005
	accuracy_policy_0: 0.6748
	loss_value_0: 0.13631
	loss_policy_1: 0.03292
	accuracy_policy_1: 0.62469
	loss_value_1: 0.02872
	loss_reward_1: 0.00441
	loss_policy_2: 0.03573
	accuracy_policy_2: 0.59422
	loss_value_2: 0.03005
	loss_reward_2: 0.00427
	loss_policy_3: 0.0385
	accuracy_policy_3: 0.57326
	loss_value_3: 0.03122
	loss_reward_3: 0.00455
	loss_policy_4: 0.04083
	accuracy_policy_4: 0.54955
	loss_value_4: 0.03224
	loss_reward_4: 0.00502
	loss_policy_5: 0.04359
	accuracy_policy_5: 0.52154
	loss_value_5: 0.03333
	loss_reward_5: 0.00576
	loss_policy: 0.33161
	loss_value: 0.29187
	loss_reward: 0.024
[2024-05-13 02:14:58] nn step 43200, lr: 0.053144.
	loss_policy_0: 0.12096
	accuracy_policy_0: 0.72162
	loss_value_0: 0.14213
	loss_policy_1: 0.03029
	accuracy_policy_1: 0.65916
	loss_value_1: 0.02989
	loss_reward_1: 0.00467
	loss_policy_2: 0.03415
	accuracy_policy_2: 0.62361
	loss_value_2: 0.03117
	loss_reward_2: 0.00442
	loss_policy_3: 0.03706
	accuracy_policy_3: 0.59592
	loss_value_3: 0.03233
	loss_reward_3: 0.00474
	loss_policy_4: 0.03948
	accuracy_policy_4: 0.57541
	loss_value_4: 0.03359
	loss_reward_4: 0.00521
	loss_policy_5: 0.0422
	accuracy_policy_5: 0.54787
	loss_value_5: 0.03461
	loss_reward_5: 0.00595
	loss_policy: 0.30414
	loss_value: 0.30373
	loss_reward: 0.02499
Optimization_Done 43200
[2024-05-13 02:17:14] [command] train weight_iter_43200.pkl 215 217
[2024-05-13 02:17:59] nn step 43300, lr: 0.053144.
	loss_policy_0: 0.16575
	accuracy_policy_0: 0.6734
	loss_value_0: 0.15829
	loss_policy_1: 0.03865
	accuracy_policy_1: 0.62273
	loss_value_1: 0.03342
	loss_reward_1: 0.00523
	loss_policy_2: 0.04227
	accuracy_policy_2: 0.59082
	loss_value_2: 0.03502
	loss_reward_2: 0.00507
	loss_policy_3: 0.04538
	accuracy_policy_3: 0.56844
	loss_value_3: 0.03634
	loss_reward_3: 0.0055
	loss_policy_4: 0.04802
	accuracy_policy_4: 0.54525
	loss_value_4: 0.0377
	loss_reward_4: 0.00602
	loss_policy_5: 0.05068
	accuracy_policy_5: 0.52203
	loss_value_5: 0.03886
	loss_reward_5: 0.00699
	loss_policy: 0.39075
	loss_value: 0.33963
	loss_reward: 0.02881
[2024-05-13 02:18:43] nn step 43400, lr: 0.053144.
	loss_policy_0: 0.13895
	accuracy_policy_0: 0.72
	loss_value_0: 0.15609
	loss_policy_1: 0.03439
	accuracy_policy_1: 0.65803
	loss_value_1: 0.03276
	loss_reward_1: 0.00509
	loss_policy_2: 0.03829
	accuracy_policy_2: 0.6216
	loss_value_2: 0.03405
	loss_reward_2: 0.00493
	loss_policy_3: 0.04125
	accuracy_policy_3: 0.59936
	loss_value_3: 0.0352
	loss_reward_3: 0.00542
	loss_policy_4: 0.04456
	accuracy_policy_4: 0.57416
	loss_value_4: 0.03646
	loss_reward_4: 0.00583
	loss_policy_5: 0.04712
	accuracy_policy_5: 0.55221
	loss_value_5: 0.03771
	loss_reward_5: 0.00677
	loss_policy: 0.34455
	loss_value: 0.33227
	loss_reward: 0.02804
Optimization_Done 43400
[2024-05-13 02:20:59] [command] train weight_iter_43400.pkl 216 218
[2024-05-13 02:22:01] nn step 43500, lr: 0.053144.
	loss_policy_0: 0.16812
	accuracy_policy_0: 0.69664
	loss_value_0: 0.17676
	loss_policy_1: 0.04191
	accuracy_policy_1: 0.63471
	loss_value_1: 0.03701
	loss_reward_1: 0.00676
	loss_policy_2: 0.04632
	accuracy_policy_2: 0.60006
	loss_value_2: 0.03857
	loss_reward_2: 0.00635
	loss_policy_3: 0.05006
	accuracy_policy_3: 0.56684
	loss_value_3: 0.03999
	loss_reward_3: 0.00686
	loss_policy_4: 0.05348
	accuracy_policy_4: 0.54518
	loss_value_4: 0.04135
	loss_reward_4: 0.00754
	loss_policy_5: 0.05661
	accuracy_policy_5: 0.52248
	loss_value_5: 0.04288
	loss_reward_5: 0.00866
	loss_policy: 0.41651
	loss_value: 0.37656
	loss_reward: 0.03617
[2024-05-13 02:23:01] nn step 43600, lr: 0.053144.
	loss_policy_0: 0.14552
	accuracy_policy_0: 0.7366
	loss_value_0: 0.17538
	loss_policy_1: 0.03838
	accuracy_policy_1: 0.66197
	loss_value_1: 0.03675
	loss_reward_1: 0.00679
	loss_policy_2: 0.04252
	accuracy_policy_2: 0.62906
	loss_value_2: 0.03822
	loss_reward_2: 0.00636
	loss_policy_3: 0.04685
	accuracy_policy_3: 0.60211
	loss_value_3: 0.03975
	loss_reward_3: 0.00702
	loss_policy_4: 0.05008
	accuracy_policy_4: 0.57961
	loss_value_4: 0.04107
	loss_reward_4: 0.00739
	loss_policy_5: 0.05327
	accuracy_policy_5: 0.55672
	loss_value_5: 0.04248
	loss_reward_5: 0.00864
	loss_policy: 0.37661
	loss_value: 0.37365
	loss_reward: 0.0362
Optimization_Done 43600
[2024-05-13 02:24:54] [command] train weight_iter_43600.pkl 217 219
[2024-05-13 02:25:58] nn step 43700, lr: 0.053144.
	loss_policy_0: 0.15432
	accuracy_policy_0: 0.7001
	loss_value_0: 0.17519
	loss_policy_1: 0.03892
	accuracy_policy_1: 0.63199
	loss_value_1: 0.0366
	loss_reward_1: 0.00609
	loss_policy_2: 0.04284
	accuracy_policy_2: 0.59297
	loss_value_2: 0.03783
	loss_reward_2: 0.0061
	loss_policy_3: 0.04644
	accuracy_policy_3: 0.56502
	loss_value_3: 0.03897
	loss_reward_3: 0.00654
	loss_policy_4: 0.04942
	accuracy_policy_4: 0.54148
	loss_value_4: 0.04011
	loss_reward_4: 0.00702
	loss_policy_5: 0.05223
	accuracy_policy_5: 0.51381
	loss_value_5: 0.04118
	loss_reward_5: 0.00816
	loss_policy: 0.38418
	loss_value: 0.36989
	loss_reward: 0.03391
[2024-05-13 02:27:03] nn step 43800, lr: 0.053144.
	loss_policy_0: 0.13449
	accuracy_policy_0: 0.74055
	loss_value_0: 0.17298
	loss_policy_1: 0.03565
	accuracy_policy_1: 0.66318
	loss_value_1: 0.03633
	loss_reward_1: 0.00617
	loss_policy_2: 0.03998
	accuracy_policy_2: 0.62555
	loss_value_2: 0.03785
	loss_reward_2: 0.00602
	loss_policy_3: 0.04331
	accuracy_policy_3: 0.59984
	loss_value_3: 0.03904
	loss_reward_3: 0.0066
	loss_policy_4: 0.04688
	accuracy_policy_4: 0.5707
	loss_value_4: 0.04008
	loss_reward_4: 0.00708
	loss_policy_5: 0.05001
	accuracy_policy_5: 0.54281
	loss_value_5: 0.04139
	loss_reward_5: 0.00816
	loss_policy: 0.35031
	loss_value: 0.36767
	loss_reward: 0.03404
Optimization_Done 43800
[2024-05-13 02:29:19] [command] train weight_iter_43800.pkl 218 220
[2024-05-13 02:30:23] nn step 43900, lr: 0.053144.
	loss_policy_0: 0.15966
	accuracy_policy_0: 0.70676
	loss_value_0: 0.16667
	loss_policy_1: 0.03961
	accuracy_policy_1: 0.64674
	loss_value_1: 0.03508
	loss_reward_1: 0.0057
	loss_policy_2: 0.04377
	accuracy_policy_2: 0.61311
	loss_value_2: 0.03648
	loss_reward_2: 0.0055
	loss_policy_3: 0.04736
	accuracy_policy_3: 0.58621
	loss_value_3: 0.03777
	loss_reward_3: 0.00598
	loss_policy_4: 0.05018
	accuracy_policy_4: 0.56102
	loss_value_4: 0.03904
	loss_reward_4: 0.00646
	loss_policy_5: 0.05295
	accuracy_policy_5: 0.53793
	loss_value_5: 0.0403
	loss_reward_5: 0.00728
	loss_policy: 0.39354
	loss_value: 0.35535
	loss_reward: 0.03091
[2024-05-13 02:31:27] nn step 44000, lr: 0.053144.
	loss_policy_0: 0.14114
	accuracy_policy_0: 0.74498
	loss_value_0: 0.17124
	loss_policy_1: 0.03737
	accuracy_policy_1: 0.67191
	loss_value_1: 0.03609
	loss_reward_1: 0.00603
	loss_policy_2: 0.04208
	accuracy_policy_2: 0.63521
	loss_value_2: 0.03761
	loss_reward_2: 0.00555
	loss_policy_3: 0.0455
	accuracy_policy_3: 0.61133
	loss_value_3: 0.03885
	loss_reward_3: 0.00598
	loss_policy_4: 0.04875
	accuracy_policy_4: 0.58768
	loss_value_4: 0.04008
	loss_reward_4: 0.00663
	loss_policy_5: 0.05187
	accuracy_policy_5: 0.56232
	loss_value_5: 0.04126
	loss_reward_5: 0.00777
	loss_policy: 0.36671
	loss_value: 0.36513
	loss_reward: 0.03196
Optimization_Done 44000
[2024-05-13 02:33:33] [command] train weight_iter_44000.pkl 219 221
[2024-05-13 02:34:36] nn step 44100, lr: 0.053144.
	loss_policy_0: 0.1778
	accuracy_policy_0: 0.69664
	loss_value_0: 0.17906
	loss_policy_1: 0.04138
	accuracy_policy_1: 0.64857
	loss_value_1: 0.03735
	loss_reward_1: 0.00521
	loss_policy_2: 0.04556
	accuracy_policy_2: 0.61592
	loss_value_2: 0.03889
	loss_reward_2: 0.00521
	loss_policy_3: 0.04862
	accuracy_policy_3: 0.59297
	loss_value_3: 0.04023
	loss_reward_3: 0.00559
	loss_policy_4: 0.05174
	accuracy_policy_4: 0.57121
	loss_value_4: 0.04156
	loss_reward_4: 0.00617
	loss_policy_5: 0.05439
	accuracy_policy_5: 0.55148
	loss_value_5: 0.04295
	loss_reward_5: 0.0072
	loss_policy: 0.4195
	loss_value: 0.38003
	loss_reward: 0.02938
[2024-05-13 02:35:39] nn step 44200, lr: 0.053144.
	loss_policy_0: 0.13351
	accuracy_policy_0: 0.74668
	loss_value_0: 0.16411
	loss_policy_1: 0.03349
	accuracy_policy_1: 0.68607
	loss_value_1: 0.03443
	loss_reward_1: 0.00487
	loss_policy_2: 0.0374
	accuracy_policy_2: 0.65551
	loss_value_2: 0.03588
	loss_reward_2: 0.00468
	loss_policy_3: 0.04015
	accuracy_policy_3: 0.63428
	loss_value_3: 0.03714
	loss_reward_3: 0.00505
	loss_policy_4: 0.04347
	accuracy_policy_4: 0.61023
	loss_value_4: 0.03823
	loss_reward_4: 0.00543
	loss_policy_5: 0.04654
	accuracy_policy_5: 0.58711
	loss_value_5: 0.0394
	loss_reward_5: 0.00664
	loss_policy: 0.33455
	loss_value: 0.34918
	loss_reward: 0.02667
Optimization_Done 44200
[2024-05-13 02:37:54] [command] train weight_iter_44200.pkl 220 222
[2024-05-13 02:38:46] nn step 44300, lr: 0.053144.
	loss_policy_0: 0.16144
	accuracy_policy_0: 0.67416
	loss_value_0: 0.15437
	loss_policy_1: 0.03728
	accuracy_policy_1: 0.62598
	loss_value_1: 0.03251
	loss_reward_1: 0.00517
	loss_policy_2: 0.04069
	accuracy_policy_2: 0.59607
	loss_value_2: 0.03385
	loss_reward_2: 0.00476
	loss_policy_3: 0.04338
	accuracy_policy_3: 0.57631
	loss_value_3: 0.03499
	loss_reward_3: 0.00537
	loss_policy_4: 0.04564
	accuracy_policy_4: 0.55658
	loss_value_4: 0.03622
	loss_reward_4: 0.00575
	loss_policy_5: 0.04764
	accuracy_policy_5: 0.54164
	loss_value_5: 0.03728
	loss_reward_5: 0.00689
	loss_policy: 0.37607
	loss_value: 0.32922
	loss_reward: 0.02795
[2024-05-13 02:39:50] nn step 44400, lr: 0.053144.
	loss_policy_0: 0.14505
	accuracy_policy_0: 0.72191
	loss_value_0: 0.1624
	loss_policy_1: 0.03527
	accuracy_policy_1: 0.66711
	loss_value_1: 0.03413
	loss_reward_1: 0.00542
	loss_policy_2: 0.03891
	accuracy_policy_2: 0.63551
	loss_value_2: 0.0357
	loss_reward_2: 0.00504
	loss_policy_3: 0.04218
	accuracy_policy_3: 0.61559
	loss_value_3: 0.03702
	loss_reward_3: 0.00549
	loss_policy_4: 0.04517
	accuracy_policy_4: 0.58834
	loss_value_4: 0.03821
	loss_reward_4: 0.00591
	loss_policy_5: 0.04793
	accuracy_policy_5: 0.56977
	loss_value_5: 0.03953
	loss_reward_5: 0.00717
	loss_policy: 0.3545
	loss_value: 0.34699
	loss_reward: 0.02902
Optimization_Done 44400
[2024-05-13 02:41:42] [command] train weight_iter_44400.pkl 221 223
[2024-05-13 02:42:34] nn step 44500, lr: 0.053144.
	loss_policy_0: 0.13727
	accuracy_policy_0: 0.70523
	loss_value_0: 0.15652
	loss_policy_1: 0.03276
	accuracy_policy_1: 0.64871
	loss_value_1: 0.03266
	loss_reward_1: 0.0048
	loss_policy_2: 0.03544
	accuracy_policy_2: 0.62371
	loss_value_2: 0.03387
	loss_reward_2: 0.00453
	loss_policy_3: 0.03801
	accuracy_policy_3: 0.60117
	loss_value_3: 0.03506
	loss_reward_3: 0.00487
	loss_policy_4: 0.04043
	accuracy_policy_4: 0.57654
	loss_value_4: 0.03624
	loss_reward_4: 0.00534
	loss_policy_5: 0.04291
	accuracy_policy_5: 0.5525
	loss_value_5: 0.03727
	loss_reward_5: 0.00629
	loss_policy: 0.32681
	loss_value: 0.33163
	loss_reward: 0.02584
[2024-05-13 02:43:25] nn step 44600, lr: 0.053144.
	loss_policy_0: 0.11302
	accuracy_policy_0: 0.75699
	loss_value_0: 0.15262
	loss_policy_1: 0.02888
	accuracy_policy_1: 0.68986
	loss_value_1: 0.03192
	loss_reward_1: 0.0048
	loss_policy_2: 0.03224
	accuracy_policy_2: 0.65566
	loss_value_2: 0.03321
	loss_reward_2: 0.00454
	loss_policy_3: 0.03471
	accuracy_policy_3: 0.63723
	loss_value_3: 0.03425
	loss_reward_3: 0.00475
	loss_policy_4: 0.03723
	accuracy_policy_4: 0.61473
	loss_value_4: 0.03534
	loss_reward_4: 0.00524
	loss_policy_5: 0.03949
	accuracy_policy_5: 0.59062
	loss_value_5: 0.0364
	loss_reward_5: 0.00607
	loss_policy: 0.28557
	loss_value: 0.32373
	loss_reward: 0.0254
Optimization_Done 44600
[2024-05-13 02:45:40] [command] train weight_iter_44600.pkl 222 224
[2024-05-13 02:46:42] nn step 44700, lr: 0.053144.
	loss_policy_0: 0.17209
	accuracy_policy_0: 0.65334
	loss_value_0: 0.15643
	loss_policy_1: 0.03923
	accuracy_policy_1: 0.60932
	loss_value_1: 0.03249
	loss_reward_1: 0.00436
	loss_policy_2: 0.0424
	accuracy_policy_2: 0.57592
	loss_value_2: 0.03376
	loss_reward_2: 0.00401
	loss_policy_3: 0.04494
	accuracy_policy_3: 0.55305
	loss_value_3: 0.03492
	loss_reward_3: 0.00447
	loss_policy_4: 0.04727
	accuracy_policy_4: 0.53006
	loss_value_4: 0.03594
	loss_reward_4: 0.00481
	loss_policy_5: 0.04955
	accuracy_policy_5: 0.51293
	loss_value_5: 0.03693
	loss_reward_5: 0.00566
	loss_policy: 0.39547
	loss_value: 0.33047
	loss_reward: 0.0233
[2024-05-13 02:47:47] nn step 44800, lr: 0.053144.
	loss_policy_0: 0.12976
	accuracy_policy_0: 0.72613
	loss_value_0: 0.14965
	loss_policy_1: 0.03163
	accuracy_policy_1: 0.66326
	loss_value_1: 0.0313
	loss_reward_1: 0.00422
	loss_policy_2: 0.0353
	accuracy_policy_2: 0.62672
	loss_value_2: 0.03254
	loss_reward_2: 0.00397
	loss_policy_3: 0.03787
	accuracy_policy_3: 0.60777
	loss_value_3: 0.03366
	loss_reward_3: 0.0042
	loss_policy_4: 0.0408
	accuracy_policy_4: 0.58121
	loss_value_4: 0.0347
	loss_reward_4: 0.00462
	loss_policy_5: 0.0433
	accuracy_policy_5: 0.55996
	loss_value_5: 0.03569
	loss_reward_5: 0.00542
	loss_policy: 0.31867
	loss_value: 0.31754
	loss_reward: 0.02244
Optimization_Done 44800
[2024-05-13 02:50:02] [command] train weight_iter_44800.pkl 223 225
[2024-05-13 02:51:06] nn step 44900, lr: 0.053144.
	loss_policy_0: 0.16277
	accuracy_policy_0: 0.65668
	loss_value_0: 0.14608
	loss_policy_1: 0.03644
	accuracy_policy_1: 0.6185
	loss_value_1: 0.03044
	loss_reward_1: 0.00393
	loss_policy_2: 0.03937
	accuracy_policy_2: 0.59111
	loss_value_2: 0.03168
	loss_reward_2: 0.00359
	loss_policy_3: 0.04187
	accuracy_policy_3: 0.56625
	loss_value_3: 0.03287
	loss_reward_3: 0.00391
	loss_policy_4: 0.04431
	accuracy_policy_4: 0.54736
	loss_value_4: 0.03393
	loss_reward_4: 0.0044
	loss_policy_5: 0.04665
	accuracy_policy_5: 0.52646
	loss_value_5: 0.0349
	loss_reward_5: 0.005
	loss_policy: 0.37141
	loss_value: 0.3099
	loss_reward: 0.02082
[2024-05-13 02:52:09] nn step 45000, lr: 0.053144.
	loss_policy_0: 0.13876
	accuracy_policy_0: 0.71703
	loss_value_0: 0.15058
	loss_policy_1: 0.03269
	accuracy_policy_1: 0.66576
	loss_value_1: 0.03128
	loss_reward_1: 0.00395
	loss_policy_2: 0.03612
	accuracy_policy_2: 0.63414
	loss_value_2: 0.0325
	loss_reward_2: 0.00369
	loss_policy_3: 0.03904
	accuracy_policy_3: 0.60695
	loss_value_3: 0.03364
	loss_reward_3: 0.00395
	loss_policy_4: 0.0419
	accuracy_policy_4: 0.58846
	loss_value_4: 0.03471
	loss_reward_4: 0.00437
	loss_policy_5: 0.04461
	accuracy_policy_5: 0.56338
	loss_value_5: 0.03577
	loss_reward_5: 0.00499
	loss_policy: 0.33312
	loss_value: 0.31849
	loss_reward: 0.02094
Optimization_Done 45000
[2024-05-13 02:54:25] [command] train weight_iter_45000.pkl 224 226
[2024-05-13 02:55:25] nn step 45100, lr: 0.053144.
	loss_policy_0: 0.18554
	accuracy_policy_0: 0.64027
	loss_value_0: 0.16077
	loss_policy_1: 0.0419
	accuracy_policy_1: 0.59475
	loss_value_1: 0.03368
	loss_reward_1: 0.00512
	loss_policy_2: 0.04508
	accuracy_policy_2: 0.56912
	loss_value_2: 0.03513
	loss_reward_2: 0.00493
	loss_policy_3: 0.04848
	accuracy_policy_3: 0.5424
	loss_value_3: 0.03652
	loss_reward_3: 0.00524
	loss_policy_4: 0.05124
	accuracy_policy_4: 0.51449
	loss_value_4: 0.03788
	loss_reward_4: 0.00575
	loss_policy_5: 0.05399
	accuracy_policy_5: 0.49656
	loss_value_5: 0.03905
	loss_reward_5: 0.00667
	loss_policy: 0.42624
	loss_value: 0.34303
	loss_reward: 0.02771
[2024-05-13 02:56:30] nn step 45200, lr: 0.053144.
	loss_policy_0: 0.15668
	accuracy_policy_0: 0.69949
	loss_value_0: 0.16295
	loss_policy_1: 0.03762
	accuracy_policy_1: 0.64164
	loss_value_1: 0.03407
	loss_reward_1: 0.00506
	loss_policy_2: 0.04176
	accuracy_policy_2: 0.61088
	loss_value_2: 0.03538
	loss_reward_2: 0.00483
	loss_policy_3: 0.0454
	accuracy_policy_3: 0.5793
	loss_value_3: 0.03675
	loss_reward_3: 0.00514
	loss_policy_4: 0.0483
	accuracy_policy_4: 0.5559
	loss_value_4: 0.03799
	loss_reward_4: 0.0058
	loss_policy_5: 0.05131
	accuracy_policy_5: 0.53516
	loss_value_5: 0.03925
	loss_reward_5: 0.00678
	loss_policy: 0.38107
	loss_value: 0.34639
	loss_reward: 0.0276
Optimization_Done 45200
[2024-05-13 02:58:44] [command] train weight_iter_45200.pkl 225 227
[2024-05-13 02:59:45] nn step 45300, lr: 0.053144.
	loss_policy_0: 0.15854
	accuracy_policy_0: 0.69607
	loss_value_0: 0.18034
	loss_policy_1: 0.03775
	accuracy_policy_1: 0.64391
	loss_value_1: 0.03756
	loss_reward_1: 0.00615
	loss_policy_2: 0.04078
	accuracy_policy_2: 0.61764
	loss_value_2: 0.03896
	loss_reward_2: 0.00595
	loss_policy_3: 0.0439
	accuracy_policy_3: 0.59516
	loss_value_3: 0.04035
	loss_reward_3: 0.00647
	loss_policy_4: 0.04631
	accuracy_policy_4: 0.58023
	loss_value_4: 0.04155
	loss_reward_4: 0.00709
	loss_policy_5: 0.04844
	accuracy_policy_5: 0.56707
	loss_value_5: 0.04278
	loss_reward_5: 0.00805
	loss_policy: 0.37574
	loss_value: 0.38154
	loss_reward: 0.03372
[2024-05-13 03:00:47] nn step 45400, lr: 0.053144.
	loss_policy_0: 0.14005
	accuracy_policy_0: 0.7358
	loss_value_0: 0.17802
	loss_policy_1: 0.03482
	accuracy_policy_1: 0.67305
	loss_value_1: 0.03709
	loss_reward_1: 0.00615
	loss_policy_2: 0.03853
	accuracy_policy_2: 0.64297
	loss_value_2: 0.03858
	loss_reward_2: 0.00601
	loss_policy_3: 0.04161
	accuracy_policy_3: 0.62291
	loss_value_3: 0.04001
	loss_reward_3: 0.00641
	loss_policy_4: 0.04439
	accuracy_policy_4: 0.60221
	loss_value_4: 0.04115
	loss_reward_4: 0.00698
	loss_policy_5: 0.04673
	accuracy_policy_5: 0.58777
	loss_value_5: 0.0423
	loss_reward_5: 0.00814
	loss_policy: 0.34612
	loss_value: 0.37715
	loss_reward: 0.0337
Optimization_Done 45400
[2024-05-13 03:02:40] [command] train weight_iter_45400.pkl 226 228
[2024-05-13 03:03:43] nn step 45500, lr: 0.053144.
	loss_policy_0: 0.1575
	accuracy_policy_0: 0.69297
	loss_value_0: 0.17446
	loss_policy_1: 0.03723
	accuracy_policy_1: 0.64018
	loss_value_1: 0.03641
	loss_reward_1: 0.00571
	loss_policy_2: 0.04045
	accuracy_policy_2: 0.61184
	loss_value_2: 0.03781
	loss_reward_2: 0.00562
	loss_policy_3: 0.04361
	accuracy_policy_3: 0.58885
	loss_value_3: 0.03903
	loss_reward_3: 0.00606
	loss_policy_4: 0.04623
	accuracy_policy_4: 0.56611
	loss_value_4: 0.04018
	loss_reward_4: 0.00655
	loss_policy_5: 0.04852
	accuracy_policy_5: 0.54834
	loss_value_5: 0.04114
	loss_reward_5: 0.00773
	loss_policy: 0.37353
	loss_value: 0.36903
	loss_reward: 0.03168
[2024-05-13 03:04:50] nn step 45600, lr: 0.053144.
	loss_policy_0: 0.1302
	accuracy_policy_0: 0.73672
	loss_value_0: 0.16706
	loss_policy_1: 0.03262
	accuracy_policy_1: 0.67572
	loss_value_1: 0.03494
	loss_reward_1: 0.00564
	loss_policy_2: 0.03567
	accuracy_policy_2: 0.64494
	loss_value_2: 0.03618
	loss_reward_2: 0.00553
	loss_policy_3: 0.03873
	accuracy_policy_3: 0.6199
	loss_value_3: 0.03733
	loss_reward_3: 0.0059
	loss_policy_4: 0.04105
	accuracy_policy_4: 0.60119
	loss_value_4: 0.0385
	loss_reward_4: 0.00628
	loss_policy_5: 0.04401
	accuracy_policy_5: 0.5799
	loss_value_5: 0.03967
	loss_reward_5: 0.00742
	loss_policy: 0.32228
	loss_value: 0.35368
	loss_reward: 0.03076
Optimization_Done 45600
[2024-05-13 03:07:02] [command] train weight_iter_45600.pkl 227 229
[2024-05-13 03:08:04] nn step 45700, lr: 0.053144.
	loss_policy_0: 0.14361
	accuracy_policy_0: 0.68467
	loss_value_0: 0.15136
	loss_policy_1: 0.03387
	accuracy_policy_1: 0.63248
	loss_value_1: 0.03171
	loss_reward_1: 0.00496
	loss_policy_2: 0.03713
	accuracy_policy_2: 0.60178
	loss_value_2: 0.03303
	loss_reward_2: 0.00488
	loss_policy_3: 0.04008
	accuracy_policy_3: 0.58066
	loss_value_3: 0.03428
	loss_reward_3: 0.00531
	loss_policy_4: 0.0423
	accuracy_policy_4: 0.55771
	loss_value_4: 0.03554
	loss_reward_4: 0.00583
	loss_policy_5: 0.04501
	accuracy_policy_5: 0.53705
	loss_value_5: 0.03656
	loss_reward_5: 0.00675
	loss_policy: 0.342
	loss_value: 0.32248
	loss_reward: 0.02773
[2024-05-13 03:09:04] nn step 45800, lr: 0.053144.
	loss_policy_0: 0.12523
	accuracy_policy_0: 0.73533
	loss_value_0: 0.15724
	loss_policy_1: 0.03147
	accuracy_policy_1: 0.67605
	loss_value_1: 0.03316
	loss_reward_1: 0.00538
	loss_policy_2: 0.0352
	accuracy_policy_2: 0.64334
	loss_value_2: 0.03465
	loss_reward_2: 0.00514
	loss_policy_3: 0.03828
	accuracy_policy_3: 0.61754
	loss_value_3: 0.03609
	loss_reward_3: 0.00544
	loss_policy_4: 0.04116
	accuracy_policy_4: 0.59555
	loss_value_4: 0.03726
	loss_reward_4: 0.00599
	loss_policy_5: 0.04437
	accuracy_policy_5: 0.56758
	loss_value_5: 0.03843
	loss_reward_5: 0.00709
	loss_policy: 0.3157
	loss_value: 0.33684
	loss_reward: 0.02904
Optimization_Done 45800
[2024-05-13 03:11:18] [command] train weight_iter_45800.pkl 228 230
[2024-05-13 03:12:22] nn step 45900, lr: 0.053144.
	loss_policy_0: 0.16383
	accuracy_policy_0: 0.65143
	loss_value_0: 0.16068
	loss_policy_1: 0.0379
	accuracy_policy_1: 0.6073
	loss_value_1: 0.03367
	loss_reward_1: 0.00466
	loss_policy_2: 0.04155
	accuracy_policy_2: 0.57555
	loss_value_2: 0.0352
	loss_reward_2: 0.00454
	loss_policy_3: 0.04489
	accuracy_policy_3: 0.55105
	loss_value_3: 0.03649
	loss_reward_3: 0.00484
	loss_policy_4: 0.04821
	accuracy_policy_4: 0.52129
	loss_value_4: 0.03764
	loss_reward_4: 0.00532
	loss_policy_5: 0.051
	accuracy_policy_5: 0.4975
	loss_value_5: 0.03892
	loss_reward_5: 0.00652
	loss_policy: 0.38738
	loss_value: 0.34259
	loss_reward: 0.02589
[2024-05-13 03:13:28] nn step 46000, lr: 0.053144.
	loss_policy_0: 0.13209
	accuracy_policy_0: 0.71064
	loss_value_0: 0.1563
	loss_policy_1: 0.0328
	accuracy_policy_1: 0.64877
	loss_value_1: 0.03291
	loss_reward_1: 0.00451
	loss_policy_2: 0.03669
	accuracy_policy_2: 0.6159
	loss_value_2: 0.03439
	loss_reward_2: 0.00458
	loss_policy_3: 0.0401
	accuracy_policy_3: 0.58613
	loss_value_3: 0.03557
	loss_reward_3: 0.00481
	loss_policy_4: 0.04337
	accuracy_policy_4: 0.56197
	loss_value_4: 0.03683
	loss_reward_4: 0.00513
	loss_policy_5: 0.04662
	accuracy_policy_5: 0.53754
	loss_value_5: 0.03796
	loss_reward_5: 0.00644
	loss_policy: 0.33167
	loss_value: 0.33396
	loss_reward: 0.02546
Optimization_Done 46000
[2024-05-13 03:15:40] [command] train weight_iter_46000.pkl 229 231
[2024-05-13 03:16:44] nn step 46100, lr: 0.053144.
	loss_policy_0: 0.15088
	accuracy_policy_0: 0.66811
	loss_value_0: 0.15683
	loss_policy_1: 0.03622
	accuracy_policy_1: 0.60621
	loss_value_1: 0.03289
	loss_reward_1: 0.00467
	loss_policy_2: 0.03992
	accuracy_policy_2: 0.57514
	loss_value_2: 0.03429
	loss_reward_2: 0.00457
	loss_policy_3: 0.04302
	accuracy_policy_3: 0.54955
	loss_value_3: 0.03561
	loss_reward_3: 0.00487
	loss_policy_4: 0.04623
	accuracy_policy_4: 0.52166
	loss_value_4: 0.03675
	loss_reward_4: 0.00541
	loss_policy_5: 0.04899
	accuracy_policy_5: 0.49973
	loss_value_5: 0.03794
	loss_reward_5: 0.00635
	loss_policy: 0.36526
	loss_value: 0.33431
	loss_reward: 0.02588
[2024-05-13 03:17:51] nn step 46200, lr: 0.053144.
	loss_policy_0: 0.12908
	accuracy_policy_0: 0.71492
	loss_value_0: 0.15965
	loss_policy_1: 0.03331
	accuracy_policy_1: 0.64639
	loss_value_1: 0.03347
	loss_reward_1: 0.00474
	loss_policy_2: 0.03741
	accuracy_policy_2: 0.61305
	loss_value_2: 0.03487
	loss_reward_2: 0.00461
	loss_policy_3: 0.0408
	accuracy_policy_3: 0.5841
	loss_value_3: 0.03607
	loss_reward_3: 0.00481
	loss_policy_4: 0.04406
	accuracy_policy_4: 0.55391
	loss_value_4: 0.03717
	loss_reward_4: 0.00534
	loss_policy_5: 0.04728
	accuracy_policy_5: 0.52812
	loss_value_5: 0.0382
	loss_reward_5: 0.00649
	loss_policy: 0.33193
	loss_value: 0.33943
	loss_reward: 0.02599
Optimization_Done 46200
[2024-05-13 03:19:53] [command] train weight_iter_46200.pkl 230 232
[2024-05-13 03:20:55] nn step 46300, lr: 0.053144.
	loss_policy_0: 0.15313
	accuracy_policy_0: 0.68121
	loss_value_0: 0.16784
	loss_policy_1: 0.03661
	accuracy_policy_1: 0.62271
	loss_value_1: 0.03523
	loss_reward_1: 0.00437
	loss_policy_2: 0.04027
	accuracy_policy_2: 0.58959
	loss_value_2: 0.03669
	loss_reward_2: 0.00423
	loss_policy_3: 0.04312
	accuracy_policy_3: 0.56863
	loss_value_3: 0.03793
	loss_reward_3: 0.00447
	loss_policy_4: 0.04589
	accuracy_policy_4: 0.54635
	loss_value_4: 0.03907
	loss_reward_4: 0.00513
	loss_policy_5: 0.04864
	accuracy_policy_5: 0.52309
	loss_value_5: 0.04021
	loss_reward_5: 0.00591
	loss_policy: 0.36766
	loss_value: 0.35696
	loss_reward: 0.02411
[2024-05-13 03:21:56] nn step 46400, lr: 0.053144.
	loss_policy_0: 0.11697
	accuracy_policy_0: 0.73355
	loss_value_0: 0.15533
	loss_policy_1: 0.03024
	accuracy_policy_1: 0.66438
	loss_value_1: 0.03245
	loss_reward_1: 0.00422
	loss_policy_2: 0.03371
	accuracy_policy_2: 0.62988
	loss_value_2: 0.0337
	loss_reward_2: 0.004
	loss_policy_3: 0.03708
	accuracy_policy_3: 0.59986
	loss_value_3: 0.03488
	loss_reward_3: 0.00432
	loss_policy_4: 0.03993
	accuracy_policy_4: 0.57412
	loss_value_4: 0.03588
	loss_reward_4: 0.00463
	loss_policy_5: 0.04296
	accuracy_policy_5: 0.54906
	loss_value_5: 0.03696
	loss_reward_5: 0.00551
	loss_policy: 0.30089
	loss_value: 0.32919
	loss_reward: 0.02267
Optimization_Done 46400
[2024-05-13 03:24:12] [command] train weight_iter_46400.pkl 231 233
[2024-05-13 03:25:15] nn step 46500, lr: 0.053144.
	loss_policy_0: 0.17019
	accuracy_policy_0: 0.63625
	loss_value_0: 0.15592
	loss_policy_1: 0.03883
	accuracy_policy_1: 0.59018
	loss_value_1: 0.03258
	loss_reward_1: 0.00418
	loss_policy_2: 0.04209
	accuracy_policy_2: 0.55918
	loss_value_2: 0.03382
	loss_reward_2: 0.00404
	loss_policy_3: 0.04464
	accuracy_policy_3: 0.54049
	loss_value_3: 0.03515
	loss_reward_3: 0.00439
	loss_policy_4: 0.04695
	accuracy_policy_4: 0.52312
	loss_value_4: 0.03639
	loss_reward_4: 0.00486
	loss_policy_5: 0.04932
	accuracy_policy_5: 0.49854
	loss_value_5: 0.03753
	loss_reward_5: 0.00557
	loss_policy: 0.39203
	loss_value: 0.33139
	loss_reward: 0.02303
[2024-05-13 03:26:15] nn step 46600, lr: 0.053144.
	loss_policy_0: 0.1269
	accuracy_policy_0: 0.71184
	loss_value_0: 0.14566
	loss_policy_1: 0.03122
	accuracy_policy_1: 0.65258
	loss_value_1: 0.03055
	loss_reward_1: 0.00395
	loss_policy_2: 0.03509
	accuracy_policy_2: 0.61219
	loss_value_2: 0.03176
	loss_reward_2: 0.00383
	loss_policy_3: 0.03757
	accuracy_policy_3: 0.59057
	loss_value_3: 0.03289
	loss_reward_3: 0.00402
	loss_policy_4: 0.04024
	accuracy_policy_4: 0.56887
	loss_value_4: 0.03397
	loss_reward_4: 0.00437
	loss_policy_5: 0.04268
	accuracy_policy_5: 0.54559
	loss_value_5: 0.03506
	loss_reward_5: 0.00523
	loss_policy: 0.3137
	loss_value: 0.30988
	loss_reward: 0.02139
Optimization_Done 46600
[2024-05-13 03:28:10] [command] train weight_iter_46600.pkl 232 234
[2024-05-13 03:29:13] nn step 46700, lr: 0.053144.
	loss_policy_0: 0.1599
	accuracy_policy_0: 0.64018
	loss_value_0: 0.14749
	loss_policy_1: 0.03572
	accuracy_policy_1: 0.59885
	loss_value_1: 0.03094
	loss_reward_1: 0.004
	loss_policy_2: 0.03874
	accuracy_policy_2: 0.56875
	loss_value_2: 0.03228
	loss_reward_2: 0.00394
	loss_policy_3: 0.04147
	accuracy_policy_3: 0.54523
	loss_value_3: 0.0335
	loss_reward_3: 0.00416
	loss_policy_4: 0.04369
	accuracy_policy_4: 0.52836
	loss_value_4: 0.03466
	loss_reward_4: 0.00463
	loss_policy_5: 0.04597
	accuracy_policy_5: 0.50582
	loss_value_5: 0.03571
	loss_reward_5: 0.00543
	loss_policy: 0.36549
	loss_value: 0.31458
	loss_reward: 0.02216
[2024-05-13 03:30:17] nn step 46800, lr: 0.053144.
	loss_policy_0: 0.13758
	accuracy_policy_0: 0.69453
	loss_value_0: 0.14574
	loss_policy_1: 0.03214
	accuracy_policy_1: 0.64098
	loss_value_1: 0.03072
	loss_reward_1: 0.00385
	loss_policy_2: 0.0356
	accuracy_policy_2: 0.60893
	loss_value_2: 0.03198
	loss_reward_2: 0.00391
	loss_policy_3: 0.03837
	accuracy_policy_3: 0.58238
	loss_value_3: 0.03308
	loss_reward_3: 0.00424
	loss_policy_4: 0.04126
	accuracy_policy_4: 0.55699
	loss_value_4: 0.03428
	loss_reward_4: 0.0046
	loss_policy_5: 0.04368
	accuracy_policy_5: 0.53777
	loss_value_5: 0.03545
	loss_reward_5: 0.00544
	loss_policy: 0.32863
	loss_value: 0.31126
	loss_reward: 0.02203
Optimization_Done 46800
[2024-05-13 03:32:32] [command] train weight_iter_46800.pkl 233 235
[2024-05-13 03:33:35] nn step 46900, lr: 0.053144.
	loss_policy_0: 0.18017
	accuracy_policy_0: 0.66506
	loss_value_0: 0.181
	loss_policy_1: 0.0416
	accuracy_policy_1: 0.62164
	loss_value_1: 0.03787
	loss_reward_1: 0.00579
	loss_policy_2: 0.04537
	accuracy_policy_2: 0.58922
	loss_value_2: 0.03947
	loss_reward_2: 0.00539
	loss_policy_3: 0.04867
	accuracy_policy_3: 0.56279
	loss_value_3: 0.04091
	loss_reward_3: 0.00583
	loss_policy_4: 0.05188
	accuracy_policy_4: 0.54492
	loss_value_4: 0.04225
	loss_reward_4: 0.00646
	loss_policy_5: 0.0542
	accuracy_policy_5: 0.52906
	loss_value_5: 0.04358
	loss_reward_5: 0.00749
	loss_policy: 0.42189
	loss_value: 0.38507
	loss_reward: 0.03096
[2024-05-13 03:34:42] nn step 47000, lr: 0.053144.
	loss_policy_0: 0.15609
	accuracy_policy_0: 0.70752
	loss_value_0: 0.17893
	loss_policy_1: 0.0383
	accuracy_policy_1: 0.64613
	loss_value_1: 0.03737
	loss_reward_1: 0.00559
	loss_policy_2: 0.04243
	accuracy_policy_2: 0.61381
	loss_value_2: 0.03885
	loss_reward_2: 0.00545
	loss_policy_3: 0.0456
	accuracy_policy_3: 0.5884
	loss_value_3: 0.04022
	loss_reward_3: 0.00583
	loss_policy_4: 0.04875
	accuracy_policy_4: 0.56697
	loss_value_4: 0.0416
	loss_reward_4: 0.00649
	loss_policy_5: 0.05201
	accuracy_policy_5: 0.5435
	loss_value_5: 0.04301
	loss_reward_5: 0.00758
	loss_policy: 0.38318
	loss_value: 0.37997
	loss_reward: 0.03094
Optimization_Done 47000
[2024-05-13 03:36:33] [command] train weight_iter_47000.pkl 234 236
[2024-05-13 03:37:33] nn step 47100, lr: 0.053144.
	loss_policy_0: 0.17967
	accuracy_policy_0: 0.65529
	loss_value_0: 0.18204
	loss_policy_1: 0.04251
	accuracy_policy_1: 0.60605
	loss_value_1: 0.03797
	loss_reward_1: 0.00599
	loss_policy_2: 0.04641
	accuracy_policy_2: 0.57117
	loss_value_2: 0.03948
	loss_reward_2: 0.00578
	loss_policy_3: 0.04973
	accuracy_policy_3: 0.54631
	loss_value_3: 0.04076
	loss_reward_3: 0.00614
	loss_policy_4: 0.05242
	accuracy_policy_4: 0.52553
	loss_value_4: 0.04188
	loss_reward_4: 0.00669
	loss_policy_5: 0.05472
	accuracy_policy_5: 0.50852
	loss_value_5: 0.04299
	loss_reward_5: 0.00768
	loss_policy: 0.42546
	loss_value: 0.38511
	loss_reward: 0.03228
[2024-05-13 03:38:34] nn step 47200, lr: 0.053144.
	loss_policy_0: 0.14477
	accuracy_policy_0: 0.70867
	loss_value_0: 0.17032
	loss_policy_1: 0.03625
	accuracy_policy_1: 0.64168
	loss_value_1: 0.03558
	loss_reward_1: 0.00569
	loss_policy_2: 0.0403
	accuracy_policy_2: 0.60844
	loss_value_2: 0.03696
	loss_reward_2: 0.00531
	loss_policy_3: 0.04316
	accuracy_policy_3: 0.58543
	loss_value_3: 0.03813
	loss_reward_3: 0.00573
	loss_policy_4: 0.04655
	accuracy_policy_4: 0.55848
	loss_value_4: 0.03938
	loss_reward_4: 0.00638
	loss_policy_5: 0.04889
	accuracy_policy_5: 0.53891
	loss_value_5: 0.04053
	loss_reward_5: 0.00741
	loss_policy: 0.35992
	loss_value: 0.36089
	loss_reward: 0.03052
Optimization_Done 47200
[2024-05-13 03:40:51] [command] train weight_iter_47200.pkl 235 237
[2024-05-13 03:41:52] nn step 47300, lr: 0.053144.
	loss_policy_0: 0.16041
	accuracy_policy_0: 0.67635
	loss_value_0: 0.16826
	loss_policy_1: 0.03861
	accuracy_policy_1: 0.61996
	loss_value_1: 0.03517
	loss_reward_1: 0.00558
	loss_policy_2: 0.04247
	accuracy_policy_2: 0.58637
	loss_value_2: 0.03655
	loss_reward_2: 0.00526
	loss_policy_3: 0.04549
	accuracy_policy_3: 0.56557
	loss_value_3: 0.03781
	loss_reward_3: 0.00563
	loss_policy_4: 0.04849
	accuracy_policy_4: 0.54086
	loss_value_4: 0.03903
	loss_reward_4: 0.006
	loss_policy_5: 0.05139
	accuracy_policy_5: 0.51967
	loss_value_5: 0.04019
	loss_reward_5: 0.00735
	loss_policy: 0.38686
	loss_value: 0.357
	loss_reward: 0.02982
[2024-05-13 03:42:55] nn step 47400, lr: 0.053144.
	loss_policy_0: 0.13899
	accuracy_policy_0: 0.72582
	loss_value_0: 0.17218
	loss_policy_1: 0.03589
	accuracy_policy_1: 0.65629
	loss_value_1: 0.03605
	loss_reward_1: 0.00584
	loss_policy_2: 0.04013
	accuracy_policy_2: 0.61951
	loss_value_2: 0.03745
	loss_reward_2: 0.00532
	loss_policy_3: 0.0438
	accuracy_policy_3: 0.59182
	loss_value_3: 0.0387
	loss_reward_3: 0.00582
	loss_policy_4: 0.04694
	accuracy_policy_4: 0.56764
	loss_value_4: 0.03991
	loss_reward_4: 0.00617
	loss_policy_5: 0.0498
	accuracy_policy_5: 0.55184
	loss_value_5: 0.04123
	loss_reward_5: 0.00762
	loss_policy: 0.35556
	loss_value: 0.36552
	loss_reward: 0.03077
Optimization_Done 47400
[2024-05-13 03:45:09] [command] train weight_iter_47400.pkl 236 238
[2024-05-13 03:46:13] nn step 47500, lr: 0.053144.
	loss_policy_0: 0.14359
	accuracy_policy_0: 0.69211
	loss_value_0: 0.15345
	loss_policy_1: 0.03458
	accuracy_policy_1: 0.63994
	loss_value_1: 0.03218
	loss_reward_1: 0.00472
	loss_policy_2: 0.03839
	accuracy_policy_2: 0.60625
	loss_value_2: 0.0336
	loss_reward_2: 0.00448
	loss_policy_3: 0.04196
	accuracy_policy_3: 0.57643
	loss_value_3: 0.03502
	loss_reward_3: 0.00478
	loss_policy_4: 0.04481
	accuracy_policy_4: 0.55266
	loss_value_4: 0.03619
	loss_reward_4: 0.00526
	loss_policy_5: 0.04757
	accuracy_policy_5: 0.52908
	loss_value_5: 0.03718
	loss_reward_5: 0.00615
	loss_policy: 0.3509
	loss_value: 0.32762
	loss_reward: 0.02539
[2024-05-13 03:47:15] nn step 47600, lr: 0.053144.
	loss_policy_0: 0.12747
	accuracy_policy_0: 0.74002
	loss_value_0: 0.16126
	loss_policy_1: 0.03236
	accuracy_policy_1: 0.67332
	loss_value_1: 0.03401
	loss_reward_1: 0.00501
	loss_policy_2: 0.03701
	accuracy_policy_2: 0.6375
	loss_value_2: 0.03552
	loss_reward_2: 0.0047
	loss_policy_3: 0.04059
	accuracy_policy_3: 0.60809
	loss_value_3: 0.0368
	loss_reward_3: 0.00513
	loss_policy_4: 0.04388
	accuracy_policy_4: 0.58512
	loss_value_4: 0.03807
	loss_reward_4: 0.00562
	loss_policy_5: 0.04732
	accuracy_policy_5: 0.55668
	loss_value_5: 0.03918
	loss_reward_5: 0.00663
	loss_policy: 0.32864
	loss_value: 0.34484
	loss_reward: 0.02709
Optimization_Done 47600
[2024-05-13 03:49:30] [command] train weight_iter_47600.pkl 237 239
[2024-05-13 03:50:31] nn step 47700, lr: 0.053144.
	loss_policy_0: 0.15374
	accuracy_policy_0: 0.68365
	loss_value_0: 0.1619
	loss_policy_1: 0.03676
	accuracy_policy_1: 0.62656
	loss_value_1: 0.03421
	loss_reward_1: 0.00546
	loss_policy_2: 0.04055
	accuracy_policy_2: 0.59861
	loss_value_2: 0.03577
	loss_reward_2: 0.00514
	loss_policy_3: 0.0437
	accuracy_policy_3: 0.5708
	loss_value_3: 0.03725
	loss_reward_3: 0.00543
	loss_policy_4: 0.04701
	accuracy_policy_4: 0.54967
	loss_value_4: 0.03844
	loss_reward_4: 0.00599
	loss_policy_5: 0.04982
	accuracy_policy_5: 0.52877
	loss_value_5: 0.03969
	loss_reward_5: 0.00692
	loss_policy: 0.37157
	loss_value: 0.34727
	loss_reward: 0.02894
[2024-05-13 03:51:30] nn step 47800, lr: 0.053144.
	loss_policy_0: 0.1281
	accuracy_policy_0: 0.72943
	loss_value_0: 0.15995
	loss_policy_1: 0.03241
	accuracy_policy_1: 0.66883
	loss_value_1: 0.03371
	loss_reward_1: 0.00529
	loss_policy_2: 0.0367
	accuracy_policy_2: 0.63379
	loss_value_2: 0.03518
	loss_reward_2: 0.005
	loss_policy_3: 0.04029
	accuracy_policy_3: 0.60762
	loss_value_3: 0.03662
	loss_reward_3: 0.00549
	loss_policy_4: 0.04372
	accuracy_policy_4: 0.57951
	loss_value_4: 0.03794
	loss_reward_4: 0.00585
	loss_policy_5: 0.04711
	accuracy_policy_5: 0.55506
	loss_value_5: 0.03918
	loss_reward_5: 0.00695
	loss_policy: 0.32834
	loss_value: 0.34257
	loss_reward: 0.02857
Optimization_Done 47800
[2024-05-13 03:53:45] [command] train weight_iter_47800.pkl 238 240
[2024-05-13 03:54:49] nn step 47900, lr: 0.053144.
	loss_policy_0: 0.11353
	accuracy_policy_0: 0.70945
	loss_value_0: 0.1396
	loss_policy_1: 0.02864
	accuracy_policy_1: 0.6443
	loss_value_1: 0.02938
	loss_reward_1: 0.00451
	loss_policy_2: 0.03216
	accuracy_policy_2: 0.61295
	loss_value_2: 0.03061
	loss_reward_2: 0.00428
	loss_policy_3: 0.03498
	accuracy_policy_3: 0.58512
	loss_value_3: 0.03165
	loss_reward_3: 0.00468
	loss_policy_4: 0.03754
	accuracy_policy_4: 0.56189
	loss_value_4: 0.03266
	loss_reward_4: 0.00503
	loss_policy_5: 0.04005
	accuracy_policy_5: 0.53932
	loss_value_5: 0.03361
	loss_reward_5: 0.00584
	loss_policy: 0.28691
	loss_value: 0.29752
	loss_reward: 0.02434
[2024-05-13 03:55:56] nn step 48000, lr: 0.053144.
	loss_policy_0: 0.10073
	accuracy_policy_0: 0.75119
	loss_value_0: 0.14432
	loss_policy_1: 0.02706
	accuracy_policy_1: 0.67797
	loss_value_1: 0.03026
	loss_reward_1: 0.00464
	loss_policy_2: 0.03047
	accuracy_policy_2: 0.64412
	loss_value_2: 0.03149
	loss_reward_2: 0.0044
	loss_policy_3: 0.03329
	accuracy_policy_3: 0.61986
	loss_value_3: 0.03257
	loss_reward_3: 0.00471
	loss_policy_4: 0.03642
	accuracy_policy_4: 0.5941
	loss_value_4: 0.03358
	loss_reward_4: 0.00501
	loss_policy_5: 0.03889
	accuracy_policy_5: 0.56959
	loss_value_5: 0.03461
	loss_reward_5: 0.00609
	loss_policy: 0.26686
	loss_value: 0.30684
	loss_reward: 0.02485
Optimization_Done 48000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-13 04:05:33] [command] train weight_iter_48000.pkl 239 241
[2024-05-13 04:06:34] nn step 48100, lr: 0.04783.
	loss_policy_0: 0.11496
	accuracy_policy_0: 0.7049
	loss_value_0: 0.13862
	loss_policy_1: 0.02868
	accuracy_policy_1: 0.639
	loss_value_1: 0.02908
	loss_reward_1: 0.00444
	loss_policy_2: 0.03177
	accuracy_policy_2: 0.60959
	loss_value_2: 0.03017
	loss_reward_2: 0.00427
	loss_policy_3: 0.0347
	accuracy_policy_3: 0.58146
	loss_value_3: 0.03126
	loss_reward_3: 0.00449
	loss_policy_4: 0.03736
	accuracy_policy_4: 0.5543
	loss_value_4: 0.03218
	loss_reward_4: 0.00498
	loss_policy_5: 0.03989
	accuracy_policy_5: 0.52758
	loss_value_5: 0.03318
	loss_reward_5: 0.00575
	loss_policy: 0.28735
	loss_value: 0.2945
	loss_reward: 0.02391
[2024-05-13 04:07:25] nn step 48200, lr: 0.04783.
	loss_policy_0: 0.09855
	accuracy_policy_0: 0.74566
	loss_value_0: 0.14397
	loss_policy_1: 0.02649
	accuracy_policy_1: 0.67098
	loss_value_1: 0.03036
	loss_reward_1: 0.00458
	loss_policy_2: 0.03005
	accuracy_policy_2: 0.6426
	loss_value_2: 0.03153
	loss_reward_2: 0.00441
	loss_policy_3: 0.03298
	accuracy_policy_3: 0.61637
	loss_value_3: 0.03254
	loss_reward_3: 0.0046
	loss_policy_4: 0.03569
	accuracy_policy_4: 0.59562
	loss_value_4: 0.03348
	loss_reward_4: 0.00506
	loss_policy_5: 0.03868
	accuracy_policy_5: 0.56648
	loss_value_5: 0.03454
	loss_reward_5: 0.00596
	loss_policy: 0.26244
	loss_value: 0.30642
	loss_reward: 0.0246
Optimization_Done 48200
[2024-05-13 04:09:45] [command] train weight_iter_48200.pkl 240 242
[2024-05-13 04:10:46] nn step 48300, lr: 0.04783.
	loss_policy_0: 0.1235
	accuracy_policy_0: 0.69984
	loss_value_0: 0.13714
	loss_policy_1: 0.02959
	accuracy_policy_1: 0.6485
	loss_value_1: 0.02899
	loss_reward_1: 0.00377
	loss_policy_2: 0.03273
	accuracy_policy_2: 0.61785
	loss_value_2: 0.03034
	loss_reward_2: 0.00375
	loss_policy_3: 0.03554
	accuracy_policy_3: 0.59912
	loss_value_3: 0.0316
	loss_reward_3: 0.00404
	loss_policy_4: 0.03801
	accuracy_policy_4: 0.57254
	loss_value_4: 0.03273
	loss_reward_4: 0.00433
	loss_policy_5: 0.04055
	accuracy_policy_5: 0.55396
	loss_value_5: 0.0339
	loss_reward_5: 0.00505
	loss_policy: 0.29991
	loss_value: 0.29469
	loss_reward: 0.02095
[2024-05-13 04:11:48] nn step 48400, lr: 0.04783.
	loss_policy_0: 0.09111
	accuracy_policy_0: 0.75729
	loss_value_0: 0.13442
	loss_policy_1: 0.0243
	accuracy_policy_1: 0.69016
	loss_value_1: 0.0285
	loss_reward_1: 0.00371
	loss_policy_2: 0.0276
	accuracy_policy_2: 0.66193
	loss_value_2: 0.0298
	loss_reward_2: 0.00361
	loss_policy_3: 0.03048
	accuracy_policy_3: 0.6343
	loss_value_3: 0.03094
	loss_reward_3: 0.00398
	loss_policy_4: 0.03319
	accuracy_policy_4: 0.60738
	loss_value_4: 0.03202
	loss_reward_4: 0.0041
	loss_policy_5: 0.03596
	accuracy_policy_5: 0.58453
	loss_value_5: 0.03315
	loss_reward_5: 0.00492
	loss_policy: 0.24265
	loss_value: 0.28884
	loss_reward: 0.02032
Optimization_Done 48400
[2024-05-13 04:14:08] [command] train weight_iter_48400.pkl 241 243
[2024-05-13 04:15:10] nn step 48500, lr: 0.04783.
	loss_policy_0: 0.13165
	accuracy_policy_0: 0.68539
	loss_value_0: 0.14514
	loss_policy_1: 0.03181
	accuracy_policy_1: 0.6334
	loss_value_1: 0.03069
	loss_reward_1: 0.00481
	loss_policy_2: 0.03557
	accuracy_policy_2: 0.60217
	loss_value_2: 0.03226
	loss_reward_2: 0.00458
	loss_policy_3: 0.03877
	accuracy_policy_3: 0.57371
	loss_value_3: 0.03349
	loss_reward_3: 0.00509
	loss_policy_4: 0.04155
	accuracy_policy_4: 0.55205
	loss_value_4: 0.03466
	loss_reward_4: 0.00529
	loss_policy_5: 0.04382
	accuracy_policy_5: 0.5293
	loss_value_5: 0.03582
	loss_reward_5: 0.00626
	loss_policy: 0.32318
	loss_value: 0.31206
	loss_reward: 0.02603
[2024-05-13 04:16:09] nn step 48600, lr: 0.04783.
	loss_policy_0: 0.10885
	accuracy_policy_0: 0.74631
	loss_value_0: 0.14886
	loss_policy_1: 0.0287
	accuracy_policy_1: 0.67365
	loss_value_1: 0.0314
	loss_reward_1: 0.00489
	loss_policy_2: 0.03283
	accuracy_policy_2: 0.63648
	loss_value_2: 0.03289
	loss_reward_2: 0.00461
	loss_policy_3: 0.03633
	accuracy_policy_3: 0.60684
	loss_value_3: 0.03427
	loss_reward_3: 0.00505
	loss_policy_4: 0.03921
	accuracy_policy_4: 0.58117
	loss_value_4: 0.03557
	loss_reward_4: 0.00522
	loss_policy_5: 0.0418
	accuracy_policy_5: 0.56215
	loss_value_5: 0.03683
	loss_reward_5: 0.00641
	loss_policy: 0.28771
	loss_value: 0.31983
	loss_reward: 0.02618
Optimization_Done 48600
[2024-05-13 04:18:27] [command] train weight_iter_48600.pkl 242 244
[2024-05-13 04:19:29] nn step 48700, lr: 0.04783.
	loss_policy_0: 0.12185
	accuracy_policy_0: 0.71662
	loss_value_0: 0.14537
	loss_policy_1: 0.03043
	accuracy_policy_1: 0.65646
	loss_value_1: 0.03055
	loss_reward_1: 0.00465
	loss_policy_2: 0.03392
	accuracy_policy_2: 0.62262
	loss_value_2: 0.03167
	loss_reward_2: 0.00443
	loss_policy_3: 0.03697
	accuracy_policy_3: 0.59787
	loss_value_3: 0.0328
	loss_reward_3: 0.00505
	loss_policy_4: 0.0394
	accuracy_policy_4: 0.5782
	loss_value_4: 0.0339
	loss_reward_4: 0.00509
	loss_policy_5: 0.04223
	accuracy_policy_5: 0.55234
	loss_value_5: 0.035
	loss_reward_5: 0.00609
	loss_policy: 0.3048
	loss_value: 0.30928
	loss_reward: 0.0253
[2024-05-13 04:20:27] nn step 48800, lr: 0.04783.
	loss_policy_0: 0.10398
	accuracy_policy_0: 0.75672
	loss_value_0: 0.14393
	loss_policy_1: 0.02778
	accuracy_policy_1: 0.68131
	loss_value_1: 0.03022
	loss_reward_1: 0.00478
	loss_policy_2: 0.03151
	accuracy_policy_2: 0.64369
	loss_value_2: 0.0315
	loss_reward_2: 0.00427
	loss_policy_3: 0.03452
	accuracy_policy_3: 0.62146
	loss_value_3: 0.03266
	loss_reward_3: 0.00481
	loss_policy_4: 0.03694
	accuracy_policy_4: 0.60332
	loss_value_4: 0.03374
	loss_reward_4: 0.00495
	loss_policy_5: 0.03957
	accuracy_policy_5: 0.58021
	loss_value_5: 0.03505
	loss_reward_5: 0.00593
	loss_policy: 0.2743
	loss_value: 0.3071
	loss_reward: 0.02474
Optimization_Done 48800
[2024-05-13 04:22:46] [command] train weight_iter_48800.pkl 243 245
[2024-05-13 04:23:46] nn step 48900, lr: 0.04783.
	loss_policy_0: 0.15069
	accuracy_policy_0: 0.6635
	loss_value_0: 0.14814
	loss_policy_1: 0.03531
	accuracy_policy_1: 0.61584
	loss_value_1: 0.03114
	loss_reward_1: 0.00443
	loss_policy_2: 0.0382
	accuracy_policy_2: 0.58766
	loss_value_2: 0.0324
	loss_reward_2: 0.00432
	loss_policy_3: 0.0408
	accuracy_policy_3: 0.56586
	loss_value_3: 0.03358
	loss_reward_3: 0.00472
	loss_policy_4: 0.04311
	accuracy_policy_4: 0.54395
	loss_value_4: 0.03472
	loss_reward_4: 0.00493
	loss_policy_5: 0.04566
	accuracy_policy_5: 0.52309
	loss_value_5: 0.03598
	loss_reward_5: 0.00593
	loss_policy: 0.35378
	loss_value: 0.31596
	loss_reward: 0.02432
[2024-05-13 04:24:38] nn step 49000, lr: 0.04783.
	loss_policy_0: 0.11921
	accuracy_policy_0: 0.73049
	loss_value_0: 0.14574
	loss_policy_1: 0.03023
	accuracy_policy_1: 0.66633
	loss_value_1: 0.03064
	loss_reward_1: 0.00445
	loss_policy_2: 0.034
	accuracy_policy_2: 0.62422
	loss_value_2: 0.03187
	loss_reward_2: 0.00432
	loss_policy_3: 0.03644
	accuracy_policy_3: 0.60748
	loss_value_3: 0.03283
	loss_reward_3: 0.00468
	loss_policy_4: 0.03923
	accuracy_policy_4: 0.58068
	loss_value_4: 0.03389
	loss_reward_4: 0.00484
	loss_policy_5: 0.04188
	accuracy_policy_5: 0.55844
	loss_value_5: 0.03508
	loss_reward_5: 0.00591
	loss_policy: 0.30099
	loss_value: 0.31005
	loss_reward: 0.0242
Optimization_Done 49000
[2024-05-13 04:26:57] [command] train weight_iter_49000.pkl 244 246
[2024-05-13 04:28:01] nn step 49100, lr: 0.04783.
	loss_policy_0: 0.16311
	accuracy_policy_0: 0.64533
	loss_value_0: 0.14521
	loss_policy_1: 0.0374
	accuracy_policy_1: 0.60377
	loss_value_1: 0.03051
	loss_reward_1: 0.00426
	loss_policy_2: 0.04071
	accuracy_policy_2: 0.57576
	loss_value_2: 0.03195
	loss_reward_2: 0.00439
	loss_policy_3: 0.04348
	accuracy_policy_3: 0.55037
	loss_value_3: 0.03333
	loss_reward_3: 0.00448
	loss_policy_4: 0.04595
	accuracy_policy_4: 0.52791
	loss_value_4: 0.03464
	loss_reward_4: 0.00484
	loss_policy_5: 0.04802
	accuracy_policy_5: 0.50547
	loss_value_5: 0.03579
	loss_reward_5: 0.00563
	loss_policy: 0.37865
	loss_value: 0.31143
	loss_reward: 0.0236
[2024-05-13 04:29:00] nn step 49200, lr: 0.04783.
	loss_policy_0: 0.1346
	accuracy_policy_0: 0.70699
	loss_value_0: 0.14428
	loss_policy_1: 0.03328
	accuracy_policy_1: 0.6476
	loss_value_1: 0.03036
	loss_reward_1: 0.00419
	loss_policy_2: 0.03711
	accuracy_policy_2: 0.61072
	loss_value_2: 0.03172
	loss_reward_2: 0.00423
	loss_policy_3: 0.03992
	accuracy_policy_3: 0.5898
	loss_value_3: 0.033
	loss_reward_3: 0.00446
	loss_policy_4: 0.04252
	accuracy_policy_4: 0.56627
	loss_value_4: 0.03415
	loss_reward_4: 0.00481
	loss_policy_5: 0.04515
	accuracy_policy_5: 0.54695
	loss_value_5: 0.03527
	loss_reward_5: 0.00556
	loss_policy: 0.33257
	loss_value: 0.30879
	loss_reward: 0.02324
Optimization_Done 49200
[2024-05-13 04:31:19] [command] train weight_iter_49200.pkl 245 247
[2024-05-13 04:32:22] nn step 49300, lr: 0.04783.
	loss_policy_0: 0.19408
	accuracy_policy_0: 0.64186
	loss_value_0: 0.16945
	loss_policy_1: 0.04476
	accuracy_policy_1: 0.59176
	loss_value_1: 0.03563
	loss_reward_1: 0.00544
	loss_policy_2: 0.04864
	accuracy_policy_2: 0.56172
	loss_value_2: 0.0372
	loss_reward_2: 0.00524
	loss_policy_3: 0.05173
	accuracy_policy_3: 0.53771
	loss_value_3: 0.03863
	loss_reward_3: 0.00566
	loss_policy_4: 0.05489
	accuracy_policy_4: 0.51559
	loss_value_4: 0.03989
	loss_reward_4: 0.00619
	loss_policy_5: 0.05721
	accuracy_policy_5: 0.50158
	loss_value_5: 0.04131
	loss_reward_5: 0.00706
	loss_policy: 0.45132
	loss_value: 0.36211
	loss_reward: 0.02959
[2024-05-13 04:33:29] nn step 49400, lr: 0.04783.
	loss_policy_0: 0.15996
	accuracy_policy_0: 0.69469
	loss_value_0: 0.16163
	loss_policy_1: 0.03874
	accuracy_policy_1: 0.63424
	loss_value_1: 0.03403
	loss_reward_1: 0.00526
	loss_policy_2: 0.04286
	accuracy_policy_2: 0.60021
	loss_value_2: 0.03553
	loss_reward_2: 0.00526
	loss_policy_3: 0.04582
	accuracy_policy_3: 0.57756
	loss_value_3: 0.03675
	loss_reward_3: 0.00548
	loss_policy_4: 0.04931
	accuracy_policy_4: 0.55072
	loss_value_4: 0.03798
	loss_reward_4: 0.00598
	loss_policy_5: 0.05173
	accuracy_policy_5: 0.53594
	loss_value_5: 0.03927
	loss_reward_5: 0.00692
	loss_policy: 0.38843
	loss_value: 0.34519
	loss_reward: 0.0289
Optimization_Done 49400
[2024-05-13 04:35:47] [command] train weight_iter_49400.pkl 246 248
[2024-05-13 04:36:52] nn step 49500, lr: 0.04783.
	loss_policy_0: 0.17406
	accuracy_policy_0: 0.68266
	loss_value_0: 0.17315
	loss_policy_1: 0.04202
	accuracy_policy_1: 0.62293
	loss_value_1: 0.03625
	loss_reward_1: 0.006
	loss_policy_2: 0.04627
	accuracy_policy_2: 0.58914
	loss_value_2: 0.03774
	loss_reward_2: 0.006
	loss_policy_3: 0.0497
	accuracy_policy_3: 0.56656
	loss_value_3: 0.03892
	loss_reward_3: 0.00655
	loss_policy_4: 0.05249
	accuracy_policy_4: 0.54557
	loss_value_4: 0.0401
	loss_reward_4: 0.00702
	loss_policy_5: 0.05501
	accuracy_policy_5: 0.52762
	loss_value_5: 0.04139
	loss_reward_5: 0.00813
	loss_policy: 0.41955
	loss_value: 0.36755
	loss_reward: 0.03369
[2024-05-13 04:37:58] nn step 49600, lr: 0.04783.
	loss_policy_0: 0.15024
	accuracy_policy_0: 0.71875
	loss_value_0: 0.17124
	loss_policy_1: 0.03859
	accuracy_policy_1: 0.64422
	loss_value_1: 0.03587
	loss_reward_1: 0.00606
	loss_policy_2: 0.04285
	accuracy_policy_2: 0.6135
	loss_value_2: 0.03727
	loss_reward_2: 0.00596
	loss_policy_3: 0.04609
	accuracy_policy_3: 0.59072
	loss_value_3: 0.03851
	loss_reward_3: 0.00647
	loss_policy_4: 0.0492
	accuracy_policy_4: 0.56619
	loss_value_4: 0.03965
	loss_reward_4: 0.00677
	loss_policy_5: 0.05191
	accuracy_policy_5: 0.54748
	loss_value_5: 0.04085
	loss_reward_5: 0.00798
	loss_policy: 0.37888
	loss_value: 0.36338
	loss_reward: 0.03325
Optimization_Done 49600
[2024-05-13 04:39:54] [command] train weight_iter_49600.pkl 247 249
[2024-05-13 04:40:56] nn step 49700, lr: 0.04783.
	loss_policy_0: 0.15732
	accuracy_policy_0: 0.69871
	loss_value_0: 0.17137
	loss_policy_1: 0.03856
	accuracy_policy_1: 0.63771
	loss_value_1: 0.03585
	loss_reward_1: 0.00554
	loss_policy_2: 0.04215
	accuracy_policy_2: 0.61008
	loss_value_2: 0.03716
	loss_reward_2: 0.00529
	loss_policy_3: 0.04519
	accuracy_policy_3: 0.58496
	loss_value_3: 0.03835
	loss_reward_3: 0.00591
	loss_policy_4: 0.04763
	accuracy_policy_4: 0.56016
	loss_value_4: 0.03936
	loss_reward_4: 0.00632
	loss_policy_5: 0.05007
	accuracy_policy_5: 0.53992
	loss_value_5: 0.0404
	loss_reward_5: 0.00717
	loss_policy: 0.38092
	loss_value: 0.36249
	loss_reward: 0.03022
[2024-05-13 04:41:56] nn step 49800, lr: 0.04783.
	loss_policy_0: 0.13894
	accuracy_policy_0: 0.73846
	loss_value_0: 0.17484
	loss_policy_1: 0.03608
	accuracy_policy_1: 0.66619
	loss_value_1: 0.03645
	loss_reward_1: 0.00582
	loss_policy_2: 0.04011
	accuracy_policy_2: 0.63338
	loss_value_2: 0.03754
	loss_reward_2: 0.00547
	loss_policy_3: 0.04346
	accuracy_policy_3: 0.60607
	loss_value_3: 0.03877
	loss_reward_3: 0.00603
	loss_policy_4: 0.04626
	accuracy_policy_4: 0.5832
	loss_value_4: 0.03997
	loss_reward_4: 0.0065
	loss_policy_5: 0.04895
	accuracy_policy_5: 0.56549
	loss_value_5: 0.04098
	loss_reward_5: 0.00747
	loss_policy: 0.3538
	loss_value: 0.36854
	loss_reward: 0.0313
Optimization_Done 49800
[2024-05-13 04:44:14] [command] train weight_iter_49800.pkl 248 250
[2024-05-13 04:45:17] nn step 49900, lr: 0.04783.
	loss_policy_0: 0.15766
	accuracy_policy_0: 0.69938
	loss_value_0: 0.16646
	loss_policy_1: 0.03815
	accuracy_policy_1: 0.64076
	loss_value_1: 0.03483
	loss_reward_1: 0.0052
	loss_policy_2: 0.04136
	accuracy_policy_2: 0.61113
	loss_value_2: 0.03616
	loss_reward_2: 0.00495
	loss_policy_3: 0.04417
	accuracy_policy_3: 0.58635
	loss_value_3: 0.03743
	loss_reward_3: 0.00551
	loss_policy_4: 0.0469
	accuracy_policy_4: 0.56863
	loss_value_4: 0.03866
	loss_reward_4: 0.00586
	loss_policy_5: 0.04982
	accuracy_policy_5: 0.54332
	loss_value_5: 0.03986
	loss_reward_5: 0.00694
	loss_policy: 0.37807
	loss_value: 0.3534
	loss_reward: 0.02846
[2024-05-13 04:46:15] nn step 50000, lr: 0.04783.
	loss_policy_0: 0.1323
	accuracy_policy_0: 0.7432
	loss_value_0: 0.165
	loss_policy_1: 0.03407
	accuracy_policy_1: 0.67502
	loss_value_1: 0.03447
	loss_reward_1: 0.00523
	loss_policy_2: 0.03795
	accuracy_policy_2: 0.63773
	loss_value_2: 0.03569
	loss_reward_2: 0.00496
	loss_policy_3: 0.04077
	accuracy_policy_3: 0.61705
	loss_value_3: 0.03694
	loss_reward_3: 0.00562
	loss_policy_4: 0.04375
	accuracy_policy_4: 0.59662
	loss_value_4: 0.03813
	loss_reward_4: 0.00587
	loss_policy_5: 0.04669
	accuracy_policy_5: 0.57363
	loss_value_5: 0.03922
	loss_reward_5: 0.00688
	loss_policy: 0.33553
	loss_value: 0.34945
	loss_reward: 0.02856
Optimization_Done 50000
[2024-05-13 04:48:24] [command] train weight_iter_50000.pkl 249 251
[2024-05-13 04:49:28] nn step 50100, lr: 0.04783.
	loss_policy_0: 0.16071
	accuracy_policy_0: 0.70271
	loss_value_0: 0.16111
	loss_policy_1: 0.03697
	accuracy_policy_1: 0.65941
	loss_value_1: 0.03388
	loss_reward_1: 0.00438
	loss_policy_2: 0.04086
	accuracy_policy_2: 0.62924
	loss_value_2: 0.03561
	loss_reward_2: 0.00436
	loss_policy_3: 0.04345
	accuracy_policy_3: 0.60709
	loss_value_3: 0.03699
	loss_reward_3: 0.00484
	loss_policy_4: 0.04598
	accuracy_policy_4: 0.59023
	loss_value_4: 0.03836
	loss_reward_4: 0.00512
	loss_policy_5: 0.04862
	accuracy_policy_5: 0.5716
	loss_value_5: 0.03965
	loss_reward_5: 0.00613
	loss_policy: 0.3766
	loss_value: 0.34559
	loss_reward: 0.02483
[2024-05-13 04:50:33] nn step 50200, lr: 0.04783.
	loss_policy_0: 0.12871
	accuracy_policy_0: 0.75082
	loss_value_0: 0.15716
	loss_policy_1: 0.03179
	accuracy_policy_1: 0.69865
	loss_value_1: 0.03325
	loss_reward_1: 0.00438
	loss_policy_2: 0.03516
	accuracy_policy_2: 0.66734
	loss_value_2: 0.03464
	loss_reward_2: 0.00424
	loss_policy_3: 0.03827
	accuracy_policy_3: 0.64234
	loss_value_3: 0.03602
	loss_reward_3: 0.00472
	loss_policy_4: 0.04147
	accuracy_policy_4: 0.62207
	loss_value_4: 0.03732
	loss_reward_4: 0.005
	loss_policy_5: 0.04422
	accuracy_policy_5: 0.60133
	loss_value_5: 0.03846
	loss_reward_5: 0.006
	loss_policy: 0.31961
	loss_value: 0.33686
	loss_reward: 0.02434
Optimization_Done 50200
[2024-05-13 04:52:38] [command] train weight_iter_50200.pkl 250 252
[2024-05-13 04:53:40] nn step 50300, lr: 0.04783.
	loss_policy_0: 0.17713
	accuracy_policy_0: 0.6559
	loss_value_0: 0.16644
	loss_policy_1: 0.04036
	accuracy_policy_1: 0.61178
	loss_value_1: 0.03523
	loss_reward_1: 0.00484
	loss_policy_2: 0.04361
	accuracy_policy_2: 0.58508
	loss_value_2: 0.03679
	loss_reward_2: 0.00473
	loss_policy_3: 0.04646
	accuracy_policy_3: 0.56125
	loss_value_3: 0.03823
	loss_reward_3: 0.00518
	loss_policy_4: 0.04885
	accuracy_policy_4: 0.54801
	loss_value_4: 0.03971
	loss_reward_4: 0.00567
	loss_policy_5: 0.05127
	accuracy_policy_5: 0.53166
	loss_value_5: 0.0411
	loss_reward_5: 0.00654
	loss_policy: 0.40768
	loss_value: 0.3575
	loss_reward: 0.02697
[2024-05-13 04:54:46] nn step 50400, lr: 0.04783.
	loss_policy_0: 0.14366
	accuracy_policy_0: 0.71562
	loss_value_0: 0.16213
	loss_policy_1: 0.03481
	accuracy_policy_1: 0.66256
	loss_value_1: 0.0342
	loss_reward_1: 0.0048
	loss_policy_2: 0.03847
	accuracy_policy_2: 0.63221
	loss_value_2: 0.03568
	loss_reward_2: 0.00462
	loss_policy_3: 0.04167
	accuracy_policy_3: 0.60779
	loss_value_3: 0.03701
	loss_reward_3: 0.00506
	loss_policy_4: 0.04455
	accuracy_policy_4: 0.58664
	loss_value_4: 0.03821
	loss_reward_4: 0.00535
	loss_policy_5: 0.04718
	accuracy_policy_5: 0.56219
	loss_value_5: 0.03939
	loss_reward_5: 0.00634
	loss_policy: 0.35034
	loss_value: 0.34662
	loss_reward: 0.02617
Optimization_Done 50400
[2024-05-13 04:56:51] [command] train weight_iter_50400.pkl 251 253
[2024-05-13 04:57:54] nn step 50500, lr: 0.04783.
	loss_policy_0: 0.15826
	accuracy_policy_0: 0.67285
	loss_value_0: 0.16448
	loss_policy_1: 0.03669
	accuracy_policy_1: 0.62238
	loss_value_1: 0.03441
	loss_reward_1: 0.00466
	loss_policy_2: 0.04008
	accuracy_policy_2: 0.59496
	loss_value_2: 0.0356
	loss_reward_2: 0.00455
	loss_policy_3: 0.04255
	accuracy_policy_3: 0.56975
	loss_value_3: 0.03692
	loss_reward_3: 0.00495
	loss_policy_4: 0.04508
	accuracy_policy_4: 0.55352
	loss_value_4: 0.03816
	loss_reward_4: 0.00525
	loss_policy_5: 0.04716
	accuracy_policy_5: 0.53943
	loss_value_5: 0.03935
	loss_reward_5: 0.0062
	loss_policy: 0.36982
	loss_value: 0.34892
	loss_reward: 0.02561
[2024-05-13 04:59:00] nn step 50600, lr: 0.04783.
	loss_policy_0: 0.13359
	accuracy_policy_0: 0.72111
	loss_value_0: 0.16087
	loss_policy_1: 0.03299
	accuracy_policy_1: 0.6608
	loss_value_1: 0.03372
	loss_reward_1: 0.00459
	loss_policy_2: 0.03647
	accuracy_policy_2: 0.63178
	loss_value_2: 0.03502
	loss_reward_2: 0.00444
	loss_policy_3: 0.03922
	accuracy_policy_3: 0.60809
	loss_value_3: 0.03623
	loss_reward_3: 0.00489
	loss_policy_4: 0.04177
	accuracy_policy_4: 0.58826
	loss_value_4: 0.03747
	loss_reward_4: 0.00518
	loss_policy_5: 0.04445
	accuracy_policy_5: 0.56807
	loss_value_5: 0.03861
	loss_reward_5: 0.00613
	loss_policy: 0.32849
	loss_value: 0.34192
	loss_reward: 0.02524
Optimization_Done 50600
[2024-05-13 05:01:16] [command] train weight_iter_50600.pkl 252 254
[2024-05-13 05:02:19] nn step 50700, lr: 0.04783.
	loss_policy_0: 0.18108
	accuracy_policy_0: 0.64574
	loss_value_0: 0.15874
	loss_policy_1: 0.0415
	accuracy_policy_1: 0.59504
	loss_value_1: 0.0332
	loss_reward_1: 0.00471
	loss_policy_2: 0.04451
	accuracy_policy_2: 0.57113
	loss_value_2: 0.03472
	loss_reward_2: 0.00457
	loss_policy_3: 0.04697
	accuracy_policy_3: 0.54969
	loss_value_3: 0.03605
	loss_reward_3: 0.00496
	loss_policy_4: 0.0495
	accuracy_policy_4: 0.5301
	loss_value_4: 0.03721
	loss_reward_4: 0.00546
	loss_policy_5: 0.05191
	accuracy_policy_5: 0.511
	loss_value_5: 0.03833
	loss_reward_5: 0.00627
	loss_policy: 0.41547
	loss_value: 0.33823
	loss_reward: 0.02597
[2024-05-13 05:03:24] nn step 50800, lr: 0.04783.
	loss_policy_0: 0.14279
	accuracy_policy_0: 0.70508
	loss_value_0: 0.15021
	loss_policy_1: 0.03497
	accuracy_policy_1: 0.64123
	loss_value_1: 0.03151
	loss_reward_1: 0.00455
	loss_policy_2: 0.0383
	accuracy_policy_2: 0.61199
	loss_value_2: 0.03274
	loss_reward_2: 0.00436
	loss_policy_3: 0.04099
	accuracy_policy_3: 0.59209
	loss_value_3: 0.03384
	loss_reward_3: 0.00483
	loss_policy_4: 0.04377
	accuracy_policy_4: 0.57037
	loss_value_4: 0.03492
	loss_reward_4: 0.00514
	loss_policy_5: 0.04676
	accuracy_policy_5: 0.54965
	loss_value_5: 0.03612
	loss_reward_5: 0.00596
	loss_policy: 0.34758
	loss_value: 0.31934
	loss_reward: 0.02485
Optimization_Done 50800
[2024-05-13 05:05:40] [command] train weight_iter_50800.pkl 253 255
[2024-05-13 05:06:42] nn step 50900, lr: 0.04783.
	loss_policy_0: 0.16524
	accuracy_policy_0: 0.64797
	loss_value_0: 0.14518
	loss_policy_1: 0.03773
	accuracy_policy_1: 0.60461
	loss_value_1: 0.03054
	loss_reward_1: 0.00432
	loss_policy_2: 0.04082
	accuracy_policy_2: 0.57037
	loss_value_2: 0.03204
	loss_reward_2: 0.00439
	loss_policy_3: 0.04361
	accuracy_policy_3: 0.55271
	loss_value_3: 0.03328
	loss_reward_3: 0.00459
	loss_policy_4: 0.04575
	accuracy_policy_4: 0.53572
	loss_value_4: 0.03439
	loss_reward_4: 0.00513
	loss_policy_5: 0.04828
	accuracy_policy_5: 0.51871
	loss_value_5: 0.03549
	loss_reward_5: 0.00592
	loss_policy: 0.38143
	loss_value: 0.31092
	loss_reward: 0.02436
[2024-05-13 05:07:49] nn step 51000, lr: 0.04783.
	loss_policy_0: 0.13976
	accuracy_policy_0: 0.70148
	loss_value_0: 0.14297
	loss_policy_1: 0.03376
	accuracy_policy_1: 0.64572
	loss_value_1: 0.02989
	loss_reward_1: 0.00442
	loss_policy_2: 0.03675
	accuracy_policy_2: 0.61598
	loss_value_2: 0.03117
	loss_reward_2: 0.00431
	loss_policy_3: 0.03996
	accuracy_policy_3: 0.59217
	loss_value_3: 0.03241
	loss_reward_3: 0.00457
	loss_policy_4: 0.04247
	accuracy_policy_4: 0.5727
	loss_value_4: 0.03352
	loss_reward_4: 0.00504
	loss_policy_5: 0.0449
	accuracy_policy_5: 0.54885
	loss_value_5: 0.03478
	loss_reward_5: 0.00594
	loss_policy: 0.3376
	loss_value: 0.30474
	loss_reward: 0.02428
Optimization_Done 51000
[2024-05-13 05:10:03] [command] train weight_iter_51000.pkl 254 256
[2024-05-13 05:11:07] nn step 51100, lr: 0.04783.
	loss_policy_0: 0.19519
	accuracy_policy_0: 0.63434
	loss_value_0: 0.16548
	loss_policy_1: 0.04353
	accuracy_policy_1: 0.59586
	loss_value_1: 0.03445
	loss_reward_1: 0.00487
	loss_policy_2: 0.04691
	accuracy_policy_2: 0.57008
	loss_value_2: 0.03589
	loss_reward_2: 0.00461
	loss_policy_3: 0.04945
	accuracy_policy_3: 0.55182
	loss_value_3: 0.03722
	loss_reward_3: 0.005
	loss_policy_4: 0.05221
	accuracy_policy_4: 0.53363
	loss_value_4: 0.03845
	loss_reward_4: 0.00549
	loss_policy_5: 0.05467
	accuracy_policy_5: 0.51807
	loss_value_5: 0.03968
	loss_reward_5: 0.00633
	loss_policy: 0.44195
	loss_value: 0.35117
	loss_reward: 0.02629
[2024-05-13 05:12:13] nn step 51200, lr: 0.04783.
	loss_policy_0: 0.16987
	accuracy_policy_0: 0.69102
	loss_value_0: 0.16685
	loss_policy_1: 0.04044
	accuracy_policy_1: 0.63801
	loss_value_1: 0.03494
	loss_reward_1: 0.00502
	loss_policy_2: 0.04433
	accuracy_policy_2: 0.60557
	loss_value_2: 0.03641
	loss_reward_2: 0.00496
	loss_policy_3: 0.0477
	accuracy_policy_3: 0.58145
	loss_value_3: 0.03772
	loss_reward_3: 0.00524
	loss_policy_4: 0.05053
	accuracy_policy_4: 0.56426
	loss_value_4: 0.03906
	loss_reward_4: 0.00563
	loss_policy_5: 0.05352
	accuracy_policy_5: 0.54279
	loss_value_5: 0.04031
	loss_reward_5: 0.00666
	loss_policy: 0.40639
	loss_value: 0.3553
	loss_reward: 0.02752
Optimization_Done 51200
[2024-05-13 05:14:27] [command] train weight_iter_51200.pkl 255 257
[2024-05-13 05:15:31] nn step 51300, lr: 0.04783.
	loss_policy_0: 0.20178
	accuracy_policy_0: 0.63898
	loss_value_0: 0.17417
	loss_policy_1: 0.04651
	accuracy_policy_1: 0.58732
	loss_value_1: 0.03642
	loss_reward_1: 0.00554
	loss_policy_2: 0.05008
	accuracy_policy_2: 0.55795
	loss_value_2: 0.03789
	loss_reward_2: 0.0054
	loss_policy_3: 0.05305
	accuracy_policy_3: 0.53979
	loss_value_3: 0.03933
	loss_reward_3: 0.00589
	loss_policy_4: 0.05562
	accuracy_policy_4: 0.52326
	loss_value_4: 0.04059
	loss_reward_4: 0.00637
	loss_policy_5: 0.05843
	accuracy_policy_5: 0.50398
	loss_value_5: 0.04181
	loss_reward_5: 0.00737
	loss_policy: 0.46547
	loss_value: 0.3702
	loss_reward: 0.03057
[2024-05-13 05:16:33] nn step 51400, lr: 0.04783.
	loss_policy_0: 0.16556
	accuracy_policy_0: 0.69021
	loss_value_0: 0.16581
	loss_policy_1: 0.04008
	accuracy_policy_1: 0.62824
	loss_value_1: 0.03482
	loss_reward_1: 0.00538
	loss_policy_2: 0.04383
	accuracy_policy_2: 0.59803
	loss_value_2: 0.03606
	loss_reward_2: 0.00522
	loss_policy_3: 0.04697
	accuracy_policy_3: 0.57703
	loss_value_3: 0.03731
	loss_reward_3: 0.00553
	loss_policy_4: 0.05026
	accuracy_policy_4: 0.55225
	loss_value_4: 0.03847
	loss_reward_4: 0.00589
	loss_policy_5: 0.05315
	accuracy_policy_5: 0.52875
	loss_value_5: 0.03963
	loss_reward_5: 0.00694
	loss_policy: 0.39984
	loss_value: 0.3521
	loss_reward: 0.02897
Optimization_Done 51400
[2024-05-13 05:18:30] [command] train weight_iter_51400.pkl 256 258
[2024-05-13 05:19:32] nn step 51500, lr: 0.04783.
	loss_policy_0: 0.16828
	accuracy_policy_0: 0.67689
	loss_value_0: 0.16581
	loss_policy_1: 0.04019
	accuracy_policy_1: 0.61945
	loss_value_1: 0.03471
	loss_reward_1: 0.005
	loss_policy_2: 0.04348
	accuracy_policy_2: 0.58924
	loss_value_2: 0.03603
	loss_reward_2: 0.00486
	loss_policy_3: 0.0469
	accuracy_policy_3: 0.56158
	loss_value_3: 0.03717
	loss_reward_3: 0.00541
	loss_policy_4: 0.04943
	accuracy_policy_4: 0.54221
	loss_value_4: 0.03808
	loss_reward_4: 0.00584
	loss_policy_5: 0.05213
	accuracy_policy_5: 0.52107
	loss_value_5: 0.03909
	loss_reward_5: 0.00699
	loss_policy: 0.40042
	loss_value: 0.35089
	loss_reward: 0.02808
[2024-05-13 05:20:39] nn step 51600, lr: 0.04783.
	loss_policy_0: 0.15155
	accuracy_policy_0: 0.71439
	loss_value_0: 0.16718
	loss_policy_1: 0.03746
	accuracy_policy_1: 0.6473
	loss_value_1: 0.03508
	loss_reward_1: 0.00511
	loss_policy_2: 0.0414
	accuracy_policy_2: 0.61459
	loss_value_2: 0.03628
	loss_reward_2: 0.00497
	loss_policy_3: 0.04461
	accuracy_policy_3: 0.5909
	loss_value_3: 0.03734
	loss_reward_3: 0.00548
	loss_policy_4: 0.04688
	accuracy_policy_4: 0.57488
	loss_value_4: 0.03854
	loss_reward_4: 0.00583
	loss_policy_5: 0.05026
	accuracy_policy_5: 0.5508
	loss_value_5: 0.03956
	loss_reward_5: 0.00702
	loss_policy: 0.37216
	loss_value: 0.35397
	loss_reward: 0.0284
Optimization_Done 51600
[2024-05-13 05:22:54] [command] train weight_iter_51600.pkl 257 259
[2024-05-13 05:23:58] nn step 51700, lr: 0.04783.
	loss_policy_0: 0.1485
	accuracy_policy_0: 0.69293
	loss_value_0: 0.15233
	loss_policy_1: 0.03552
	accuracy_policy_1: 0.64119
	loss_value_1: 0.03211
	loss_reward_1: 0.00503
	loss_policy_2: 0.0389
	accuracy_policy_2: 0.61187
	loss_value_2: 0.03363
	loss_reward_2: 0.00468
	loss_policy_3: 0.04206
	accuracy_policy_3: 0.5849
	loss_value_3: 0.03495
	loss_reward_3: 0.00514
	loss_policy_4: 0.04499
	accuracy_policy_4: 0.56141
	loss_value_4: 0.03618
	loss_reward_4: 0.00564
	loss_policy_5: 0.04761
	accuracy_policy_5: 0.53717
	loss_value_5: 0.03727
	loss_reward_5: 0.00697
	loss_policy: 0.35759
	loss_value: 0.32647
	loss_reward: 0.02746
[2024-05-13 05:25:02] nn step 51800, lr: 0.04783.
	loss_policy_0: 0.12077
	accuracy_policy_0: 0.73893
	loss_value_0: 0.14793
	loss_policy_1: 0.03173
	accuracy_policy_1: 0.67088
	loss_value_1: 0.03122
	loss_reward_1: 0.00482
	loss_policy_2: 0.03509
	accuracy_policy_2: 0.63969
	loss_value_2: 0.03266
	loss_reward_2: 0.00467
	loss_policy_3: 0.03817
	accuracy_policy_3: 0.61359
	loss_value_3: 0.03394
	loss_reward_3: 0.00515
	loss_policy_4: 0.04132
	accuracy_policy_4: 0.59084
	loss_value_4: 0.0351
	loss_reward_4: 0.00536
	loss_policy_5: 0.04418
	accuracy_policy_5: 0.56393
	loss_value_5: 0.03628
	loss_reward_5: 0.00679
	loss_policy: 0.31125
	loss_value: 0.31714
	loss_reward: 0.02679
Optimization_Done 51800
[2024-05-13 05:27:17] [command] train weight_iter_51800.pkl 258 260
[2024-05-13 05:28:18] nn step 51900, lr: 0.04783.
	loss_policy_0: 0.14512
	accuracy_policy_0: 0.69609
	loss_value_0: 0.15119
	loss_policy_1: 0.03354
	accuracy_policy_1: 0.65408
	loss_value_1: 0.03187
	loss_reward_1: 0.00471
	loss_policy_2: 0.03673
	accuracy_policy_2: 0.63244
	loss_value_2: 0.03325
	loss_reward_2: 0.00463
	loss_policy_3: 0.03949
	accuracy_policy_3: 0.61146
	loss_value_3: 0.03449
	loss_reward_3: 0.00496
	loss_policy_4: 0.04176
	accuracy_policy_4: 0.59352
	loss_value_4: 0.03581
	loss_reward_4: 0.0056
	loss_policy_5: 0.04362
	accuracy_policy_5: 0.58242
	loss_value_5: 0.03701
	loss_reward_5: 0.00673
	loss_policy: 0.34025
	loss_value: 0.32362
	loss_reward: 0.02663
[2024-05-13 05:29:21] nn step 52000, lr: 0.04783.
	loss_policy_0: 0.1201
	accuracy_policy_0: 0.75348
	loss_value_0: 0.1567
	loss_policy_1: 0.03015
	accuracy_policy_1: 0.7034
	loss_value_1: 0.03304
	loss_reward_1: 0.00491
	loss_policy_2: 0.03348
	accuracy_policy_2: 0.67506
	loss_value_2: 0.03458
	loss_reward_2: 0.0048
	loss_policy_3: 0.0368
	accuracy_policy_3: 0.6502
	loss_value_3: 0.03586
	loss_reward_3: 0.00528
	loss_policy_4: 0.03967
	accuracy_policy_4: 0.62562
	loss_value_4: 0.0371
	loss_reward_4: 0.00563
	loss_policy_5: 0.04278
	accuracy_policy_5: 0.60443
	loss_value_5: 0.03839
	loss_reward_5: 0.00687
	loss_policy: 0.30298
	loss_value: 0.33567
	loss_reward: 0.02748
Optimization_Done 52000
[2024-05-13 05:31:36] [command] train weight_iter_52000.pkl 259 261
[2024-05-13 05:32:37] nn step 52100, lr: 0.04783.
	loss_policy_0: 0.13292
	accuracy_policy_0: 0.70336
	loss_value_0: 0.15624
	loss_policy_1: 0.0319
	accuracy_policy_1: 0.65533
	loss_value_1: 0.03295
	loss_reward_1: 0.00503
	loss_policy_2: 0.03547
	accuracy_policy_2: 0.62254
	loss_value_2: 0.03416
	loss_reward_2: 0.00489
	loss_policy_3: 0.0383
	accuracy_policy_3: 0.60303
	loss_value_3: 0.03546
	loss_reward_3: 0.00542
	loss_policy_4: 0.0408
	accuracy_policy_4: 0.57977
	loss_value_4: 0.03674
	loss_reward_4: 0.00577
	loss_policy_5: 0.04334
	accuracy_policy_5: 0.56385
	loss_value_5: 0.03788
	loss_reward_5: 0.00697
	loss_policy: 0.32273
	loss_value: 0.33343
	loss_reward: 0.02808
[2024-05-13 05:33:31] nn step 52200, lr: 0.04783.
	loss_policy_0: 0.10877
	accuracy_policy_0: 0.75588
	loss_value_0: 0.15548
	loss_policy_1: 0.02835
	accuracy_policy_1: 0.68902
	loss_value_1: 0.03276
	loss_reward_1: 0.00502
	loss_policy_2: 0.03202
	accuracy_policy_2: 0.65855
	loss_value_2: 0.034
	loss_reward_2: 0.00486
	loss_policy_3: 0.03479
	accuracy_policy_3: 0.63576
	loss_value_3: 0.03514
	loss_reward_3: 0.00515
	loss_policy_4: 0.03733
	accuracy_policy_4: 0.61434
	loss_value_4: 0.03633
	loss_reward_4: 0.00563
	loss_policy_5: 0.04018
	accuracy_policy_5: 0.59381
	loss_value_5: 0.03747
	loss_reward_5: 0.00665
	loss_policy: 0.28145
	loss_value: 0.3312
	loss_reward: 0.02732
Optimization_Done 52200
[2024-05-13 05:35:15] [command] train weight_iter_52200.pkl 260 262
[2024-05-13 05:36:17] nn step 52300, lr: 0.04783.
	loss_policy_0: 0.12364
	accuracy_policy_0: 0.71998
	loss_value_0: 0.15253
	loss_policy_1: 0.03079
	accuracy_policy_1: 0.65945
	loss_value_1: 0.032
	loss_reward_1: 0.0045
	loss_policy_2: 0.03427
	accuracy_policy_2: 0.62561
	loss_value_2: 0.03325
	loss_reward_2: 0.00427
	loss_policy_3: 0.03641
	accuracy_policy_3: 0.60912
	loss_value_3: 0.03431
	loss_reward_3: 0.00456
	loss_policy_4: 0.03883
	accuracy_policy_4: 0.58824
	loss_value_4: 0.03546
	loss_reward_4: 0.0049
	loss_policy_5: 0.04078
	accuracy_policy_5: 0.57293
	loss_value_5: 0.03649
	loss_reward_5: 0.00583
	loss_policy: 0.30473
	loss_value: 0.32405
	loss_reward: 0.02405
[2024-05-13 05:37:17] nn step 52400, lr: 0.04783.
	loss_policy_0: 0.09995
	accuracy_policy_0: 0.7699
	loss_value_0: 0.14951
	loss_policy_1: 0.02649
	accuracy_policy_1: 0.69916
	loss_value_1: 0.03136
	loss_reward_1: 0.00438
	loss_policy_2: 0.02976
	accuracy_policy_2: 0.66686
	loss_value_2: 0.0325
	loss_reward_2: 0.00414
	loss_policy_3: 0.03271
	accuracy_policy_3: 0.64209
	loss_value_3: 0.03357
	loss_reward_3: 0.00447
	loss_policy_4: 0.03482
	accuracy_policy_4: 0.6232
	loss_value_4: 0.03453
	loss_reward_4: 0.00484
	loss_policy_5: 0.0372
	accuracy_policy_5: 0.60643
	loss_value_5: 0.03556
	loss_reward_5: 0.00578
	loss_policy: 0.26093
	loss_value: 0.31703
	loss_reward: 0.02361
Optimization_Done 52400
[2024-05-13 05:39:34] [command] train weight_iter_52400.pkl 261 263
[2024-05-13 05:40:35] nn step 52500, lr: 0.04783.
	loss_policy_0: 0.14054
	accuracy_policy_0: 0.67555
	loss_value_0: 0.13857
	loss_policy_1: 0.03297
	accuracy_policy_1: 0.62596
	loss_value_1: 0.02908
	loss_reward_1: 0.00422
	loss_policy_2: 0.03662
	accuracy_policy_2: 0.59471
	loss_value_2: 0.03023
	loss_reward_2: 0.0042
	loss_policy_3: 0.03893
	accuracy_policy_3: 0.5715
	loss_value_3: 0.0315
	loss_reward_3: 0.00438
	loss_policy_4: 0.04127
	accuracy_policy_4: 0.5518
	loss_value_4: 0.03258
	loss_reward_4: 0.00493
	loss_policy_5: 0.04319
	accuracy_policy_5: 0.53904
	loss_value_5: 0.03362
	loss_reward_5: 0.00581
	loss_policy: 0.33352
	loss_value: 0.29559
	loss_reward: 0.02354
[2024-05-13 05:41:41] nn step 52600, lr: 0.04783.
	loss_policy_0: 0.11617
	accuracy_policy_0: 0.73344
	loss_value_0: 0.1411
	loss_policy_1: 0.0297
	accuracy_policy_1: 0.66859
	loss_value_1: 0.02968
	loss_reward_1: 0.00434
	loss_policy_2: 0.03294
	accuracy_policy_2: 0.63719
	loss_value_2: 0.03092
	loss_reward_2: 0.00415
	loss_policy_3: 0.03575
	accuracy_policy_3: 0.61301
	loss_value_3: 0.03204
	loss_reward_3: 0.00443
	loss_policy_4: 0.03837
	accuracy_policy_4: 0.58867
	loss_value_4: 0.03314
	loss_reward_4: 0.00476
	loss_policy_5: 0.04084
	accuracy_policy_5: 0.56717
	loss_value_5: 0.03429
	loss_reward_5: 0.00574
	loss_policy: 0.29377
	loss_value: 0.30118
	loss_reward: 0.02341
Optimization_Done 52600
[2024-05-13 05:43:57] [command] train weight_iter_52600.pkl 262 264
[2024-05-13 05:44:59] nn step 52700, lr: 0.04783.
	loss_policy_0: 0.17259
	accuracy_policy_0: 0.65189
	loss_value_0: 0.1549
	loss_policy_1: 0.03951
	accuracy_policy_1: 0.60701
	loss_value_1: 0.03265
	loss_reward_1: 0.00459
	loss_policy_2: 0.04281
	accuracy_policy_2: 0.57955
	loss_value_2: 0.03433
	loss_reward_2: 0.00449
	loss_policy_3: 0.04545
	accuracy_policy_3: 0.55893
	loss_value_3: 0.03558
	loss_reward_3: 0.00501
	loss_policy_4: 0.04774
	accuracy_policy_4: 0.5407
	loss_value_4: 0.03688
	loss_reward_4: 0.00524
	loss_policy_5: 0.04987
	accuracy_policy_5: 0.5273
	loss_value_5: 0.03803
	loss_reward_5: 0.00616
	loss_policy: 0.39797
	loss_value: 0.33237
	loss_reward: 0.0255
[2024-05-13 05:45:55] nn step 52800, lr: 0.04783.
	loss_policy_0: 0.13675
	accuracy_policy_0: 0.71309
	loss_value_0: 0.15071
	loss_policy_1: 0.0335
	accuracy_policy_1: 0.65223
	loss_value_1: 0.0317
	loss_reward_1: 0.00457
	loss_policy_2: 0.03696
	accuracy_policy_2: 0.62197
	loss_value_2: 0.03296
	loss_reward_2: 0.0043
	loss_policy_3: 0.03956
	accuracy_policy_3: 0.59984
	loss_value_3: 0.03404
	loss_reward_3: 0.00473
	loss_policy_4: 0.04239
	accuracy_policy_4: 0.57584
	loss_value_4: 0.0352
	loss_reward_4: 0.00508
	loss_policy_5: 0.04523
	accuracy_policy_5: 0.55459
	loss_value_5: 0.03627
	loss_reward_5: 0.00614
	loss_policy: 0.33439
	loss_value: 0.32089
	loss_reward: 0.02482
Optimization_Done 52800
[2024-05-13 05:48:11] [command] train weight_iter_52800.pkl 263 265
[2024-05-13 05:49:12] nn step 52900, lr: 0.04783.
	loss_policy_0: 0.17536
	accuracy_policy_0: 0.65543
	loss_value_0: 0.16694
	loss_policy_1: 0.04111
	accuracy_policy_1: 0.60102
	loss_value_1: 0.03501
	loss_reward_1: 0.00567
	loss_policy_2: 0.04459
	accuracy_policy_2: 0.57619
	loss_value_2: 0.03643
	loss_reward_2: 0.00547
	loss_policy_3: 0.04734
	accuracy_policy_3: 0.5557
	loss_value_3: 0.03763
	loss_reward_3: 0.00591
	loss_policy_4: 0.05
	accuracy_policy_4: 0.53521
	loss_value_4: 0.03894
	loss_reward_4: 0.00659
	loss_policy_5: 0.05255
	accuracy_policy_5: 0.51832
	loss_value_5: 0.04011
	loss_reward_5: 0.00753
	loss_policy: 0.41095
	loss_value: 0.35506
	loss_reward: 0.03117
[2024-05-13 05:50:19] nn step 53000, lr: 0.04783.
	loss_policy_0: 0.15213
	accuracy_policy_0: 0.70311
	loss_value_0: 0.16716
	loss_policy_1: 0.0381
	accuracy_policy_1: 0.63621
	loss_value_1: 0.0349
	loss_reward_1: 0.00576
	loss_policy_2: 0.04144
	accuracy_policy_2: 0.60959
	loss_value_2: 0.03623
	loss_reward_2: 0.00537
	loss_policy_3: 0.04463
	accuracy_policy_3: 0.58441
	loss_value_3: 0.03753
	loss_reward_3: 0.00575
	loss_policy_4: 0.04754
	accuracy_policy_4: 0.56053
	loss_value_4: 0.03872
	loss_reward_4: 0.00622
	loss_policy_5: 0.05028
	accuracy_policy_5: 0.54035
	loss_value_5: 0.03994
	loss_reward_5: 0.0076
	loss_policy: 0.37412
	loss_value: 0.35448
	loss_reward: 0.03071
Optimization_Done 53000
[2024-05-13 05:52:23] [command] train weight_iter_53000.pkl 264 266
[2024-05-13 05:53:25] nn step 53100, lr: 0.04783.
	loss_policy_0: 0.16774
	accuracy_policy_0: 0.66283
	loss_value_0: 0.16476
	loss_policy_1: 0.03951
	accuracy_policy_1: 0.60309
	loss_value_1: 0.03413
	loss_reward_1: 0.0051
	loss_policy_2: 0.04295
	accuracy_policy_2: 0.57592
	loss_value_2: 0.03536
	loss_reward_2: 0.00488
	loss_policy_3: 0.04569
	accuracy_policy_3: 0.55133
	loss_value_3: 0.03638
	loss_reward_3: 0.00537
	loss_policy_4: 0.04824
	accuracy_policy_4: 0.52762
	loss_value_4: 0.03729
	loss_reward_4: 0.00596
	loss_policy_5: 0.05084
	accuracy_policy_5: 0.51143
	loss_value_5: 0.03825
	loss_reward_5: 0.00691
	loss_policy: 0.39497
	loss_value: 0.34617
	loss_reward: 0.02823
[2024-05-13 05:54:21] nn step 53200, lr: 0.04783.
	loss_policy_0: 0.13924
	accuracy_policy_0: 0.70715
	loss_value_0: 0.15885
	loss_policy_1: 0.03455
	accuracy_policy_1: 0.64297
	loss_value_1: 0.03312
	loss_reward_1: 0.00509
	loss_policy_2: 0.03822
	accuracy_policy_2: 0.6116
	loss_value_2: 0.0342
	loss_reward_2: 0.00467
	loss_policy_3: 0.04087
	accuracy_policy_3: 0.58639
	loss_value_3: 0.03514
	loss_reward_3: 0.00523
	loss_policy_4: 0.04362
	accuracy_policy_4: 0.56514
	loss_value_4: 0.03604
	loss_reward_4: 0.00569
	loss_policy_5: 0.04642
	accuracy_policy_5: 0.54312
	loss_value_5: 0.03698
	loss_reward_5: 0.00673
	loss_policy: 0.34291
	loss_value: 0.33434
	loss_reward: 0.02741
Optimization_Done 53200
[2024-05-13 05:56:26] [command] train weight_iter_53200.pkl 265 267
[2024-05-13 05:57:28] nn step 53300, lr: 0.04783.
	loss_policy_0: 0.16968
	accuracy_policy_0: 0.65518
	loss_value_0: 0.16219
	loss_policy_1: 0.04036
	accuracy_policy_1: 0.60049
	loss_value_1: 0.03381
	loss_reward_1: 0.00471
	loss_policy_2: 0.04353
	accuracy_policy_2: 0.5758
	loss_value_2: 0.03508
	loss_reward_2: 0.00471
	loss_policy_3: 0.04637
	accuracy_policy_3: 0.55186
	loss_value_3: 0.03617
	loss_reward_3: 0.00497
	loss_policy_4: 0.04922
	accuracy_policy_4: 0.53275
	loss_value_4: 0.03721
	loss_reward_4: 0.00554
	loss_policy_5: 0.05186
	accuracy_policy_5: 0.50877
	loss_value_5: 0.03836
	loss_reward_5: 0.00646
	loss_policy: 0.40102
	loss_value: 0.34282
	loss_reward: 0.02638
[2024-05-13 05:58:34] nn step 53400, lr: 0.04783.
	loss_policy_0: 0.13645
	accuracy_policy_0: 0.71365
	loss_value_0: 0.15711
	loss_policy_1: 0.03447
	accuracy_policy_1: 0.64266
	loss_value_1: 0.03287
	loss_reward_1: 0.0048
	loss_policy_2: 0.03809
	accuracy_policy_2: 0.6127
	loss_value_2: 0.03401
	loss_reward_2: 0.00434
	loss_policy_3: 0.04127
	accuracy_policy_3: 0.58885
	loss_value_3: 0.03513
	loss_reward_3: 0.00477
	loss_policy_4: 0.04415
	accuracy_policy_4: 0.56354
	loss_value_4: 0.03616
	loss_reward_4: 0.00525
	loss_policy_5: 0.04724
	accuracy_policy_5: 0.5342
	loss_value_5: 0.03715
	loss_reward_5: 0.00629
	loss_policy: 0.34167
	loss_value: 0.33244
	loss_reward: 0.02545
Optimization_Done 53400
[2024-05-13 06:00:51] [command] train weight_iter_53400.pkl 266 268
[2024-05-13 06:01:47] nn step 53500, lr: 0.04783.
	loss_policy_0: 0.15646
	accuracy_policy_0: 0.66574
	loss_value_0: 0.15776
	loss_policy_1: 0.03717
	accuracy_policy_1: 0.61117
	loss_value_1: 0.0329
	loss_reward_1: 0.00444
	loss_policy_2: 0.04089
	accuracy_policy_2: 0.57977
	loss_value_2: 0.03446
	loss_reward_2: 0.00426
	loss_policy_3: 0.04377
	accuracy_policy_3: 0.55906
	loss_value_3: 0.03581
	loss_reward_3: 0.00453
	loss_policy_4: 0.04663
	accuracy_policy_4: 0.53852
	loss_value_4: 0.037
	loss_reward_4: 0.00511
	loss_policy_5: 0.04901
	accuracy_policy_5: 0.51895
	loss_value_5: 0.03815
	loss_reward_5: 0.00606
	loss_policy: 0.37394
	loss_value: 0.33607
	loss_reward: 0.02441
[2024-05-13 06:02:37] nn step 53600, lr: 0.04783.
	loss_policy_0: 0.12847
	accuracy_policy_0: 0.71797
	loss_value_0: 0.15382
	loss_policy_1: 0.03283
	accuracy_policy_1: 0.64934
	loss_value_1: 0.03215
	loss_reward_1: 0.00431
	loss_policy_2: 0.03621
	accuracy_policy_2: 0.61883
	loss_value_2: 0.03352
	loss_reward_2: 0.00412
	loss_policy_3: 0.03941
	accuracy_policy_3: 0.59057
	loss_value_3: 0.03468
	loss_reward_3: 0.00447
	loss_policy_4: 0.04222
	accuracy_policy_4: 0.57262
	loss_value_4: 0.03582
	loss_reward_4: 0.00481
	loss_policy_5: 0.04498
	accuracy_policy_5: 0.54711
	loss_value_5: 0.03681
	loss_reward_5: 0.00585
	loss_policy: 0.32413
	loss_value: 0.32681
	loss_reward: 0.02356
Optimization_Done 53600
[2024-05-13 06:04:53] [command] train weight_iter_53600.pkl 267 269
[2024-05-13 06:05:56] nn step 53700, lr: 0.04783.
	loss_policy_0: 0.15718
	accuracy_policy_0: 0.68631
	loss_value_0: 0.16501
	loss_policy_1: 0.03792
	accuracy_policy_1: 0.62887
	loss_value_1: 0.03461
	loss_reward_1: 0.00523
	loss_policy_2: 0.04159
	accuracy_policy_2: 0.6
	loss_value_2: 0.03609
	loss_reward_2: 0.00515
	loss_policy_3: 0.04425
	accuracy_policy_3: 0.58012
	loss_value_3: 0.03747
	loss_reward_3: 0.0055
	loss_policy_4: 0.04695
	accuracy_policy_4: 0.56164
	loss_value_4: 0.03853
	loss_reward_4: 0.00608
	loss_policy_5: 0.04962
	accuracy_policy_5: 0.54281
	loss_value_5: 0.03971
	loss_reward_5: 0.00711
	loss_policy: 0.37752
	loss_value: 0.35141
	loss_reward: 0.02908
[2024-05-13 06:07:01] nn step 53800, lr: 0.04783.
	loss_policy_0: 0.12765
	accuracy_policy_0: 0.72828
	loss_value_0: 0.15879
	loss_policy_1: 0.03273
	accuracy_policy_1: 0.66217
	loss_value_1: 0.03341
	loss_reward_1: 0.00506
	loss_policy_2: 0.03633
	accuracy_policy_2: 0.63576
	loss_value_2: 0.03476
	loss_reward_2: 0.00476
	loss_policy_3: 0.03943
	accuracy_policy_3: 0.61029
	loss_value_3: 0.03601
	loss_reward_3: 0.0054
	loss_policy_4: 0.0421
	accuracy_policy_4: 0.59268
	loss_value_4: 0.03723
	loss_reward_4: 0.00569
	loss_policy_5: 0.04499
	accuracy_policy_5: 0.57219
	loss_value_5: 0.03837
	loss_reward_5: 0.00698
	loss_policy: 0.32324
	loss_value: 0.33857
	loss_reward: 0.0279
Optimization_Done 53800
[2024-05-13 06:09:15] [command] train weight_iter_53800.pkl 268 270
[2024-05-13 06:10:19] nn step 53900, lr: 0.04783.
	loss_policy_0: 0.13152
	accuracy_policy_0: 0.71574
	loss_value_0: 0.16152
	loss_policy_1: 0.03281
	accuracy_policy_1: 0.65609
	loss_value_1: 0.03348
	loss_reward_1: 0.00544
	loss_policy_2: 0.03644
	accuracy_policy_2: 0.62674
	loss_value_2: 0.03488
	loss_reward_2: 0.00514
	loss_policy_3: 0.03902
	accuracy_policy_3: 0.60467
	loss_value_3: 0.03608
	loss_reward_3: 0.00569
	loss_policy_4: 0.04191
	accuracy_policy_4: 0.58279
	loss_value_4: 0.03719
	loss_reward_4: 0.00617
	loss_policy_5: 0.04462
	accuracy_policy_5: 0.56012
	loss_value_5: 0.03822
	loss_reward_5: 0.00721
	loss_policy: 0.32631
	loss_value: 0.34138
	loss_reward: 0.02965
[2024-05-13 06:11:25] nn step 54000, lr: 0.04783.
	loss_policy_0: 0.10621
	accuracy_policy_0: 0.75818
	loss_value_0: 0.15358
	loss_policy_1: 0.02885
	accuracy_policy_1: 0.68248
	loss_value_1: 0.03208
	loss_reward_1: 0.00499
	loss_policy_2: 0.03216
	accuracy_policy_2: 0.6524
	loss_value_2: 0.03312
	loss_reward_2: 0.0048
	loss_policy_3: 0.03495
	accuracy_policy_3: 0.62793
	loss_value_3: 0.03431
	loss_reward_3: 0.00521
	loss_policy_4: 0.03761
	accuracy_policy_4: 0.60783
	loss_value_4: 0.03548
	loss_reward_4: 0.00553
	loss_policy_5: 0.04041
	accuracy_policy_5: 0.58381
	loss_value_5: 0.03667
	loss_reward_5: 0.00679
	loss_policy: 0.28019
	loss_value: 0.32523
	loss_reward: 0.02732
Optimization_Done 54000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-13 06:18:41] [command] train weight_iter_54000.pkl 269 271
[2024-05-13 06:19:42] nn step 54100, lr: 0.043047.
	loss_policy_0: 0.12402
	accuracy_policy_0: 0.73719
	loss_value_0: 0.16852
	loss_policy_1: 0.03199
	accuracy_policy_1: 0.67377
	loss_value_1: 0.03511
	loss_reward_1: 0.00537
	loss_policy_2: 0.03558
	accuracy_policy_2: 0.64463
	loss_value_2: 0.03651
	loss_reward_2: 0.00528
	loss_policy_3: 0.03898
	accuracy_policy_3: 0.61541
	loss_value_3: 0.0376
	loss_reward_3: 0.00567
	loss_policy_4: 0.04168
	accuracy_policy_4: 0.5927
	loss_value_4: 0.03867
	loss_reward_4: 0.0061
	loss_policy_5: 0.04448
	accuracy_policy_5: 0.57012
	loss_value_5: 0.03956
	loss_reward_5: 0.00717
	loss_policy: 0.31673
	loss_value: 0.35598
	loss_reward: 0.02959
[2024-05-13 06:20:31] nn step 54200, lr: 0.043047.
	loss_policy_0: 0.10057
	accuracy_policy_0: 0.77262
	loss_value_0: 0.16169
	loss_policy_1: 0.02781
	accuracy_policy_1: 0.70348
	loss_value_1: 0.03396
	loss_reward_1: 0.0052
	loss_policy_2: 0.03142
	accuracy_policy_2: 0.67131
	loss_value_2: 0.03497
	loss_reward_2: 0.00504
	loss_policy_3: 0.03435
	accuracy_policy_3: 0.65061
	loss_value_3: 0.03589
	loss_reward_3: 0.00536
	loss_policy_4: 0.03746
	accuracy_policy_4: 0.62539
	loss_value_4: 0.03698
	loss_reward_4: 0.00591
	loss_policy_5: 0.04005
	accuracy_policy_5: 0.60186
	loss_value_5: 0.03805
	loss_reward_5: 0.00678
	loss_policy: 0.27166
	loss_value: 0.34154
	loss_reward: 0.02829
Optimization_Done 54200
[2024-05-13 06:22:48] [command] train weight_iter_54200.pkl 270 272
[2024-05-13 06:23:50] nn step 54300, lr: 0.043047.
	loss_policy_0: 0.13328
	accuracy_policy_0: 0.70354
	loss_value_0: 0.15909
	loss_policy_1: 0.03301
	accuracy_policy_1: 0.64863
	loss_value_1: 0.03321
	loss_reward_1: 0.00427
	loss_policy_2: 0.0371
	accuracy_policy_2: 0.61889
	loss_value_2: 0.03447
	loss_reward_2: 0.0042
	loss_policy_3: 0.04035
	accuracy_policy_3: 0.59121
	loss_value_3: 0.03567
	loss_reward_3: 0.00463
	loss_policy_4: 0.04309
	accuracy_policy_4: 0.57021
	loss_value_4: 0.0369
	loss_reward_4: 0.00506
	loss_policy_5: 0.04556
	accuracy_policy_5: 0.54902
	loss_value_5: 0.03796
	loss_reward_5: 0.00595
	loss_policy: 0.33239
	loss_value: 0.3373
	loss_reward: 0.02411
[2024-05-13 06:24:51] nn step 54400, lr: 0.043047.
	loss_policy_0: 0.10337
	accuracy_policy_0: 0.75748
	loss_value_0: 0.15607
	loss_policy_1: 0.02795
	accuracy_policy_1: 0.6907
	loss_value_1: 0.03266
	loss_reward_1: 0.00427
	loss_policy_2: 0.03198
	accuracy_policy_2: 0.65668
	loss_value_2: 0.03378
	loss_reward_2: 0.00418
	loss_policy_3: 0.03555
	accuracy_policy_3: 0.63064
	loss_value_3: 0.03497
	loss_reward_3: 0.0045
	loss_policy_4: 0.03858
	accuracy_policy_4: 0.60371
	loss_value_4: 0.03604
	loss_reward_4: 0.00488
	loss_policy_5: 0.04114
	accuracy_policy_5: 0.58475
	loss_value_5: 0.03718
	loss_reward_5: 0.00585
	loss_policy: 0.27857
	loss_value: 0.3307
	loss_reward: 0.02367
Optimization_Done 54400
[2024-05-13 06:27:05] [command] train weight_iter_54400.pkl 271 273
[2024-05-13 06:28:03] nn step 54500, lr: 0.043047.
	loss_policy_0: 0.14184
	accuracy_policy_0: 0.70051
	loss_value_0: 0.15541
	loss_policy_1: 0.03494
	accuracy_policy_1: 0.64225
	loss_value_1: 0.03289
	loss_reward_1: 0.00507
	loss_policy_2: 0.0385
	accuracy_policy_2: 0.6175
	loss_value_2: 0.03433
	loss_reward_2: 0.00489
	loss_policy_3: 0.04139
	accuracy_policy_3: 0.59744
	loss_value_3: 0.03548
	loss_reward_3: 0.00535
	loss_policy_4: 0.04423
	accuracy_policy_4: 0.57809
	loss_value_4: 0.03669
	loss_reward_4: 0.00577
	loss_policy_5: 0.04692
	accuracy_policy_5: 0.55994
	loss_value_5: 0.03779
	loss_reward_5: 0.00662
	loss_policy: 0.34782
	loss_value: 0.33259
	loss_reward: 0.02769
[2024-05-13 06:29:01] nn step 54600, lr: 0.043047.
	loss_policy_0: 0.11548
	accuracy_policy_0: 0.75525
	loss_value_0: 0.15462
	loss_policy_1: 0.03093
	accuracy_policy_1: 0.68332
	loss_value_1: 0.03268
	loss_reward_1: 0.00494
	loss_policy_2: 0.03461
	accuracy_policy_2: 0.65547
	loss_value_2: 0.03412
	loss_reward_2: 0.00481
	loss_policy_3: 0.03792
	accuracy_policy_3: 0.63018
	loss_value_3: 0.03534
	loss_reward_3: 0.00519
	loss_policy_4: 0.04126
	accuracy_policy_4: 0.60625
	loss_value_4: 0.0366
	loss_reward_4: 0.00562
	loss_policy_5: 0.04411
	accuracy_policy_5: 0.5859
	loss_value_5: 0.03773
	loss_reward_5: 0.00673
	loss_policy: 0.3043
	loss_value: 0.3311
	loss_reward: 0.02728
Optimization_Done 54600
[2024-05-13 06:31:13] [command] train weight_iter_54600.pkl 272 274
[2024-05-13 06:32:12] nn step 54700, lr: 0.043047.
	loss_policy_0: 0.10104
	accuracy_policy_0: 0.75277
	loss_value_0: 0.13645
	loss_policy_1: 0.02673
	accuracy_policy_1: 0.68236
	loss_value_1: 0.0285
	loss_reward_1: 0.00426
	loss_policy_2: 0.02986
	accuracy_policy_2: 0.65625
	loss_value_2: 0.02958
	loss_reward_2: 0.00402
	loss_policy_3: 0.03254
	accuracy_policy_3: 0.63391
	loss_value_3: 0.03056
	loss_reward_3: 0.00433
	loss_policy_4: 0.03501
	accuracy_policy_4: 0.61104
	loss_value_4: 0.03148
	loss_reward_4: 0.00467
	loss_policy_5: 0.03746
	accuracy_policy_5: 0.59018
	loss_value_5: 0.03234
	loss_reward_5: 0.00546
	loss_policy: 0.26264
	loss_value: 0.28891
	loss_reward: 0.02273
[2024-05-13 06:33:16] nn step 54800, lr: 0.043047.
	loss_policy_0: 0.09135
	accuracy_policy_0: 0.7777
	loss_value_0: 0.14073
	loss_policy_1: 0.0256
	accuracy_policy_1: 0.69699
	loss_value_1: 0.02929
	loss_reward_1: 0.00422
	loss_policy_2: 0.02868
	accuracy_policy_2: 0.67021
	loss_value_2: 0.03042
	loss_reward_2: 0.00399
	loss_policy_3: 0.03121
	accuracy_policy_3: 0.64779
	loss_value_3: 0.03137
	loss_reward_3: 0.00438
	loss_policy_4: 0.03412
	accuracy_policy_4: 0.6267
	loss_value_4: 0.0323
	loss_reward_4: 0.00464
	loss_policy_5: 0.0368
	accuracy_policy_5: 0.60098
	loss_value_5: 0.03323
	loss_reward_5: 0.0056
	loss_policy: 0.24776
	loss_value: 0.29735
	loss_reward: 0.02283
Optimization_Done 54800
[2024-05-13 06:35:35] [command] train weight_iter_54800.pkl 273 275
[2024-05-13 06:36:38] nn step 54900, lr: 0.043047.
	loss_policy_0: 0.11288
	accuracy_policy_0: 0.70939
	loss_value_0: 0.13847
	loss_policy_1: 0.02812
	accuracy_policy_1: 0.64795
	loss_value_1: 0.02888
	loss_reward_1: 0.00386
	loss_policy_2: 0.03063
	accuracy_policy_2: 0.62191
	loss_value_2: 0.03015
	loss_reward_2: 0.0038
	loss_policy_3: 0.03265
	accuracy_policy_3: 0.60262
	loss_value_3: 0.03127
	loss_reward_3: 0.00406
	loss_policy_4: 0.03468
	accuracy_policy_4: 0.58055
	loss_value_4: 0.03222
	loss_reward_4: 0.00437
	loss_policy_5: 0.03682
	accuracy_policy_5: 0.56568
	loss_value_5: 0.03309
	loss_reward_5: 0.00525
	loss_policy: 0.27578
	loss_value: 0.29409
	loss_reward: 0.02134
[2024-05-13 06:37:37] nn step 55000, lr: 0.043047.
	loss_policy_0: 0.08849
	accuracy_policy_0: 0.77686
	loss_value_0: 0.13753
	loss_policy_1: 0.02448
	accuracy_policy_1: 0.6991
	loss_value_1: 0.02865
	loss_reward_1: 0.00386
	loss_policy_2: 0.02714
	accuracy_policy_2: 0.66881
	loss_value_2: 0.02974
	loss_reward_2: 0.00382
	loss_policy_3: 0.02933
	accuracy_policy_3: 0.64893
	loss_value_3: 0.03072
	loss_reward_3: 0.00397
	loss_policy_4: 0.03173
	accuracy_policy_4: 0.63156
	loss_value_4: 0.0317
	loss_reward_4: 0.00436
	loss_policy_5: 0.03398
	accuracy_policy_5: 0.60834
	loss_value_5: 0.03265
	loss_reward_5: 0.00529
	loss_policy: 0.23515
	loss_value: 0.29098
	loss_reward: 0.02131
Optimization_Done 55000
[2024-05-13 06:39:58] [command] train weight_iter_55000.pkl 274 276
[2024-05-13 06:41:00] nn step 55100, lr: 0.043047.
	loss_policy_0: 0.12924
	accuracy_policy_0: 0.67674
	loss_value_0: 0.1305
	loss_policy_1: 0.03013
	accuracy_policy_1: 0.63166
	loss_value_1: 0.02749
	loss_reward_1: 0.00321
	loss_policy_2: 0.03263
	accuracy_policy_2: 0.60838
	loss_value_2: 0.02863
	loss_reward_2: 0.00329
	loss_policy_3: 0.03443
	accuracy_policy_3: 0.59035
	loss_value_3: 0.02973
	loss_reward_3: 0.00352
	loss_policy_4: 0.03662
	accuracy_policy_4: 0.57049
	loss_value_4: 0.03073
	loss_reward_4: 0.00382
	loss_policy_5: 0.03815
	accuracy_policy_5: 0.55844
	loss_value_5: 0.0317
	loss_reward_5: 0.00447
	loss_policy: 0.3012
	loss_value: 0.27878
	loss_reward: 0.01831
[2024-05-13 06:42:03] nn step 55200, lr: 0.043047.
	loss_policy_0: 0.10382
	accuracy_policy_0: 0.73961
	loss_value_0: 0.13119
	loss_policy_1: 0.02625
	accuracy_policy_1: 0.68137
	loss_value_1: 0.02761
	loss_reward_1: 0.00335
	loss_policy_2: 0.02896
	accuracy_policy_2: 0.65377
	loss_value_2: 0.02864
	loss_reward_2: 0.00334
	loss_policy_3: 0.03087
	accuracy_policy_3: 0.63797
	loss_value_3: 0.02968
	loss_reward_3: 0.00337
	loss_policy_4: 0.03322
	accuracy_policy_4: 0.61389
	loss_value_4: 0.03072
	loss_reward_4: 0.00381
	loss_policy_5: 0.03534
	accuracy_policy_5: 0.59697
	loss_value_5: 0.03175
	loss_reward_5: 0.00445
	loss_policy: 0.25847
	loss_value: 0.27958
	loss_reward: 0.01832
Optimization_Done 55200
[2024-05-13 06:44:27] [command] train weight_iter_55200.pkl 275 277
[2024-05-13 06:45:15] nn step 55300, lr: 0.043047.
	loss_policy_0: 0.17663
	accuracy_policy_0: 0.64043
	loss_value_0: 0.15553
	loss_policy_1: 0.03961
	accuracy_policy_1: 0.59932
	loss_value_1: 0.03256
	loss_reward_1: 0.00421
	loss_policy_2: 0.04231
	accuracy_policy_2: 0.57461
	loss_value_2: 0.03378
	loss_reward_2: 0.00421
	loss_policy_3: 0.04496
	accuracy_policy_3: 0.55545
	loss_value_3: 0.03503
	loss_reward_3: 0.00455
	loss_policy_4: 0.04693
	accuracy_policy_4: 0.54117
	loss_value_4: 0.03613
	loss_reward_4: 0.00498
	loss_policy_5: 0.04903
	accuracy_policy_5: 0.52414
	loss_value_5: 0.0374
	loss_reward_5: 0.00549
	loss_policy: 0.39947
	loss_value: 0.33043
	loss_reward: 0.02344
[2024-05-13 06:45:59] nn step 55400, lr: 0.043047.
	loss_policy_0: 0.14537
	accuracy_policy_0: 0.71061
	loss_value_0: 0.15463
	loss_policy_1: 0.03467
	accuracy_policy_1: 0.65449
	loss_value_1: 0.03243
	loss_reward_1: 0.00442
	loss_policy_2: 0.0382
	accuracy_policy_2: 0.62492
	loss_value_2: 0.03377
	loss_reward_2: 0.00434
	loss_policy_3: 0.04081
	accuracy_policy_3: 0.60494
	loss_value_3: 0.03522
	loss_reward_3: 0.00465
	loss_policy_4: 0.04349
	accuracy_policy_4: 0.58404
	loss_value_4: 0.03638
	loss_reward_4: 0.00501
	loss_policy_5: 0.04605
	accuracy_policy_5: 0.56486
	loss_value_5: 0.03745
	loss_reward_5: 0.00604
	loss_policy: 0.3486
	loss_value: 0.32988
	loss_reward: 0.02445
Optimization_Done 55400
[2024-05-13 06:48:14] [command] train weight_iter_55400.pkl 276 278
[2024-05-13 06:49:14] nn step 55500, lr: 0.043047.
	loss_policy_0: 0.18649
	accuracy_policy_0: 0.65592
	loss_value_0: 0.15925
	loss_policy_1: 0.04269
	accuracy_policy_1: 0.609
	loss_value_1: 0.03338
	loss_reward_1: 0.00478
	loss_policy_2: 0.04615
	accuracy_policy_2: 0.5817
	loss_value_2: 0.03475
	loss_reward_2: 0.00448
	loss_policy_3: 0.04938
	accuracy_policy_3: 0.55311
	loss_value_3: 0.03594
	loss_reward_3: 0.00476
	loss_policy_4: 0.05176
	accuracy_policy_4: 0.54191
	loss_value_4: 0.03718
	loss_reward_4: 0.00529
	loss_policy_5: 0.05419
	accuracy_policy_5: 0.52293
	loss_value_5: 0.03817
	loss_reward_5: 0.00596
	loss_policy: 0.43066
	loss_value: 0.33868
	loss_reward: 0.02527
[2024-05-13 06:50:09] nn step 55600, lr: 0.043047.
	loss_policy_0: 0.16406
	accuracy_policy_0: 0.69143
	loss_value_0: 0.15447
	loss_policy_1: 0.03901
	accuracy_policy_1: 0.64135
	loss_value_1: 0.03229
	loss_reward_1: 0.00468
	loss_policy_2: 0.04237
	accuracy_policy_2: 0.61307
	loss_value_2: 0.03378
	loss_reward_2: 0.00455
	loss_policy_3: 0.04541
	accuracy_policy_3: 0.58865
	loss_value_3: 0.03499
	loss_reward_3: 0.00479
	loss_policy_4: 0.04838
	accuracy_policy_4: 0.56688
	loss_value_4: 0.0361
	loss_reward_4: 0.00531
	loss_policy_5: 0.0509
	accuracy_policy_5: 0.54963
	loss_value_5: 0.03728
	loss_reward_5: 0.00592
	loss_policy: 0.39013
	loss_value: 0.3289
	loss_reward: 0.02525
Optimization_Done 55600
[2024-05-13 06:52:00] [command] train weight_iter_55600.pkl 277 279
[2024-05-13 06:53:05] nn step 55700, lr: 0.043047.
	loss_policy_0: 0.17334
	accuracy_policy_0: 0.68215
	loss_value_0: 0.16668
	loss_policy_1: 0.04092
	accuracy_policy_1: 0.63107
	loss_value_1: 0.03488
	loss_reward_1: 0.00508
	loss_policy_2: 0.0444
	accuracy_policy_2: 0.59881
	loss_value_2: 0.03624
	loss_reward_2: 0.00492
	loss_policy_3: 0.04748
	accuracy_policy_3: 0.57613
	loss_value_3: 0.03732
	loss_reward_3: 0.00529
	loss_policy_4: 0.05014
	accuracy_policy_4: 0.55373
	loss_value_4: 0.03849
	loss_reward_4: 0.00584
	loss_policy_5: 0.0524
	accuracy_policy_5: 0.52963
	loss_value_5: 0.03943
	loss_reward_5: 0.00632
	loss_policy: 0.40868
	loss_value: 0.35304
	loss_reward: 0.02744
[2024-05-13 06:54:11] nn step 55800, lr: 0.043047.
	loss_policy_0: 0.15367
	accuracy_policy_0: 0.71994
	loss_value_0: 0.16584
	loss_policy_1: 0.03808
	accuracy_policy_1: 0.65709
	loss_value_1: 0.03486
	loss_reward_1: 0.00505
	loss_policy_2: 0.04208
	accuracy_policy_2: 0.62268
	loss_value_2: 0.03611
	loss_reward_2: 0.00496
	loss_policy_3: 0.0451
	accuracy_policy_3: 0.59787
	loss_value_3: 0.03727
	loss_reward_3: 0.00517
	loss_policy_4: 0.04758
	accuracy_policy_4: 0.57973
	loss_value_4: 0.03842
	loss_reward_4: 0.0057
	loss_policy_5: 0.05029
	accuracy_policy_5: 0.55426
	loss_value_5: 0.0396
	loss_reward_5: 0.00652
	loss_policy: 0.3768
	loss_value: 0.3521
	loss_reward: 0.0274
Optimization_Done 55800
[2024-05-13 06:56:29] [command] train weight_iter_55800.pkl 278 280
[2024-05-13 06:57:28] nn step 55900, lr: 0.043047.
	loss_policy_0: 0.16198
	accuracy_policy_0: 0.67211
	loss_value_0: 0.14833
	loss_policy_1: 0.03878
	accuracy_policy_1: 0.61904
	loss_value_1: 0.03126
	loss_reward_1: 0.0046
	loss_policy_2: 0.0424
	accuracy_policy_2: 0.59066
	loss_value_2: 0.03281
	loss_reward_2: 0.00451
	loss_policy_3: 0.04555
	accuracy_policy_3: 0.56639
	loss_value_3: 0.03418
	loss_reward_3: 0.00486
	loss_policy_4: 0.04814
	accuracy_policy_4: 0.54396
	loss_value_4: 0.03551
	loss_reward_4: 0.00539
	loss_policy_5: 0.05052
	accuracy_policy_5: 0.52467
	loss_value_5: 0.03661
	loss_reward_5: 0.00623
	loss_policy: 0.38738
	loss_value: 0.3187
	loss_reward: 0.02559
[2024-05-13 06:58:29] nn step 56000, lr: 0.043047.
	loss_policy_0: 0.13664
	accuracy_policy_0: 0.71984
	loss_value_0: 0.14763
	loss_policy_1: 0.03479
	accuracy_policy_1: 0.65699
	loss_value_1: 0.03119
	loss_reward_1: 0.0047
	loss_policy_2: 0.03888
	accuracy_policy_2: 0.62191
	loss_value_2: 0.03273
	loss_reward_2: 0.00442
	loss_policy_3: 0.04198
	accuracy_policy_3: 0.59957
	loss_value_3: 0.03406
	loss_reward_3: 0.00484
	loss_policy_4: 0.045
	accuracy_policy_4: 0.57248
	loss_value_4: 0.03535
	loss_reward_4: 0.00522
	loss_policy_5: 0.04792
	accuracy_policy_5: 0.55098
	loss_value_5: 0.03651
	loss_reward_5: 0.00615
	loss_policy: 0.34521
	loss_value: 0.31748
	loss_reward: 0.02534
Optimization_Done 56000
[2024-05-13 07:00:12] [command] train weight_iter_56000.pkl 279 281
[2024-05-13 07:01:14] nn step 56100, lr: 0.043047.
	loss_policy_0: 0.15932
	accuracy_policy_0: 0.67758
	loss_value_0: 0.15085
	loss_policy_1: 0.0374
	accuracy_policy_1: 0.63445
	loss_value_1: 0.03181
	loss_reward_1: 0.00464
	loss_policy_2: 0.04143
	accuracy_policy_2: 0.60006
	loss_value_2: 0.03338
	loss_reward_2: 0.00459
	loss_policy_3: 0.04429
	accuracy_policy_3: 0.57512
	loss_value_3: 0.03472
	loss_reward_3: 0.00498
	loss_policy_4: 0.0474
	accuracy_policy_4: 0.5509
	loss_value_4: 0.03596
	loss_reward_4: 0.00541
	loss_policy_5: 0.04985
	accuracy_policy_5: 0.53068
	loss_value_5: 0.03713
	loss_reward_5: 0.00652
	loss_policy: 0.37969
	loss_value: 0.32385
	loss_reward: 0.02614
[2024-05-13 07:02:15] nn step 56200, lr: 0.043047.
	loss_policy_0: 0.13375
	accuracy_policy_0: 0.72881
	loss_value_0: 0.15394
	loss_policy_1: 0.0338
	accuracy_policy_1: 0.66715
	loss_value_1: 0.03256
	loss_reward_1: 0.00478
	loss_policy_2: 0.0379
	accuracy_policy_2: 0.63158
	loss_value_2: 0.03395
	loss_reward_2: 0.00449
	loss_policy_3: 0.0412
	accuracy_policy_3: 0.60549
	loss_value_3: 0.03525
	loss_reward_3: 0.00503
	loss_policy_4: 0.04423
	accuracy_policy_4: 0.58439
	loss_value_4: 0.0364
	loss_reward_4: 0.00549
	loss_policy_5: 0.04712
	accuracy_policy_5: 0.5598
	loss_value_5: 0.03739
	loss_reward_5: 0.00661
	loss_policy: 0.33801
	loss_value: 0.3295
	loss_reward: 0.0264
Optimization_Done 56200
[2024-05-13 07:04:30] [command] train weight_iter_56200.pkl 280 282
[2024-05-13 07:05:32] nn step 56300, lr: 0.043047.
	loss_policy_0: 0.18346
	accuracy_policy_0: 0.66189
	loss_value_0: 0.15877
	loss_policy_1: 0.04282
	accuracy_policy_1: 0.61309
	loss_value_1: 0.03341
	loss_reward_1: 0.00494
	loss_policy_2: 0.04652
	accuracy_policy_2: 0.58301
	loss_value_2: 0.03488
	loss_reward_2: 0.0047
	loss_policy_3: 0.04972
	accuracy_policy_3: 0.56053
	loss_value_3: 0.03619
	loss_reward_3: 0.00512
	loss_policy_4: 0.05247
	accuracy_policy_4: 0.54055
	loss_value_4: 0.03749
	loss_reward_4: 0.00557
	loss_policy_5: 0.05531
	accuracy_policy_5: 0.52367
	loss_value_5: 0.03869
	loss_reward_5: 0.00656
	loss_policy: 0.4303
	loss_value: 0.33942
	loss_reward: 0.0269
[2024-05-13 07:06:35] nn step 56400, lr: 0.043047.
	loss_policy_0: 0.14369
	accuracy_policy_0: 0.71936
	loss_value_0: 0.15488
	loss_policy_1: 0.03557
	accuracy_policy_1: 0.65422
	loss_value_1: 0.03261
	loss_reward_1: 0.00469
	loss_policy_2: 0.03987
	accuracy_policy_2: 0.61795
	loss_value_2: 0.034
	loss_reward_2: 0.0044
	loss_policy_3: 0.04329
	accuracy_policy_3: 0.59426
	loss_value_3: 0.03519
	loss_reward_3: 0.00478
	loss_policy_4: 0.04657
	accuracy_policy_4: 0.57092
	loss_value_4: 0.03646
	loss_reward_4: 0.00523
	loss_policy_5: 0.0495
	accuracy_policy_5: 0.54887
	loss_value_5: 0.03751
	loss_reward_5: 0.00636
	loss_policy: 0.35849
	loss_value: 0.33065
	loss_reward: 0.02547
Optimization_Done 56400
[2024-05-13 07:08:49] [command] train weight_iter_56400.pkl 281 283
[2024-05-13 07:09:53] nn step 56500, lr: 0.043047.
	loss_policy_0: 0.1429
	accuracy_policy_0: 0.68773
	loss_value_0: 0.15195
	loss_policy_1: 0.03474
	accuracy_policy_1: 0.6291
	loss_value_1: 0.0318
	loss_reward_1: 0.00428
	loss_policy_2: 0.03828
	accuracy_policy_2: 0.59996
	loss_value_2: 0.03312
	loss_reward_2: 0.00414
	loss_policy_3: 0.04136
	accuracy_policy_3: 0.57551
	loss_value_3: 0.03403
	loss_reward_3: 0.00441
	loss_policy_4: 0.04407
	accuracy_policy_4: 0.55154
	loss_value_4: 0.03525
	loss_reward_4: 0.00473
	loss_policy_5: 0.04667
	accuracy_policy_5: 0.53365
	loss_value_5: 0.03627
	loss_reward_5: 0.00559
	loss_policy: 0.34802
	loss_value: 0.32241
	loss_reward: 0.02314
[2024-05-13 07:10:53] nn step 56600, lr: 0.043047.
	loss_policy_0: 0.13148
	accuracy_policy_0: 0.72299
	loss_value_0: 0.16012
	loss_policy_1: 0.03342
	accuracy_policy_1: 0.65547
	loss_value_1: 0.03361
	loss_reward_1: 0.0044
	loss_policy_2: 0.03729
	accuracy_policy_2: 0.62701
	loss_value_2: 0.03486
	loss_reward_2: 0.00424
	loss_policy_3: 0.0406
	accuracy_policy_3: 0.59881
	loss_value_3: 0.03597
	loss_reward_3: 0.00442
	loss_policy_4: 0.04334
	accuracy_policy_4: 0.57812
	loss_value_4: 0.03696
	loss_reward_4: 0.00471
	loss_policy_5: 0.04628
	accuracy_policy_5: 0.55576
	loss_value_5: 0.03804
	loss_reward_5: 0.00588
	loss_policy: 0.33241
	loss_value: 0.33956
	loss_reward: 0.02364
Optimization_Done 56600
[2024-05-13 07:13:10] [command] train weight_iter_56600.pkl 282 284
[2024-05-13 07:14:11] nn step 56700, lr: 0.043047.
	loss_policy_0: 0.17073
	accuracy_policy_0: 0.63521
	loss_value_0: 0.14643
	loss_policy_1: 0.03922
	accuracy_policy_1: 0.58643
	loss_value_1: 0.0307
	loss_reward_1: 0.00449
	loss_policy_2: 0.04238
	accuracy_policy_2: 0.55838
	loss_value_2: 0.03198
	loss_reward_2: 0.00431
	loss_policy_3: 0.04445
	accuracy_policy_3: 0.54025
	loss_value_3: 0.03313
	loss_reward_3: 0.00463
	loss_policy_4: 0.04661
	accuracy_policy_4: 0.52479
	loss_value_4: 0.03424
	loss_reward_4: 0.00502
	loss_policy_5: 0.04941
	accuracy_policy_5: 0.49916
	loss_value_5: 0.0353
	loss_reward_5: 0.00582
	loss_policy: 0.39281
	loss_value: 0.31178
	loss_reward: 0.02427
[2024-05-13 07:15:04] nn step 56800, lr: 0.043047.
	loss_policy_0: 0.14265
	accuracy_policy_0: 0.70162
	loss_value_0: 0.15414
	loss_policy_1: 0.03549
	accuracy_policy_1: 0.63654
	loss_value_1: 0.03224
	loss_reward_1: 0.00465
	loss_policy_2: 0.03901
	accuracy_policy_2: 0.60529
	loss_value_2: 0.03354
	loss_reward_2: 0.00438
	loss_policy_3: 0.04191
	accuracy_policy_3: 0.58277
	loss_value_3: 0.03469
	loss_reward_3: 0.00474
	loss_policy_4: 0.04452
	accuracy_policy_4: 0.56537
	loss_value_4: 0.03582
	loss_reward_4: 0.00516
	loss_policy_5: 0.04735
	accuracy_policy_5: 0.54539
	loss_value_5: 0.03684
	loss_reward_5: 0.00631
	loss_policy: 0.35093
	loss_value: 0.32727
	loss_reward: 0.02525
Optimization_Done 56800
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-13 07:20:02] [command] train weight_iter_56800.pkl 283 285
[2024-05-13 07:21:04] nn step 56900, lr: 0.043047.
	loss_policy_0: 0.14482
	accuracy_policy_0: 0.68023
	loss_value_0: 0.15066
	loss_policy_1: 0.03489
	accuracy_policy_1: 0.62371
	loss_value_1: 0.0315
	loss_reward_1: 0.00434
	loss_policy_2: 0.03844
	accuracy_policy_2: 0.59332
	loss_value_2: 0.03277
	loss_reward_2: 0.00443
	loss_policy_3: 0.04146
	accuracy_policy_3: 0.56957
	loss_value_3: 0.03396
	loss_reward_3: 0.0048
	loss_policy_4: 0.0439
	accuracy_policy_4: 0.54932
	loss_value_4: 0.03515
	loss_reward_4: 0.00525
	loss_policy_5: 0.04637
	accuracy_policy_5: 0.52537
	loss_value_5: 0.03617
	loss_reward_5: 0.00595
	loss_policy: 0.34989
	loss_value: 0.32021
	loss_reward: 0.02477
[2024-05-13 07:21:53] nn step 57000, lr: 0.043047.
	loss_policy_0: 0.11973
	accuracy_policy_0: 0.72674
	loss_value_0: 0.14584
	loss_policy_1: 0.03059
	accuracy_policy_1: 0.65934
	loss_value_1: 0.0306
	loss_reward_1: 0.00436
	loss_policy_2: 0.03421
	accuracy_policy_2: 0.62736
	loss_value_2: 0.03186
	loss_reward_2: 0.00419
	loss_policy_3: 0.03696
	accuracy_policy_3: 0.60822
	loss_value_3: 0.03302
	loss_reward_3: 0.00469
	loss_policy_4: 0.03959
	accuracy_policy_4: 0.58754
	loss_value_4: 0.0342
	loss_reward_4: 0.00504
	loss_policy_5: 0.04243
	accuracy_policy_5: 0.56205
	loss_value_5: 0.03517
	loss_reward_5: 0.0059
	loss_policy: 0.30351
	loss_value: 0.31069
	loss_reward: 0.02418
Optimization_Done 57000
[2024-05-13 07:24:15] [command] train weight_iter_57000.pkl 284 286
[2024-05-13 07:25:17] nn step 57100, lr: 0.043047.
	loss_policy_0: 0.16583
	accuracy_policy_0: 0.67594
	loss_value_0: 0.1572
	loss_policy_1: 0.03844
	accuracy_policy_1: 0.62873
	loss_value_1: 0.03325
	loss_reward_1: 0.00458
	loss_policy_2: 0.043
	accuracy_policy_2: 0.5917
	loss_value_2: 0.03483
	loss_reward_2: 0.00447
	loss_policy_3: 0.04589
	accuracy_policy_3: 0.5717
	loss_value_3: 0.0362
	loss_reward_3: 0.00481
	loss_policy_4: 0.04885
	accuracy_policy_4: 0.55311
	loss_value_4: 0.03749
	loss_reward_4: 0.00536
	loss_policy_5: 0.05227
	accuracy_policy_5: 0.52816
	loss_value_5: 0.03858
	loss_reward_5: 0.00628
	loss_policy: 0.39428
	loss_value: 0.33755
	loss_reward: 0.02551
[2024-05-13 07:26:16] nn step 57200, lr: 0.043047.
	loss_policy_0: 0.13905
	accuracy_policy_0: 0.72541
	loss_value_0: 0.15951
	loss_policy_1: 0.035
	accuracy_policy_1: 0.66359
	loss_value_1: 0.03374
	loss_reward_1: 0.00465
	loss_policy_2: 0.03934
	accuracy_policy_2: 0.63066
	loss_value_2: 0.03527
	loss_reward_2: 0.00449
	loss_policy_3: 0.04265
	accuracy_policy_3: 0.60969
	loss_value_3: 0.03676
	loss_reward_3: 0.00479
	loss_policy_4: 0.04564
	accuracy_policy_4: 0.58564
	loss_value_4: 0.03798
	loss_reward_4: 0.00533
	loss_policy_5: 0.04884
	accuracy_policy_5: 0.56182
	loss_value_5: 0.03916
	loss_reward_5: 0.0064
	loss_policy: 0.35054
	loss_value: 0.34242
	loss_reward: 0.02566
Optimization_Done 57200
[2024-05-13 07:28:31] [command] train weight_iter_57200.pkl 285 287
[2024-05-13 07:29:37] nn step 57300, lr: 0.043047.
	loss_policy_0: 0.16474
	accuracy_policy_0: 0.66775
	loss_value_0: 0.15966
	loss_policy_1: 0.03898
	accuracy_policy_1: 0.61885
	loss_value_1: 0.03356
	loss_reward_1: 0.00519
	loss_policy_2: 0.04282
	accuracy_policy_2: 0.59096
	loss_value_2: 0.03506
	loss_reward_2: 0.00491
	loss_policy_3: 0.04574
	accuracy_policy_3: 0.56701
	loss_value_3: 0.03641
	loss_reward_3: 0.00541
	loss_policy_4: 0.04905
	accuracy_policy_4: 0.54318
	loss_value_4: 0.03759
	loss_reward_4: 0.00591
	loss_policy_5: 0.05217
	accuracy_policy_5: 0.51799
	loss_value_5: 0.03865
	loss_reward_5: 0.00693
	loss_policy: 0.39348
	loss_value: 0.34094
	loss_reward: 0.02835
[2024-05-13 07:30:41] nn step 57400, lr: 0.043047.
	loss_policy_0: 0.13279
	accuracy_policy_0: 0.72701
	loss_value_0: 0.15788
	loss_policy_1: 0.03374
	accuracy_policy_1: 0.66297
	loss_value_1: 0.0332
	loss_reward_1: 0.00496
	loss_policy_2: 0.03817
	accuracy_policy_2: 0.6277
	loss_value_2: 0.03454
	loss_reward_2: 0.00476
	loss_policy_3: 0.04172
	accuracy_policy_3: 0.60152
	loss_value_3: 0.03563
	loss_reward_3: 0.00524
	loss_policy_4: 0.04473
	accuracy_policy_4: 0.57623
	loss_value_4: 0.0367
	loss_reward_4: 0.00571
	loss_policy_5: 0.04812
	accuracy_policy_5: 0.55225
	loss_value_5: 0.03781
	loss_reward_5: 0.00679
	loss_policy: 0.33926
	loss_value: 0.33576
	loss_reward: 0.02746
Optimization_Done 57400
[2024-05-13 07:33:01] [command] train weight_iter_57400.pkl 286 288
[2024-05-13 07:33:54] nn step 57500, lr: 0.043047.
	loss_policy_0: 0.1293
	accuracy_policy_0: 0.70717
	loss_value_0: 0.15164
	loss_policy_1: 0.03268
	accuracy_policy_1: 0.64412
	loss_value_1: 0.0317
	loss_reward_1: 0.00478
	loss_policy_2: 0.03661
	accuracy_policy_2: 0.60973
	loss_value_2: 0.03284
	loss_reward_2: 0.00443
	loss_policy_3: 0.03968
	accuracy_policy_3: 0.58434
	loss_value_3: 0.03385
	loss_reward_3: 0.00488
	loss_policy_4: 0.04256
	accuracy_policy_4: 0.55727
	loss_value_4: 0.03479
	loss_reward_4: 0.00518
	loss_policy_5: 0.04555
	accuracy_policy_5: 0.53221
	loss_value_5: 0.03587
	loss_reward_5: 0.00632
	loss_policy: 0.32639
	loss_value: 0.32068
	loss_reward: 0.02559
[2024-05-13 07:34:50] nn step 57600, lr: 0.043047.
	loss_policy_0: 0.11318
	accuracy_policy_0: 0.74398
	loss_value_0: 0.15141
	loss_policy_1: 0.03019
	accuracy_policy_1: 0.66838
	loss_value_1: 0.03164
	loss_reward_1: 0.00473
	loss_policy_2: 0.03405
	accuracy_policy_2: 0.63691
	loss_value_2: 0.0328
	loss_reward_2: 0.00427
	loss_policy_3: 0.03725
	accuracy_policy_3: 0.61115
	loss_value_3: 0.03377
	loss_reward_3: 0.00479
	loss_policy_4: 0.04001
	accuracy_policy_4: 0.58543
	loss_value_4: 0.03483
	loss_reward_4: 0.00496
	loss_policy_5: 0.04327
	accuracy_policy_5: 0.55963
	loss_value_5: 0.03582
	loss_reward_5: 0.00614
	loss_policy: 0.29794
	loss_value: 0.32027
	loss_reward: 0.0249
Optimization_Done 57600
[2024-05-13 07:37:11] [command] train weight_iter_57600.pkl 287 289
[2024-05-13 07:38:14] nn step 57700, lr: 0.043047.
	loss_policy_0: 0.13082
	accuracy_policy_0: 0.67822
	loss_value_0: 0.13862
	loss_policy_1: 0.03182
	accuracy_policy_1: 0.62227
	loss_value_1: 0.02906
	loss_reward_1: 0.00432
	loss_policy_2: 0.03479
	accuracy_policy_2: 0.59314
	loss_value_2: 0.03041
	loss_reward_2: 0.00418
	loss_policy_3: 0.03725
	accuracy_policy_3: 0.57115
	loss_value_3: 0.03148
	loss_reward_3: 0.0045
	loss_policy_4: 0.03992
	accuracy_policy_4: 0.54668
	loss_value_4: 0.03244
	loss_reward_4: 0.00479
	loss_policy_5: 0.04243
	accuracy_policy_5: 0.52066
	loss_value_5: 0.03341
	loss_reward_5: 0.00583
	loss_policy: 0.31704
	loss_value: 0.29543
	loss_reward: 0.02362
[2024-05-13 07:39:22] nn step 57800, lr: 0.043047.
	loss_policy_0: 0.10508
	accuracy_policy_0: 0.7391
	loss_value_0: 0.13495
	loss_policy_1: 0.02805
	accuracy_policy_1: 0.6574
	loss_value_1: 0.02827
	loss_reward_1: 0.00421
	loss_policy_2: 0.03093
	accuracy_policy_2: 0.62908
	loss_value_2: 0.02934
	loss_reward_2: 0.00399
	loss_policy_3: 0.03383
	accuracy_policy_3: 0.60496
	loss_value_3: 0.03045
	loss_reward_3: 0.00428
	loss_policy_4: 0.03621
	accuracy_policy_4: 0.58137
	loss_value_4: 0.03133
	loss_reward_4: 0.00464
	loss_policy_5: 0.03883
	accuracy_policy_5: 0.56033
	loss_value_5: 0.03241
	loss_reward_5: 0.00546
	loss_policy: 0.27293
	loss_value: 0.28675
	loss_reward: 0.02259
Optimization_Done 57800
[2024-05-13 07:41:36] [command] train weight_iter_57800.pkl 288 290
[2024-05-13 07:42:43] nn step 57900, lr: 0.043047.
	loss_policy_0: 0.13357
	accuracy_policy_0: 0.67105
	loss_value_0: 0.13793
	loss_policy_1: 0.03105
	accuracy_policy_1: 0.62186
	loss_value_1: 0.02891
	loss_reward_1: 0.00382
	loss_policy_2: 0.03373
	accuracy_policy_2: 0.59561
	loss_value_2: 0.03009
	loss_reward_2: 0.00376
	loss_policy_3: 0.03585
	accuracy_policy_3: 0.57816
	loss_value_3: 0.03108
	loss_reward_3: 0.0039
	loss_policy_4: 0.03793
	accuracy_policy_4: 0.55994
	loss_value_4: 0.03205
	loss_reward_4: 0.00431
	loss_policy_5: 0.03998
	accuracy_policy_5: 0.53879
	loss_value_5: 0.03304
	loss_reward_5: 0.00512
	loss_policy: 0.31211
	loss_value: 0.29308
	loss_reward: 0.0209
[2024-05-13 07:43:49] nn step 58000, lr: 0.043047.
	loss_policy_0: 0.11016
	accuracy_policy_0: 0.73498
	loss_value_0: 0.14368
	loss_policy_1: 0.02801
	accuracy_policy_1: 0.66891
	loss_value_1: 0.02994
	loss_reward_1: 0.00397
	loss_policy_2: 0.03111
	accuracy_policy_2: 0.64182
	loss_value_2: 0.0311
	loss_reward_2: 0.00385
	loss_policy_3: 0.03347
	accuracy_policy_3: 0.6207
	loss_value_3: 0.03225
	loss_reward_3: 0.00406
	loss_policy_4: 0.03591
	accuracy_policy_4: 0.59809
	loss_value_4: 0.03325
	loss_reward_4: 0.00435
	loss_policy_5: 0.03809
	accuracy_policy_5: 0.57701
	loss_value_5: 0.03419
	loss_reward_5: 0.00527
	loss_policy: 0.27676
	loss_value: 0.3044
	loss_reward: 0.0215
Optimization_Done 58000
[2024-05-13 07:46:07] [command] train weight_iter_58000.pkl 289 291
[2024-05-13 07:47:15] nn step 58100, lr: 0.043047.
	loss_policy_0: 0.16276
	accuracy_policy_0: 0.64625
	loss_value_0: 0.15465
	loss_policy_1: 0.03765
	accuracy_policy_1: 0.59553
	loss_value_1: 0.03238
	loss_reward_1: 0.00479
	loss_policy_2: 0.04061
	accuracy_policy_2: 0.5724
	loss_value_2: 0.03372
	loss_reward_2: 0.00462
	loss_policy_3: 0.04326
	accuracy_policy_3: 0.54787
	loss_value_3: 0.03507
	loss_reward_3: 0.00503
	loss_policy_4: 0.04579
	accuracy_policy_4: 0.53215
	loss_value_4: 0.03615
	loss_reward_4: 0.0055
	loss_policy_5: 0.04809
	accuracy_policy_5: 0.51334
	loss_value_5: 0.03737
	loss_reward_5: 0.00629
	loss_policy: 0.37815
	loss_value: 0.32933
	loss_reward: 0.02623
[2024-05-13 07:48:19] nn step 58200, lr: 0.043047.
	loss_policy_0: 0.13843
	accuracy_policy_0: 0.7057
	loss_value_0: 0.15721
	loss_policy_1: 0.03419
	accuracy_policy_1: 0.64586
	loss_value_1: 0.03284
	loss_reward_1: 0.00493
	loss_policy_2: 0.03795
	accuracy_policy_2: 0.61158
	loss_value_2: 0.034
	loss_reward_2: 0.00464
	loss_policy_3: 0.04071
	accuracy_policy_3: 0.59055
	loss_value_3: 0.03513
	loss_reward_3: 0.00502
	loss_policy_4: 0.04352
	accuracy_policy_4: 0.56371
	loss_value_4: 0.03614
	loss_reward_4: 0.0055
	loss_policy_5: 0.04649
	accuracy_policy_5: 0.54434
	loss_value_5: 0.03726
	loss_reward_5: 0.00643
	loss_policy: 0.34128
	loss_value: 0.33258
	loss_reward: 0.02652
Optimization_Done 58200
[2024-05-13 07:50:30] [command] train weight_iter_58200.pkl 290 292
[2024-05-13 07:51:18] nn step 58300, lr: 0.043047.
	loss_policy_0: 0.17669
	accuracy_policy_0: 0.67074
	loss_value_0: 0.18275
	loss_policy_1: 0.04315
	accuracy_policy_1: 0.61285
	loss_value_1: 0.03807
	loss_reward_1: 0.00645
	loss_policy_2: 0.04765
	accuracy_policy_2: 0.58135
	loss_value_2: 0.03937
	loss_reward_2: 0.00591
	loss_policy_3: 0.05123
	accuracy_policy_3: 0.55334
	loss_value_3: 0.04077
	loss_reward_3: 0.00652
	loss_policy_4: 0.05424
	accuracy_policy_4: 0.53576
	loss_value_4: 0.04223
	loss_reward_4: 0.00703
	loss_policy_5: 0.05785
	accuracy_policy_5: 0.50643
	loss_value_5: 0.0435
	loss_reward_5: 0.0083
	loss_policy: 0.43081
	loss_value: 0.38669
	loss_reward: 0.03422
[2024-05-13 07:52:02] nn step 58400, lr: 0.043047.
	loss_policy_0: 0.14779
	accuracy_policy_0: 0.71617
	loss_value_0: 0.17252
	loss_policy_1: 0.03878
	accuracy_policy_1: 0.63805
	loss_value_1: 0.0362
	loss_reward_1: 0.00603
	loss_policy_2: 0.04313
	accuracy_policy_2: 0.60621
	loss_value_2: 0.03753
	loss_reward_2: 0.00589
	loss_policy_3: 0.04649
	accuracy_policy_3: 0.58164
	loss_value_3: 0.03887
	loss_reward_3: 0.00621
	loss_policy_4: 0.05
	accuracy_policy_4: 0.55803
	loss_value_4: 0.04007
	loss_reward_4: 0.00667
	loss_policy_5: 0.05337
	accuracy_policy_5: 0.52773
	loss_value_5: 0.04126
	loss_reward_5: 0.00791
	loss_policy: 0.37954
	loss_value: 0.36645
	loss_reward: 0.03271
Optimization_Done 58400
[2024-05-13 07:53:57] [command] train weight_iter_58400.pkl 291 293
[2024-05-13 07:55:01] nn step 58500, lr: 0.043047.
	loss_policy_0: 0.18441
	accuracy_policy_0: 0.65822
	loss_value_0: 0.17943
	loss_policy_1: 0.04482
	accuracy_policy_1: 0.59219
	loss_value_1: 0.03762
	loss_reward_1: 0.00592
	loss_policy_2: 0.04877
	accuracy_policy_2: 0.56385
	loss_value_2: 0.03909
	loss_reward_2: 0.00576
	loss_policy_3: 0.05163
	accuracy_policy_3: 0.54066
	loss_value_3: 0.04032
	loss_reward_3: 0.00604
	loss_policy_4: 0.0547
	accuracy_policy_4: 0.51574
	loss_value_4: 0.04132
	loss_reward_4: 0.00644
	loss_policy_5: 0.05786
	accuracy_policy_5: 0.48875
	loss_value_5: 0.0425
	loss_reward_5: 0.00768
	loss_policy: 0.4422
	loss_value: 0.38028
	loss_reward: 0.03183
[2024-05-13 07:56:08] nn step 58600, lr: 0.043047.
	loss_policy_0: 0.15076
	accuracy_policy_0: 0.71182
	loss_value_0: 0.17203
	loss_policy_1: 0.039
	accuracy_policy_1: 0.63723
	loss_value_1: 0.03605
	loss_reward_1: 0.00595
	loss_policy_2: 0.04344
	accuracy_policy_2: 0.60281
	loss_value_2: 0.03737
	loss_reward_2: 0.00568
	loss_policy_3: 0.0468
	accuracy_policy_3: 0.57781
	loss_value_3: 0.03858
	loss_reward_3: 0.00609
	loss_policy_4: 0.05035
	accuracy_policy_4: 0.55051
	loss_value_4: 0.03979
	loss_reward_4: 0.00639
	loss_policy_5: 0.05367
	accuracy_policy_5: 0.52131
	loss_value_5: 0.04086
	loss_reward_5: 0.00765
	loss_policy: 0.38401
	loss_value: 0.36468
	loss_reward: 0.03174
Optimization_Done 58600
[2024-05-13 07:58:26] [command] train weight_iter_58600.pkl 292 294
[2024-05-13 07:59:26] nn step 58700, lr: 0.043047.
	loss_policy_0: 0.17889
	accuracy_policy_0: 0.65404
	loss_value_0: 0.16438
	loss_policy_1: 0.0434
	accuracy_policy_1: 0.59355
	loss_value_1: 0.03435
	loss_reward_1: 0.00531
	loss_policy_2: 0.0471
	accuracy_policy_2: 0.56437
	loss_value_2: 0.03579
	loss_reward_2: 0.00518
	loss_policy_3: 0.05054
	accuracy_policy_3: 0.53992
	loss_value_3: 0.03708
	loss_reward_3: 0.00577
	loss_policy_4: 0.05378
	accuracy_policy_4: 0.514
	loss_value_4: 0.03843
	loss_reward_4: 0.00615
	loss_policy_5: 0.05752
	accuracy_policy_5: 0.48646
	loss_value_5: 0.03962
	loss_reward_5: 0.00734
	loss_policy: 0.43123
	loss_value: 0.34965
	loss_reward: 0.02974
[2024-05-13 08:00:20] nn step 58800, lr: 0.043047.
	loss_policy_0: 0.14657
	accuracy_policy_0: 0.71246
	loss_value_0: 0.16182
	loss_policy_1: 0.03829
	accuracy_policy_1: 0.63344
	loss_value_1: 0.0341
	loss_reward_1: 0.00524
	loss_policy_2: 0.04251
	accuracy_policy_2: 0.60197
	loss_value_2: 0.03548
	loss_reward_2: 0.00502
	loss_policy_3: 0.0459
	accuracy_policy_3: 0.57338
	loss_value_3: 0.03692
	loss_reward_3: 0.00553
	loss_policy_4: 0.04952
	accuracy_policy_4: 0.54631
	loss_value_4: 0.03806
	loss_reward_4: 0.00589
	loss_policy_5: 0.05303
	accuracy_policy_5: 0.51959
	loss_value_5: 0.03933
	loss_reward_5: 0.00705
	loss_policy: 0.37582
	loss_value: 0.34571
	loss_reward: 0.02874
Optimization_Done 58800
[2024-05-13 08:02:29] [command] train weight_iter_58800.pkl 293 295
[2024-05-13 08:03:21] nn step 58900, lr: 0.043047.
	loss_policy_0: 0.1596
	accuracy_policy_0: 0.6792
	loss_value_0: 0.16345
	loss_policy_1: 0.03777
	accuracy_policy_1: 0.62768
	loss_value_1: 0.03404
	loss_reward_1: 0.00451
	loss_policy_2: 0.04079
	accuracy_policy_2: 0.59881
	loss_value_2: 0.03544
	loss_reward_2: 0.00433
	loss_policy_3: 0.04397
	accuracy_policy_3: 0.57525
	loss_value_3: 0.03676
	loss_reward_3: 0.00478
	loss_policy_4: 0.04668
	accuracy_policy_4: 0.55676
	loss_value_4: 0.03816
	loss_reward_4: 0.00527
	loss_policy_5: 0.04978
	accuracy_policy_5: 0.52961
	loss_value_5: 0.03933
	loss_reward_5: 0.00614
	loss_policy: 0.37859
	loss_value: 0.34718
	loss_reward: 0.02502
[2024-05-13 08:04:13] nn step 59000, lr: 0.043047.
	loss_policy_0: 0.13441
	accuracy_policy_0: 0.72811
	loss_value_0: 0.15913
	loss_policy_1: 0.03407
	accuracy_policy_1: 0.66082
	loss_value_1: 0.03352
	loss_reward_1: 0.00456
	loss_policy_2: 0.038
	accuracy_policy_2: 0.62871
	loss_value_2: 0.03478
	loss_reward_2: 0.00438
	loss_policy_3: 0.04086
	accuracy_policy_3: 0.60732
	loss_value_3: 0.03606
	loss_reward_3: 0.00472
	loss_policy_4: 0.0437
	accuracy_policy_4: 0.58447
	loss_value_4: 0.03737
	loss_reward_4: 0.00513
	loss_policy_5: 0.04699
	accuracy_policy_5: 0.56027
	loss_value_5: 0.0383
	loss_reward_5: 0.00598
	loss_policy: 0.33803
	loss_value: 0.33917
	loss_reward: 0.02477
Optimization_Done 59000
[2024-05-13 08:06:32] [command] train weight_iter_59000.pkl 294 296
[2024-05-13 08:07:26] nn step 59100, lr: 0.043047.
	loss_policy_0: 0.15062
	accuracy_policy_0: 0.68994
	loss_value_0: 0.14982
	loss_policy_1: 0.03604
	accuracy_policy_1: 0.63338
	loss_value_1: 0.03152
	loss_reward_1: 0.00539
	loss_policy_2: 0.03892
	accuracy_policy_2: 0.6101
	loss_value_2: 0.03281
	loss_reward_2: 0.00511
	loss_policy_3: 0.04187
	accuracy_policy_3: 0.58727
	loss_value_3: 0.03392
	loss_reward_3: 0.00559
	loss_policy_4: 0.04452
	accuracy_policy_4: 0.56432
	loss_value_4: 0.03524
	loss_reward_4: 0.00599
	loss_policy_5: 0.04765
	accuracy_policy_5: 0.54012
	loss_value_5: 0.03651
	loss_reward_5: 0.00697
	loss_policy: 0.35961
	loss_value: 0.31983
	loss_reward: 0.02905
[2024-05-13 08:08:18] nn step 59200, lr: 0.043047.
	loss_policy_0: 0.13481
	accuracy_policy_0: 0.73092
	loss_value_0: 0.15533
	loss_policy_1: 0.03439
	accuracy_policy_1: 0.66754
	loss_value_1: 0.0327
	loss_reward_1: 0.00576
	loss_policy_2: 0.03794
	accuracy_policy_2: 0.6393
	loss_value_2: 0.03391
	loss_reward_2: 0.00535
	loss_policy_3: 0.04065
	accuracy_policy_3: 0.62236
	loss_value_3: 0.03518
	loss_reward_3: 0.00583
	loss_policy_4: 0.044
	accuracy_policy_4: 0.59389
	loss_value_4: 0.03647
	loss_reward_4: 0.00613
	loss_policy_5: 0.04737
	accuracy_policy_5: 0.56602
	loss_value_5: 0.03749
	loss_reward_5: 0.00747
	loss_policy: 0.33916
	loss_value: 0.33108
	loss_reward: 0.03053
Optimization_Done 59200
[2024-05-13 08:10:14] [command] train weight_iter_59200.pkl 295 297
[2024-05-13 08:11:06] nn step 59300, lr: 0.043047.
	loss_policy_0: 0.12191
	accuracy_policy_0: 0.71049
	loss_value_0: 0.14078
	loss_policy_1: 0.03036
	accuracy_policy_1: 0.65029
	loss_value_1: 0.02939
	loss_reward_1: 0.00474
	loss_policy_2: 0.03314
	accuracy_policy_2: 0.62449
	loss_value_2: 0.03054
	loss_reward_2: 0.00448
	loss_policy_3: 0.03526
	accuracy_policy_3: 0.60168
	loss_value_3: 0.03164
	loss_reward_3: 0.00501
	loss_policy_4: 0.03748
	accuracy_policy_4: 0.58486
	loss_value_4: 0.03262
	loss_reward_4: 0.00524
	loss_policy_5: 0.04012
	accuracy_policy_5: 0.56078
	loss_value_5: 0.03365
	loss_reward_5: 0.00617
	loss_policy: 0.29827
	loss_value: 0.29863
	loss_reward: 0.02564
[2024-05-13 08:11:57] nn step 59400, lr: 0.043047.
	loss_policy_0: 0.10564
	accuracy_policy_0: 0.75217
	loss_value_0: 0.14227
	loss_policy_1: 0.0279
	accuracy_policy_1: 0.68492
	loss_value_1: 0.02963
	loss_reward_1: 0.00479
	loss_policy_2: 0.03114
	accuracy_policy_2: 0.65316
	loss_value_2: 0.03089
	loss_reward_2: 0.0045
	loss_policy_3: 0.03333
	accuracy_policy_3: 0.63555
	loss_value_3: 0.03193
	loss_reward_3: 0.00485
	loss_policy_4: 0.03584
	accuracy_policy_4: 0.61318
	loss_value_4: 0.03292
	loss_reward_4: 0.00515
	loss_policy_5: 0.03865
	accuracy_policy_5: 0.58504
	loss_value_5: 0.03405
	loss_reward_5: 0.00634
	loss_policy: 0.27249
	loss_value: 0.3017
	loss_reward: 0.02563
Optimization_Done 59400
[2024-05-13 08:14:20] [command] train weight_iter_59400.pkl 296 298
[2024-05-13 08:15:10] nn step 59500, lr: 0.043047.
	loss_policy_0: 0.14618
	accuracy_policy_0: 0.66975
	loss_value_0: 0.13835
	loss_policy_1: 0.03393
	accuracy_policy_1: 0.62496
	loss_value_1: 0.0289
	loss_reward_1: 0.00466
	loss_policy_2: 0.03632
	accuracy_policy_2: 0.59938
	loss_value_2: 0.03018
	loss_reward_2: 0.00451
	loss_policy_3: 0.03821
	accuracy_policy_3: 0.5865
	loss_value_3: 0.03121
	loss_reward_3: 0.00486
	loss_policy_4: 0.04036
	accuracy_policy_4: 0.56385
	loss_value_4: 0.03212
	loss_reward_4: 0.00544
	loss_policy_5: 0.04269
	accuracy_policy_5: 0.54258
	loss_value_5: 0.03318
	loss_reward_5: 0.00623
	loss_policy: 0.33768
	loss_value: 0.29394
	loss_reward: 0.0257
[2024-05-13 08:16:00] nn step 59600, lr: 0.043047.
	loss_policy_0: 0.11745
	accuracy_policy_0: 0.72586
	loss_value_0: 0.13625
	loss_policy_1: 0.02967
	accuracy_policy_1: 0.66096
	loss_value_1: 0.02861
	loss_reward_1: 0.0046
	loss_policy_2: 0.0325
	accuracy_policy_2: 0.63488
	loss_value_2: 0.02965
	loss_reward_2: 0.00445
	loss_policy_3: 0.03468
	accuracy_policy_3: 0.61744
	loss_value_3: 0.03069
	loss_reward_3: 0.00477
	loss_policy_4: 0.03696
	accuracy_policy_4: 0.59762
	loss_value_4: 0.03178
	loss_reward_4: 0.00516
	loss_policy_5: 0.03926
	accuracy_policy_5: 0.5742
	loss_value_5: 0.03283
	loss_reward_5: 0.00624
	loss_policy: 0.29051
	loss_value: 0.28981
	loss_reward: 0.02522
Optimization_Done 59600
[2024-05-13 08:18:17] [command] train weight_iter_59600.pkl 297 299
[2024-05-13 08:19:09] nn step 59700, lr: 0.043047.
	loss_policy_0: 0.16694
	accuracy_policy_0: 0.65
	loss_value_0: 0.14997
	loss_policy_1: 0.0383
	accuracy_policy_1: 0.60268
	loss_value_1: 0.03143
	loss_reward_1: 0.00425
	loss_policy_2: 0.04132
	accuracy_policy_2: 0.57619
	loss_value_2: 0.03277
	loss_reward_2: 0.00434
	loss_policy_3: 0.04375
	accuracy_policy_3: 0.55697
	loss_value_3: 0.03403
	loss_reward_3: 0.00463
	loss_policy_4: 0.04628
	accuracy_policy_4: 0.54219
	loss_value_4: 0.0352
	loss_reward_4: 0.00494
	loss_policy_5: 0.04843
	accuracy_policy_5: 0.52186
	loss_value_5: 0.03641
	loss_reward_5: 0.006
	loss_policy: 0.38501
	loss_value: 0.31982
	loss_reward: 0.02417
[2024-05-13 08:19:58] nn step 59800, lr: 0.043047.
	loss_policy_0: 0.13661
	accuracy_policy_0: 0.70912
	loss_value_0: 0.1481
	loss_policy_1: 0.03402
	accuracy_policy_1: 0.6434
	loss_value_1: 0.03132
	loss_reward_1: 0.00434
	loss_policy_2: 0.03717
	accuracy_policy_2: 0.61355
	loss_value_2: 0.03258
	loss_reward_2: 0.00434
	loss_policy_3: 0.04011
	accuracy_policy_3: 0.59205
	loss_value_3: 0.03378
	loss_reward_3: 0.00462
	loss_policy_4: 0.04292
	accuracy_policy_4: 0.57104
	loss_value_4: 0.03494
	loss_reward_4: 0.00503
	loss_policy_5: 0.04545
	accuracy_policy_5: 0.54967
	loss_value_5: 0.03603
	loss_reward_5: 0.00609
	loss_policy: 0.33629
	loss_value: 0.31675
	loss_reward: 0.02443
Optimization_Done 59800
[2024-05-13 08:22:17] [command] train weight_iter_59800.pkl 298 300
[2024-05-13 08:23:09] nn step 59900, lr: 0.043047.
	loss_policy_0: 0.19898
	accuracy_policy_0: 0.63059
	loss_value_0: 0.16349
	loss_policy_1: 0.04487
	accuracy_policy_1: 0.59514
	loss_value_1: 0.03428
	loss_reward_1: 0.00536
	loss_policy_2: 0.04815
	accuracy_policy_2: 0.57006
	loss_value_2: 0.03577
	loss_reward_2: 0.00523
	loss_policy_3: 0.05085
	accuracy_policy_3: 0.55371
	loss_value_3: 0.03719
	loss_reward_3: 0.0057
	loss_policy_4: 0.05359
	accuracy_policy_4: 0.53686
	loss_value_4: 0.03852
	loss_reward_4: 0.00607
	loss_policy_5: 0.05581
	accuracy_policy_5: 0.51953
	loss_value_5: 0.03984
	loss_reward_5: 0.00711
	loss_policy: 0.45225
	loss_value: 0.34908
	loss_reward: 0.02948
[2024-05-13 08:24:01] nn step 60000, lr: 0.043047.
	loss_policy_0: 0.16797
	accuracy_policy_0: 0.69346
	loss_value_0: 0.15986
	loss_policy_1: 0.04042
	accuracy_policy_1: 0.6359
	loss_value_1: 0.0336
	loss_reward_1: 0.00543
	loss_policy_2: 0.04357
	accuracy_policy_2: 0.61406
	loss_value_2: 0.03495
	loss_reward_2: 0.00514
	loss_policy_3: 0.04692
	accuracy_policy_3: 0.58912
	loss_value_3: 0.0363
	loss_reward_3: 0.00549
	loss_policy_4: 0.05004
	accuracy_policy_4: 0.57109
	loss_value_4: 0.03768
	loss_reward_4: 0.00609
	loss_policy_5: 0.05254
	accuracy_policy_5: 0.55352
	loss_value_5: 0.03914
	loss_reward_5: 0.00709
	loss_policy: 0.40146
	loss_value: 0.34152
	loss_reward: 0.02924
Optimization_Done 60000
