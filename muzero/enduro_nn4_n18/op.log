A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-06 15:12:20] [command] train weight_iter_0.pkl 1 1
[2024-05-06 15:13:01] nn step 100, lr: 0.005.
	loss_policy_0: 0.06269
	accuracy_policy_0: 0.87547
	loss_value_0: 0.3833
	loss_policy_1: 0.01556
	accuracy_policy_1: 0.86791
	loss_value_1: 0.06925
	loss_reward_1: 0.03209
	loss_policy_2: 0.01539
	accuracy_policy_2: 0.87309
	loss_value_2: 0.06875
	loss_reward_2: 0.03169
	loss_policy_3: 0.01559
	accuracy_policy_3: 0.86217
	loss_value_3: 0.06849
	loss_reward_3: 0.03169
	loss_policy_4: 0.01529
	accuracy_policy_4: 0.87613
	loss_value_4: 0.06827
	loss_reward_4: 0.03161
	loss_policy_5: 0.01506
	accuracy_policy_5: 0.88908
	loss_value_5: 0.06796
	loss_reward_5: 0.03161
	loss_policy: 0.13959
	loss_value: 0.72602
	loss_reward: 0.15869
[2024-05-06 15:13:38] nn step 200, lr: 0.005.
	loss_policy_0: 0.01381
	accuracy_policy_0: 0.9125
	loss_value_0: 0.17311
	loss_policy_1: 0.00695
	accuracy_policy_1: 0.90496
	loss_value_1: 0.03506
	loss_reward_1: 1e-05
	loss_policy_2: 0.00785
	accuracy_policy_2: 0.90801
	loss_value_2: 0.03504
	loss_reward_2: 0.0
	loss_policy_3: 0.00793
	accuracy_policy_3: 0.89832
	loss_value_3: 0.03494
	loss_reward_3: 0.0
	loss_policy_4: 0.0079
	accuracy_policy_4: 0.91432
	loss_value_4: 0.03528
	loss_reward_4: 0.0
	loss_policy_5: 0.00765
	accuracy_policy_5: 0.915
	loss_value_5: 0.03538
	loss_reward_5: 0.0
	loss_policy: 0.05208
	loss_value: 0.34881
	loss_reward: 3e-05
Optimization_Done 200
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-06 15:18:01] [command] train weight_iter_200.pkl 1 2
[2024-05-06 15:18:43] nn step 300, lr: 0.01.
	loss_policy_0: 0.09146
	accuracy_policy_0: 0.78777
	loss_value_0: 0.16455
	loss_policy_1: 0.02343
	accuracy_policy_1: 0.76035
	loss_value_1: 0.03286
	loss_reward_1: 8e-05
	loss_policy_2: 0.02399
	accuracy_policy_2: 0.78176
	loss_value_2: 0.03272
	loss_reward_2: 2e-05
	loss_policy_3: 0.02381
	accuracy_policy_3: 0.76885
	loss_value_3: 0.03266
	loss_reward_3: 3e-05
	loss_policy_4: 0.02357
	accuracy_policy_4: 0.785
	loss_value_4: 0.03256
	loss_reward_4: 3e-05
	loss_policy_5: 0.023
	accuracy_policy_5: 0.79109
	loss_value_5: 0.03252
	loss_reward_5: 3e-05
	loss_policy: 0.20927
	loss_value: 0.32787
	loss_reward: 0.00019
[2024-05-06 15:19:19] nn step 400, lr: 0.01.
	loss_policy_0: 0.04335
	accuracy_policy_0: 0.86061
	loss_value_0: 0.15226
	loss_policy_1: 0.01357
	accuracy_policy_1: 0.83062
	loss_value_1: 0.03037
	loss_reward_1: 2e-05
	loss_policy_2: 0.01513
	accuracy_policy_2: 0.8427
	loss_value_2: 0.03025
	loss_reward_2: 1e-05
	loss_policy_3: 0.01455
	accuracy_policy_3: 0.84426
	loss_value_3: 0.03016
	loss_reward_3: 1e-05
	loss_policy_4: 0.01442
	accuracy_policy_4: 0.85682
	loss_value_4: 0.03007
	loss_reward_4: 1e-05
	loss_policy_5: 0.01427
	accuracy_policy_5: 0.86436
	loss_value_5: 0.03
	loss_reward_5: 1e-05
	loss_policy: 0.1153
	loss_value: 0.30311
	loss_reward: 6e-05
Optimization_Done 400
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-06 15:22:24] [command] train weight_iter_400.pkl 1 3
[2024-05-06 15:23:06] nn step 500, lr: 0.02.
	loss_policy_0: 0.10435
	accuracy_policy_0: 0.69273
	loss_value_0: 0.15848
	loss_policy_1: 0.02459
	accuracy_policy_1: 0.66035
	loss_value_1: 0.03167
	loss_reward_1: 1e-05
	loss_policy_2: 0.02569
	accuracy_policy_2: 0.67158
	loss_value_2: 0.0315
	loss_reward_2: 1e-05
	loss_policy_3: 0.02551
	accuracy_policy_3: 0.66293
	loss_value_3: 0.03141
	loss_reward_3: 1e-05
	loss_policy_4: 0.02546
	accuracy_policy_4: 0.67984
	loss_value_4: 0.0313
	loss_reward_4: 1e-05
	loss_policy_5: 0.02496
	accuracy_policy_5: 0.69023
	loss_value_5: 0.03127
	loss_reward_5: 1e-05
	loss_policy: 0.23056
	loss_value: 0.31563
	loss_reward: 4e-05
[2024-05-06 15:23:41] nn step 600, lr: 0.02.
	loss_policy_0: 0.08958
	accuracy_policy_0: 0.71076
	loss_value_0: 0.15124
	loss_policy_1: 0.02135
	accuracy_policy_1: 0.68186
	loss_value_1: 0.03022
	loss_reward_1: 1e-05
	loss_policy_2: 0.02243
	accuracy_policy_2: 0.691
	loss_value_2: 0.03011
	loss_reward_2: 1e-05
	loss_policy_3: 0.02279
	accuracy_policy_3: 0.68555
	loss_value_3: 0.03001
	loss_reward_3: 0.0
	loss_policy_4: 0.02272
	accuracy_policy_4: 0.70104
	loss_value_4: 0.02994
	loss_reward_4: 1e-05
	loss_policy_5: 0.02239
	accuracy_policy_5: 0.70814
	loss_value_5: 0.02987
	loss_reward_5: 1e-05
	loss_policy: 0.20127
	loss_value: 0.30141
	loss_reward: 3e-05
Optimization_Done 600
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-06 15:27:15] [command] train weight_iter_600.pkl 1 4
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-06 22:29:47] [command] train weight_iter_600.pkl 1 4
[2024-05-06 22:30:33] nn step 700, lr: 0.08.
	loss_policy_0: 0.14058
	accuracy_policy_0: 0.58547
	loss_value_0: 0.28076
	loss_policy_1: 0.03064
	accuracy_policy_1: 0.53588
	loss_value_1: 0.0572
	loss_reward_1: 0.0001
	loss_policy_2: 0.0309
	accuracy_policy_2: 0.55559
	loss_value_2: 0.05693
	loss_reward_2: 6e-05
	loss_policy_3: 0.03029
	accuracy_policy_3: 0.54395
	loss_value_3: 0.05661
	loss_reward_3: 8e-05
	loss_policy_4: 0.03047
	accuracy_policy_4: 0.5501
	loss_value_4: 0.05661
	loss_reward_4: 8e-05
	loss_policy_5: 0.03036
	accuracy_policy_5: 0.55662
	loss_value_5: 0.05652
	loss_reward_5: 8e-05
	loss_policy: 0.29323
	loss_value: 0.56463
	loss_reward: 0.0004
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-06 23:20:15] [command] train weight_iter_600.pkl 1 4
[2024-05-06 23:21:01] nn step 700, lr: 0.08.
	loss_policy_0: 0.10922
	accuracy_policy_0: 0.65305
	loss_value_0: 0.19226
	loss_policy_1: 0.02594
	accuracy_policy_1: 0.61035
	loss_value_1: 0.0379
	loss_reward_1: 2e-05
	loss_policy_2: 0.02675
	accuracy_policy_2: 0.61562
	loss_value_2: 0.03777
	loss_reward_2: 2e-05
	loss_policy_3: 0.02634
	accuracy_policy_3: 0.60701
	loss_value_3: 0.03754
	loss_reward_3: 3e-05
	loss_policy_4: 0.02642
	accuracy_policy_4: 0.61699
	loss_value_4: 0.03751
	loss_reward_4: 3e-05
	loss_policy_5: 0.02624
	accuracy_policy_5: 0.62502
	loss_value_5: 0.03743
	loss_reward_5: 3e-05
	loss_policy: 0.24092
	loss_value: 0.38041
	loss_reward: 0.00012
[2024-05-06 23:21:36] nn step 800, lr: 0.08.
	loss_policy_0: 0.10167
	accuracy_policy_0: 0.66514
	loss_value_0: 0.14593
	loss_policy_1: 0.0243
	accuracy_policy_1: 0.62328
	loss_value_1: 0.02908
	loss_reward_1: 0.0
	loss_policy_2: 0.0258
	accuracy_policy_2: 0.6208
	loss_value_2: 0.02899
	loss_reward_2: 0.0
	loss_policy_3: 0.02539
	accuracy_policy_3: 0.61723
	loss_value_3: 0.02891
	loss_reward_3: 0.0
	loss_policy_4: 0.02537
	accuracy_policy_4: 0.62586
	loss_value_4: 0.02882
	loss_reward_4: 0.0
	loss_policy_5: 0.02517
	accuracy_policy_5: 0.63461
	loss_value_5: 0.02876
	loss_reward_5: 0.0
	loss_policy: 0.2277
	loss_value: 0.29049
	loss_reward: 1e-05
Optimization_Done 800
[2024-05-06 23:23:52] [command] train weight_iter_800.pkl 1 5
[2024-05-06 23:24:28] nn step 900, lr: 0.08.
	loss_policy_0: 0.08063
	accuracy_policy_0: 0.75602
	loss_value_0: 0.14853
	loss_policy_1: 0.01841
	accuracy_policy_1: 0.73531
	loss_value_1: 0.02925
	loss_reward_1: 0.00288
	loss_policy_2: 0.0195
	accuracy_policy_2: 0.72748
	loss_value_2: 0.02836
	loss_reward_2: 0.00284
	loss_policy_3: 0.01941
	accuracy_policy_3: 0.73021
	loss_value_3: 0.02752
	loss_reward_3: 0.0029
	loss_policy_4: 0.01928
	accuracy_policy_4: 0.73857
	loss_value_4: 0.02662
	loss_reward_4: 0.0029
	loss_policy_5: 0.01924
	accuracy_policy_5: 0.74316
	loss_value_5: 0.02562
	loss_reward_5: 0.00298
	loss_policy: 0.17648
	loss_value: 0.2859
	loss_reward: 0.0145
[2024-05-06 23:25:03] nn step 1000, lr: 0.08.
	loss_policy_0: 0.06715
	accuracy_policy_0: 0.78041
	loss_value_0: 0.12951
	loss_policy_1: 0.01557
	accuracy_policy_1: 0.75855
	loss_value_1: 0.02568
	loss_reward_1: 0.0019
	loss_policy_2: 0.01625
	accuracy_policy_2: 0.75689
	loss_value_2: 0.02521
	loss_reward_2: 0.00188
	loss_policy_3: 0.01641
	accuracy_policy_3: 0.75443
	loss_value_3: 0.02473
	loss_reward_3: 0.00189
	loss_policy_4: 0.01649
	accuracy_policy_4: 0.75738
	loss_value_4: 0.02424
	loss_reward_4: 0.00184
	loss_policy_5: 0.01635
	accuracy_policy_5: 0.7642
	loss_value_5: 0.02375
	loss_reward_5: 0.00189
	loss_policy: 0.14823
	loss_value: 0.25311
	loss_reward: 0.00939
Optimization_Done 1000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-06 23:30:28] [command] train weight_iter_1000.pkl 2 6
[2024-05-06 23:31:19] nn step 1100, lr: 0.1.
	loss_policy_0: 0.0605
	accuracy_policy_0: 0.81484
	loss_value_0: 0.1393
	loss_policy_1: 0.0151
	accuracy_policy_1: 0.7773
	loss_value_1: 0.02766
	loss_reward_1: 0.00178
	loss_policy_2: 0.01598
	accuracy_policy_2: 0.77053
	loss_value_2: 0.02725
	loss_reward_2: 0.0017
	loss_policy_3: 0.01651
	accuracy_policy_3: 0.7566
	loss_value_3: 0.02684
	loss_reward_3: 0.00173
	loss_policy_4: 0.01729
	accuracy_policy_4: 0.746
	loss_value_4: 0.02635
	loss_reward_4: 0.00171
	loss_policy_5: 0.01696
	accuracy_policy_5: 0.75084
	loss_value_5: 0.02587
	loss_reward_5: 0.00171
	loss_policy: 0.14234
	loss_value: 0.27327
	loss_reward: 0.00863
[2024-05-06 23:31:54] nn step 1200, lr: 0.1.
	loss_policy_0: 0.0544
	accuracy_policy_0: 0.82146
	loss_value_0: 0.13378
	loss_policy_1: 0.01381
	accuracy_policy_1: 0.78281
	loss_value_1: 0.02663
	loss_reward_1: 0.00152
	loss_policy_2: 0.0147
	accuracy_policy_2: 0.77457
	loss_value_2: 0.02633
	loss_reward_2: 0.00145
	loss_policy_3: 0.01516
	accuracy_policy_3: 0.75689
	loss_value_3: 0.02602
	loss_reward_3: 0.00148
	loss_policy_4: 0.01579
	accuracy_policy_4: 0.75377
	loss_value_4: 0.02567
	loss_reward_4: 0.0015
	loss_policy_5: 0.01581
	accuracy_policy_5: 0.75277
	loss_value_5: 0.02526
	loss_reward_5: 0.00148
	loss_policy: 0.12966
	loss_value: 0.2637
	loss_reward: 0.00744
Optimization_Done 1200
[2024-05-06 23:34:12] [command] train weight_iter_1200.pkl 3 7
[2024-05-06 23:34:48] nn step 1300, lr: 0.1.
	loss_policy_0: 0.07079
	accuracy_policy_0: 0.74377
	loss_value_0: 0.13192
	loss_policy_1: 0.01736
	accuracy_policy_1: 0.70107
	loss_value_1: 0.02622
	loss_reward_1: 0.00178
	loss_policy_2: 0.01828
	accuracy_policy_2: 0.69154
	loss_value_2: 0.02588
	loss_reward_2: 0.00171
	loss_policy_3: 0.01889
	accuracy_policy_3: 0.68773
	loss_value_3: 0.02554
	loss_reward_3: 0.00165
	loss_policy_4: 0.01975
	accuracy_policy_4: 0.67785
	loss_value_4: 0.02515
	loss_reward_4: 0.00167
	loss_policy_5: 0.01965
	accuracy_policy_5: 0.67811
	loss_value_5: 0.02475
	loss_reward_5: 0.00167
	loss_policy: 0.16472
	loss_value: 0.25945
	loss_reward: 0.00847
[2024-05-06 23:35:23] nn step 1400, lr: 0.1.
	loss_policy_0: 0.05922
	accuracy_policy_0: 0.76498
	loss_value_0: 0.12314
	loss_policy_1: 0.0146
	accuracy_policy_1: 0.73451
	loss_value_1: 0.02454
	loss_reward_1: 0.00161
	loss_policy_2: 0.0152
	accuracy_policy_2: 0.72883
	loss_value_2: 0.02428
	loss_reward_2: 0.00155
	loss_policy_3: 0.01589
	accuracy_policy_3: 0.72201
	loss_value_3: 0.02399
	loss_reward_3: 0.00153
	loss_policy_4: 0.0165
	accuracy_policy_4: 0.71217
	loss_value_4: 0.02365
	loss_reward_4: 0.0016
	loss_policy_5: 0.01672
	accuracy_policy_5: 0.71072
	loss_value_5: 0.0233
	loss_reward_5: 0.00159
	loss_policy: 0.13814
	loss_value: 0.24291
	loss_reward: 0.00788
Optimization_Done 1400
[2024-05-06 23:37:37] [command] train weight_iter_1400.pkl 4 8
[2024-05-06 23:38:14] nn step 1500, lr: 0.1.
	loss_policy_0: 0.04983
	accuracy_policy_0: 0.78082
	loss_value_0: 0.12411
	loss_policy_1: 0.01341
	accuracy_policy_1: 0.73338
	loss_value_1: 0.02476
	loss_reward_1: 0.0016
	loss_policy_2: 0.01399
	accuracy_policy_2: 0.73682
	loss_value_2: 0.02451
	loss_reward_2: 0.00149
	loss_policy_3: 0.01475
	accuracy_policy_3: 0.73051
	loss_value_3: 0.02428
	loss_reward_3: 0.00152
	loss_policy_4: 0.0153
	accuracy_policy_4: 0.72078
	loss_value_4: 0.02399
	loss_reward_4: 0.00158
	loss_policy_5: 0.01562
	accuracy_policy_5: 0.72225
	loss_value_5: 0.02368
	loss_reward_5: 0.00153
	loss_policy: 0.12291
	loss_value: 0.24533
	loss_reward: 0.00771
[2024-05-06 23:38:49] nn step 1600, lr: 0.1.
	loss_policy_0: 0.04255
	accuracy_policy_0: 0.80332
	loss_value_0: 0.12192
	loss_policy_1: 0.01159
	accuracy_policy_1: 0.76635
	loss_value_1: 0.02436
	loss_reward_1: 0.00154
	loss_policy_2: 0.01223
	accuracy_policy_2: 0.76191
	loss_value_2: 0.02413
	loss_reward_2: 0.00151
	loss_policy_3: 0.0129
	accuracy_policy_3: 0.7601
	loss_value_3: 0.02392
	loss_reward_3: 0.00149
	loss_policy_4: 0.01347
	accuracy_policy_4: 0.75051
	loss_value_4: 0.02364
	loss_reward_4: 0.00152
	loss_policy_5: 0.01385
	accuracy_policy_5: 0.74354
	loss_value_5: 0.02333
	loss_reward_5: 0.00154
	loss_policy: 0.10659
	loss_value: 0.2413
	loss_reward: 0.0076
Optimization_Done 1600
[2024-05-06 23:41:03] [command] train weight_iter_1600.pkl 5 9
[2024-05-06 23:41:39] nn step 1700, lr: 0.1.
	loss_policy_0: 0.03786
	accuracy_policy_0: 0.81479
	loss_value_0: 0.12408
	loss_policy_1: 0.01099
	accuracy_policy_1: 0.77955
	loss_value_1: 0.02478
	loss_reward_1: 0.00197
	loss_policy_2: 0.01181
	accuracy_policy_2: 0.77453
	loss_value_2: 0.02447
	loss_reward_2: 0.00195
	loss_policy_3: 0.01214
	accuracy_policy_3: 0.76818
	loss_value_3: 0.02418
	loss_reward_3: 0.00187
	loss_policy_4: 0.01303
	accuracy_policy_4: 0.75715
	loss_value_4: 0.02388
	loss_reward_4: 0.0019
	loss_policy_5: 0.01335
	accuracy_policy_5: 0.75848
	loss_value_5: 0.02349
	loss_reward_5: 0.00203
	loss_policy: 0.09918
	loss_value: 0.24488
	loss_reward: 0.00971
[2024-05-06 23:42:15] nn step 1800, lr: 0.1.
	loss_policy_0: 0.03345
	accuracy_policy_0: 0.83166
	loss_value_0: 0.12081
	loss_policy_1: 0.0097
	accuracy_policy_1: 0.79762
	loss_value_1: 0.02414
	loss_reward_1: 0.00183
	loss_policy_2: 0.01026
	accuracy_policy_2: 0.79256
	loss_value_2: 0.02389
	loss_reward_2: 0.00179
	loss_policy_3: 0.01081
	accuracy_policy_3: 0.79186
	loss_value_3: 0.02362
	loss_reward_3: 0.00173
	loss_policy_4: 0.0115
	accuracy_policy_4: 0.7776
	loss_value_4: 0.02332
	loss_reward_4: 0.00179
	loss_policy_5: 0.01206
	accuracy_policy_5: 0.77781
	loss_value_5: 0.02303
	loss_reward_5: 0.00183
	loss_policy: 0.08777
	loss_value: 0.2388
	loss_reward: 0.00898
Optimization_Done 1800
[2024-05-06 23:44:20] [command] train weight_iter_1800.pkl 6 10
[2024-05-06 23:44:58] nn step 1900, lr: 0.1.
	loss_policy_0: 0.04328
	accuracy_policy_0: 0.76961
	loss_value_0: 0.12182
	loss_policy_1: 0.01355
	accuracy_policy_1: 0.70432
	loss_value_1: 0.02427
	loss_reward_1: 0.00064
	loss_policy_2: 0.01486
	accuracy_policy_2: 0.67764
	loss_value_2: 0.02409
	loss_reward_2: 0.00067
	loss_policy_3: 0.01574
	accuracy_policy_3: 0.67098
	loss_value_3: 0.02395
	loss_reward_3: 0.00061
	loss_policy_4: 0.01694
	accuracy_policy_4: 0.64805
	loss_value_4: 0.02378
	loss_reward_4: 0.00064
	loss_policy_5: 0.01794
	accuracy_policy_5: 0.63975
	loss_value_5: 0.0236
	loss_reward_5: 0.00066
	loss_policy: 0.12231
	loss_value: 0.24152
	loss_reward: 0.00323
[2024-05-06 23:45:35] nn step 2000, lr: 0.1.
	loss_policy_0: 0.03822
	accuracy_policy_0: 0.79295
	loss_value_0: 0.12294
	loss_policy_1: 0.01237
	accuracy_policy_1: 0.72504
	loss_value_1: 0.02447
	loss_reward_1: 0.00067
	loss_policy_2: 0.0135
	accuracy_policy_2: 0.70154
	loss_value_2: 0.02432
	loss_reward_2: 0.00062
	loss_policy_3: 0.01429
	accuracy_policy_3: 0.69203
	loss_value_3: 0.02417
	loss_reward_3: 0.00064
	loss_policy_4: 0.0153
	accuracy_policy_4: 0.67428
	loss_value_4: 0.02399
	loss_reward_4: 0.00067
	loss_policy_5: 0.01611
	accuracy_policy_5: 0.66623
	loss_value_5: 0.02383
	loss_reward_5: 0.00066
	loss_policy: 0.1098
	loss_value: 0.24372
	loss_reward: 0.00326
Optimization_Done 2000
[2024-05-06 23:47:53] [command] train weight_iter_2000.pkl 7 11
[2024-05-06 23:48:29] nn step 2100, lr: 0.1.
	loss_policy_0: 0.03665
	accuracy_policy_0: 0.80186
	loss_value_0: 0.12351
	loss_policy_1: 0.01158
	accuracy_policy_1: 0.73947
	loss_value_1: 0.02461
	loss_reward_1: 0.00068
	loss_policy_2: 0.01268
	accuracy_policy_2: 0.72008
	loss_value_2: 0.02445
	loss_reward_2: 0.00061
	loss_policy_3: 0.01325
	accuracy_policy_3: 0.71611
	loss_value_3: 0.02429
	loss_reward_3: 0.00065
	loss_policy_4: 0.01421
	accuracy_policy_4: 0.69545
	loss_value_4: 0.02416
	loss_reward_4: 0.00065
	loss_policy_5: 0.01502
	accuracy_policy_5: 0.68352
	loss_value_5: 0.02403
	loss_reward_5: 0.00065
	loss_policy: 0.10339
	loss_value: 0.24506
	loss_reward: 0.00324
[2024-05-06 23:49:05] nn step 2200, lr: 0.1.
	loss_policy_0: 0.03335
	accuracy_policy_0: 0.80932
	loss_value_0: 0.11683
	loss_policy_1: 0.01075
	accuracy_policy_1: 0.7457
	loss_value_1: 0.0233
	loss_reward_1: 0.0006
	loss_policy_2: 0.01163
	accuracy_policy_2: 0.72725
	loss_value_2: 0.02316
	loss_reward_2: 0.0006
	loss_policy_3: 0.01234
	accuracy_policy_3: 0.72027
	loss_value_3: 0.023
	loss_reward_3: 0.00062
	loss_policy_4: 0.01313
	accuracy_policy_4: 0.70205
	loss_value_4: 0.02287
	loss_reward_4: 0.00059
	loss_policy_5: 0.01375
	accuracy_policy_5: 0.69471
	loss_value_5: 0.02272
	loss_reward_5: 0.00061
	loss_policy: 0.09495
	loss_value: 0.23187
	loss_reward: 0.00302
Optimization_Done 2200
[2024-05-06 23:51:18] [command] train weight_iter_2200.pkl 8 12
[2024-05-06 23:51:55] nn step 2300, lr: 0.1.
	loss_policy_0: 0.03603
	accuracy_policy_0: 0.80111
	loss_value_0: 0.11543
	loss_policy_1: 0.01091
	accuracy_policy_1: 0.75084
	loss_value_1: 0.02301
	loss_reward_1: 0.0005
	loss_policy_2: 0.01167
	accuracy_policy_2: 0.73531
	loss_value_2: 0.02286
	loss_reward_2: 0.00051
	loss_policy_3: 0.01226
	accuracy_policy_3: 0.73006
	loss_value_3: 0.02272
	loss_reward_3: 0.00049
	loss_policy_4: 0.01293
	accuracy_policy_4: 0.71455
	loss_value_4: 0.0226
	loss_reward_4: 0.00051
	loss_policy_5: 0.01354
	accuracy_policy_5: 0.70699
	loss_value_5: 0.02247
	loss_reward_5: 0.0005
	loss_policy: 0.09734
	loss_value: 0.2291
	loss_reward: 0.00251
[2024-05-06 23:52:31] nn step 2400, lr: 0.1.
	loss_policy_0: 0.03325
	accuracy_policy_0: 0.81621
	loss_value_0: 0.11742
	loss_policy_1: 0.01027
	accuracy_policy_1: 0.76721
	loss_value_1: 0.02343
	loss_reward_1: 0.00053
	loss_policy_2: 0.01092
	accuracy_policy_2: 0.75195
	loss_value_2: 0.02328
	loss_reward_2: 0.0005
	loss_policy_3: 0.01147
	accuracy_policy_3: 0.74914
	loss_value_3: 0.02312
	loss_reward_3: 0.00051
	loss_policy_4: 0.01217
	accuracy_policy_4: 0.7348
	loss_value_4: 0.023
	loss_reward_4: 0.00051
	loss_policy_5: 0.01282
	accuracy_policy_5: 0.72264
	loss_value_5: 0.02286
	loss_reward_5: 0.00052
	loss_policy: 0.0909
	loss_value: 0.23311
	loss_reward: 0.00257
Optimization_Done 2400
[2024-05-06 23:54:42] [command] train weight_iter_2400.pkl 9 13
[2024-05-06 23:55:19] nn step 2500, lr: 0.1.
	loss_policy_0: 0.02921
	accuracy_policy_0: 0.83053
	loss_value_0: 0.11528
	loss_policy_1: 0.00932
	accuracy_policy_1: 0.7835
	loss_value_1: 0.02298
	loss_reward_1: 0.00052
	loss_policy_2: 0.01013
	accuracy_policy_2: 0.76965
	loss_value_2: 0.02284
	loss_reward_2: 0.00046
	loss_policy_3: 0.01067
	accuracy_policy_3: 0.76162
	loss_value_3: 0.02271
	loss_reward_3: 0.00048
	loss_policy_4: 0.01119
	accuracy_policy_4: 0.75488
	loss_value_4: 0.02258
	loss_reward_4: 0.00049
	loss_policy_5: 0.0118
	accuracy_policy_5: 0.74539
	loss_value_5: 0.02245
	loss_reward_5: 0.00051
	loss_policy: 0.08233
	loss_value: 0.22884
	loss_reward: 0.00247
[2024-05-06 23:55:55] nn step 2600, lr: 0.1.
	loss_policy_0: 0.02399
	accuracy_policy_0: 0.84492
	loss_value_0: 0.10683
	loss_policy_1: 0.00811
	accuracy_policy_1: 0.7942
	loss_value_1: 0.02129
	loss_reward_1: 0.00047
	loss_policy_2: 0.00877
	accuracy_policy_2: 0.78
	loss_value_2: 0.02116
	loss_reward_2: 0.00042
	loss_policy_3: 0.00914
	accuracy_policy_3: 0.77543
	loss_value_3: 0.02104
	loss_reward_3: 0.00042
	loss_policy_4: 0.00961
	accuracy_policy_4: 0.76859
	loss_value_4: 0.02093
	loss_reward_4: 0.00043
	loss_policy_5: 0.01024
	accuracy_policy_5: 0.75453
	loss_value_5: 0.0208
	loss_reward_5: 0.00046
	loss_policy: 0.06986
	loss_value: 0.21206
	loss_reward: 0.00219
Optimization_Done 2600
[2024-05-06 23:57:59] [command] train weight_iter_2600.pkl 10 14
[2024-05-06 23:58:36] nn step 2700, lr: 0.1.
	loss_policy_0: 0.02534
	accuracy_policy_0: 0.82184
	loss_value_0: 0.10805
	loss_policy_1: 0.0088
	accuracy_policy_1: 0.76309
	loss_value_1: 0.02159
	loss_reward_1: 1e-05
	loss_policy_2: 0.00952
	accuracy_policy_2: 0.75256
	loss_value_2: 0.02152
	loss_reward_2: 0.0
	loss_policy_3: 0.01008
	accuracy_policy_3: 0.7391
	loss_value_3: 0.02145
	loss_reward_3: 0.0
	loss_policy_4: 0.01052
	accuracy_policy_4: 0.7283
	loss_value_4: 0.02137
	loss_reward_4: 0.0
	loss_policy_5: 0.01122
	accuracy_policy_5: 0.70953
	loss_value_5: 0.02129
	loss_reward_5: 0.0
	loss_policy: 0.07548
	loss_value: 0.21527
	loss_reward: 2e-05
[2024-05-06 23:59:12] nn step 2800, lr: 0.1.
	loss_policy_0: 0.02248
	accuracy_policy_0: 0.8348
	loss_value_0: 0.10566
	loss_policy_1: 0.00816
	accuracy_policy_1: 0.77504
	loss_value_1: 0.02111
	loss_reward_1: 0.0
	loss_policy_2: 0.00888
	accuracy_policy_2: 0.7593
	loss_value_2: 0.02103
	loss_reward_2: 0.0
	loss_policy_3: 0.00919
	accuracy_policy_3: 0.75174
	loss_value_3: 0.02095
	loss_reward_3: 0.0
	loss_policy_4: 0.00989
	accuracy_policy_4: 0.73967
	loss_value_4: 0.02087
	loss_reward_4: 0.0
	loss_policy_5: 0.0102
	accuracy_policy_5: 0.72846
	loss_value_5: 0.0208
	loss_reward_5: 0.0
	loss_policy: 0.06881
	loss_value: 0.21042
	loss_reward: 1e-05
Optimization_Done 2800
[2024-05-07 00:01:24] [command] train weight_iter_2800.pkl 11 15
[2024-05-07 00:02:02] nn step 2900, lr: 0.1.
	loss_policy_0: 0.0192
	accuracy_policy_0: 0.85539
	loss_value_0: 0.10907
	loss_policy_1: 0.00724
	accuracy_policy_1: 0.79666
	loss_value_1: 0.0218
	loss_reward_1: 0.0001
	loss_policy_2: 0.00801
	accuracy_policy_2: 0.78365
	loss_value_2: 0.02172
	loss_reward_2: 9e-05
	loss_policy_3: 0.00834
	accuracy_policy_3: 0.77693
	loss_value_3: 0.02164
	loss_reward_3: 0.0001
	loss_policy_4: 0.00887
	accuracy_policy_4: 0.76732
	loss_value_4: 0.02154
	loss_reward_4: 9e-05
	loss_policy_5: 0.00941
	accuracy_policy_5: 0.75502
	loss_value_5: 0.02144
	loss_reward_5: 0.00011
	loss_policy: 0.06107
	loss_value: 0.21721
	loss_reward: 0.00049
[2024-05-07 00:02:38] nn step 3000, lr: 0.1.
	loss_policy_0: 0.01888
	accuracy_policy_0: 0.85828
	loss_value_0: 0.10764
	loss_policy_1: 0.00697
	accuracy_policy_1: 0.80285
	loss_value_1: 0.02151
	loss_reward_1: 8e-05
	loss_policy_2: 0.00764
	accuracy_policy_2: 0.79324
	loss_value_2: 0.02143
	loss_reward_2: 7e-05
	loss_policy_3: 0.00798
	accuracy_policy_3: 0.78238
	loss_value_3: 0.02133
	loss_reward_3: 7e-05
	loss_policy_4: 0.00843
	accuracy_policy_4: 0.77582
	loss_value_4: 0.02123
	loss_reward_4: 7e-05
	loss_policy_5: 0.00893
	accuracy_policy_5: 0.76344
	loss_value_5: 0.02114
	loss_reward_5: 8e-05
	loss_policy: 0.05883
	loss_value: 0.21428
	loss_reward: 0.00036
Optimization_Done 3000
[2024-05-07 00:04:18] [command] train weight_iter_3000.pkl 12 16
[2024-05-07 00:04:55] nn step 3100, lr: 0.1.
	loss_policy_0: 0.01981
	accuracy_policy_0: 0.85688
	loss_value_0: 0.10666
	loss_policy_1: 0.00757
	accuracy_policy_1: 0.79859
	loss_value_1: 0.0213
	loss_reward_1: 8e-05
	loss_policy_2: 0.00841
	accuracy_policy_2: 0.78744
	loss_value_2: 0.02122
	loss_reward_2: 7e-05
	loss_policy_3: 0.00876
	accuracy_policy_3: 0.78068
	loss_value_3: 0.02115
	loss_reward_3: 8e-05
	loss_policy_4: 0.0093
	accuracy_policy_4: 0.76998
	loss_value_4: 0.02105
	loss_reward_4: 9e-05
	loss_policy_5: 0.0101
	accuracy_policy_5: 0.75391
	loss_value_5: 0.02095
	loss_reward_5: 8e-05
	loss_policy: 0.06394
	loss_value: 0.21233
	loss_reward: 0.0004
[2024-05-07 00:05:31] nn step 3200, lr: 0.1.
	loss_policy_0: 0.01778
	accuracy_policy_0: 0.86209
	loss_value_0: 0.10369
	loss_policy_1: 0.0069
	accuracy_policy_1: 0.80971
	loss_value_1: 0.02069
	loss_reward_1: 0.0001
	loss_policy_2: 0.0075
	accuracy_policy_2: 0.79678
	loss_value_2: 0.02062
	loss_reward_2: 6e-05
	loss_policy_3: 0.00791
	accuracy_policy_3: 0.79084
	loss_value_3: 0.02053
	loss_reward_3: 7e-05
	loss_policy_4: 0.00843
	accuracy_policy_4: 0.78289
	loss_value_4: 0.02044
	loss_reward_4: 7e-05
	loss_policy_5: 0.00906
	accuracy_policy_5: 0.76592
	loss_value_5: 0.02034
	loss_reward_5: 8e-05
	loss_policy: 0.05758
	loss_value: 0.20631
	loss_reward: 0.00038
Optimization_Done 3200
[2024-05-07 00:07:43] [command] train weight_iter_3200.pkl 13 17
[2024-05-07 00:08:20] nn step 3300, lr: 0.1.
	loss_policy_0: 0.01461
	accuracy_policy_0: 0.87066
	loss_value_0: 0.09748
	loss_policy_1: 0.00599
	accuracy_policy_1: 0.81654
	loss_value_1: 0.01947
	loss_reward_1: 8e-05
	loss_policy_2: 0.00661
	accuracy_policy_2: 0.80096
	loss_value_2: 0.01939
	loss_reward_2: 7e-05
	loss_policy_3: 0.00702
	accuracy_policy_3: 0.79668
	loss_value_3: 0.0193
	loss_reward_3: 7e-05
	loss_policy_4: 0.00741
	accuracy_policy_4: 0.78627
	loss_value_4: 0.01921
	loss_reward_4: 7e-05
	loss_policy_5: 0.00791
	accuracy_policy_5: 0.77627
	loss_value_5: 0.01913
	loss_reward_5: 7e-05
	loss_policy: 0.04956
	loss_value: 0.19398
	loss_reward: 0.00036
[2024-05-07 00:08:56] nn step 3400, lr: 0.1.
	loss_policy_0: 0.01473
	accuracy_policy_0: 0.87314
	loss_value_0: 0.09944
	loss_policy_1: 0.00603
	accuracy_policy_1: 0.81734
	loss_value_1: 0.01986
	loss_reward_1: 8e-05
	loss_policy_2: 0.00647
	accuracy_policy_2: 0.81039
	loss_value_2: 0.01978
	loss_reward_2: 7e-05
	loss_policy_3: 0.00687
	accuracy_policy_3: 0.79986
	loss_value_3: 0.01969
	loss_reward_3: 8e-05
	loss_policy_4: 0.00728
	accuracy_policy_4: 0.79516
	loss_value_4: 0.0196
	loss_reward_4: 6e-05
	loss_policy_5: 0.00776
	accuracy_policy_5: 0.78365
	loss_value_5: 0.01951
	loss_reward_5: 7e-05
	loss_policy: 0.04915
	loss_value: 0.19789
	loss_reward: 0.00037
Optimization_Done 3400
[2024-05-07 00:10:58] [command] train weight_iter_3400.pkl 14 18
[2024-05-07 00:11:34] nn step 3500, lr: 0.1.
	loss_policy_0: 0.01288
	accuracy_policy_0: 0.86877
	loss_value_0: 0.09001
	loss_policy_1: 0.00568
	accuracy_policy_1: 0.81209
	loss_value_1: 0.01796
	loss_reward_1: 0.00022
	loss_policy_2: 0.00616
	accuracy_policy_2: 0.80158
	loss_value_2: 0.01785
	loss_reward_2: 0.00019
	loss_policy_3: 0.00654
	accuracy_policy_3: 0.79203
	loss_value_3: 0.01773
	loss_reward_3: 0.00019
	loss_policy_4: 0.00697
	accuracy_policy_4: 0.7809
	loss_value_4: 0.01765
	loss_reward_4: 0.00019
	loss_policy_5: 0.00748
	accuracy_policy_5: 0.76771
	loss_value_5: 0.01753
	loss_reward_5: 0.00021
	loss_policy: 0.04571
	loss_value: 0.17874
	loss_reward: 0.00099
[2024-05-07 00:12:10] nn step 3600, lr: 0.1.
	loss_policy_0: 0.01262
	accuracy_policy_0: 0.87492
	loss_value_0: 0.09105
	loss_policy_1: 0.00561
	accuracy_policy_1: 0.81695
	loss_value_1: 0.01816
	loss_reward_1: 0.00018
	loss_policy_2: 0.00627
	accuracy_policy_2: 0.80555
	loss_value_2: 0.01804
	loss_reward_2: 0.00019
	loss_policy_3: 0.00661
	accuracy_policy_3: 0.79273
	loss_value_3: 0.01791
	loss_reward_3: 0.00018
	loss_policy_4: 0.00704
	accuracy_policy_4: 0.78461
	loss_value_4: 0.01779
	loss_reward_4: 0.00016
	loss_policy_5: 0.00752
	accuracy_policy_5: 0.77166
	loss_value_5: 0.01771
	loss_reward_5: 0.00018
	loss_policy: 0.04567
	loss_value: 0.18066
	loss_reward: 0.00088
Optimization_Done 3600
[2024-05-07 00:14:27] [command] train weight_iter_3600.pkl 15 19
[2024-05-07 00:15:04] nn step 3700, lr: 0.1.
	loss_policy_0: 0.0109
	accuracy_policy_0: 0.88594
	loss_value_0: 0.09186
	loss_policy_1: 0.00499
	accuracy_policy_1: 0.83418
	loss_value_1: 0.01835
	loss_reward_1: 0.00026
	loss_policy_2: 0.00558
	accuracy_policy_2: 0.82367
	loss_value_2: 0.01823
	loss_reward_2: 0.00023
	loss_policy_3: 0.00603
	accuracy_policy_3: 0.80877
	loss_value_3: 0.01811
	loss_reward_3: 0.00023
	loss_policy_4: 0.00652
	accuracy_policy_4: 0.8009
	loss_value_4: 0.01799
	loss_reward_4: 0.00024
	loss_policy_5: 0.00701
	accuracy_policy_5: 0.78887
	loss_value_5: 0.01789
	loss_reward_5: 0.00025
	loss_policy: 0.04103
	loss_value: 0.18242
	loss_reward: 0.00122
[2024-05-07 00:15:41] nn step 3800, lr: 0.1.
	loss_policy_0: 0.01023
	accuracy_policy_0: 0.89084
	loss_value_0: 0.08796
	loss_policy_1: 0.00467
	accuracy_policy_1: 0.83551
	loss_value_1: 0.01756
	loss_reward_1: 0.00022
	loss_policy_2: 0.00512
	accuracy_policy_2: 0.83002
	loss_value_2: 0.01744
	loss_reward_2: 0.00021
	loss_policy_3: 0.00549
	accuracy_policy_3: 0.81668
	loss_value_3: 0.01733
	loss_reward_3: 0.00019
	loss_policy_4: 0.00595
	accuracy_policy_4: 0.81043
	loss_value_4: 0.01722
	loss_reward_4: 0.00021
	loss_policy_5: 0.00642
	accuracy_policy_5: 0.79775
	loss_value_5: 0.01711
	loss_reward_5: 0.00021
	loss_policy: 0.03789
	loss_value: 0.17461
	loss_reward: 0.00105
Optimization_Done 3800
[2024-05-07 00:17:55] [command] train weight_iter_3800.pkl 16 20
[2024-05-07 00:18:32] nn step 3900, lr: 0.1.
	loss_policy_0: 0.00993
	accuracy_policy_0: 0.88635
	loss_value_0: 0.08686
	loss_policy_1: 0.00456
	accuracy_policy_1: 0.8301
	loss_value_1: 0.01733
	loss_reward_1: 0.00017
	loss_policy_2: 0.00495
	accuracy_policy_2: 0.8234
	loss_value_2: 0.01723
	loss_reward_2: 0.00015
	loss_policy_3: 0.00539
	accuracy_policy_3: 0.81689
	loss_value_3: 0.01712
	loss_reward_3: 0.00015
	loss_policy_4: 0.00593
	accuracy_policy_4: 0.80217
	loss_value_4: 0.01702
	loss_reward_4: 0.00015
	loss_policy_5: 0.00642
	accuracy_policy_5: 0.79057
	loss_value_5: 0.01692
	loss_reward_5: 0.00016
	loss_policy: 0.03718
	loss_value: 0.17248
	loss_reward: 0.00079
[2024-05-07 00:19:08] nn step 4000, lr: 0.1.
	loss_policy_0: 0.00904
	accuracy_policy_0: 0.89271
	loss_value_0: 0.08528
	loss_policy_1: 0.00433
	accuracy_policy_1: 0.83689
	loss_value_1: 0.01703
	loss_reward_1: 0.00014
	loss_policy_2: 0.00471
	accuracy_policy_2: 0.83021
	loss_value_2: 0.01692
	loss_reward_2: 0.00014
	loss_policy_3: 0.0051
	accuracy_policy_3: 0.82336
	loss_value_3: 0.01681
	loss_reward_3: 0.00012
	loss_policy_4: 0.00558
	accuracy_policy_4: 0.81092
	loss_value_4: 0.01672
	loss_reward_4: 0.00014
	loss_policy_5: 0.00593
	accuracy_policy_5: 0.80348
	loss_value_5: 0.01664
	loss_reward_5: 0.00013
	loss_policy: 0.03469
	loss_value: 0.1694
	loss_reward: 0.00067
Optimization_Done 4000
[2024-05-07 00:21:17] [command] train weight_iter_4000.pkl 17 21
[2024-05-07 00:21:54] nn step 4100, lr: 0.1.
	loss_policy_0: 0.01066
	accuracy_policy_0: 0.88514
	loss_value_0: 0.08177
	loss_policy_1: 0.00457
	accuracy_policy_1: 0.82832
	loss_value_1: 0.01629
	loss_reward_1: 0.00045
	loss_policy_2: 0.005
	accuracy_policy_2: 0.81672
	loss_value_2: 0.01615
	loss_reward_2: 0.00043
	loss_policy_3: 0.00528
	accuracy_policy_3: 0.81318
	loss_value_3: 0.01602
	loss_reward_3: 0.00043
	loss_policy_4: 0.00571
	accuracy_policy_4: 0.79867
	loss_value_4: 0.0159
	loss_reward_4: 0.00043
	loss_policy_5: 0.00619
	accuracy_policy_5: 0.79412
	loss_value_5: 0.01575
	loss_reward_5: 0.00047
	loss_policy: 0.0374
	loss_value: 0.16188
	loss_reward: 0.00222
[2024-05-07 00:22:30] nn step 4200, lr: 0.1.
	loss_policy_0: 0.00924
	accuracy_policy_0: 0.89287
	loss_value_0: 0.08162
	loss_policy_1: 0.00417
	accuracy_policy_1: 0.83826
	loss_value_1: 0.01625
	loss_reward_1: 0.00042
	loss_policy_2: 0.00442
	accuracy_policy_2: 0.83375
	loss_value_2: 0.0161
	loss_reward_2: 0.00037
	loss_policy_3: 0.00477
	accuracy_policy_3: 0.82713
	loss_value_3: 0.01596
	loss_reward_3: 0.00034
	loss_policy_4: 0.00519
	accuracy_policy_4: 0.81742
	loss_value_4: 0.01583
	loss_reward_4: 0.00036
	loss_policy_5: 0.00563
	accuracy_policy_5: 0.80744
	loss_value_5: 0.0157
	loss_reward_5: 0.00036
	loss_policy: 0.03341
	loss_value: 0.16144
	loss_reward: 0.00186
Optimization_Done 4200
[2024-05-07 00:24:38] [command] train weight_iter_4200.pkl 18 22
[2024-05-07 00:25:15] nn step 4300, lr: 0.1.
	loss_policy_0: 0.01287
	accuracy_policy_0: 0.85887
	loss_value_0: 0.07929
	loss_policy_1: 0.00474
	accuracy_policy_1: 0.80637
	loss_value_1: 0.0157
	loss_reward_1: 0.00116
	loss_policy_2: 0.005
	accuracy_policy_2: 0.79662
	loss_value_2: 0.01543
	loss_reward_2: 0.00109
	loss_policy_3: 0.00535
	accuracy_policy_3: 0.79471
	loss_value_3: 0.01522
	loss_reward_3: 0.00105
	loss_policy_4: 0.00573
	accuracy_policy_4: 0.79113
	loss_value_4: 0.01504
	loss_reward_4: 0.00102
	loss_policy_5: 0.00619
	accuracy_policy_5: 0.77635
	loss_value_5: 0.01483
	loss_reward_5: 0.00117
	loss_policy: 0.03988
	loss_value: 0.1555
	loss_reward: 0.00549
[2024-05-07 00:25:51] nn step 4400, lr: 0.1.
	loss_policy_0: 0.00996
	accuracy_policy_0: 0.87797
	loss_value_0: 0.07536
	loss_policy_1: 0.00392
	accuracy_policy_1: 0.82549
	loss_value_1: 0.01495
	loss_reward_1: 0.00089
	loss_policy_2: 0.00413
	accuracy_policy_2: 0.81578
	loss_value_2: 0.01475
	loss_reward_2: 0.00084
	loss_policy_3: 0.00439
	accuracy_policy_3: 0.81461
	loss_value_3: 0.01456
	loss_reward_3: 0.00086
	loss_policy_4: 0.00469
	accuracy_policy_4: 0.81004
	loss_value_4: 0.01442
	loss_reward_4: 0.00081
	loss_policy_5: 0.00512
	accuracy_policy_5: 0.7966
	loss_value_5: 0.01427
	loss_reward_5: 0.00087
	loss_policy: 0.03221
	loss_value: 0.1483
	loss_reward: 0.00426
Optimization_Done 4400
[2024-05-07 00:27:50] [command] train weight_iter_4400.pkl 19 23
[2024-05-07 00:28:27] nn step 4500, lr: 0.1.
	loss_policy_0: 0.0106
	accuracy_policy_0: 0.8842
	loss_value_0: 0.08072
	loss_policy_1: 0.00417
	accuracy_policy_1: 0.83045
	loss_value_1: 0.01605
	loss_reward_1: 0.00087
	loss_policy_2: 0.00451
	accuracy_policy_2: 0.82168
	loss_value_2: 0.01585
	loss_reward_2: 0.00078
	loss_policy_3: 0.00469
	accuracy_policy_3: 0.8209
	loss_value_3: 0.01567
	loss_reward_3: 0.0008
	loss_policy_4: 0.00515
	accuracy_policy_4: 0.81312
	loss_value_4: 0.01552
	loss_reward_4: 0.00079
	loss_policy_5: 0.00564
	accuracy_policy_5: 0.7982
	loss_value_5: 0.01537
	loss_reward_5: 0.00085
	loss_policy: 0.03475
	loss_value: 0.15918
	loss_reward: 0.00409
[2024-05-07 00:29:02] nn step 4600, lr: 0.1.
	loss_policy_0: 0.00898
	accuracy_policy_0: 0.89105
	loss_value_0: 0.07432
	loss_policy_1: 0.00372
	accuracy_policy_1: 0.836
	loss_value_1: 0.01485
	loss_reward_1: 0.00065
	loss_policy_2: 0.00392
	accuracy_policy_2: 0.8316
	loss_value_2: 0.01468
	loss_reward_2: 0.00057
	loss_policy_3: 0.00419
	accuracy_policy_3: 0.82906
	loss_value_3: 0.01455
	loss_reward_3: 0.00058
	loss_policy_4: 0.00454
	accuracy_policy_4: 0.82145
	loss_value_4: 0.01443
	loss_reward_4: 0.00059
	loss_policy_5: 0.00502
	accuracy_policy_5: 0.80604
	loss_value_5: 0.01431
	loss_reward_5: 0.00062
	loss_policy: 0.03038
	loss_value: 0.14714
	loss_reward: 0.00301
Optimization_Done 4600
[2024-05-07 00:31:11] [command] train weight_iter_4600.pkl 20 24
[2024-05-07 00:31:48] nn step 4700, lr: 0.1.
	loss_policy_0: 0.0093
	accuracy_policy_0: 0.89582
	loss_value_0: 0.08116
	loss_policy_1: 0.00382
	accuracy_policy_1: 0.8458
	loss_value_1: 0.01627
	loss_reward_1: 0.00056
	loss_policy_2: 0.0041
	accuracy_policy_2: 0.83611
	loss_value_2: 0.01611
	loss_reward_2: 0.00054
	loss_policy_3: 0.00432
	accuracy_policy_3: 0.83502
	loss_value_3: 0.01601
	loss_reward_3: 0.00049
	loss_policy_4: 0.00473
	accuracy_policy_4: 0.83051
	loss_value_4: 0.01588
	loss_reward_4: 0.00052
	loss_policy_5: 0.00525
	accuracy_policy_5: 0.81518
	loss_value_5: 0.01577
	loss_reward_5: 0.00057
	loss_policy: 0.03152
	loss_value: 0.16121
	loss_reward: 0.00268
[2024-05-07 00:32:24] nn step 4800, lr: 0.1.
	loss_policy_0: 0.00845
	accuracy_policy_0: 0.89922
	loss_value_0: 0.07975
	loss_policy_1: 0.0035
	accuracy_policy_1: 0.85168
	loss_value_1: 0.01598
	loss_reward_1: 0.00048
	loss_policy_2: 0.00368
	accuracy_policy_2: 0.85172
	loss_value_2: 0.01588
	loss_reward_2: 0.00044
	loss_policy_3: 0.00399
	accuracy_policy_3: 0.84377
	loss_value_3: 0.01579
	loss_reward_3: 0.00042
	loss_policy_4: 0.00429
	accuracy_policy_4: 0.83992
	loss_value_4: 0.01572
	loss_reward_4: 0.00041
	loss_policy_5: 0.00475
	accuracy_policy_5: 0.82551
	loss_value_5: 0.01564
	loss_reward_5: 0.00045
	loss_policy: 0.02867
	loss_value: 0.15877
	loss_reward: 0.00221
Optimization_Done 4800
[2024-05-07 00:34:33] [command] train weight_iter_4800.pkl 21 25
[2024-05-07 00:35:10] nn step 4900, lr: 0.1.
	loss_policy_0: 0.01554
	accuracy_policy_0: 0.86061
	loss_value_0: 0.08203
	loss_policy_1: 0.00535
	accuracy_policy_1: 0.81049
	loss_value_1: 0.01645
	loss_reward_1: 0.00081
	loss_policy_2: 0.0056
	accuracy_policy_2: 0.80771
	loss_value_2: 0.01632
	loss_reward_2: 0.00075
	loss_policy_3: 0.00591
	accuracy_policy_3: 0.80215
	loss_value_3: 0.01623
	loss_reward_3: 0.00069
	loss_policy_4: 0.00643
	accuracy_policy_4: 0.79506
	loss_value_4: 0.01611
	loss_reward_4: 0.00076
	loss_policy_5: 0.00693
	accuracy_policy_5: 0.78488
	loss_value_5: 0.016
	loss_reward_5: 0.00079
	loss_policy: 0.04576
	loss_value: 0.16314
	loss_reward: 0.00381
[2024-05-07 00:35:46] nn step 5000, lr: 0.1.
	loss_policy_0: 0.0133
	accuracy_policy_0: 0.87186
	loss_value_0: 0.08033
	loss_policy_1: 0.00462
	accuracy_policy_1: 0.82584
	loss_value_1: 0.01612
	loss_reward_1: 0.00061
	loss_policy_2: 0.00492
	accuracy_policy_2: 0.82234
	loss_value_2: 0.01604
	loss_reward_2: 0.00058
	loss_policy_3: 0.00522
	accuracy_policy_3: 0.8182
	loss_value_3: 0.01599
	loss_reward_3: 0.00054
	loss_policy_4: 0.00566
	accuracy_policy_4: 0.81027
	loss_value_4: 0.01594
	loss_reward_4: 0.00056
	loss_policy_5: 0.00613
	accuracy_policy_5: 0.79256
	loss_value_5: 0.01588
	loss_reward_5: 0.00062
	loss_policy: 0.03985
	loss_value: 0.16029
	loss_reward: 0.00291
Optimization_Done 5000
[2024-05-07 00:37:53] [command] train weight_iter_5000.pkl 22 26
[2024-05-07 00:38:31] nn step 5100, lr: 0.1.
	loss_policy_0: 0.05682
	accuracy_policy_0: 0.71445
	loss_value_0: 0.08603
	loss_policy_1: 0.01325
	accuracy_policy_1: 0.68002
	loss_value_1: 0.01739
	loss_reward_1: 0.00151
	loss_policy_2: 0.0135
	accuracy_policy_2: 0.67512
	loss_value_2: 0.01751
	loss_reward_2: 0.00148
	loss_policy_3: 0.01366
	accuracy_policy_3: 0.67045
	loss_value_3: 0.01757
	loss_reward_3: 0.00152
	loss_policy_4: 0.01412
	accuracy_policy_4: 0.65953
	loss_value_4: 0.01761
	loss_reward_4: 0.00161
	loss_policy_5: 0.01475
	accuracy_policy_5: 0.65188
	loss_value_5: 0.01767
	loss_reward_5: 0.00164
	loss_policy: 0.1261
	loss_value: 0.17378
	loss_reward: 0.00776
[2024-05-07 00:39:07] nn step 5200, lr: 0.1.
	loss_policy_0: 0.04508
	accuracy_policy_0: 0.76139
	loss_value_0: 0.0851
	loss_policy_1: 0.01089
	accuracy_policy_1: 0.72445
	loss_value_1: 0.01721
	loss_reward_1: 0.00151
	loss_policy_2: 0.01113
	accuracy_policy_2: 0.72389
	loss_value_2: 0.0173
	loss_reward_2: 0.00144
	loss_policy_3: 0.01152
	accuracy_policy_3: 0.71367
	loss_value_3: 0.01736
	loss_reward_3: 0.00144
	loss_policy_4: 0.01184
	accuracy_policy_4: 0.70645
	loss_value_4: 0.01738
	loss_reward_4: 0.0015
	loss_policy_5: 0.01233
	accuracy_policy_5: 0.69818
	loss_value_5: 0.01739
	loss_reward_5: 0.00156
	loss_policy: 0.1028
	loss_value: 0.17174
	loss_reward: 0.00745
Optimization_Done 5200
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 01:00:29] [command] train weight_iter_5200.pkl 24 27
[2024-05-07 01:01:25] nn step 5300, lr: 0.08.
	loss_policy_0: 0.04805
	accuracy_policy_0: 0.77838
	loss_value_0: 0.09217
	loss_policy_1: 0.01196
	accuracy_policy_1: 0.73928
	loss_value_1: 0.01852
	loss_reward_1: 0.00147
	loss_policy_2: 0.01213
	accuracy_policy_2: 0.73611
	loss_value_2: 0.01857
	loss_reward_2: 0.00152
	loss_policy_3: 0.01256
	accuracy_policy_3: 0.73248
	loss_value_3: 0.01856
	loss_reward_3: 0.00146
	loss_policy_4: 0.01295
	accuracy_policy_4: 0.72824
	loss_value_4: 0.01852
	loss_reward_4: 0.00153
	loss_policy_5: 0.01377
	accuracy_policy_5: 0.71529
	loss_value_5: 0.01846
	loss_reward_5: 0.00158
	loss_policy: 0.11142
	loss_value: 0.1848
	loss_reward: 0.00757
[2024-05-07 01:02:01] nn step 5400, lr: 0.08.
	loss_policy_0: 0.0403
	accuracy_policy_0: 0.81457
	loss_value_0: 0.09367
	loss_policy_1: 0.01043
	accuracy_policy_1: 0.77398
	loss_value_1: 0.01891
	loss_reward_1: 0.00149
	loss_policy_2: 0.01047
	accuracy_policy_2: 0.77361
	loss_value_2: 0.01894
	loss_reward_2: 0.00148
	loss_policy_3: 0.01088
	accuracy_policy_3: 0.76764
	loss_value_3: 0.01894
	loss_reward_3: 0.00146
	loss_policy_4: 0.01139
	accuracy_policy_4: 0.75811
	loss_value_4: 0.01889
	loss_reward_4: 0.00153
	loss_policy_5: 0.01193
	accuracy_policy_5: 0.75621
	loss_value_5: 0.01879
	loss_reward_5: 0.00165
	loss_policy: 0.09541
	loss_value: 0.18814
	loss_reward: 0.0076
Optimization_Done 5400
[2024-05-07 01:04:19] [command] train weight_iter_5400.pkl 25 28
[2024-05-07 01:04:56] nn step 5500, lr: 0.08.
	loss_policy_0: 0.0464
	accuracy_policy_0: 0.83289
	loss_value_0: 0.10572
	loss_policy_1: 0.0115
	accuracy_policy_1: 0.8025
	loss_value_1: 0.02131
	loss_reward_1: 0.0029
	loss_policy_2: 0.01193
	accuracy_policy_2: 0.79453
	loss_value_2: 0.02135
	loss_reward_2: 0.00283
	loss_policy_3: 0.01254
	accuracy_policy_3: 0.78869
	loss_value_3: 0.02129
	loss_reward_3: 0.00288
	loss_policy_4: 0.01303
	accuracy_policy_4: 0.7825
	loss_value_4: 0.02115
	loss_reward_4: 0.00303
	loss_policy_5: 0.01391
	accuracy_policy_5: 0.77357
	loss_value_5: 0.02094
	loss_reward_5: 0.00317
	loss_policy: 0.10932
	loss_value: 0.21176
	loss_reward: 0.01481
[2024-05-07 01:05:32] nn step 5600, lr: 0.08.
	loss_policy_0: 0.03661
	accuracy_policy_0: 0.85959
	loss_value_0: 0.10624
	loss_policy_1: 0.00933
	accuracy_policy_1: 0.83732
	loss_value_1: 0.02155
	loss_reward_1: 0.0028
	loss_policy_2: 0.00949
	accuracy_policy_2: 0.83486
	loss_value_2: 0.02166
	loss_reward_2: 0.00269
	loss_policy_3: 0.00998
	accuracy_policy_3: 0.82656
	loss_value_3: 0.02175
	loss_reward_3: 0.00276
	loss_policy_4: 0.01039
	accuracy_policy_4: 0.82443
	loss_value_4: 0.02175
	loss_reward_4: 0.00291
	loss_policy_5: 0.01109
	accuracy_policy_5: 0.81607
	loss_value_5: 0.02174
	loss_reward_5: 0.00293
	loss_policy: 0.08689
	loss_value: 0.21468
	loss_reward: 0.01409
Optimization_Done 5600
[2024-05-07 01:07:48] [command] train weight_iter_5600.pkl 26 29
[2024-05-07 01:08:25] nn step 5700, lr: 0.08.
	loss_policy_0: 0.05019
	accuracy_policy_0: 0.82775
	loss_value_0: 0.11337
	loss_policy_1: 0.01206
	accuracy_policy_1: 0.80188
	loss_value_1: 0.02303
	loss_reward_1: 0.00306
	loss_policy_2: 0.01224
	accuracy_policy_2: 0.79719
	loss_value_2: 0.02322
	loss_reward_2: 0.00312
	loss_policy_3: 0.01258
	accuracy_policy_3: 0.79104
	loss_value_3: 0.02337
	loss_reward_3: 0.003
	loss_policy_4: 0.01291
	accuracy_policy_4: 0.78857
	loss_value_4: 0.0234
	loss_reward_4: 0.00318
	loss_policy_5: 0.01359
	accuracy_policy_5: 0.78557
	loss_value_5: 0.02346
	loss_reward_5: 0.00327
	loss_policy: 0.11358
	loss_value: 0.22984
	loss_reward: 0.01564
[2024-05-07 01:09:01] nn step 5800, lr: 0.08.
	loss_policy_0: 0.04558
	accuracy_policy_0: 0.8418
	loss_value_0: 0.11364
	loss_policy_1: 0.01117
	accuracy_policy_1: 0.81564
	loss_value_1: 0.02306
	loss_reward_1: 0.00293
	loss_policy_2: 0.01126
	accuracy_policy_2: 0.8142
	loss_value_2: 0.02331
	loss_reward_2: 0.00298
	loss_policy_3: 0.0116
	accuracy_policy_3: 0.81166
	loss_value_3: 0.02351
	loss_reward_3: 0.0029
	loss_policy_4: 0.01204
	accuracy_policy_4: 0.80354
	loss_value_4: 0.02359
	loss_reward_4: 0.00306
	loss_policy_5: 0.01271
	accuracy_policy_5: 0.79975
	loss_value_5: 0.02366
	loss_reward_5: 0.00321
	loss_policy: 0.10437
	loss_value: 0.23078
	loss_reward: 0.01509
Optimization_Done 5800
[2024-05-07 01:11:16] [command] train weight_iter_5800.pkl 27 30
[2024-05-07 01:11:53] nn step 5900, lr: 0.08.
	loss_policy_0: 0.06506
	accuracy_policy_0: 0.74945
	loss_value_0: 0.11217
	loss_policy_1: 0.01592
	accuracy_policy_1: 0.70572
	loss_value_1: 0.02284
	loss_reward_1: 0.00363
	loss_policy_2: 0.01671
	accuracy_policy_2: 0.69213
	loss_value_2: 0.0231
	loss_reward_2: 0.00357
	loss_policy_3: 0.01727
	accuracy_policy_3: 0.68508
	loss_value_3: 0.02325
	loss_reward_3: 0.00361
	loss_policy_4: 0.01801
	accuracy_policy_4: 0.67492
	loss_value_4: 0.02329
	loss_reward_4: 0.00378
	loss_policy_5: 0.01868
	accuracy_policy_5: 0.66717
	loss_value_5: 0.02326
	loss_reward_5: 0.00395
	loss_policy: 0.15165
	loss_value: 0.22791
	loss_reward: 0.01853
[2024-05-07 01:12:30] nn step 6000, lr: 0.08.
	loss_policy_0: 0.05413
	accuracy_policy_0: 0.7893
	loss_value_0: 0.11473
	loss_policy_1: 0.01379
	accuracy_policy_1: 0.74646
	loss_value_1: 0.02336
	loss_reward_1: 0.00372
	loss_policy_2: 0.01448
	accuracy_policy_2: 0.73688
	loss_value_2: 0.0236
	loss_reward_2: 0.0037
	loss_policy_3: 0.01509
	accuracy_policy_3: 0.72637
	loss_value_3: 0.02366
	loss_reward_3: 0.00364
	loss_policy_4: 0.01568
	accuracy_policy_4: 0.71834
	loss_value_4: 0.02365
	loss_reward_4: 0.0039
	loss_policy_5: 0.01634
	accuracy_policy_5: 0.7133
	loss_value_5: 0.02358
	loss_reward_5: 0.00398
	loss_policy: 0.12951
	loss_value: 0.23259
	loss_reward: 0.01895
Optimization_Done 6000
[2024-05-07 01:14:35] [command] train weight_iter_6000.pkl 28 31
[2024-05-07 01:15:13] nn step 6100, lr: 0.08.
	loss_policy_0: 0.06135
	accuracy_policy_0: 0.76828
	loss_value_0: 0.11977
	loss_policy_1: 0.01541
	accuracy_policy_1: 0.72465
	loss_value_1: 0.02436
	loss_reward_1: 0.00403
	loss_policy_2: 0.01623
	accuracy_policy_2: 0.71354
	loss_value_2: 0.02449
	loss_reward_2: 0.00399
	loss_policy_3: 0.01706
	accuracy_policy_3: 0.70324
	loss_value_3: 0.02455
	loss_reward_3: 0.00395
	loss_policy_4: 0.0178
	accuracy_policy_4: 0.6951
	loss_value_4: 0.02451
	loss_reward_4: 0.00418
	loss_policy_5: 0.01843
	accuracy_policy_5: 0.68676
	loss_value_5: 0.02441
	loss_reward_5: 0.00436
	loss_policy: 0.14629
	loss_value: 0.2421
	loss_reward: 0.02051
[2024-05-07 01:15:49] nn step 6200, lr: 0.08.
	loss_policy_0: 0.05248
	accuracy_policy_0: 0.79139
	loss_value_0: 0.11535
	loss_policy_1: 0.01365
	accuracy_policy_1: 0.74578
	loss_value_1: 0.02351
	loss_reward_1: 0.00394
	loss_policy_2: 0.01442
	accuracy_policy_2: 0.73281
	loss_value_2: 0.02362
	loss_reward_2: 0.0039
	loss_policy_3: 0.01501
	accuracy_policy_3: 0.72557
	loss_value_3: 0.02371
	loss_reward_3: 0.00382
	loss_policy_4: 0.01547
	accuracy_policy_4: 0.72045
	loss_value_4: 0.02366
	loss_reward_4: 0.00403
	loss_policy_5: 0.01608
	accuracy_policy_5: 0.70928
	loss_value_5: 0.02357
	loss_reward_5: 0.00422
	loss_policy: 0.12712
	loss_value: 0.23341
	loss_reward: 0.01991
Optimization_Done 6200
[2024-05-07 01:18:05] [command] train weight_iter_6200.pkl 29 32
[2024-05-07 01:18:42] nn step 6300, lr: 0.08.
	loss_policy_0: 0.07482
	accuracy_policy_0: 0.7168
	loss_value_0: 0.11597
	loss_policy_1: 0.01804
	accuracy_policy_1: 0.67299
	loss_value_1: 0.02353
	loss_reward_1: 0.00399
	loss_policy_2: 0.01889
	accuracy_policy_2: 0.65625
	loss_value_2: 0.02366
	loss_reward_2: 0.0039
	loss_policy_3: 0.01986
	accuracy_policy_3: 0.64324
	loss_value_3: 0.02373
	loss_reward_3: 0.00394
	loss_policy_4: 0.0206
	accuracy_policy_4: 0.63426
	loss_value_4: 0.02368
	loss_reward_4: 0.00404
	loss_policy_5: 0.02165
	accuracy_policy_5: 0.62279
	loss_value_5: 0.02358
	loss_reward_5: 0.00423
	loss_policy: 0.17386
	loss_value: 0.23415
	loss_reward: 0.0201
[2024-05-07 01:19:17] nn step 6400, lr: 0.08.
	loss_policy_0: 0.0654
	accuracy_policy_0: 0.74906
	loss_value_0: 0.11804
	loss_policy_1: 0.01651
	accuracy_policy_1: 0.69844
	loss_value_1: 0.024
	loss_reward_1: 0.00419
	loss_policy_2: 0.01728
	accuracy_policy_2: 0.68367
	loss_value_2: 0.02414
	loss_reward_2: 0.00392
	loss_policy_3: 0.01811
	accuracy_policy_3: 0.67422
	loss_value_3: 0.02421
	loss_reward_3: 0.0039
	loss_policy_4: 0.01884
	accuracy_policy_4: 0.66381
	loss_value_4: 0.0242
	loss_reward_4: 0.00412
	loss_policy_5: 0.0195
	accuracy_policy_5: 0.65523
	loss_value_5: 0.02412
	loss_reward_5: 0.00444
	loss_policy: 0.15564
	loss_value: 0.23871
	loss_reward: 0.02058
Optimization_Done 6400
[2024-05-07 01:21:34] [command] train weight_iter_6400.pkl 30 33
[2024-05-07 01:22:11] nn step 6500, lr: 0.08.
	loss_policy_0: 0.06957
	accuracy_policy_0: 0.7534
	loss_value_0: 0.12551
	loss_policy_1: 0.01763
	accuracy_policy_1: 0.70426
	loss_value_1: 0.02553
	loss_reward_1: 0.00453
	loss_policy_2: 0.01832
	accuracy_policy_2: 0.69096
	loss_value_2: 0.02571
	loss_reward_2: 0.00447
	loss_policy_3: 0.01948
	accuracy_policy_3: 0.67307
	loss_value_3: 0.02574
	loss_reward_3: 0.00449
	loss_policy_4: 0.02009
	accuracy_policy_4: 0.66941
	loss_value_4: 0.02575
	loss_reward_4: 0.00455
	loss_policy_5: 0.02093
	accuracy_policy_5: 0.65547
	loss_value_5: 0.02578
	loss_reward_5: 0.00479
	loss_policy: 0.16602
	loss_value: 0.25402
	loss_reward: 0.02283
[2024-05-07 01:22:47] nn step 6600, lr: 0.08.
	loss_policy_0: 0.06259
	accuracy_policy_0: 0.76844
	loss_value_0: 0.12503
	loss_policy_1: 0.01641
	accuracy_policy_1: 0.71885
	loss_value_1: 0.02536
	loss_reward_1: 0.00449
	loss_policy_2: 0.01731
	accuracy_policy_2: 0.70311
	loss_value_2: 0.02555
	loss_reward_2: 0.00437
	loss_policy_3: 0.01803
	accuracy_policy_3: 0.69307
	loss_value_3: 0.02562
	loss_reward_3: 0.00443
	loss_policy_4: 0.01867
	accuracy_policy_4: 0.68477
	loss_value_4: 0.02569
	loss_reward_4: 0.00452
	loss_policy_5: 0.0198
	accuracy_policy_5: 0.67244
	loss_value_5: 0.02572
	loss_reward_5: 0.00474
	loss_policy: 0.15281
	loss_value: 0.25298
	loss_reward: 0.02254
Optimization_Done 6600
[2024-05-07 01:25:00] [command] train weight_iter_6600.pkl 31 34
[2024-05-07 01:25:38] nn step 6700, lr: 0.08.
	loss_policy_0: 0.04746
	accuracy_policy_0: 0.81783
	loss_value_0: 0.11969
	loss_policy_1: 0.01238
	accuracy_policy_1: 0.77562
	loss_value_1: 0.02443
	loss_reward_1: 0.00409
	loss_policy_2: 0.01317
	accuracy_policy_2: 0.7601
	loss_value_2: 0.02482
	loss_reward_2: 0.00395
	loss_policy_3: 0.01411
	accuracy_policy_3: 0.7452
	loss_value_3: 0.02506
	loss_reward_3: 0.00393
	loss_policy_4: 0.01486
	accuracy_policy_4: 0.73271
	loss_value_4: 0.02518
	loss_reward_4: 0.00408
	loss_policy_5: 0.01565
	accuracy_policy_5: 0.72301
	loss_value_5: 0.02528
	loss_reward_5: 0.00444
	loss_policy: 0.11763
	loss_value: 0.24446
	loss_reward: 0.02048
[2024-05-07 01:26:14] nn step 6800, lr: 0.08.
	loss_policy_0: 0.04224
	accuracy_policy_0: 0.83213
	loss_value_0: 0.12204
	loss_policy_1: 0.01158
	accuracy_policy_1: 0.78895
	loss_value_1: 0.02481
	loss_reward_1: 0.00407
	loss_policy_2: 0.01234
	accuracy_policy_2: 0.77271
	loss_value_2: 0.02516
	loss_reward_2: 0.00389
	loss_policy_3: 0.01337
	accuracy_policy_3: 0.7543
	loss_value_3: 0.02541
	loss_reward_3: 0.00389
	loss_policy_4: 0.014
	accuracy_policy_4: 0.74605
	loss_value_4: 0.02557
	loss_reward_4: 0.0041
	loss_policy_5: 0.01492
	accuracy_policy_5: 0.7334
	loss_value_5: 0.02568
	loss_reward_5: 0.00438
	loss_policy: 0.10845
	loss_value: 0.24868
	loss_reward: 0.02034
Optimization_Done 6800
[2024-05-07 01:28:17] [command] train weight_iter_6800.pkl 32 35
[2024-05-07 01:28:54] nn step 6900, lr: 0.08.
	loss_policy_0: 0.04625
	accuracy_policy_0: 0.82479
	loss_value_0: 0.12377
	loss_policy_1: 0.01257
	accuracy_policy_1: 0.7842
	loss_value_1: 0.02523
	loss_reward_1: 0.0043
	loss_policy_2: 0.01349
	accuracy_policy_2: 0.76256
	loss_value_2: 0.02566
	loss_reward_2: 0.00404
	loss_policy_3: 0.01457
	accuracy_policy_3: 0.74551
	loss_value_3: 0.02595
	loss_reward_3: 0.00403
	loss_policy_4: 0.01533
	accuracy_policy_4: 0.73582
	loss_value_4: 0.02622
	loss_reward_4: 0.00432
	loss_policy_5: 0.0162
	accuracy_policy_5: 0.72562
	loss_value_5: 0.02635
	loss_reward_5: 0.00457
	loss_policy: 0.11841
	loss_value: 0.25318
	loss_reward: 0.02126
[2024-05-07 01:29:30] nn step 7000, lr: 0.08.
	loss_policy_0: 0.04147
	accuracy_policy_0: 0.84078
	loss_value_0: 0.12298
	loss_policy_1: 0.01147
	accuracy_policy_1: 0.7959
	loss_value_1: 0.02506
	loss_reward_1: 0.00425
	loss_policy_2: 0.01241
	accuracy_policy_2: 0.77883
	loss_value_2: 0.02541
	loss_reward_2: 0.00398
	loss_policy_3: 0.01337
	accuracy_policy_3: 0.76666
	loss_value_3: 0.02576
	loss_reward_3: 0.00395
	loss_policy_4: 0.01419
	accuracy_policy_4: 0.75248
	loss_value_4: 0.02607
	loss_reward_4: 0.0042
	loss_policy_5: 0.01515
	accuracy_policy_5: 0.73916
	loss_value_5: 0.02635
	loss_reward_5: 0.00453
	loss_policy: 0.10807
	loss_value: 0.25163
	loss_reward: 0.02091
Optimization_Done 7000
[2024-05-07 01:31:43] [command] train weight_iter_7000.pkl 33 36
[2024-05-07 01:32:21] nn step 7100, lr: 0.08.
	loss_policy_0: 0.05166
	accuracy_policy_0: 0.80758
	loss_value_0: 0.12527
	loss_policy_1: 0.01335
	accuracy_policy_1: 0.76875
	loss_value_1: 0.02548
	loss_reward_1: 0.00407
	loss_policy_2: 0.01433
	accuracy_policy_2: 0.75047
	loss_value_2: 0.02594
	loss_reward_2: 0.00386
	loss_policy_3: 0.01529
	accuracy_policy_3: 0.7365
	loss_value_3: 0.02632
	loss_reward_3: 0.00394
	loss_policy_4: 0.01602
	accuracy_policy_4: 0.72357
	loss_value_4: 0.02666
	loss_reward_4: 0.00429
	loss_policy_5: 0.01706
	accuracy_policy_5: 0.71191
	loss_value_5: 0.02684
	loss_reward_5: 0.00445
	loss_policy: 0.1277
	loss_value: 0.25651
	loss_reward: 0.02062
[2024-05-07 01:32:57] nn step 7200, lr: 0.08.
	loss_policy_0: 0.04634
	accuracy_policy_0: 0.83045
	loss_value_0: 0.12854
	loss_policy_1: 0.01252
	accuracy_policy_1: 0.78795
	loss_value_1: 0.02623
	loss_reward_1: 0.00432
	loss_policy_2: 0.01365
	accuracy_policy_2: 0.76863
	loss_value_2: 0.02659
	loss_reward_2: 0.00398
	loss_policy_3: 0.0146
	accuracy_policy_3: 0.75354
	loss_value_3: 0.02688
	loss_reward_3: 0.00411
	loss_policy_4: 0.01533
	accuracy_policy_4: 0.74379
	loss_value_4: 0.0271
	loss_reward_4: 0.00435
	loss_policy_5: 0.01639
	accuracy_policy_5: 0.72691
	loss_value_5: 0.02737
	loss_reward_5: 0.00468
	loss_policy: 0.11883
	loss_value: 0.26271
	loss_reward: 0.02144
Optimization_Done 7200
[2024-05-07 01:34:40] [command] train weight_iter_7200.pkl 34 37
[2024-05-07 01:35:17] nn step 7300, lr: 0.08.
	loss_policy_0: 0.05265
	accuracy_policy_0: 0.82135
	loss_value_0: 0.13447
	loss_policy_1: 0.01362
	accuracy_policy_1: 0.77865
	loss_value_1: 0.02741
	loss_reward_1: 0.0047
	loss_policy_2: 0.01458
	accuracy_policy_2: 0.76145
	loss_value_2: 0.02781
	loss_reward_2: 0.00425
	loss_policy_3: 0.01543
	accuracy_policy_3: 0.74918
	loss_value_3: 0.02823
	loss_reward_3: 0.00438
	loss_policy_4: 0.01626
	accuracy_policy_4: 0.73707
	loss_value_4: 0.0285
	loss_reward_4: 0.00466
	loss_policy_5: 0.01721
	accuracy_policy_5: 0.725
	loss_value_5: 0.02874
	loss_reward_5: 0.00483
	loss_policy: 0.12974
	loss_value: 0.27517
	loss_reward: 0.02282
[2024-05-07 01:35:54] nn step 7400, lr: 0.08.
	loss_policy_0: 0.04909
	accuracy_policy_0: 0.83506
	loss_value_0: 0.13883
	loss_policy_1: 0.01319
	accuracy_policy_1: 0.79232
	loss_value_1: 0.02826
	loss_reward_1: 0.00483
	loss_policy_2: 0.01419
	accuracy_policy_2: 0.77551
	loss_value_2: 0.02875
	loss_reward_2: 0.00448
	loss_policy_3: 0.01515
	accuracy_policy_3: 0.75799
	loss_value_3: 0.02911
	loss_reward_3: 0.00438
	loss_policy_4: 0.016
	accuracy_policy_4: 0.74814
	loss_value_4: 0.02948
	loss_reward_4: 0.00469
	loss_policy_5: 0.01704
	accuracy_policy_5: 0.7334
	loss_value_5: 0.02974
	loss_reward_5: 0.0052
	loss_policy: 0.12465
	loss_value: 0.28417
	loss_reward: 0.02358
Optimization_Done 7400
[2024-05-07 01:38:06] [command] train weight_iter_7400.pkl 35 38
[2024-05-07 01:38:42] nn step 7500, lr: 0.08.
	loss_policy_0: 0.0671
	accuracy_policy_0: 0.78502
	loss_value_0: 0.14115
	loss_policy_1: 0.01678
	accuracy_policy_1: 0.7423
	loss_value_1: 0.02874
	loss_reward_1: 0.00564
	loss_policy_2: 0.01771
	accuracy_policy_2: 0.72641
	loss_value_2: 0.02936
	loss_reward_2: 0.00512
	loss_policy_3: 0.01856
	accuracy_policy_3: 0.71137
	loss_value_3: 0.02984
	loss_reward_3: 0.00505
	loss_policy_4: 0.01947
	accuracy_policy_4: 0.69945
	loss_value_4: 0.03022
	loss_reward_4: 0.00528
	loss_policy_5: 0.02036
	accuracy_policy_5: 0.68828
	loss_value_5: 0.03045
	loss_reward_5: 0.00581
	loss_policy: 0.15998
	loss_value: 0.28976
	loss_reward: 0.02691
[2024-05-07 01:39:18] nn step 7600, lr: 0.08.
	loss_policy_0: 0.05602
	accuracy_policy_0: 0.81375
	loss_value_0: 0.13957
	loss_policy_1: 0.01469
	accuracy_policy_1: 0.76191
	loss_value_1: 0.02837
	loss_reward_1: 0.00548
	loss_policy_2: 0.01594
	accuracy_policy_2: 0.74256
	loss_value_2: 0.02885
	loss_reward_2: 0.00487
	loss_policy_3: 0.01666
	accuracy_policy_3: 0.7358
	loss_value_3: 0.02931
	loss_reward_3: 0.00489
	loss_policy_4: 0.0176
	accuracy_policy_4: 0.72449
	loss_value_4: 0.02965
	loss_reward_4: 0.00522
	loss_policy_5: 0.01847
	accuracy_policy_5: 0.71117
	loss_value_5: 0.02993
	loss_reward_5: 0.00564
	loss_policy: 0.13939
	loss_value: 0.28568
	loss_reward: 0.0261
Optimization_Done 7600
[2024-05-07 01:41:16] [command] train weight_iter_7600.pkl 36 39
[2024-05-07 01:41:54] nn step 7700, lr: 0.08.
	loss_policy_0: 0.075
	accuracy_policy_0: 0.7859
	loss_value_0: 0.16004
	loss_policy_1: 0.01888
	accuracy_policy_1: 0.7351
	loss_value_1: 0.03255
	loss_reward_1: 0.00655
	loss_policy_2: 0.02038
	accuracy_policy_2: 0.71207
	loss_value_2: 0.03324
	loss_reward_2: 0.00595
	loss_policy_3: 0.02127
	accuracy_policy_3: 0.69746
	loss_value_3: 0.03372
	loss_reward_3: 0.00577
	loss_policy_4: 0.02213
	accuracy_policy_4: 0.68793
	loss_value_4: 0.03413
	loss_reward_4: 0.0063
	loss_policy_5: 0.02329
	accuracy_policy_5: 0.67422
	loss_value_5: 0.03441
	loss_reward_5: 0.00678
	loss_policy: 0.18096
	loss_value: 0.32809
	loss_reward: 0.03135
[2024-05-07 01:42:30] nn step 7800, lr: 0.08.
	loss_policy_0: 0.06596
	accuracy_policy_0: 0.79773
	loss_value_0: 0.15193
	loss_policy_1: 0.01719
	accuracy_policy_1: 0.74635
	loss_value_1: 0.03092
	loss_reward_1: 0.00614
	loss_policy_2: 0.01816
	accuracy_policy_2: 0.72684
	loss_value_2: 0.03148
	loss_reward_2: 0.00549
	loss_policy_3: 0.01914
	accuracy_policy_3: 0.71525
	loss_value_3: 0.03195
	loss_reward_3: 0.00542
	loss_policy_4: 0.02034
	accuracy_policy_4: 0.69723
	loss_value_4: 0.03229
	loss_reward_4: 0.0058
	loss_policy_5: 0.02128
	accuracy_policy_5: 0.68484
	loss_value_5: 0.03255
	loss_reward_5: 0.00638
	loss_policy: 0.16206
	loss_value: 0.31111
	loss_reward: 0.02922
Optimization_Done 7800
[2024-05-07 01:44:42] [command] train weight_iter_7800.pkl 37 40
[2024-05-07 01:45:19] nn step 7900, lr: 0.08.
	loss_policy_0: 0.07728
	accuracy_policy_0: 0.78141
	loss_value_0: 0.16033
	loss_policy_1: 0.01964
	accuracy_policy_1: 0.72623
	loss_value_1: 0.03251
	loss_reward_1: 0.00635
	loss_policy_2: 0.02056
	accuracy_policy_2: 0.71572
	loss_value_2: 0.03305
	loss_reward_2: 0.00569
	loss_policy_3: 0.02187
	accuracy_policy_3: 0.69801
	loss_value_3: 0.0336
	loss_reward_3: 0.00554
	loss_policy_4: 0.02297
	accuracy_policy_4: 0.68252
	loss_value_4: 0.03401
	loss_reward_4: 0.00603
	loss_policy_5: 0.02388
	accuracy_policy_5: 0.67295
	loss_value_5: 0.03441
	loss_reward_5: 0.00649
	loss_policy: 0.1862
	loss_value: 0.32792
	loss_reward: 0.0301
[2024-05-07 01:45:55] nn step 8000, lr: 0.08.
	loss_policy_0: 0.0702
	accuracy_policy_0: 0.79879
	loss_value_0: 0.16265
	loss_policy_1: 0.01841
	accuracy_policy_1: 0.74717
	loss_value_1: 0.03301
	loss_reward_1: 0.00649
	loss_policy_2: 0.01982
	accuracy_policy_2: 0.72576
	loss_value_2: 0.03357
	loss_reward_2: 0.00575
	loss_policy_3: 0.02119
	accuracy_policy_3: 0.70652
	loss_value_3: 0.03396
	loss_reward_3: 0.00579
	loss_policy_4: 0.0222
	accuracy_policy_4: 0.69672
	loss_value_4: 0.0344
	loss_reward_4: 0.00609
	loss_policy_5: 0.02316
	accuracy_policy_5: 0.68221
	loss_value_5: 0.03479
	loss_reward_5: 0.00653
	loss_policy: 0.17498
	loss_value: 0.33238
	loss_reward: 0.03065
Optimization_Done 8000
[2024-05-07 01:48:10] [command] train weight_iter_8000.pkl 38 41
[2024-05-07 01:48:47] nn step 8100, lr: 0.08.
	loss_policy_0: 0.07687
	accuracy_policy_0: 0.77715
	loss_value_0: 0.16082
	loss_policy_1: 0.01997
	accuracy_policy_1: 0.72904
	loss_value_1: 0.03264
	loss_reward_1: 0.00632
	loss_policy_2: 0.0213
	accuracy_policy_2: 0.71096
	loss_value_2: 0.0332
	loss_reward_2: 0.00552
	loss_policy_3: 0.02266
	accuracy_policy_3: 0.69484
	loss_value_3: 0.03368
	loss_reward_3: 0.0054
	loss_policy_4: 0.02372
	accuracy_policy_4: 0.67822
	loss_value_4: 0.03409
	loss_reward_4: 0.0059
	loss_policy_5: 0.02493
	accuracy_policy_5: 0.66459
	loss_value_5: 0.03445
	loss_reward_5: 0.00645
	loss_policy: 0.18944
	loss_value: 0.32888
	loss_reward: 0.02959
[2024-05-07 01:49:23] nn step 8200, lr: 0.08.
	loss_policy_0: 0.06929
	accuracy_policy_0: 0.79598
	loss_value_0: 0.163
	loss_policy_1: 0.01877
	accuracy_policy_1: 0.74424
	loss_value_1: 0.0331
	loss_reward_1: 0.00624
	loss_policy_2: 0.01996
	accuracy_policy_2: 0.72547
	loss_value_2: 0.03358
	loss_reward_2: 0.00564
	loss_policy_3: 0.02135
	accuracy_policy_3: 0.70719
	loss_value_3: 0.03411
	loss_reward_3: 0.00561
	loss_policy_4: 0.02235
	accuracy_policy_4: 0.69475
	loss_value_4: 0.03465
	loss_reward_4: 0.00604
	loss_policy_5: 0.02392
	accuracy_policy_5: 0.68145
	loss_value_5: 0.03489
	loss_reward_5: 0.00663
	loss_policy: 0.17564
	loss_value: 0.33333
	loss_reward: 0.03016
Optimization_Done 8200
[2024-05-07 01:51:38] [command] train weight_iter_8200.pkl 39 42
[2024-05-07 01:52:16] nn step 8300, lr: 0.08.
	loss_policy_0: 0.08199
	accuracy_policy_0: 0.79436
	loss_value_0: 0.17594
	loss_policy_1: 0.02176
	accuracy_policy_1: 0.73736
	loss_value_1: 0.03555
	loss_reward_1: 0.00706
	loss_policy_2: 0.02362
	accuracy_policy_2: 0.70822
	loss_value_2: 0.03626
	loss_reward_2: 0.00593
	loss_policy_3: 0.02507
	accuracy_policy_3: 0.68844
	loss_value_3: 0.03697
	loss_reward_3: 0.00596
	loss_policy_4: 0.02615
	accuracy_policy_4: 0.67604
	loss_value_4: 0.03753
	loss_reward_4: 0.00655
	loss_policy_5: 0.0278
	accuracy_policy_5: 0.6567
	loss_value_5: 0.03804
	loss_reward_5: 0.00714
	loss_policy: 0.20639
	loss_value: 0.36029
	loss_reward: 0.03265
[2024-05-07 01:52:52] nn step 8400, lr: 0.08.
	loss_policy_0: 0.06804
	accuracy_policy_0: 0.81125
	loss_value_0: 0.1669
	loss_policy_1: 0.01925
	accuracy_policy_1: 0.74781
	loss_value_1: 0.03391
	loss_reward_1: 0.00675
	loss_policy_2: 0.0206
	accuracy_policy_2: 0.72559
	loss_value_2: 0.03435
	loss_reward_2: 0.00566
	loss_policy_3: 0.02205
	accuracy_policy_3: 0.70674
	loss_value_3: 0.03492
	loss_reward_3: 0.00573
	loss_policy_4: 0.02339
	accuracy_policy_4: 0.69199
	loss_value_4: 0.03552
	loss_reward_4: 0.00606
	loss_policy_5: 0.02461
	accuracy_policy_5: 0.67617
	loss_value_5: 0.03605
	loss_reward_5: 0.00679
	loss_policy: 0.17793
	loss_value: 0.34164
	loss_reward: 0.031
Optimization_Done 8400
[2024-05-07 01:55:06] [command] train weight_iter_8400.pkl 40 43
[2024-05-07 01:55:44] nn step 8500, lr: 0.08.
	loss_policy_0: 0.06954
	accuracy_policy_0: 0.81145
	loss_value_0: 0.16732
	loss_policy_1: 0.01891
	accuracy_policy_1: 0.75781
	loss_value_1: 0.03404
	loss_reward_1: 0.00667
	loss_policy_2: 0.02041
	accuracy_policy_2: 0.73535
	loss_value_2: 0.03466
	loss_reward_2: 0.00575
	loss_policy_3: 0.02221
	accuracy_policy_3: 0.71355
	loss_value_3: 0.03532
	loss_reward_3: 0.00578
	loss_policy_4: 0.02323
	accuracy_policy_4: 0.69678
	loss_value_4: 0.03601
	loss_reward_4: 0.00638
	loss_policy_5: 0.02432
	accuracy_policy_5: 0.68039
	loss_value_5: 0.03675
	loss_reward_5: 0.00686
	loss_policy: 0.17862
	loss_value: 0.3441
	loss_reward: 0.03145
[2024-05-07 01:56:21] nn step 8600, lr: 0.08.
	loss_policy_0: 0.06782
	accuracy_policy_0: 0.82258
	loss_value_0: 0.17488
	loss_policy_1: 0.01876
	accuracy_policy_1: 0.76941
	loss_value_1: 0.03571
	loss_reward_1: 0.00685
	loss_policy_2: 0.02042
	accuracy_policy_2: 0.74789
	loss_value_2: 0.03633
	loss_reward_2: 0.00602
	loss_policy_3: 0.02214
	accuracy_policy_3: 0.72502
	loss_value_3: 0.03698
	loss_reward_3: 0.00607
	loss_policy_4: 0.02345
	accuracy_policy_4: 0.70783
	loss_value_4: 0.03758
	loss_reward_4: 0.00652
	loss_policy_5: 0.02497
	accuracy_policy_5: 0.69564
	loss_value_5: 0.03828
	loss_reward_5: 0.00712
	loss_policy: 0.17757
	loss_value: 0.35975
	loss_reward: 0.03258
Optimization_Done 8600
[2024-05-07 01:58:25] [command] train weight_iter_8600.pkl 41 44
[2024-05-07 01:59:02] nn step 8700, lr: 0.08.
	loss_policy_0: 0.06684
	accuracy_policy_0: 0.81943
	loss_value_0: 0.1686
	loss_policy_1: 0.01852
	accuracy_policy_1: 0.7615
	loss_value_1: 0.03441
	loss_reward_1: 0.00659
	loss_policy_2: 0.02032
	accuracy_policy_2: 0.74043
	loss_value_2: 0.03499
	loss_reward_2: 0.00569
	loss_policy_3: 0.02177
	accuracy_policy_3: 0.71814
	loss_value_3: 0.03562
	loss_reward_3: 0.00589
	loss_policy_4: 0.02306
	accuracy_policy_4: 0.70119
	loss_value_4: 0.03626
	loss_reward_4: 0.00624
	loss_policy_5: 0.02406
	accuracy_policy_5: 0.6908
	loss_value_5: 0.03683
	loss_reward_5: 0.00668
	loss_policy: 0.17456
	loss_value: 0.34671
	loss_reward: 0.03109
[2024-05-07 01:59:38] nn step 8800, lr: 0.08.
	loss_policy_0: 0.06574
	accuracy_policy_0: 0.8277
	loss_value_0: 0.17808
	loss_policy_1: 0.01842
	accuracy_policy_1: 0.7733
	loss_value_1: 0.0363
	loss_reward_1: 0.00694
	loss_policy_2: 0.02021
	accuracy_policy_2: 0.75408
	loss_value_2: 0.03685
	loss_reward_2: 0.00588
	loss_policy_3: 0.0218
	accuracy_policy_3: 0.73311
	loss_value_3: 0.03748
	loss_reward_3: 0.00598
	loss_policy_4: 0.0231
	accuracy_policy_4: 0.71564
	loss_value_4: 0.03815
	loss_reward_4: 0.0065
	loss_policy_5: 0.02447
	accuracy_policy_5: 0.70197
	loss_value_5: 0.03892
	loss_reward_5: 0.00715
	loss_policy: 0.17373
	loss_value: 0.3658
	loss_reward: 0.03246
Optimization_Done 8800
[2024-05-07 02:01:52] [command] train weight_iter_8800.pkl 42 45
[2024-05-07 02:02:28] nn step 8900, lr: 0.08.
	loss_policy_0: 0.07158
	accuracy_policy_0: 0.82188
	loss_value_0: 0.1849
	loss_policy_1: 0.0197
	accuracy_policy_1: 0.76361
	loss_value_1: 0.03762
	loss_reward_1: 0.0068
	loss_policy_2: 0.02191
	accuracy_policy_2: 0.73863
	loss_value_2: 0.03828
	loss_reward_2: 0.00609
	loss_policy_3: 0.02367
	accuracy_policy_3: 0.7182
	loss_value_3: 0.03894
	loss_reward_3: 0.00621
	loss_policy_4: 0.02518
	accuracy_policy_4: 0.70084
	loss_value_4: 0.03961
	loss_reward_4: 0.00671
	loss_policy_5: 0.02673
	accuracy_policy_5: 0.68705
	loss_value_5: 0.04034
	loss_reward_5: 0.00744
	loss_policy: 0.18878
	loss_value: 0.37969
	loss_reward: 0.03325
[2024-05-07 02:03:04] nn step 9000, lr: 0.08.
	loss_policy_0: 0.05864
	accuracy_policy_0: 0.84254
	loss_value_0: 0.17263
	loss_policy_1: 0.01759
	accuracy_policy_1: 0.77875
	loss_value_1: 0.03531
	loss_reward_1: 0.00669
	loss_policy_2: 0.01943
	accuracy_policy_2: 0.75225
	loss_value_2: 0.0358
	loss_reward_2: 0.00572
	loss_policy_3: 0.02122
	accuracy_policy_3: 0.73391
	loss_value_3: 0.03637
	loss_reward_3: 0.00571
	loss_policy_4: 0.02265
	accuracy_policy_4: 0.715
	loss_value_4: 0.03715
	loss_reward_4: 0.00629
	loss_policy_5: 0.02384
	accuracy_policy_5: 0.69818
	loss_value_5: 0.03788
	loss_reward_5: 0.007
	loss_policy: 0.16338
	loss_value: 0.35513
	loss_reward: 0.03141
Optimization_Done 9000
[2024-05-07 02:05:17] [command] train weight_iter_9000.pkl 43 46
[2024-05-07 02:05:55] nn step 9100, lr: 0.08.
	loss_policy_0: 0.08175
	accuracy_policy_0: 0.79867
	loss_value_0: 0.17594
	loss_policy_1: 0.02183
	accuracy_policy_1: 0.74627
	loss_value_1: 0.03579
	loss_reward_1: 0.00623
	loss_policy_2: 0.02417
	accuracy_policy_2: 0.7257
	loss_value_2: 0.03659
	loss_reward_2: 0.00549
	loss_policy_3: 0.0258
	accuracy_policy_3: 0.70486
	loss_value_3: 0.03718
	loss_reward_3: 0.00531
	loss_policy_4: 0.0276
	accuracy_policy_4: 0.68557
	loss_value_4: 0.03781
	loss_reward_4: 0.00556
	loss_policy_5: 0.02941
	accuracy_policy_5: 0.66096
	loss_value_5: 0.03841
	loss_reward_5: 0.00641
	loss_policy: 0.21055
	loss_value: 0.36172
	loss_reward: 0.02901
[2024-05-07 02:06:32] nn step 9200, lr: 0.08.
	loss_policy_0: 0.07295
	accuracy_policy_0: 0.81719
	loss_value_0: 0.1775
	loss_policy_1: 0.02028
	accuracy_policy_1: 0.76543
	loss_value_1: 0.03626
	loss_reward_1: 0.00638
	loss_policy_2: 0.02269
	accuracy_policy_2: 0.74172
	loss_value_2: 0.03695
	loss_reward_2: 0.00548
	loss_policy_3: 0.02434
	accuracy_policy_3: 0.726
	loss_value_3: 0.03766
	loss_reward_3: 0.00556
	loss_policy_4: 0.02628
	accuracy_policy_4: 0.70355
	loss_value_4: 0.03829
	loss_reward_4: 0.0059
	loss_policy_5: 0.02802
	accuracy_policy_5: 0.68377
	loss_value_5: 0.03906
	loss_reward_5: 0.00654
	loss_policy: 0.19457
	loss_value: 0.36572
	loss_reward: 0.02986
Optimization_Done 9200
[2024-05-07 02:08:44] [command] train weight_iter_9200.pkl 44 47
[2024-05-07 02:09:22] nn step 9300, lr: 0.08.
	loss_policy_0: 0.09111
	accuracy_policy_0: 0.78834
	loss_value_0: 0.17741
	loss_policy_1: 0.02511
	accuracy_policy_1: 0.7275
	loss_value_1: 0.0362
	loss_reward_1: 0.00656
	loss_policy_2: 0.02735
	accuracy_policy_2: 0.70223
	loss_value_2: 0.0369
	loss_reward_2: 0.00542
	loss_policy_3: 0.02919
	accuracy_policy_3: 0.68779
	loss_value_3: 0.0376
	loss_reward_3: 0.00556
	loss_policy_4: 0.031
	accuracy_policy_4: 0.66748
	loss_value_4: 0.03826
	loss_reward_4: 0.00594
	loss_policy_5: 0.03291
	accuracy_policy_5: 0.64457
	loss_value_5: 0.03889
	loss_reward_5: 0.00646
	loss_policy: 0.23667
	loss_value: 0.36525
	loss_reward: 0.02993
[2024-05-07 02:09:58] nn step 9400, lr: 0.08.
	loss_policy_0: 0.07852
	accuracy_policy_0: 0.80738
	loss_value_0: 0.17423
	loss_policy_1: 0.02299
	accuracy_policy_1: 0.74477
	loss_value_1: 0.03556
	loss_reward_1: 0.00619
	loss_policy_2: 0.02551
	accuracy_policy_2: 0.7198
	loss_value_2: 0.03624
	loss_reward_2: 0.00538
	loss_policy_3: 0.02711
	accuracy_policy_3: 0.7023
	loss_value_3: 0.03682
	loss_reward_3: 0.00535
	loss_policy_4: 0.02888
	accuracy_policy_4: 0.6851
	loss_value_4: 0.03756
	loss_reward_4: 0.00586
	loss_policy_5: 0.03076
	accuracy_policy_5: 0.66588
	loss_value_5: 0.03828
	loss_reward_5: 0.00638
	loss_policy: 0.21378
	loss_value: 0.35869
	loss_reward: 0.02915
Optimization_Done 9400
[2024-05-07 02:11:30] [command] train weight_iter_9400.pkl 45 48
[2024-05-07 02:12:07] nn step 9500, lr: 0.08.
	loss_policy_0: 0.08533
	accuracy_policy_0: 0.80178
	loss_value_0: 0.17463
	loss_policy_1: 0.02428
	accuracy_policy_1: 0.73785
	loss_value_1: 0.03575
	loss_reward_1: 0.00664
	loss_policy_2: 0.02736
	accuracy_policy_2: 0.70725
	loss_value_2: 0.03643
	loss_reward_2: 0.00606
	loss_policy_3: 0.02932
	accuracy_policy_3: 0.68686
	loss_value_3: 0.03716
	loss_reward_3: 0.00595
	loss_policy_4: 0.0313
	accuracy_policy_4: 0.66543
	loss_value_4: 0.03786
	loss_reward_4: 0.0066
	loss_policy_5: 0.03316
	accuracy_policy_5: 0.64521
	loss_value_5: 0.03859
	loss_reward_5: 0.00731
	loss_policy: 0.23075
	loss_value: 0.36041
	loss_reward: 0.03256
[2024-05-07 02:12:44] nn step 9600, lr: 0.08.
	loss_policy_0: 0.08222
	accuracy_policy_0: 0.80762
	loss_value_0: 0.1786
	loss_policy_1: 0.02419
	accuracy_policy_1: 0.74051
	loss_value_1: 0.03646
	loss_reward_1: 0.00671
	loss_policy_2: 0.02713
	accuracy_policy_2: 0.71166
	loss_value_2: 0.03722
	loss_reward_2: 0.00599
	loss_policy_3: 0.02892
	accuracy_policy_3: 0.69539
	loss_value_3: 0.03804
	loss_reward_3: 0.00606
	loss_policy_4: 0.03141
	accuracy_policy_4: 0.67215
	loss_value_4: 0.03873
	loss_reward_4: 0.00652
	loss_policy_5: 0.03254
	accuracy_policy_5: 0.65852
	loss_value_5: 0.03938
	loss_reward_5: 0.00732
	loss_policy: 0.22641
	loss_value: 0.36844
	loss_reward: 0.03261
Optimization_Done 9600
[2024-05-07 02:14:59] [command] train weight_iter_9600.pkl 46 49
[2024-05-07 02:15:36] nn step 9700, lr: 0.08.
	loss_policy_0: 0.09869
	accuracy_policy_0: 0.78998
	loss_value_0: 0.18527
	loss_policy_1: 0.02755
	accuracy_policy_1: 0.7285
	loss_value_1: 0.03786
	loss_reward_1: 0.00709
	loss_policy_2: 0.03071
	accuracy_policy_2: 0.69594
	loss_value_2: 0.03871
	loss_reward_2: 0.00622
	loss_policy_3: 0.03292
	accuracy_policy_3: 0.67646
	loss_value_3: 0.0396
	loss_reward_3: 0.0064
	loss_policy_4: 0.03515
	accuracy_policy_4: 0.65584
	loss_value_4: 0.04034
	loss_reward_4: 0.00691
	loss_policy_5: 0.03724
	accuracy_policy_5: 0.636
	loss_value_5: 0.04102
	loss_reward_5: 0.00801
	loss_policy: 0.26228
	loss_value: 0.38281
	loss_reward: 0.03464
[2024-05-07 02:16:12] nn step 9800, lr: 0.08.
	loss_policy_0: 0.08713
	accuracy_policy_0: 0.79797
	loss_value_0: 0.17387
	loss_policy_1: 0.02487
	accuracy_policy_1: 0.73285
	loss_value_1: 0.03562
	loss_reward_1: 0.00673
	loss_policy_2: 0.02772
	accuracy_policy_2: 0.71061
	loss_value_2: 0.03635
	loss_reward_2: 0.00598
	loss_policy_3: 0.03021
	accuracy_policy_3: 0.68412
	loss_value_3: 0.03712
	loss_reward_3: 0.006
	loss_policy_4: 0.03231
	accuracy_policy_4: 0.66162
	loss_value_4: 0.0378
	loss_reward_4: 0.00646
	loss_policy_5: 0.03387
	accuracy_policy_5: 0.64729
	loss_value_5: 0.03852
	loss_reward_5: 0.00734
	loss_policy: 0.2361
	loss_value: 0.35927
	loss_reward: 0.0325
Optimization_Done 9800
[2024-05-07 02:18:27] [command] train weight_iter_9800.pkl 47 50
[2024-05-07 02:19:04] nn step 9900, lr: 0.08.
	loss_policy_0: 0.1006
	accuracy_policy_0: 0.7752
	loss_value_0: 0.17596
	loss_policy_1: 0.02661
	accuracy_policy_1: 0.71938
	loss_value_1: 0.03595
	loss_reward_1: 0.00645
	loss_policy_2: 0.02947
	accuracy_policy_2: 0.69408
	loss_value_2: 0.0366
	loss_reward_2: 0.00558
	loss_policy_3: 0.03155
	accuracy_policy_3: 0.67445
	loss_value_3: 0.03733
	loss_reward_3: 0.00576
	loss_policy_4: 0.03363
	accuracy_policy_4: 0.65543
	loss_value_4: 0.03805
	loss_reward_4: 0.00629
	loss_policy_5: 0.03501
	accuracy_policy_5: 0.64057
	loss_value_5: 0.03855
	loss_reward_5: 0.00702
	loss_policy: 0.25687
	loss_value: 0.36245
	loss_reward: 0.0311
[2024-05-07 02:19:40] nn step 10000, lr: 0.08.
	loss_policy_0: 0.09295
	accuracy_policy_0: 0.79385
	loss_value_0: 0.182
	loss_policy_1: 0.02601
	accuracy_policy_1: 0.73453
	loss_value_1: 0.0371
	loss_reward_1: 0.00674
	loss_policy_2: 0.02894
	accuracy_policy_2: 0.70639
	loss_value_2: 0.03789
	loss_reward_2: 0.00601
	loss_policy_3: 0.03132
	accuracy_policy_3: 0.68783
	loss_value_3: 0.03864
	loss_reward_3: 0.00617
	loss_policy_4: 0.03349
	accuracy_policy_4: 0.66668
	loss_value_4: 0.03939
	loss_reward_4: 0.00659
	loss_policy_5: 0.03525
	accuracy_policy_5: 0.65309
	loss_value_5: 0.0401
	loss_reward_5: 0.00746
	loss_policy: 0.24797
	loss_value: 0.37512
	loss_reward: 0.03297
Optimization_Done 10000
[2024-05-07 02:21:54] [command] train weight_iter_10000.pkl 48 51
[2024-05-07 02:22:31] nn step 10100, lr: 0.08.
	loss_policy_0: 0.11431
	accuracy_policy_0: 0.76588
	loss_value_0: 0.18277
	loss_policy_1: 0.02926
	accuracy_policy_1: 0.71365
	loss_value_1: 0.03721
	loss_reward_1: 0.00661
	loss_policy_2: 0.03207
	accuracy_policy_2: 0.68836
	loss_value_2: 0.03808
	loss_reward_2: 0.00598
	loss_policy_3: 0.03441
	accuracy_policy_3: 0.66674
	loss_value_3: 0.03889
	loss_reward_3: 0.00602
	loss_policy_4: 0.03658
	accuracy_policy_4: 0.64332
	loss_value_4: 0.03966
	loss_reward_4: 0.00678
	loss_policy_5: 0.03861
	accuracy_policy_5: 0.62568
	loss_value_5: 0.04035
	loss_reward_5: 0.00749
	loss_policy: 0.28524
	loss_value: 0.37696
	loss_reward: 0.03288
[2024-05-07 02:23:08] nn step 10200, lr: 0.08.
	loss_policy_0: 0.09608
	accuracy_policy_0: 0.79061
	loss_value_0: 0.17317
	loss_policy_1: 0.02607
	accuracy_policy_1: 0.72641
	loss_value_1: 0.03539
	loss_reward_1: 0.00626
	loss_policy_2: 0.02888
	accuracy_policy_2: 0.70277
	loss_value_2: 0.03606
	loss_reward_2: 0.00563
	loss_policy_3: 0.03096
	accuracy_policy_3: 0.68193
	loss_value_3: 0.03678
	loss_reward_3: 0.00572
	loss_policy_4: 0.03318
	accuracy_policy_4: 0.66258
	loss_value_4: 0.03756
	loss_reward_4: 0.00635
	loss_policy_5: 0.03501
	accuracy_policy_5: 0.64568
	loss_value_5: 0.03822
	loss_reward_5: 0.00715
	loss_policy: 0.25018
	loss_value: 0.35717
	loss_reward: 0.0311
Optimization_Done 10200
[2024-05-07 02:25:11] [command] train weight_iter_10200.pkl 49 52
[2024-05-07 02:25:49] nn step 10300, lr: 0.08.
	loss_policy_0: 0.11218
	accuracy_policy_0: 0.76631
	loss_value_0: 0.17186
	loss_policy_1: 0.02928
	accuracy_policy_1: 0.71143
	loss_value_1: 0.03496
	loss_reward_1: 0.00594
	loss_policy_2: 0.032
	accuracy_policy_2: 0.68438
	loss_value_2: 0.03579
	loss_reward_2: 0.00531
	loss_policy_3: 0.03474
	accuracy_policy_3: 0.66223
	loss_value_3: 0.03667
	loss_reward_3: 0.00541
	loss_policy_4: 0.03688
	accuracy_policy_4: 0.63986
	loss_value_4: 0.03737
	loss_reward_4: 0.00578
	loss_policy_5: 0.03865
	accuracy_policy_5: 0.62373
	loss_value_5: 0.03814
	loss_reward_5: 0.00658
	loss_policy: 0.28373
	loss_value: 0.35479
	loss_reward: 0.02901
[2024-05-07 02:26:26] nn step 10400, lr: 0.08.
	loss_policy_0: 0.10495
	accuracy_policy_0: 0.7815
	loss_value_0: 0.17742
	loss_policy_1: 0.02887
	accuracy_policy_1: 0.72107
	loss_value_1: 0.03623
	loss_reward_1: 0.00609
	loss_policy_2: 0.03167
	accuracy_policy_2: 0.69152
	loss_value_2: 0.03705
	loss_reward_2: 0.00535
	loss_policy_3: 0.03418
	accuracy_policy_3: 0.67135
	loss_value_3: 0.03786
	loss_reward_3: 0.0054
	loss_policy_4: 0.03661
	accuracy_policy_4: 0.6517
	loss_value_4: 0.03869
	loss_reward_4: 0.00607
	loss_policy_5: 0.0383
	accuracy_policy_5: 0.63883
	loss_value_5: 0.03951
	loss_reward_5: 0.00666
	loss_policy: 0.27458
	loss_value: 0.36676
	loss_reward: 0.02957
Optimization_Done 10400
[2024-05-07 02:28:41] [command] train weight_iter_10400.pkl 50 53
[2024-05-07 02:29:18] nn step 10500, lr: 0.08.
	loss_policy_0: 0.12131
	accuracy_policy_0: 0.75293
	loss_value_0: 0.17519
	loss_policy_1: 0.03142
	accuracy_policy_1: 0.69791
	loss_value_1: 0.0359
	loss_reward_1: 0.00599
	loss_policy_2: 0.03483
	accuracy_policy_2: 0.66557
	loss_value_2: 0.03679
	loss_reward_2: 0.00523
	loss_policy_3: 0.0373
	accuracy_policy_3: 0.64672
	loss_value_3: 0.0376
	loss_reward_3: 0.00522
	loss_policy_4: 0.03999
	accuracy_policy_4: 0.62566
	loss_value_4: 0.03843
	loss_reward_4: 0.00597
	loss_policy_5: 0.04167
	accuracy_policy_5: 0.60902
	loss_value_5: 0.03927
	loss_reward_5: 0.00657
	loss_policy: 0.30652
	loss_value: 0.36317
	loss_reward: 0.02899
[2024-05-07 02:29:54] nn step 10600, lr: 0.08.
	loss_policy_0: 0.11073
	accuracy_policy_0: 0.7683
	loss_value_0: 0.17646
	loss_policy_1: 0.02992
	accuracy_policy_1: 0.70953
	loss_value_1: 0.03621
	loss_reward_1: 0.00589
	loss_policy_2: 0.03307
	accuracy_policy_2: 0.68285
	loss_value_2: 0.03718
	loss_reward_2: 0.00514
	loss_policy_3: 0.03544
	accuracy_policy_3: 0.664
	loss_value_3: 0.03798
	loss_reward_3: 0.00517
	loss_policy_4: 0.0381
	accuracy_policy_4: 0.64354
	loss_value_4: 0.03877
	loss_reward_4: 0.00577
	loss_policy_5: 0.04037
	accuracy_policy_5: 0.62355
	loss_value_5: 0.03961
	loss_reward_5: 0.0066
	loss_policy: 0.28762
	loss_value: 0.36622
	loss_reward: 0.02857
Optimization_Done 10600
[2024-05-07 02:32:06] [command] train weight_iter_10600.pkl 51 54
[2024-05-07 02:32:43] nn step 10700, lr: 0.08.
	loss_policy_0: 0.10739
	accuracy_policy_0: 0.7749
	loss_value_0: 0.1755
	loss_policy_1: 0.02935
	accuracy_policy_1: 0.71094
	loss_value_1: 0.03601
	loss_reward_1: 0.00596
	loss_policy_2: 0.03239
	accuracy_policy_2: 0.6834
	loss_value_2: 0.03695
	loss_reward_2: 0.00538
	loss_policy_3: 0.03508
	accuracy_policy_3: 0.66154
	loss_value_3: 0.03784
	loss_reward_3: 0.0054
	loss_policy_4: 0.03771
	accuracy_policy_4: 0.63969
	loss_value_4: 0.03864
	loss_reward_4: 0.00583
	loss_policy_5: 0.03942
	accuracy_policy_5: 0.62611
	loss_value_5: 0.03948
	loss_reward_5: 0.0069
	loss_policy: 0.28134
	loss_value: 0.36443
	loss_reward: 0.02947
[2024-05-07 02:33:19] nn step 10800, lr: 0.08.
	loss_policy_0: 0.09932
	accuracy_policy_0: 0.78717
	loss_value_0: 0.17442
	loss_policy_1: 0.02779
	accuracy_policy_1: 0.72291
	loss_value_1: 0.03576
	loss_reward_1: 0.00586
	loss_policy_2: 0.03042
	accuracy_policy_2: 0.70002
	loss_value_2: 0.03673
	loss_reward_2: 0.00529
	loss_policy_3: 0.03341
	accuracy_policy_3: 0.67514
	loss_value_3: 0.03758
	loss_reward_3: 0.00539
	loss_policy_4: 0.03565
	accuracy_policy_4: 0.65449
	loss_value_4: 0.0384
	loss_reward_4: 0.00574
	loss_policy_5: 0.03752
	accuracy_policy_5: 0.63881
	loss_value_5: 0.03923
	loss_reward_5: 0.00686
	loss_policy: 0.26411
	loss_value: 0.36214
	loss_reward: 0.02915
Optimization_Done 10800
[2024-05-07 02:35:30] [command] train weight_iter_10800.pkl 52 55
[2024-05-07 02:36:07] nn step 10900, lr: 0.08.
	loss_policy_0: 0.12851
	accuracy_policy_0: 0.74588
	loss_value_0: 0.18396
	loss_policy_1: 0.03347
	accuracy_policy_1: 0.68682
	loss_value_1: 0.038
	loss_reward_1: 0.00626
	loss_policy_2: 0.03693
	accuracy_policy_2: 0.66135
	loss_value_2: 0.03905
	loss_reward_2: 0.00547
	loss_policy_3: 0.03967
	accuracy_policy_3: 0.63793
	loss_value_3: 0.04005
	loss_reward_3: 0.00563
	loss_policy_4: 0.04194
	accuracy_policy_4: 0.62074
	loss_value_4: 0.04112
	loss_reward_4: 0.00634
	loss_policy_5: 0.04419
	accuracy_policy_5: 0.60223
	loss_value_5: 0.04207
	loss_reward_5: 0.00697
	loss_policy: 0.32472
	loss_value: 0.38425
	loss_reward: 0.03067
[2024-05-07 02:36:43] nn step 11000, lr: 0.08.
	loss_policy_0: 0.11338
	accuracy_policy_0: 0.76756
	loss_value_0: 0.17858
	loss_policy_1: 0.03088
	accuracy_policy_1: 0.69758
	loss_value_1: 0.03698
	loss_reward_1: 0.00596
	loss_policy_2: 0.0335
	accuracy_policy_2: 0.67832
	loss_value_2: 0.03786
	loss_reward_2: 0.00541
	loss_policy_3: 0.03635
	accuracy_policy_3: 0.65516
	loss_value_3: 0.03875
	loss_reward_3: 0.00544
	loss_policy_4: 0.03886
	accuracy_policy_4: 0.63506
	loss_value_4: 0.03952
	loss_reward_4: 0.006
	loss_policy_5: 0.04127
	accuracy_policy_5: 0.6182
	loss_value_5: 0.04051
	loss_reward_5: 0.00701
	loss_policy: 0.29425
	loss_value: 0.37219
	loss_reward: 0.02983
Optimization_Done 11000
[2024-05-07 02:38:52] [command] train weight_iter_11000.pkl 53 56
[2024-05-07 02:39:30] nn step 11100, lr: 0.08.
	loss_policy_0: 0.13368
	accuracy_policy_0: 0.7426
	loss_value_0: 0.18739
	loss_policy_1: 0.03437
	accuracy_policy_1: 0.68469
	loss_value_1: 0.0387
	loss_reward_1: 0.00638
	loss_policy_2: 0.03779
	accuracy_policy_2: 0.66139
	loss_value_2: 0.03992
	loss_reward_2: 0.00574
	loss_policy_3: 0.04049
	accuracy_policy_3: 0.63787
	loss_value_3: 0.04097
	loss_reward_3: 0.0059
	loss_policy_4: 0.04297
	accuracy_policy_4: 0.61812
	loss_value_4: 0.0421
	loss_reward_4: 0.00642
	loss_policy_5: 0.04553
	accuracy_policy_5: 0.59914
	loss_value_5: 0.04312
	loss_reward_5: 0.00709
	loss_policy: 0.33482
	loss_value: 0.3922
	loss_reward: 0.03152
[2024-05-07 02:40:07] nn step 11200, lr: 0.08.
	loss_policy_0: 0.1182
	accuracy_policy_0: 0.76051
	loss_value_0: 0.18098
	loss_policy_1: 0.03224
	accuracy_policy_1: 0.69635
	loss_value_1: 0.03743
	loss_reward_1: 0.00613
	loss_policy_2: 0.03512
	accuracy_policy_2: 0.67066
	loss_value_2: 0.03854
	loss_reward_2: 0.00553
	loss_policy_3: 0.03751
	accuracy_policy_3: 0.65318
	loss_value_3: 0.03957
	loss_reward_3: 0.00578
	loss_policy_4: 0.04005
	accuracy_policy_4: 0.63115
	loss_value_4: 0.04069
	loss_reward_4: 0.00629
	loss_policy_5: 0.04244
	accuracy_policy_5: 0.61375
	loss_value_5: 0.0418
	loss_reward_5: 0.00717
	loss_policy: 0.30556
	loss_value: 0.37902
	loss_reward: 0.0309
Optimization_Done 11200
[2024-05-07 02:42:07] [command] train weight_iter_11200.pkl 54 57
[2024-05-07 02:42:45] nn step 11300, lr: 0.08.
	loss_policy_0: 0.13531
	accuracy_policy_0: 0.72494
	loss_value_0: 0.17906
	loss_policy_1: 0.03423
	accuracy_policy_1: 0.67264
	loss_value_1: 0.03713
	loss_reward_1: 0.00599
	loss_policy_2: 0.03735
	accuracy_policy_2: 0.64436
	loss_value_2: 0.03837
	loss_reward_2: 0.00558
	loss_policy_3: 0.03969
	accuracy_policy_3: 0.62666
	loss_value_3: 0.03942
	loss_reward_3: 0.00579
	loss_policy_4: 0.04206
	accuracy_policy_4: 0.60549
	loss_value_4: 0.04044
	loss_reward_4: 0.00625
	loss_policy_5: 0.04401
	accuracy_policy_5: 0.58984
	loss_value_5: 0.04137
	loss_reward_5: 0.00687
	loss_policy: 0.33264
	loss_value: 0.37578
	loss_reward: 0.03048
[2024-05-07 02:43:21] nn step 11400, lr: 0.08.
	loss_policy_0: 0.1282
	accuracy_policy_0: 0.74875
	loss_value_0: 0.18599
	loss_policy_1: 0.03429
	accuracy_policy_1: 0.68334
	loss_value_1: 0.03857
	loss_reward_1: 0.00642
	loss_policy_2: 0.03747
	accuracy_policy_2: 0.65924
	loss_value_2: 0.03977
	loss_reward_2: 0.00589
	loss_policy_3: 0.0398
	accuracy_policy_3: 0.64045
	loss_value_3: 0.0409
	loss_reward_3: 0.00598
	loss_policy_4: 0.04228
	accuracy_policy_4: 0.6234
	loss_value_4: 0.04192
	loss_reward_4: 0.00649
	loss_policy_5: 0.04443
	accuracy_policy_5: 0.6059
	loss_value_5: 0.04293
	loss_reward_5: 0.00711
	loss_policy: 0.32647
	loss_value: 0.39007
	loss_reward: 0.03188
Optimization_Done 11400
[2024-05-07 02:45:12] [command] train weight_iter_11400.pkl 55 58
[2024-05-07 02:45:49] nn step 11500, lr: 0.08.
	loss_policy_0: 0.14225
	accuracy_policy_0: 0.71561
	loss_value_0: 0.17943
	loss_policy_1: 0.03639
	accuracy_policy_1: 0.64947
	loss_value_1: 0.03726
	loss_reward_1: 0.00601
	loss_policy_2: 0.03994
	accuracy_policy_2: 0.62766
	loss_value_2: 0.0383
	loss_reward_2: 0.00556
	loss_policy_3: 0.04269
	accuracy_policy_3: 0.60617
	loss_value_3: 0.03939
	loss_reward_3: 0.00579
	loss_policy_4: 0.04476
	accuracy_policy_4: 0.58711
	loss_value_4: 0.04038
	loss_reward_4: 0.00614
	loss_policy_5: 0.04727
	accuracy_policy_5: 0.56885
	loss_value_5: 0.04139
	loss_reward_5: 0.00707
	loss_policy: 0.3533
	loss_value: 0.37615
	loss_reward: 0.03057
[2024-05-07 02:46:25] nn step 11600, lr: 0.08.
	loss_policy_0: 0.13377
	accuracy_policy_0: 0.73459
	loss_value_0: 0.1806
	loss_policy_1: 0.03492
	accuracy_policy_1: 0.67012
	loss_value_1: 0.0374
	loss_reward_1: 0.00622
	loss_policy_2: 0.03827
	accuracy_policy_2: 0.64586
	loss_value_2: 0.03847
	loss_reward_2: 0.00563
	loss_policy_3: 0.04111
	accuracy_policy_3: 0.625
	loss_value_3: 0.03956
	loss_reward_3: 0.00576
	loss_policy_4: 0.04388
	accuracy_policy_4: 0.59871
	loss_value_4: 0.04059
	loss_reward_4: 0.0063
	loss_policy_5: 0.04639
	accuracy_policy_5: 0.58057
	loss_value_5: 0.04152
	loss_reward_5: 0.0071
	loss_policy: 0.33834
	loss_value: 0.37815
	loss_reward: 0.03102
Optimization_Done 11600
[2024-05-07 02:48:37] [command] train weight_iter_11600.pkl 56 59
[2024-05-07 02:49:14] nn step 11700, lr: 0.08.
	loss_policy_0: 0.13475
	accuracy_policy_0: 0.73545
	loss_value_0: 0.18465
	loss_policy_1: 0.03427
	accuracy_policy_1: 0.67934
	loss_value_1: 0.03831
	loss_reward_1: 0.00639
	loss_policy_2: 0.03752
	accuracy_policy_2: 0.65189
	loss_value_2: 0.03952
	loss_reward_2: 0.00589
	loss_policy_3: 0.04055
	accuracy_policy_3: 0.62703
	loss_value_3: 0.04054
	loss_reward_3: 0.00623
	loss_policy_4: 0.0436
	accuracy_policy_4: 0.59869
	loss_value_4: 0.04163
	loss_reward_4: 0.00664
	loss_policy_5: 0.04583
	accuracy_policy_5: 0.57672
	loss_value_5: 0.04259
	loss_reward_5: 0.00748
	loss_policy: 0.33652
	loss_value: 0.38723
	loss_reward: 0.03263
[2024-05-07 02:49:51] nn step 11800, lr: 0.08.
	loss_policy_0: 0.12538
	accuracy_policy_0: 0.74562
	loss_value_0: 0.17983
	loss_policy_1: 0.03271
	accuracy_policy_1: 0.68967
	loss_value_1: 0.03748
	loss_reward_1: 0.00632
	loss_policy_2: 0.03605
	accuracy_policy_2: 0.66053
	loss_value_2: 0.03855
	loss_reward_2: 0.00595
	loss_policy_3: 0.03915
	accuracy_policy_3: 0.6332
	loss_value_3: 0.0396
	loss_reward_3: 0.00618
	loss_policy_4: 0.04148
	accuracy_policy_4: 0.61172
	loss_value_4: 0.0407
	loss_reward_4: 0.00652
	loss_policy_5: 0.04389
	accuracy_policy_5: 0.59193
	loss_value_5: 0.04185
	loss_reward_5: 0.00743
	loss_policy: 0.31866
	loss_value: 0.37801
	loss_reward: 0.03241
Optimization_Done 11800
[2024-05-07 02:52:00] [command] train weight_iter_11800.pkl 57 60
[2024-05-07 02:52:38] nn step 11900, lr: 0.08.
	loss_policy_0: 0.11633
	accuracy_policy_0: 0.76217
	loss_value_0: 0.17694
	loss_policy_1: 0.03037
	accuracy_policy_1: 0.70268
	loss_value_1: 0.0368
	loss_reward_1: 0.00679
	loss_policy_2: 0.03362
	accuracy_policy_2: 0.67236
	loss_value_2: 0.03809
	loss_reward_2: 0.00606
	loss_policy_3: 0.03636
	accuracy_policy_3: 0.6509
	loss_value_3: 0.03913
	loss_reward_3: 0.00663
	loss_policy_4: 0.03883
	accuracy_policy_4: 0.62605
	loss_value_4: 0.0403
	loss_reward_4: 0.00732
	loss_policy_5: 0.04103
	accuracy_policy_5: 0.60352
	loss_value_5: 0.04146
	loss_reward_5: 0.00793
	loss_policy: 0.29654
	loss_value: 0.37272
	loss_reward: 0.03473
[2024-05-07 02:53:14] nn step 12000, lr: 0.08.
	loss_policy_0: 0.11318
	accuracy_policy_0: 0.77322
	loss_value_0: 0.17636
	loss_policy_1: 0.03018
	accuracy_policy_1: 0.70566
	loss_value_1: 0.03677
	loss_reward_1: 0.00685
	loss_policy_2: 0.03376
	accuracy_policy_2: 0.67711
	loss_value_2: 0.03791
	loss_reward_2: 0.00619
	loss_policy_3: 0.03667
	accuracy_policy_3: 0.65186
	loss_value_3: 0.03905
	loss_reward_3: 0.00664
	loss_policy_4: 0.03906
	accuracy_policy_4: 0.62627
	loss_value_4: 0.04012
	loss_reward_4: 0.00715
	loss_policy_5: 0.04148
	accuracy_policy_5: 0.60578
	loss_value_5: 0.04129
	loss_reward_5: 0.00813
	loss_policy: 0.29433
	loss_value: 0.37149
	loss_reward: 0.03495
Optimization_Done 12000
[2024-05-07 02:55:14] [command] train weight_iter_12000.pkl 58 61
[2024-05-07 02:55:52] nn step 12100, lr: 0.08.
	loss_policy_0: 0.11651
	accuracy_policy_0: 0.77158
	loss_value_0: 0.17946
	loss_policy_1: 0.03084
	accuracy_policy_1: 0.70746
	loss_value_1: 0.03746
	loss_reward_1: 0.00666
	loss_policy_2: 0.03473
	accuracy_policy_2: 0.67375
	loss_value_2: 0.03869
	loss_reward_2: 0.00605
	loss_policy_3: 0.03759
	accuracy_policy_3: 0.6523
	loss_value_3: 0.03988
	loss_reward_3: 0.00648
	loss_policy_4: 0.03974
	accuracy_policy_4: 0.62605
	loss_value_4: 0.04095
	loss_reward_4: 0.00702
	loss_policy_5: 0.04192
	accuracy_policy_5: 0.60291
	loss_value_5: 0.04205
	loss_reward_5: 0.00792
	loss_policy: 0.30133
	loss_value: 0.37849
	loss_reward: 0.03413
[2024-05-07 02:56:29] nn step 12200, lr: 0.08.
	loss_policy_0: 0.10501
	accuracy_policy_0: 0.78842
	loss_value_0: 0.17174
	loss_policy_1: 0.02921
	accuracy_policy_1: 0.71699
	loss_value_1: 0.03591
	loss_reward_1: 0.00647
	loss_policy_2: 0.03286
	accuracy_policy_2: 0.68398
	loss_value_2: 0.03729
	loss_reward_2: 0.00591
	loss_policy_3: 0.03571
	accuracy_policy_3: 0.66316
	loss_value_3: 0.03839
	loss_reward_3: 0.0063
	loss_policy_4: 0.038
	accuracy_policy_4: 0.64084
	loss_value_4: 0.03938
	loss_reward_4: 0.00695
	loss_policy_5: 0.04002
	accuracy_policy_5: 0.6159
	loss_value_5: 0.04054
	loss_reward_5: 0.00788
	loss_policy: 0.28081
	loss_value: 0.36325
	loss_reward: 0.03352
Optimization_Done 12200
[2024-05-07 02:58:41] [command] train weight_iter_12200.pkl 59 62
[2024-05-07 02:59:18] nn step 12300, lr: 0.08.
	loss_policy_0: 0.11999
	accuracy_policy_0: 0.76279
	loss_value_0: 0.17222
	loss_policy_1: 0.03154
	accuracy_policy_1: 0.69912
	loss_value_1: 0.03585
	loss_reward_1: 0.00642
	loss_policy_2: 0.03515
	accuracy_policy_2: 0.6673
	loss_value_2: 0.03713
	loss_reward_2: 0.00589
	loss_policy_3: 0.03763
	accuracy_policy_3: 0.64395
	loss_value_3: 0.0383
	loss_reward_3: 0.00647
	loss_policy_4: 0.04016
	accuracy_policy_4: 0.61822
	loss_value_4: 0.03928
	loss_reward_4: 0.00672
	loss_policy_5: 0.04212
	accuracy_policy_5: 0.60197
	loss_value_5: 0.0404
	loss_reward_5: 0.00805
	loss_policy: 0.30659
	loss_value: 0.36317
	loss_reward: 0.03355
[2024-05-07 02:59:54] nn step 12400, lr: 0.08.
	loss_policy_0: 0.11082
	accuracy_policy_0: 0.78332
	loss_value_0: 0.17414
	loss_policy_1: 0.03033
	accuracy_policy_1: 0.71883
	loss_value_1: 0.03633
	loss_reward_1: 0.00649
	loss_policy_2: 0.03417
	accuracy_policy_2: 0.68373
	loss_value_2: 0.03759
	loss_reward_2: 0.00618
	loss_policy_3: 0.03693
	accuracy_policy_3: 0.66199
	loss_value_3: 0.03877
	loss_reward_3: 0.00669
	loss_policy_4: 0.0398
	accuracy_policy_4: 0.63697
	loss_value_4: 0.03994
	loss_reward_4: 0.00707
	loss_policy_5: 0.04172
	accuracy_policy_5: 0.61756
	loss_value_5: 0.04116
	loss_reward_5: 0.00838
	loss_policy: 0.29376
	loss_value: 0.36793
	loss_reward: 0.03481
Optimization_Done 12400
[2024-05-07 03:02:06] [command] train weight_iter_12400.pkl 60 63
[2024-05-07 03:02:44] nn step 12500, lr: 0.08.
	loss_policy_0: 0.12106
	accuracy_policy_0: 0.7533
	loss_value_0: 0.16476
	loss_policy_1: 0.03106
	accuracy_policy_1: 0.69549
	loss_value_1: 0.03437
	loss_reward_1: 0.00598
	loss_policy_2: 0.03444
	accuracy_policy_2: 0.6634
	loss_value_2: 0.03575
	loss_reward_2: 0.00571
	loss_policy_3: 0.03663
	accuracy_policy_3: 0.64533
	loss_value_3: 0.03693
	loss_reward_3: 0.00606
	loss_policy_4: 0.03906
	accuracy_policy_4: 0.62219
	loss_value_4: 0.03804
	loss_reward_4: 0.00643
	loss_policy_5: 0.04108
	accuracy_policy_5: 0.60176
	loss_value_5: 0.03925
	loss_reward_5: 0.00764
	loss_policy: 0.30333
	loss_value: 0.34911
	loss_reward: 0.03182
[2024-05-07 03:03:20] nn step 12600, lr: 0.08.
	loss_policy_0: 0.11364
	accuracy_policy_0: 0.77072
	loss_value_0: 0.16489
	loss_policy_1: 0.03064
	accuracy_policy_1: 0.7059
	loss_value_1: 0.03438
	loss_reward_1: 0.00604
	loss_policy_2: 0.03379
	accuracy_policy_2: 0.67785
	loss_value_2: 0.03569
	loss_reward_2: 0.0058
	loss_policy_3: 0.03638
	accuracy_policy_3: 0.65625
	loss_value_3: 0.03699
	loss_reward_3: 0.00605
	loss_policy_4: 0.03877
	accuracy_policy_4: 0.63473
	loss_value_4: 0.03823
	loss_reward_4: 0.00681
	loss_policy_5: 0.04066
	accuracy_policy_5: 0.61797
	loss_value_5: 0.0394
	loss_reward_5: 0.00763
	loss_policy: 0.29387
	loss_value: 0.34958
	loss_reward: 0.03233
Optimization_Done 12600
[2024-05-07 03:05:30] [command] train weight_iter_12600.pkl 61 64
[2024-05-07 03:06:08] nn step 12700, lr: 0.08.
	loss_policy_0: 0.13003
	accuracy_policy_0: 0.73922
	loss_value_0: 0.16754
	loss_policy_1: 0.03307
	accuracy_policy_1: 0.67318
	loss_value_1: 0.03503
	loss_reward_1: 0.00643
	loss_policy_2: 0.03684
	accuracy_policy_2: 0.64242
	loss_value_2: 0.03635
	loss_reward_2: 0.00602
	loss_policy_3: 0.03923
	accuracy_policy_3: 0.62199
	loss_value_3: 0.03747
	loss_reward_3: 0.00662
	loss_policy_4: 0.04141
	accuracy_policy_4: 0.6043
	loss_value_4: 0.03851
	loss_reward_4: 0.00714
	loss_policy_5: 0.04317
	accuracy_policy_5: 0.58996
	loss_value_5: 0.03942
	loss_reward_5: 0.00802
	loss_policy: 0.32376
	loss_value: 0.35431
	loss_reward: 0.03424
[2024-05-07 03:06:45] nn step 12800, lr: 0.08.
	loss_policy_0: 0.12304
	accuracy_policy_0: 0.76039
	loss_value_0: 0.16878
	loss_policy_1: 0.03282
	accuracy_policy_1: 0.68773
	loss_value_1: 0.0353
	loss_reward_1: 0.00652
	loss_policy_2: 0.0362
	accuracy_policy_2: 0.66242
	loss_value_2: 0.03668
	loss_reward_2: 0.00628
	loss_policy_3: 0.0392
	accuracy_policy_3: 0.63826
	loss_value_3: 0.03788
	loss_reward_3: 0.00678
	loss_policy_4: 0.04119
	accuracy_policy_4: 0.62201
	loss_value_4: 0.03882
	loss_reward_4: 0.00721
	loss_policy_5: 0.04336
	accuracy_policy_5: 0.60094
	loss_value_5: 0.0399
	loss_reward_5: 0.00827
	loss_policy: 0.31582
	loss_value: 0.35734
	loss_reward: 0.03506
Optimization_Done 12800
[2024-05-07 03:08:33] [command] train weight_iter_12800.pkl 62 65
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 03:26:32] [command] train weight_iter_12800.pkl 63 65
[2024-05-07 03:27:18] nn step 12900, lr: 0.06.
	loss_policy_0: 0.14024
	accuracy_policy_0: 0.73379
	loss_value_0: 0.18565
	loss_policy_1: 0.037
	accuracy_policy_1: 0.66006
	loss_value_1: 0.03877
	loss_reward_1: 0.00691
	loss_policy_2: 0.04102
	accuracy_policy_2: 0.62953
	loss_value_2: 0.04037
	loss_reward_2: 0.00656
	loss_policy_3: 0.04483
	accuracy_policy_3: 0.59729
	loss_value_3: 0.0417
	loss_reward_3: 0.00718
	loss_policy_4: 0.04713
	accuracy_policy_4: 0.58213
	loss_value_4: 0.04284
	loss_reward_4: 0.0076
	loss_policy_5: 0.04957
	accuracy_policy_5: 0.55689
	loss_value_5: 0.04394
	loss_reward_5: 0.00884
	loss_policy: 0.35979
	loss_value: 0.39326
	loss_reward: 0.03709
[2024-05-07 03:27:53] nn step 13000, lr: 0.06.
	loss_policy_0: 0.12022
	accuracy_policy_0: 0.76318
	loss_value_0: 0.17543
	loss_policy_1: 0.0334
	accuracy_policy_1: 0.68227
	loss_value_1: 0.03693
	loss_reward_1: 0.00685
	loss_policy_2: 0.03731
	accuracy_policy_2: 0.65312
	loss_value_2: 0.03839
	loss_reward_2: 0.00647
	loss_policy_3: 0.04106
	accuracy_policy_3: 0.62316
	loss_value_3: 0.03962
	loss_reward_3: 0.00696
	loss_policy_4: 0.04347
	accuracy_policy_4: 0.60297
	loss_value_4: 0.04079
	loss_reward_4: 0.00742
	loss_policy_5: 0.04601
	accuracy_policy_5: 0.58361
	loss_value_5: 0.0418
	loss_reward_5: 0.00841
	loss_policy: 0.32147
	loss_value: 0.37296
	loss_reward: 0.03611
Optimization_Done 13000
[2024-05-07 03:30:09] [command] train weight_iter_13000.pkl 64 66
[2024-05-07 03:30:46] nn step 13100, lr: 0.06.
	loss_policy_0: 0.13587
	accuracy_policy_0: 0.72934
	loss_value_0: 0.1728
	loss_policy_1: 0.03615
	accuracy_policy_1: 0.65104
	loss_value_1: 0.03621
	loss_reward_1: 0.00635
	loss_policy_2: 0.04024
	accuracy_policy_2: 0.61986
	loss_value_2: 0.03763
	loss_reward_2: 0.00624
	loss_policy_3: 0.0436
	accuracy_policy_3: 0.59703
	loss_value_3: 0.03886
	loss_reward_3: 0.00667
	loss_policy_4: 0.04627
	accuracy_policy_4: 0.57779
	loss_value_4: 0.03999
	loss_reward_4: 0.00708
	loss_policy_5: 0.04841
	accuracy_policy_5: 0.5582
	loss_value_5: 0.04106
	loss_reward_5: 0.00819
	loss_policy: 0.35054
	loss_value: 0.36656
	loss_reward: 0.03454
[2024-05-07 03:31:21] nn step 13200, lr: 0.06.
	loss_policy_0: 0.11951
	accuracy_policy_0: 0.75811
	loss_value_0: 0.17124
	loss_policy_1: 0.03379
	accuracy_policy_1: 0.67369
	loss_value_1: 0.0357
	loss_reward_1: 0.00631
	loss_policy_2: 0.03776
	accuracy_policy_2: 0.64014
	loss_value_2: 0.03721
	loss_reward_2: 0.00613
	loss_policy_3: 0.04142
	accuracy_policy_3: 0.61324
	loss_value_3: 0.03838
	loss_reward_3: 0.00652
	loss_policy_4: 0.04379
	accuracy_policy_4: 0.59938
	loss_value_4: 0.03936
	loss_reward_4: 0.0071
	loss_policy_5: 0.04669
	accuracy_policy_5: 0.57574
	loss_value_5: 0.04049
	loss_reward_5: 0.00815
	loss_policy: 0.32296
	loss_value: 0.36239
	loss_reward: 0.0342
Optimization_Done 13200
[2024-05-07 03:33:34] [command] train weight_iter_13200.pkl 65 67
[2024-05-07 03:34:11] nn step 13300, lr: 0.06.
	loss_policy_0: 0.13256
	accuracy_policy_0: 0.72561
	loss_value_0: 0.16067
	loss_policy_1: 0.03522
	accuracy_policy_1: 0.65492
	loss_value_1: 0.03372
	loss_reward_1: 0.00582
	loss_policy_2: 0.03857
	accuracy_policy_2: 0.63051
	loss_value_2: 0.03507
	loss_reward_2: 0.00548
	loss_policy_3: 0.04172
	accuracy_policy_3: 0.61078
	loss_value_3: 0.03627
	loss_reward_3: 0.00616
	loss_policy_4: 0.04425
	accuracy_policy_4: 0.59475
	loss_value_4: 0.03747
	loss_reward_4: 0.00645
	loss_policy_5: 0.04661
	accuracy_policy_5: 0.56936
	loss_value_5: 0.0386
	loss_reward_5: 0.00753
	loss_policy: 0.33893
	loss_value: 0.3418
	loss_reward: 0.03145
[2024-05-07 03:34:48] nn step 13400, lr: 0.06.
	loss_policy_0: 0.12265
	accuracy_policy_0: 0.7543
	loss_value_0: 0.16767
	loss_policy_1: 0.03434
	accuracy_policy_1: 0.67322
	loss_value_1: 0.03509
	loss_reward_1: 0.00604
	loss_policy_2: 0.03813
	accuracy_policy_2: 0.64592
	loss_value_2: 0.03664
	loss_reward_2: 0.0057
	loss_policy_3: 0.04144
	accuracy_policy_3: 0.62676
	loss_value_3: 0.03786
	loss_reward_3: 0.00622
	loss_policy_4: 0.04438
	accuracy_policy_4: 0.60592
	loss_value_4: 0.03899
	loss_reward_4: 0.00654
	loss_policy_5: 0.04707
	accuracy_policy_5: 0.58287
	loss_value_5: 0.04011
	loss_reward_5: 0.00784
	loss_policy: 0.32801
	loss_value: 0.35636
	loss_reward: 0.03234
Optimization_Done 13400
[2024-05-07 03:36:59] [command] train weight_iter_13400.pkl 66 68
[2024-05-07 03:37:36] nn step 13500, lr: 0.06.
	loss_policy_0: 0.13928
	accuracy_policy_0: 0.7359
	loss_value_0: 0.16769
	loss_policy_1: 0.03696
	accuracy_policy_1: 0.66844
	loss_value_1: 0.03517
	loss_reward_1: 0.00553
	loss_policy_2: 0.04071
	accuracy_policy_2: 0.6391
	loss_value_2: 0.0366
	loss_reward_2: 0.0049
	loss_policy_3: 0.04374
	accuracy_policy_3: 0.61838
	loss_value_3: 0.03796
	loss_reward_3: 0.00546
	loss_policy_4: 0.04657
	accuracy_policy_4: 0.59951
	loss_value_4: 0.03925
	loss_reward_4: 0.00581
	loss_policy_5: 0.04868
	accuracy_policy_5: 0.58137
	loss_value_5: 0.04036
	loss_reward_5: 0.00671
	loss_policy: 0.35593
	loss_value: 0.35703
	loss_reward: 0.0284
[2024-05-07 03:38:11] nn step 13600, lr: 0.06.
	loss_policy_0: 0.12452
	accuracy_policy_0: 0.7591
	loss_value_0: 0.16667
	loss_policy_1: 0.0348
	accuracy_policy_1: 0.68348
	loss_value_1: 0.03515
	loss_reward_1: 0.0055
	loss_policy_2: 0.03885
	accuracy_policy_2: 0.65479
	loss_value_2: 0.03639
	loss_reward_2: 0.00517
	loss_policy_3: 0.04196
	accuracy_policy_3: 0.63363
	loss_value_3: 0.03768
	loss_reward_3: 0.00558
	loss_policy_4: 0.04465
	accuracy_policy_4: 0.61
	loss_value_4: 0.03888
	loss_reward_4: 0.00594
	loss_policy_5: 0.04691
	accuracy_policy_5: 0.59289
	loss_value_5: 0.04002
	loss_reward_5: 0.00692
	loss_policy: 0.3317
	loss_value: 0.35479
	loss_reward: 0.0291
Optimization_Done 13600
[2024-05-07 03:40:24] [command] train weight_iter_13600.pkl 67 69
[2024-05-07 03:41:01] nn step 13700, lr: 0.06.
	loss_policy_0: 0.15411
	accuracy_policy_0: 0.70258
	loss_value_0: 0.16498
	loss_policy_1: 0.03981
	accuracy_policy_1: 0.6318
	loss_value_1: 0.03477
	loss_reward_1: 0.00517
	loss_policy_2: 0.04368
	accuracy_policy_2: 0.6042
	loss_value_2: 0.03627
	loss_reward_2: 0.00494
	loss_policy_3: 0.04693
	accuracy_policy_3: 0.58207
	loss_value_3: 0.03781
	loss_reward_3: 0.00536
	loss_policy_4: 0.04928
	accuracy_policy_4: 0.56271
	loss_value_4: 0.03904
	loss_reward_4: 0.00565
	loss_policy_5: 0.05174
	accuracy_policy_5: 0.54514
	loss_value_5: 0.0401
	loss_reward_5: 0.00677
	loss_policy: 0.38555
	loss_value: 0.35296
	loss_reward: 0.0279
[2024-05-07 03:41:38] nn step 13800, lr: 0.06.
	loss_policy_0: 0.13728
	accuracy_policy_0: 0.73336
	loss_value_0: 0.16395
	loss_policy_1: 0.03656
	accuracy_policy_1: 0.6583
	loss_value_1: 0.03468
	loss_reward_1: 0.00531
	loss_policy_2: 0.04113
	accuracy_policy_2: 0.62873
	loss_value_2: 0.03611
	loss_reward_2: 0.005
	loss_policy_3: 0.04408
	accuracy_policy_3: 0.61066
	loss_value_3: 0.03742
	loss_reward_3: 0.0053
	loss_policy_4: 0.04661
	accuracy_policy_4: 0.59297
	loss_value_4: 0.03876
	loss_reward_4: 0.00565
	loss_policy_5: 0.04904
	accuracy_policy_5: 0.5698
	loss_value_5: 0.03994
	loss_reward_5: 0.00662
	loss_policy: 0.35471
	loss_value: 0.35087
	loss_reward: 0.02787
Optimization_Done 13800
[2024-05-07 03:43:47] [command] train weight_iter_13800.pkl 68 70
[2024-05-07 03:44:25] nn step 13900, lr: 0.06.
	loss_policy_0: 0.17507
	accuracy_policy_0: 0.66469
	loss_value_0: 0.17299
	loss_policy_1: 0.04264
	accuracy_policy_1: 0.60365
	loss_value_1: 0.0363
	loss_reward_1: 0.00585
	loss_policy_2: 0.04603
	accuracy_policy_2: 0.58121
	loss_value_2: 0.03772
	loss_reward_2: 0.00537
	loss_policy_3: 0.04963
	accuracy_policy_3: 0.55354
	loss_value_3: 0.03907
	loss_reward_3: 0.00553
	loss_policy_4: 0.05204
	accuracy_policy_4: 0.5376
	loss_value_4: 0.04039
	loss_reward_4: 0.00583
	loss_policy_5: 0.0547
	accuracy_policy_5: 0.51783
	loss_value_5: 0.04159
	loss_reward_5: 0.00651
	loss_policy: 0.42011
	loss_value: 0.36806
	loss_reward: 0.0291
[2024-05-07 03:45:01] nn step 14000, lr: 0.06.
	loss_policy_0: 0.14771
	accuracy_policy_0: 0.69939
	loss_value_0: 0.16467
	loss_policy_1: 0.03843
	accuracy_policy_1: 0.62887
	loss_value_1: 0.03453
	loss_reward_1: 0.00553
	loss_policy_2: 0.04171
	accuracy_policy_2: 0.60359
	loss_value_2: 0.0359
	loss_reward_2: 0.00506
	loss_policy_3: 0.04475
	accuracy_policy_3: 0.58357
	loss_value_3: 0.03715
	loss_reward_3: 0.0052
	loss_policy_4: 0.0476
	accuracy_policy_4: 0.56437
	loss_value_4: 0.03851
	loss_reward_4: 0.00562
	loss_policy_5: 0.04983
	accuracy_policy_5: 0.54926
	loss_value_5: 0.03979
	loss_reward_5: 0.00633
	loss_policy: 0.37002
	loss_value: 0.35055
	loss_reward: 0.02773
Optimization_Done 14000
[2024-05-07 03:47:10] [command] train weight_iter_14000.pkl 69 71
[2024-05-07 03:47:47] nn step 14100, lr: 0.06.
	loss_policy_0: 0.19185
	accuracy_policy_0: 0.62453
	loss_value_0: 0.16674
	loss_policy_1: 0.04517
	accuracy_policy_1: 0.56617
	loss_value_1: 0.03502
	loss_reward_1: 0.00623
	loss_policy_2: 0.0483
	accuracy_policy_2: 0.54336
	loss_value_2: 0.03655
	loss_reward_2: 0.00571
	loss_policy_3: 0.05136
	accuracy_policy_3: 0.52494
	loss_value_3: 0.03816
	loss_reward_3: 0.00615
	loss_policy_4: 0.05396
	accuracy_policy_4: 0.50221
	loss_value_4: 0.03962
	loss_reward_4: 0.0065
	loss_policy_5: 0.05644
	accuracy_policy_5: 0.48488
	loss_value_5: 0.04099
	loss_reward_5: 0.0073
	loss_policy: 0.44708
	loss_value: 0.35709
	loss_reward: 0.0319
[2024-05-07 03:48:24] nn step 14200, lr: 0.06.
	loss_policy_0: 0.16152
	accuracy_policy_0: 0.67219
	loss_value_0: 0.15884
	loss_policy_1: 0.04052
	accuracy_policy_1: 0.5999
	loss_value_1: 0.03337
	loss_reward_1: 0.00598
	loss_policy_2: 0.04412
	accuracy_policy_2: 0.57301
	loss_value_2: 0.03473
	loss_reward_2: 0.0057
	loss_policy_3: 0.04663
	accuracy_policy_3: 0.55268
	loss_value_3: 0.03606
	loss_reward_3: 0.00599
	loss_policy_4: 0.04917
	accuracy_policy_4: 0.53182
	loss_value_4: 0.03738
	loss_reward_4: 0.00624
	loss_policy_5: 0.05185
	accuracy_policy_5: 0.51385
	loss_value_5: 0.03862
	loss_reward_5: 0.00715
	loss_policy: 0.39381
	loss_value: 0.339
	loss_reward: 0.03105
Optimization_Done 14200
[2024-05-07 03:50:33] [command] train weight_iter_14200.pkl 70 72
[2024-05-07 03:51:11] nn step 14300, lr: 0.06.
	loss_policy_0: 0.18116
	accuracy_policy_0: 0.65117
	loss_value_0: 0.1669
	loss_policy_1: 0.04475
	accuracy_policy_1: 0.57713
	loss_value_1: 0.03502
	loss_reward_1: 0.00657
	loss_policy_2: 0.0485
	accuracy_policy_2: 0.55168
	loss_value_2: 0.03636
	loss_reward_2: 0.00626
	loss_policy_3: 0.05116
	accuracy_policy_3: 0.53256
	loss_value_3: 0.03775
	loss_reward_3: 0.00654
	loss_policy_4: 0.05385
	accuracy_policy_4: 0.51621
	loss_value_4: 0.03904
	loss_reward_4: 0.00697
	loss_policy_5: 0.05612
	accuracy_policy_5: 0.49979
	loss_value_5: 0.04021
	loss_reward_5: 0.00776
	loss_policy: 0.43554
	loss_value: 0.35528
	loss_reward: 0.0341
[2024-05-07 03:51:47] nn step 14400, lr: 0.06.
	loss_policy_0: 0.17347
	accuracy_policy_0: 0.67646
	loss_value_0: 0.17763
	loss_policy_1: 0.04454
	accuracy_policy_1: 0.59918
	loss_value_1: 0.03729
	loss_reward_1: 0.00696
	loss_policy_2: 0.0485
	accuracy_policy_2: 0.56871
	loss_value_2: 0.03882
	loss_reward_2: 0.00645
	loss_policy_3: 0.05144
	accuracy_policy_3: 0.55135
	loss_value_3: 0.04014
	loss_reward_3: 0.00677
	loss_policy_4: 0.05417
	accuracy_policy_4: 0.53246
	loss_value_4: 0.04137
	loss_reward_4: 0.0073
	loss_policy_5: 0.05668
	accuracy_policy_5: 0.51561
	loss_value_5: 0.04267
	loss_reward_5: 0.00814
	loss_policy: 0.42879
	loss_value: 0.37791
	loss_reward: 0.03562
Optimization_Done 14400
[2024-05-07 03:53:36] [command] train weight_iter_14400.pkl 71 73
[2024-05-07 03:54:14] nn step 14500, lr: 0.06.
	loss_policy_0: 0.18199
	accuracy_policy_0: 0.65451
	loss_value_0: 0.17549
	loss_policy_1: 0.04628
	accuracy_policy_1: 0.57033
	loss_value_1: 0.03663
	loss_reward_1: 0.0064
	loss_policy_2: 0.04976
	accuracy_policy_2: 0.5443
	loss_value_2: 0.03807
	loss_reward_2: 0.00581
	loss_policy_3: 0.05347
	accuracy_policy_3: 0.51125
	loss_value_3: 0.03918
	loss_reward_3: 0.00629
	loss_policy_4: 0.05591
	accuracy_policy_4: 0.49746
	loss_value_4: 0.04023
	loss_reward_4: 0.00674
	loss_policy_5: 0.05819
	accuracy_policy_5: 0.474
	loss_value_5: 0.04102
	loss_reward_5: 0.00771
	loss_policy: 0.4456
	loss_value: 0.37062
	loss_reward: 0.03295
[2024-05-07 03:54:50] nn step 14600, lr: 0.06.
	loss_policy_0: 0.1701
	accuracy_policy_0: 0.68377
	loss_value_0: 0.17872
	loss_policy_1: 0.04504
	accuracy_policy_1: 0.59455
	loss_value_1: 0.03741
	loss_reward_1: 0.00636
	loss_policy_2: 0.04862
	accuracy_policy_2: 0.56447
	loss_value_2: 0.03874
	loss_reward_2: 0.00604
	loss_policy_3: 0.05207
	accuracy_policy_3: 0.53859
	loss_value_3: 0.03995
	loss_reward_3: 0.00659
	loss_policy_4: 0.05462
	accuracy_policy_4: 0.51955
	loss_value_4: 0.0411
	loss_reward_4: 0.00698
	loss_policy_5: 0.05787
	accuracy_policy_5: 0.49455
	loss_value_5: 0.04226
	loss_reward_5: 0.00804
	loss_policy: 0.42832
	loss_value: 0.37819
	loss_reward: 0.03401
Optimization_Done 14600
[2024-05-07 03:57:00] [command] train weight_iter_14600.pkl 72 74
[2024-05-07 03:57:38] nn step 14700, lr: 0.06.
	loss_policy_0: 0.17082
	accuracy_policy_0: 0.6682
	loss_value_0: 0.17251
	loss_policy_1: 0.04372
	accuracy_policy_1: 0.595
	loss_value_1: 0.03599
	loss_reward_1: 0.0057
	loss_policy_2: 0.04807
	accuracy_policy_2: 0.55836
	loss_value_2: 0.03733
	loss_reward_2: 0.00547
	loss_policy_3: 0.05152
	accuracy_policy_3: 0.53768
	loss_value_3: 0.03866
	loss_reward_3: 0.00593
	loss_policy_4: 0.05426
	accuracy_policy_4: 0.51334
	loss_value_4: 0.0396
	loss_reward_4: 0.0062
	loss_policy_5: 0.05684
	accuracy_policy_5: 0.49719
	loss_value_5: 0.04077
	loss_reward_5: 0.00729
	loss_policy: 0.42522
	loss_value: 0.36485
	loss_reward: 0.03058
[2024-05-07 03:58:14] nn step 14800, lr: 0.06.
	loss_policy_0: 0.15419
	accuracy_policy_0: 0.69943
	loss_value_0: 0.16869
	loss_policy_1: 0.04103
	accuracy_policy_1: 0.61785
	loss_value_1: 0.03524
	loss_reward_1: 0.00585
	loss_policy_2: 0.04519
	accuracy_policy_2: 0.58414
	loss_value_2: 0.03674
	loss_reward_2: 0.0055
	loss_policy_3: 0.04829
	accuracy_policy_3: 0.56574
	loss_value_3: 0.038
	loss_reward_3: 0.00585
	loss_policy_4: 0.05152
	accuracy_policy_4: 0.54318
	loss_value_4: 0.03903
	loss_reward_4: 0.00634
	loss_policy_5: 0.0542
	accuracy_policy_5: 0.51748
	loss_value_5: 0.04012
	loss_reward_5: 0.00711
	loss_policy: 0.39441
	loss_value: 0.35782
	loss_reward: 0.03065
Optimization_Done 14800
[2024-05-07 04:00:05] [command] train weight_iter_14800.pkl 73 75
[2024-05-07 04:00:43] nn step 14900, lr: 0.06.
	loss_policy_0: 0.17461
	accuracy_policy_0: 0.66967
	loss_value_0: 0.16221
	loss_policy_1: 0.04326
	accuracy_policy_1: 0.59949
	loss_value_1: 0.03414
	loss_reward_1: 0.00532
	loss_policy_2: 0.04722
	accuracy_policy_2: 0.56736
	loss_value_2: 0.0356
	loss_reward_2: 0.00512
	loss_policy_3: 0.05061
	accuracy_policy_3: 0.54713
	loss_value_3: 0.03697
	loss_reward_3: 0.00541
	loss_policy_4: 0.05363
	accuracy_policy_4: 0.52549
	loss_value_4: 0.03805
	loss_reward_4: 0.00581
	loss_policy_5: 0.05647
	accuracy_policy_5: 0.50127
	loss_value_5: 0.0391
	loss_reward_5: 0.00672
	loss_policy: 0.42581
	loss_value: 0.34606
	loss_reward: 0.02839
[2024-05-07 04:01:19] nn step 15000, lr: 0.06.
	loss_policy_0: 0.15537
	accuracy_policy_0: 0.70139
	loss_value_0: 0.15793
	loss_policy_1: 0.03967
	accuracy_policy_1: 0.62785
	loss_value_1: 0.03313
	loss_reward_1: 0.00536
	loss_policy_2: 0.04394
	accuracy_policy_2: 0.59352
	loss_value_2: 0.03458
	loss_reward_2: 0.00506
	loss_policy_3: 0.04755
	accuracy_policy_3: 0.57014
	loss_value_3: 0.03577
	loss_reward_3: 0.00541
	loss_policy_4: 0.05062
	accuracy_policy_4: 0.5468
	loss_value_4: 0.03688
	loss_reward_4: 0.00585
	loss_policy_5: 0.05369
	accuracy_policy_5: 0.52105
	loss_value_5: 0.03809
	loss_reward_5: 0.00662
	loss_policy: 0.39085
	loss_value: 0.33639
	loss_reward: 0.02831
Optimization_Done 15000
[2024-05-07 04:03:19] [command] train weight_iter_15000.pkl 74 76
[2024-05-07 04:03:56] nn step 15100, lr: 0.06.
	loss_policy_0: 0.1769
	accuracy_policy_0: 0.65021
	loss_value_0: 0.16309
	loss_policy_1: 0.04383
	accuracy_policy_1: 0.57916
	loss_value_1: 0.03418
	loss_reward_1: 0.00599
	loss_policy_2: 0.04793
	accuracy_policy_2: 0.54783
	loss_value_2: 0.03573
	loss_reward_2: 0.00564
	loss_policy_3: 0.05112
	accuracy_policy_3: 0.5259
	loss_value_3: 0.03733
	loss_reward_3: 0.00622
	loss_policy_4: 0.05439
	accuracy_policy_4: 0.50385
	loss_value_4: 0.03862
	loss_reward_4: 0.00679
	loss_policy_5: 0.05721
	accuracy_policy_5: 0.4816
	loss_value_5: 0.03987
	loss_reward_5: 0.00757
	loss_policy: 0.43137
	loss_value: 0.34881
	loss_reward: 0.03222
[2024-05-07 04:04:31] nn step 15200, lr: 0.06.
	loss_policy_0: 0.16026
	accuracy_policy_0: 0.68799
	loss_value_0: 0.16564
	loss_policy_1: 0.04179
	accuracy_policy_1: 0.60984
	loss_value_1: 0.03483
	loss_reward_1: 0.00618
	loss_policy_2: 0.04579
	accuracy_policy_2: 0.57637
	loss_value_2: 0.03626
	loss_reward_2: 0.00579
	loss_policy_3: 0.0491
	accuracy_policy_3: 0.55857
	loss_value_3: 0.03761
	loss_reward_3: 0.00625
	loss_policy_4: 0.05249
	accuracy_policy_4: 0.5333
	loss_value_4: 0.03907
	loss_reward_4: 0.00665
	loss_policy_5: 0.0559
	accuracy_policy_5: 0.5074
	loss_value_5: 0.04027
	loss_reward_5: 0.00789
	loss_policy: 0.40534
	loss_value: 0.35368
	loss_reward: 0.03277
Optimization_Done 15200
[2024-05-07 04:06:42] [command] train weight_iter_15200.pkl 75 77
[2024-05-07 04:07:19] nn step 15300, lr: 0.06.
	loss_policy_0: 0.1652
	accuracy_policy_0: 0.67244
	loss_value_0: 0.16249
	loss_policy_1: 0.04214
	accuracy_policy_1: 0.59318
	loss_value_1: 0.0343
	loss_reward_1: 0.00542
	loss_policy_2: 0.04569
	accuracy_policy_2: 0.56414
	loss_value_2: 0.0359
	loss_reward_2: 0.00517
	loss_policy_3: 0.04923
	accuracy_policy_3: 0.54207
	loss_value_3: 0.03732
	loss_reward_3: 0.0055
	loss_policy_4: 0.05205
	accuracy_policy_4: 0.52037
	loss_value_4: 0.03867
	loss_reward_4: 0.00587
	loss_policy_5: 0.05477
	accuracy_policy_5: 0.49643
	loss_value_5: 0.03993
	loss_reward_5: 0.00678
	loss_policy: 0.40907
	loss_value: 0.34862
	loss_reward: 0.02875
[2024-05-07 04:07:55] nn step 15400, lr: 0.06.
	loss_policy_0: 0.15076
	accuracy_policy_0: 0.69863
	loss_value_0: 0.15925
	loss_policy_1: 0.03965
	accuracy_policy_1: 0.61693
	loss_value_1: 0.03349
	loss_reward_1: 0.00537
	loss_policy_2: 0.04388
	accuracy_policy_2: 0.58135
	loss_value_2: 0.03516
	loss_reward_2: 0.00513
	loss_policy_3: 0.04691
	accuracy_policy_3: 0.55965
	loss_value_3: 0.03667
	loss_reward_3: 0.00548
	loss_policy_4: 0.04997
	accuracy_policy_4: 0.53664
	loss_value_4: 0.03785
	loss_reward_4: 0.00601
	loss_policy_5: 0.05286
	accuracy_policy_5: 0.51396
	loss_value_5: 0.03927
	loss_reward_5: 0.00688
	loss_policy: 0.38402
	loss_value: 0.34168
	loss_reward: 0.02887
Optimization_Done 15400
[2024-05-07 04:10:07] [command] train weight_iter_15400.pkl 76 78
[2024-05-07 04:10:44] nn step 15500, lr: 0.06.
	loss_policy_0: 0.16724
	accuracy_policy_0: 0.65154
	loss_value_0: 0.14737
	loss_policy_1: 0.04142
	accuracy_policy_1: 0.58119
	loss_value_1: 0.03081
	loss_reward_1: 0.0055
	loss_policy_2: 0.045
	accuracy_policy_2: 0.55004
	loss_value_2: 0.03238
	loss_reward_2: 0.00512
	loss_policy_3: 0.04803
	accuracy_policy_3: 0.52859
	loss_value_3: 0.03357
	loss_reward_3: 0.00562
	loss_policy_4: 0.05088
	accuracy_policy_4: 0.5018
	loss_value_4: 0.03483
	loss_reward_4: 0.00594
	loss_policy_5: 0.05334
	accuracy_policy_5: 0.48412
	loss_value_5: 0.03598
	loss_reward_5: 0.00693
	loss_policy: 0.40592
	loss_value: 0.31494
	loss_reward: 0.02912
[2024-05-07 04:11:20] nn step 15600, lr: 0.06.
	loss_policy_0: 0.15321
	accuracy_policy_0: 0.69346
	loss_value_0: 0.15406
	loss_policy_1: 0.04106
	accuracy_policy_1: 0.6048
	loss_value_1: 0.03247
	loss_reward_1: 0.0058
	loss_policy_2: 0.04508
	accuracy_policy_2: 0.57316
	loss_value_2: 0.03403
	loss_reward_2: 0.00538
	loss_policy_3: 0.04814
	accuracy_policy_3: 0.55504
	loss_value_3: 0.03523
	loss_reward_3: 0.00585
	loss_policy_4: 0.0512
	accuracy_policy_4: 0.52816
	loss_value_4: 0.03648
	loss_reward_4: 0.00634
	loss_policy_5: 0.05429
	accuracy_policy_5: 0.50299
	loss_value_5: 0.03775
	loss_reward_5: 0.00742
	loss_policy: 0.39298
	loss_value: 0.33002
	loss_reward: 0.03079
Optimization_Done 15600
[2024-05-07 04:13:31] [command] train weight_iter_15600.pkl 77 79
[2024-05-07 04:14:09] nn step 15700, lr: 0.06.
	loss_policy_0: 0.17496
	accuracy_policy_0: 0.63945
	loss_value_0: 0.15392
	loss_policy_1: 0.0421
	accuracy_policy_1: 0.57563
	loss_value_1: 0.03231
	loss_reward_1: 0.0057
	loss_policy_2: 0.04489
	accuracy_policy_2: 0.55006
	loss_value_2: 0.03388
	loss_reward_2: 0.00529
	loss_policy_3: 0.04777
	accuracy_policy_3: 0.5283
	loss_value_3: 0.03528
	loss_reward_3: 0.00553
	loss_policy_4: 0.05066
	accuracy_policy_4: 0.50988
	loss_value_4: 0.03646
	loss_reward_4: 0.00606
	loss_policy_5: 0.05314
	accuracy_policy_5: 0.48219
	loss_value_5: 0.03756
	loss_reward_5: 0.00674
	loss_policy: 0.41353
	loss_value: 0.32941
	loss_reward: 0.02932
[2024-05-07 04:14:46] nn step 15800, lr: 0.06.
	loss_policy_0: 0.14753
	accuracy_policy_0: 0.67832
	loss_value_0: 0.14808
	loss_policy_1: 0.03742
	accuracy_policy_1: 0.60652
	loss_value_1: 0.03111
	loss_reward_1: 0.00532
	loss_policy_2: 0.04072
	accuracy_policy_2: 0.57648
	loss_value_2: 0.03246
	loss_reward_2: 0.00499
	loss_policy_3: 0.04371
	accuracy_policy_3: 0.55346
	loss_value_3: 0.03384
	loss_reward_3: 0.00538
	loss_policy_4: 0.04624
	accuracy_policy_4: 0.53385
	loss_value_4: 0.03487
	loss_reward_4: 0.0057
	loss_policy_5: 0.04869
	accuracy_policy_5: 0.51166
	loss_value_5: 0.03594
	loss_reward_5: 0.00652
	loss_policy: 0.36431
	loss_value: 0.3163
	loss_reward: 0.0279
Optimization_Done 15800
[2024-05-07 04:16:57] [command] train weight_iter_15800.pkl 78 80
[2024-05-07 04:17:35] nn step 15900, lr: 0.06.
	loss_policy_0: 0.17007
	accuracy_policy_0: 0.66281
	loss_value_0: 0.17096
	loss_policy_1: 0.04297
	accuracy_policy_1: 0.58812
	loss_value_1: 0.03576
	loss_reward_1: 0.0069
	loss_policy_2: 0.04648
	accuracy_policy_2: 0.56307
	loss_value_2: 0.03721
	loss_reward_2: 0.00633
	loss_policy_3: 0.04971
	accuracy_policy_3: 0.53992
	loss_value_3: 0.03859
	loss_reward_3: 0.00698
	loss_policy_4: 0.05264
	accuracy_policy_4: 0.51871
	loss_value_4: 0.03978
	loss_reward_4: 0.0075
	loss_policy_5: 0.05526
	accuracy_policy_5: 0.49453
	loss_value_5: 0.04086
	loss_reward_5: 0.00845
	loss_policy: 0.41714
	loss_value: 0.36316
	loss_reward: 0.03616
[2024-05-07 04:18:11] nn step 16000, lr: 0.06.
	loss_policy_0: 0.16713
	accuracy_policy_0: 0.68949
	loss_value_0: 0.18425
	loss_policy_1: 0.04412
	accuracy_policy_1: 0.60561
	loss_value_1: 0.03858
	loss_reward_1: 0.00731
	loss_policy_2: 0.04793
	accuracy_policy_2: 0.57859
	loss_value_2: 0.04003
	loss_reward_2: 0.00686
	loss_policy_3: 0.05165
	accuracy_policy_3: 0.55396
	loss_value_3: 0.04145
	loss_reward_3: 0.00729
	loss_policy_4: 0.05489
	accuracy_policy_4: 0.52928
	loss_value_4: 0.04293
	loss_reward_4: 0.00791
	loss_policy_5: 0.05755
	accuracy_policy_5: 0.50785
	loss_value_5: 0.04434
	loss_reward_5: 0.00905
	loss_policy: 0.42327
	loss_value: 0.39159
	loss_reward: 0.03841
Optimization_Done 16000
[2024-05-07 04:20:20] [command] train weight_iter_16000.pkl 79 81
[2024-05-07 04:20:58] nn step 16100, lr: 0.06.
	loss_policy_0: 0.17078
	accuracy_policy_0: 0.6599
	loss_value_0: 0.1751
	loss_policy_1: 0.04398
	accuracy_policy_1: 0.58094
	loss_value_1: 0.03665
	loss_reward_1: 0.00681
	loss_policy_2: 0.04774
	accuracy_policy_2: 0.54762
	loss_value_2: 0.0382
	loss_reward_2: 0.00646
	loss_policy_3: 0.05084
	accuracy_policy_3: 0.52463
	loss_value_3: 0.03961
	loss_reward_3: 0.00681
	loss_policy_4: 0.05376
	accuracy_policy_4: 0.50002
	loss_value_4: 0.0409
	loss_reward_4: 0.00737
	loss_policy_5: 0.05646
	accuracy_policy_5: 0.48379
	loss_value_5: 0.04194
	loss_reward_5: 0.00852
	loss_policy: 0.42356
	loss_value: 0.3724
	loss_reward: 0.03597
[2024-05-07 04:21:34] nn step 16200, lr: 0.06.
	loss_policy_0: 0.15707
	accuracy_policy_0: 0.69441
	loss_value_0: 0.18081
	loss_policy_1: 0.04246
	accuracy_policy_1: 0.60467
	loss_value_1: 0.03811
	loss_reward_1: 0.00705
	loss_policy_2: 0.04663
	accuracy_policy_2: 0.57432
	loss_value_2: 0.03967
	loss_reward_2: 0.00648
	loss_policy_3: 0.04958
	accuracy_policy_3: 0.55264
	loss_value_3: 0.04099
	loss_reward_3: 0.00702
	loss_policy_4: 0.05302
	accuracy_policy_4: 0.52361
	loss_value_4: 0.04233
	loss_reward_4: 0.00751
	loss_policy_5: 0.0554
	accuracy_policy_5: 0.50789
	loss_value_5: 0.0434
	loss_reward_5: 0.00869
	loss_policy: 0.40415
	loss_value: 0.3853
	loss_reward: 0.03675
Optimization_Done 16200
[2024-05-07 04:23:24] [command] train weight_iter_16200.pkl 80 82
[2024-05-07 04:24:01] nn step 16300, lr: 0.06.
	loss_policy_0: 0.17586
	accuracy_policy_0: 0.659
	loss_value_0: 0.17528
	loss_policy_1: 0.04495
	accuracy_policy_1: 0.58539
	loss_value_1: 0.03677
	loss_reward_1: 0.00635
	loss_policy_2: 0.04911
	accuracy_policy_2: 0.5491
	loss_value_2: 0.03814
	loss_reward_2: 0.0058
	loss_policy_3: 0.0524
	accuracy_policy_3: 0.52471
	loss_value_3: 0.03946
	loss_reward_3: 0.00648
	loss_policy_4: 0.05568
	accuracy_policy_4: 0.49773
	loss_value_4: 0.04055
	loss_reward_4: 0.00676
	loss_policy_5: 0.05825
	accuracy_policy_5: 0.47477
	loss_value_5: 0.0416
	loss_reward_5: 0.00787
	loss_policy: 0.43626
	loss_value: 0.3718
	loss_reward: 0.03325
[2024-05-07 04:24:37] nn step 16400, lr: 0.06.
	loss_policy_0: 0.16232
	accuracy_policy_0: 0.69588
	loss_value_0: 0.17906
	loss_policy_1: 0.04398
	accuracy_policy_1: 0.60621
	loss_value_1: 0.03767
	loss_reward_1: 0.00643
	loss_policy_2: 0.04812
	accuracy_policy_2: 0.5759
	loss_value_2: 0.03909
	loss_reward_2: 0.00584
	loss_policy_3: 0.05159
	accuracy_policy_3: 0.54881
	loss_value_3: 0.04033
	loss_reward_3: 0.00668
	loss_policy_4: 0.05481
	accuracy_policy_4: 0.52371
	loss_value_4: 0.0415
	loss_reward_4: 0.00719
	loss_policy_5: 0.05798
	accuracy_policy_5: 0.4999
	loss_value_5: 0.04265
	loss_reward_5: 0.0082
	loss_policy: 0.4188
	loss_value: 0.38029
	loss_reward: 0.03435
Optimization_Done 16400
[2024-05-07 04:26:47] [command] train weight_iter_16400.pkl 81 83
[2024-05-07 04:27:24] nn step 16500, lr: 0.06.
	loss_policy_0: 0.17678
	accuracy_policy_0: 0.65701
	loss_value_0: 0.17097
	loss_policy_1: 0.04376
	accuracy_policy_1: 0.59311
	loss_value_1: 0.0357
	loss_reward_1: 0.00585
	loss_policy_2: 0.04758
	accuracy_policy_2: 0.56328
	loss_value_2: 0.03732
	loss_reward_2: 0.00542
	loss_policy_3: 0.05073
	accuracy_policy_3: 0.53703
	loss_value_3: 0.03858
	loss_reward_3: 0.00568
	loss_policy_4: 0.05383
	accuracy_policy_4: 0.51604
	loss_value_4: 0.03979
	loss_reward_4: 0.00621
	loss_policy_5: 0.0565
	accuracy_policy_5: 0.49453
	loss_value_5: 0.04101
	loss_reward_5: 0.00703
	loss_policy: 0.42918
	loss_value: 0.36337
	loss_reward: 0.0302
[2024-05-07 04:28:01] nn step 16600, lr: 0.06.
	loss_policy_0: 0.15343
	accuracy_policy_0: 0.6966
	loss_value_0: 0.16462
	loss_policy_1: 0.04022
	accuracy_policy_1: 0.62131
	loss_value_1: 0.03448
	loss_reward_1: 0.00557
	loss_policy_2: 0.04412
	accuracy_policy_2: 0.59084
	loss_value_2: 0.03601
	loss_reward_2: 0.00534
	loss_policy_3: 0.04714
	accuracy_policy_3: 0.56563
	loss_value_3: 0.03725
	loss_reward_3: 0.00547
	loss_policy_4: 0.0504
	accuracy_policy_4: 0.53904
	loss_value_4: 0.03836
	loss_reward_4: 0.00587
	loss_policy_5: 0.05315
	accuracy_policy_5: 0.51691
	loss_value_5: 0.03955
	loss_reward_5: 0.00703
	loss_policy: 0.38846
	loss_value: 0.35027
	loss_reward: 0.02928
Optimization_Done 16600
[2024-05-07 04:30:13] [command] train weight_iter_16600.pkl 82 84
[2024-05-07 04:30:50] nn step 16700, lr: 0.06.
	loss_policy_0: 0.17985
	accuracy_policy_0: 0.65836
	loss_value_0: 0.17157
	loss_policy_1: 0.04428
	accuracy_policy_1: 0.5957
	loss_value_1: 0.03607
	loss_reward_1: 0.00581
	loss_policy_2: 0.04836
	accuracy_policy_2: 0.56412
	loss_value_2: 0.0376
	loss_reward_2: 0.00548
	loss_policy_3: 0.05163
	accuracy_policy_3: 0.54004
	loss_value_3: 0.03897
	loss_reward_3: 0.00594
	loss_policy_4: 0.05465
	accuracy_policy_4: 0.51473
	loss_value_4: 0.04013
	loss_reward_4: 0.00648
	loss_policy_5: 0.0575
	accuracy_policy_5: 0.49055
	loss_value_5: 0.0413
	loss_reward_5: 0.00728
	loss_policy: 0.43628
	loss_value: 0.36564
	loss_reward: 0.031
[2024-05-07 04:31:26] nn step 16800, lr: 0.06.
	loss_policy_0: 0.15335
	accuracy_policy_0: 0.69885
	loss_value_0: 0.16423
	loss_policy_1: 0.0404
	accuracy_policy_1: 0.61943
	loss_value_1: 0.03464
	loss_reward_1: 0.00553
	loss_policy_2: 0.04398
	accuracy_policy_2: 0.5916
	loss_value_2: 0.03606
	loss_reward_2: 0.00515
	loss_policy_3: 0.04794
	accuracy_policy_3: 0.56111
	loss_value_3: 0.03742
	loss_reward_3: 0.00588
	loss_policy_4: 0.05111
	accuracy_policy_4: 0.53811
	loss_value_4: 0.03857
	loss_reward_4: 0.00612
	loss_policy_5: 0.05424
	accuracy_policy_5: 0.51072
	loss_value_5: 0.03984
	loss_reward_5: 0.00721
	loss_policy: 0.39103
	loss_value: 0.35076
	loss_reward: 0.02989
Optimization_Done 16800
[2024-05-07 04:33:37] [command] train weight_iter_16800.pkl 83 85
[2024-05-07 04:34:14] nn step 16900, lr: 0.06.
	loss_policy_0: 0.14771
	accuracy_policy_0: 0.69793
	loss_value_0: 0.16557
	loss_policy_1: 0.0385
	accuracy_policy_1: 0.62439
	loss_value_1: 0.03489
	loss_reward_1: 0.00592
	loss_policy_2: 0.04295
	accuracy_policy_2: 0.58785
	loss_value_2: 0.03636
	loss_reward_2: 0.00557
	loss_policy_3: 0.04615
	accuracy_policy_3: 0.56104
	loss_value_3: 0.03776
	loss_reward_3: 0.00603
	loss_policy_4: 0.04926
	accuracy_policy_4: 0.53451
	loss_value_4: 0.03889
	loss_reward_4: 0.00653
	loss_policy_5: 0.05225
	accuracy_policy_5: 0.51062
	loss_value_5: 0.04014
	loss_reward_5: 0.00732
	loss_policy: 0.37681
	loss_value: 0.35362
	loss_reward: 0.03137
[2024-05-07 04:34:51] nn step 17000, lr: 0.06.
	loss_policy_0: 0.13565
	accuracy_policy_0: 0.72693
	loss_value_0: 0.16902
	loss_policy_1: 0.03735
	accuracy_policy_1: 0.63828
	loss_value_1: 0.0354
	loss_reward_1: 0.00597
	loss_policy_2: 0.04146
	accuracy_policy_2: 0.60623
	loss_value_2: 0.03684
	loss_reward_2: 0.00551
	loss_policy_3: 0.04496
	accuracy_policy_3: 0.57996
	loss_value_3: 0.03818
	loss_reward_3: 0.00607
	loss_policy_4: 0.04816
	accuracy_policy_4: 0.555
	loss_value_4: 0.03936
	loss_reward_4: 0.00647
	loss_policy_5: 0.05119
	accuracy_policy_5: 0.53295
	loss_value_5: 0.04044
	loss_reward_5: 0.00757
	loss_policy: 0.35876
	loss_value: 0.35924
	loss_reward: 0.03158
Optimization_Done 17000
[2024-05-07 04:36:51] [command] train weight_iter_17000.pkl 84 86
[2024-05-07 04:37:28] nn step 17100, lr: 0.06.
	loss_policy_0: 0.14586
	accuracy_policy_0: 0.69471
	loss_value_0: 0.16218
	loss_policy_1: 0.03878
	accuracy_policy_1: 0.61104
	loss_value_1: 0.03422
	loss_reward_1: 0.0057
	loss_policy_2: 0.0426
	accuracy_policy_2: 0.57764
	loss_value_2: 0.03568
	loss_reward_2: 0.00528
	loss_policy_3: 0.04595
	accuracy_policy_3: 0.54744
	loss_value_3: 0.03696
	loss_reward_3: 0.00581
	loss_policy_4: 0.04883
	accuracy_policy_4: 0.52389
	loss_value_4: 0.03814
	loss_reward_4: 0.00614
	loss_policy_5: 0.05148
	accuracy_policy_5: 0.50295
	loss_value_5: 0.03919
	loss_reward_5: 0.00735
	loss_policy: 0.37351
	loss_value: 0.34636
	loss_reward: 0.03028
[2024-05-07 04:38:04] nn step 17200, lr: 0.06.
	loss_policy_0: 0.12489
	accuracy_policy_0: 0.72943
	loss_value_0: 0.15732
	loss_policy_1: 0.03551
	accuracy_policy_1: 0.6316
	loss_value_1: 0.0332
	loss_reward_1: 0.00541
	loss_policy_2: 0.03905
	accuracy_policy_2: 0.60041
	loss_value_2: 0.03447
	loss_reward_2: 0.005
	loss_policy_3: 0.0425
	accuracy_policy_3: 0.57482
	loss_value_3: 0.03586
	loss_reward_3: 0.00535
	loss_policy_4: 0.04572
	accuracy_policy_4: 0.54672
	loss_value_4: 0.03707
	loss_reward_4: 0.00603
	loss_policy_5: 0.04833
	accuracy_policy_5: 0.52318
	loss_value_5: 0.03805
	loss_reward_5: 0.00698
	loss_policy: 0.33599
	loss_value: 0.33597
	loss_reward: 0.02877
Optimization_Done 17200
[2024-05-07 04:39:57] [command] train weight_iter_17200.pkl 85 87
[2024-05-07 04:40:34] nn step 17300, lr: 0.06.
	loss_policy_0: 0.14096
	accuracy_policy_0: 0.6682
	loss_value_0: 0.13745
	loss_policy_1: 0.03591
	accuracy_policy_1: 0.59559
	loss_value_1: 0.02894
	loss_reward_1: 0.00512
	loss_policy_2: 0.03931
	accuracy_policy_2: 0.56059
	loss_value_2: 0.03044
	loss_reward_2: 0.00474
	loss_policy_3: 0.0423
	accuracy_policy_3: 0.52967
	loss_value_3: 0.0316
	loss_reward_3: 0.00511
	loss_policy_4: 0.04442
	accuracy_policy_4: 0.51426
	loss_value_4: 0.03276
	loss_reward_4: 0.00552
	loss_policy_5: 0.04676
	accuracy_policy_5: 0.4916
	loss_value_5: 0.03375
	loss_reward_5: 0.00638
	loss_policy: 0.34967
	loss_value: 0.29495
	loss_reward: 0.02687
[2024-05-07 04:41:10] nn step 17400, lr: 0.06.
	loss_policy_0: 0.1306
	accuracy_policy_0: 0.71309
	loss_value_0: 0.15091
	loss_policy_1: 0.03635
	accuracy_policy_1: 0.61752
	loss_value_1: 0.03195
	loss_reward_1: 0.00556
	loss_policy_2: 0.0404
	accuracy_policy_2: 0.58477
	loss_value_2: 0.03332
	loss_reward_2: 0.00515
	loss_policy_3: 0.04337
	accuracy_policy_3: 0.55881
	loss_value_3: 0.03481
	loss_reward_3: 0.00538
	loss_policy_4: 0.0459
	accuracy_policy_4: 0.53516
	loss_value_4: 0.03597
	loss_reward_4: 0.00594
	loss_policy_5: 0.04863
	accuracy_policy_5: 0.51506
	loss_value_5: 0.03703
	loss_reward_5: 0.00685
	loss_policy: 0.34524
	loss_value: 0.324
	loss_reward: 0.02887
Optimization_Done 17400
[2024-05-07 04:43:23] [command] train weight_iter_17400.pkl 86 88
[2024-05-07 04:44:01] nn step 17500, lr: 0.06.
	loss_policy_0: 0.15977
	accuracy_policy_0: 0.65391
	loss_value_0: 0.15612
	loss_policy_1: 0.039
	accuracy_policy_1: 0.589
	loss_value_1: 0.03298
	loss_reward_1: 0.00577
	loss_policy_2: 0.04229
	accuracy_policy_2: 0.56344
	loss_value_2: 0.03432
	loss_reward_2: 0.0055
	loss_policy_3: 0.04493
	accuracy_policy_3: 0.54318
	loss_value_3: 0.03573
	loss_reward_3: 0.00604
	loss_policy_4: 0.04716
	accuracy_policy_4: 0.5274
	loss_value_4: 0.03687
	loss_reward_4: 0.00649
	loss_policy_5: 0.04931
	accuracy_policy_5: 0.50734
	loss_value_5: 0.03795
	loss_reward_5: 0.00728
	loss_policy: 0.38247
	loss_value: 0.33397
	loss_reward: 0.03108
[2024-05-07 04:44:38] nn step 17600, lr: 0.06.
	loss_policy_0: 0.14718
	accuracy_policy_0: 0.70043
	loss_value_0: 0.16478
	loss_policy_1: 0.03838
	accuracy_policy_1: 0.61828
	loss_value_1: 0.0347
	loss_reward_1: 0.00605
	loss_policy_2: 0.04195
	accuracy_policy_2: 0.58924
	loss_value_2: 0.03628
	loss_reward_2: 0.00569
	loss_policy_3: 0.04467
	accuracy_policy_3: 0.5684
	loss_value_3: 0.03773
	loss_reward_3: 0.00615
	loss_policy_4: 0.04749
	accuracy_policy_4: 0.54914
	loss_value_4: 0.03893
	loss_reward_4: 0.00656
	loss_policy_5: 0.04997
	accuracy_policy_5: 0.53086
	loss_value_5: 0.04008
	loss_reward_5: 0.00753
	loss_policy: 0.36965
	loss_value: 0.3525
	loss_reward: 0.03197
Optimization_Done 17600
[2024-05-07 04:46:50] [command] train weight_iter_17600.pkl 87 89
[2024-05-07 04:47:27] nn step 17700, lr: 0.06.
	loss_policy_0: 0.17306
	accuracy_policy_0: 0.66895
	loss_value_0: 0.18666
	loss_policy_1: 0.04473
	accuracy_policy_1: 0.58602
	loss_value_1: 0.03929
	loss_reward_1: 0.00739
	loss_policy_2: 0.04881
	accuracy_policy_2: 0.55672
	loss_value_2: 0.0409
	loss_reward_2: 0.00718
	loss_policy_3: 0.05196
	accuracy_policy_3: 0.53549
	loss_value_3: 0.04223
	loss_reward_3: 0.0076
	loss_policy_4: 0.05493
	accuracy_policy_4: 0.51648
	loss_value_4: 0.04356
	loss_reward_4: 0.00821
	loss_policy_5: 0.05761
	accuracy_policy_5: 0.49881
	loss_value_5: 0.04486
	loss_reward_5: 0.00946
	loss_policy: 0.43111
	loss_value: 0.39749
	loss_reward: 0.03984
[2024-05-07 04:48:04] nn step 17800, lr: 0.06.
	loss_policy_0: 0.15375
	accuracy_policy_0: 0.6993
	loss_value_0: 0.18049
	loss_policy_1: 0.04051
	accuracy_policy_1: 0.61977
	loss_value_1: 0.03788
	loss_reward_1: 0.00731
	loss_policy_2: 0.04449
	accuracy_policy_2: 0.58812
	loss_value_2: 0.03945
	loss_reward_2: 0.0067
	loss_policy_3: 0.04805
	accuracy_policy_3: 0.56502
	loss_value_3: 0.04098
	loss_reward_3: 0.0074
	loss_policy_4: 0.05097
	accuracy_policy_4: 0.54256
	loss_value_4: 0.04243
	loss_reward_4: 0.00797
	loss_policy_5: 0.05378
	accuracy_policy_5: 0.52309
	loss_value_5: 0.0436
	loss_reward_5: 0.00899
	loss_policy: 0.39153
	loss_value: 0.38483
	loss_reward: 0.03837
Optimization_Done 17800
[2024-05-07 04:50:04] [command] train weight_iter_17800.pkl 88 90
[2024-05-07 04:50:42] nn step 17900, lr: 0.06.
	loss_policy_0: 0.17859
	accuracy_policy_0: 0.66217
	loss_value_0: 0.18748
	loss_policy_1: 0.04571
	accuracy_policy_1: 0.58545
	loss_value_1: 0.03934
	loss_reward_1: 0.00725
	loss_policy_2: 0.04947
	accuracy_policy_2: 0.55824
	loss_value_2: 0.04081
	loss_reward_2: 0.00657
	loss_policy_3: 0.05258
	accuracy_policy_3: 0.53658
	loss_value_3: 0.0422
	loss_reward_3: 0.00718
	loss_policy_4: 0.05523
	accuracy_policy_4: 0.51561
	loss_value_4: 0.04344
	loss_reward_4: 0.00766
	loss_policy_5: 0.05798
	accuracy_policy_5: 0.49646
	loss_value_5: 0.04465
	loss_reward_5: 0.00879
	loss_policy: 0.43955
	loss_value: 0.39793
	loss_reward: 0.03745
[2024-05-07 04:51:18] nn step 18000, lr: 0.06.
	loss_policy_0: 0.15034
	accuracy_policy_0: 0.70215
	loss_value_0: 0.17494
	loss_policy_1: 0.04059
	accuracy_policy_1: 0.61404
	loss_value_1: 0.03704
	loss_reward_1: 0.00658
	loss_policy_2: 0.04452
	accuracy_policy_2: 0.58383
	loss_value_2: 0.03852
	loss_reward_2: 0.00607
	loss_policy_3: 0.04757
	accuracy_policy_3: 0.56023
	loss_value_3: 0.0397
	loss_reward_3: 0.00675
	loss_policy_4: 0.05017
	accuracy_policy_4: 0.53781
	loss_value_4: 0.04093
	loss_reward_4: 0.00718
	loss_policy_5: 0.05269
	accuracy_policy_5: 0.52059
	loss_value_5: 0.04213
	loss_reward_5: 0.00849
	loss_policy: 0.38587
	loss_value: 0.37325
	loss_reward: 0.03508
Optimization_Done 18000
[2024-05-07 04:53:20] [command] train weight_iter_18000.pkl 89 91
[2024-05-07 04:53:57] nn step 18100, lr: 0.06.
	loss_policy_0: 0.17142
	accuracy_policy_0: 0.65492
	loss_value_0: 0.1714
	loss_policy_1: 0.04349
	accuracy_policy_1: 0.57637
	loss_value_1: 0.03602
	loss_reward_1: 0.00613
	loss_policy_2: 0.04735
	accuracy_policy_2: 0.54062
	loss_value_2: 0.03751
	loss_reward_2: 0.00574
	loss_policy_3: 0.05065
	accuracy_policy_3: 0.5159
	loss_value_3: 0.03875
	loss_reward_3: 0.00624
	loss_policy_4: 0.05317
	accuracy_policy_4: 0.49951
	loss_value_4: 0.0399
	loss_reward_4: 0.00682
	loss_policy_5: 0.05564
	accuracy_policy_5: 0.48066
	loss_value_5: 0.04113
	loss_reward_5: 0.00779
	loss_policy: 0.42172
	loss_value: 0.36471
	loss_reward: 0.03271
[2024-05-07 04:54:33] nn step 18200, lr: 0.06.
	loss_policy_0: 0.14589
	accuracy_policy_0: 0.69547
	loss_value_0: 0.1595
	loss_policy_1: 0.03932
	accuracy_policy_1: 0.60156
	loss_value_1: 0.03367
	loss_reward_1: 0.0058
	loss_policy_2: 0.04359
	accuracy_policy_2: 0.56266
	loss_value_2: 0.03494
	loss_reward_2: 0.00533
	loss_policy_3: 0.04618
	accuracy_policy_3: 0.54088
	loss_value_3: 0.03628
	loss_reward_3: 0.00588
	loss_policy_4: 0.04908
	accuracy_policy_4: 0.51896
	loss_value_4: 0.03749
	loss_reward_4: 0.00632
	loss_policy_5: 0.05132
	accuracy_policy_5: 0.5034
	loss_value_5: 0.03863
	loss_reward_5: 0.00741
	loss_policy: 0.37537
	loss_value: 0.34051
	loss_reward: 0.03074
Optimization_Done 18200
[2024-05-07 04:56:46] [command] train weight_iter_18200.pkl 90 92
[2024-05-07 04:57:24] nn step 18300, lr: 0.06.
	loss_policy_0: 0.15477
	accuracy_policy_0: 0.67861
	loss_value_0: 0.16166
	loss_policy_1: 0.03847
	accuracy_policy_1: 0.61205
	loss_value_1: 0.03403
	loss_reward_1: 0.0057
	loss_policy_2: 0.04199
	accuracy_policy_2: 0.57867
	loss_value_2: 0.03552
	loss_reward_2: 0.00537
	loss_policy_3: 0.04513
	accuracy_policy_3: 0.55277
	loss_value_3: 0.0368
	loss_reward_3: 0.00582
	loss_policy_4: 0.04763
	accuracy_policy_4: 0.5323
	loss_value_4: 0.03801
	loss_reward_4: 0.00625
	loss_policy_5: 0.05004
	accuracy_policy_5: 0.515
	loss_value_5: 0.03923
	loss_reward_5: 0.00712
	loss_policy: 0.37804
	loss_value: 0.34526
	loss_reward: 0.03026
[2024-05-07 04:58:00] nn step 18400, lr: 0.06.
	loss_policy_0: 0.13798
	accuracy_policy_0: 0.71465
	loss_value_0: 0.16242
	loss_policy_1: 0.03656
	accuracy_policy_1: 0.63295
	loss_value_1: 0.03424
	loss_reward_1: 0.00553
	loss_policy_2: 0.04054
	accuracy_policy_2: 0.59455
	loss_value_2: 0.03574
	loss_reward_2: 0.00535
	loss_policy_3: 0.04369
	accuracy_policy_3: 0.57314
	loss_value_3: 0.03687
	loss_reward_3: 0.00562
	loss_policy_4: 0.04667
	accuracy_policy_4: 0.54633
	loss_value_4: 0.03807
	loss_reward_4: 0.00623
	loss_policy_5: 0.04892
	accuracy_policy_5: 0.52963
	loss_value_5: 0.03921
	loss_reward_5: 0.00706
	loss_policy: 0.35435
	loss_value: 0.34655
	loss_reward: 0.02978
Optimization_Done 18400
[2024-05-07 05:00:12] [command] train weight_iter_18400.pkl 91 93
[2024-05-07 05:00:50] nn step 18500, lr: 0.06.
	loss_policy_0: 0.15309
	accuracy_policy_0: 0.68793
	loss_value_0: 0.17488
	loss_policy_1: 0.0411
	accuracy_policy_1: 0.59764
	loss_value_1: 0.03694
	loss_reward_1: 0.00705
	loss_policy_2: 0.04519
	accuracy_policy_2: 0.56258
	loss_value_2: 0.03842
	loss_reward_2: 0.00673
	loss_policy_3: 0.04844
	accuracy_policy_3: 0.54396
	loss_value_3: 0.03971
	loss_reward_3: 0.0075
	loss_policy_4: 0.05119
	accuracy_policy_4: 0.52531
	loss_value_4: 0.04091
	loss_reward_4: 0.00809
	loss_policy_5: 0.05342
	accuracy_policy_5: 0.50682
	loss_value_5: 0.04196
	loss_reward_5: 0.00902
	loss_policy: 0.39243
	loss_value: 0.37281
	loss_reward: 0.03839
[2024-05-07 05:01:26] nn step 18600, lr: 0.06.
	loss_policy_0: 0.13357
	accuracy_policy_0: 0.72496
	loss_value_0: 0.1698
	loss_policy_1: 0.03778
	accuracy_policy_1: 0.62096
	loss_value_1: 0.03571
	loss_reward_1: 0.00694
	loss_policy_2: 0.04192
	accuracy_policy_2: 0.58871
	loss_value_2: 0.03724
	loss_reward_2: 0.0068
	loss_policy_3: 0.04559
	accuracy_policy_3: 0.56459
	loss_value_3: 0.0385
	loss_reward_3: 0.00727
	loss_policy_4: 0.0483
	accuracy_policy_4: 0.54338
	loss_value_4: 0.03975
	loss_reward_4: 0.00785
	loss_policy_5: 0.05105
	accuracy_policy_5: 0.52246
	loss_value_5: 0.04096
	loss_reward_5: 0.00885
	loss_policy: 0.35823
	loss_value: 0.36195
	loss_reward: 0.0377
Optimization_Done 18600
[2024-05-07 05:03:37] [command] train weight_iter_18600.pkl 92 94
[2024-05-07 05:04:15] nn step 18700, lr: 0.06.
	loss_policy_0: 0.13827
	accuracy_policy_0: 0.71625
	loss_value_0: 0.1746
	loss_policy_1: 0.03893
	accuracy_policy_1: 0.61957
	loss_value_1: 0.03687
	loss_reward_1: 0.00732
	loss_policy_2: 0.04315
	accuracy_policy_2: 0.58084
	loss_value_2: 0.03839
	loss_reward_2: 0.00691
	loss_policy_3: 0.04619
	accuracy_policy_3: 0.56182
	loss_value_3: 0.03979
	loss_reward_3: 0.00761
	loss_policy_4: 0.04932
	accuracy_policy_4: 0.53848
	loss_value_4: 0.04104
	loss_reward_4: 0.00806
	loss_policy_5: 0.0519
	accuracy_policy_5: 0.52275
	loss_value_5: 0.0422
	loss_reward_5: 0.00911
	loss_policy: 0.36775
	loss_value: 0.37289
	loss_reward: 0.03901
[2024-05-07 05:04:52] nn step 18800, lr: 0.06.
	loss_policy_0: 0.11791
	accuracy_policy_0: 0.74328
	loss_value_0: 0.16456
	loss_policy_1: 0.03524
	accuracy_policy_1: 0.63346
	loss_value_1: 0.03467
	loss_reward_1: 0.007
	loss_policy_2: 0.03907
	accuracy_policy_2: 0.59904
	loss_value_2: 0.03617
	loss_reward_2: 0.00655
	loss_policy_3: 0.04214
	accuracy_policy_3: 0.57889
	loss_value_3: 0.03759
	loss_reward_3: 0.00707
	loss_policy_4: 0.04485
	accuracy_policy_4: 0.55805
	loss_value_4: 0.03876
	loss_reward_4: 0.00763
	loss_policy_5: 0.04731
	accuracy_policy_5: 0.53891
	loss_value_5: 0.04
	loss_reward_5: 0.00865
	loss_policy: 0.32653
	loss_value: 0.35176
	loss_reward: 0.0369
Optimization_Done 18800
[2024-05-07 05:07:06] [command] train weight_iter_18800.pkl 93 95
[2024-05-07 05:07:43] nn step 18900, lr: 0.06.
	loss_policy_0: 0.1503
	accuracy_policy_0: 0.68742
	loss_value_0: 0.16024
	loss_policy_1: 0.04064
	accuracy_policy_1: 0.59438
	loss_value_1: 0.03371
	loss_reward_1: 0.00639
	loss_policy_2: 0.0444
	accuracy_policy_2: 0.56424
	loss_value_2: 0.03496
	loss_reward_2: 0.00602
	loss_policy_3: 0.04733
	accuracy_policy_3: 0.5435
	loss_value_3: 0.03618
	loss_reward_3: 0.00691
	loss_policy_4: 0.04986
	accuracy_policy_4: 0.52828
	loss_value_4: 0.03725
	loss_reward_4: 0.00705
	loss_policy_5: 0.05223
	accuracy_policy_5: 0.50734
	loss_value_5: 0.03828
	loss_reward_5: 0.00818
	loss_policy: 0.38476
	loss_value: 0.34062
	loss_reward: 0.03455
[2024-05-07 05:08:20] nn step 19000, lr: 0.06.
	loss_policy_0: 0.12459
	accuracy_policy_0: 0.73523
	loss_value_0: 0.15865
	loss_policy_1: 0.03693
	accuracy_policy_1: 0.62111
	loss_value_1: 0.03344
	loss_reward_1: 0.00635
	loss_policy_2: 0.04101
	accuracy_policy_2: 0.58812
	loss_value_2: 0.03481
	loss_reward_2: 0.00606
	loss_policy_3: 0.04407
	accuracy_policy_3: 0.56752
	loss_value_3: 0.03608
	loss_reward_3: 0.00673
	loss_policy_4: 0.04702
	accuracy_policy_4: 0.54457
	loss_value_4: 0.03713
	loss_reward_4: 0.00701
	loss_policy_5: 0.04965
	accuracy_policy_5: 0.52775
	loss_value_5: 0.03836
	loss_reward_5: 0.00831
	loss_policy: 0.34329
	loss_value: 0.33848
	loss_reward: 0.03446
Optimization_Done 19000
[2024-05-07 05:10:19] [command] train weight_iter_19000.pkl 94 96
[2024-05-07 05:10:55] nn step 19100, lr: 0.06.
	loss_policy_0: 0.15213
	accuracy_policy_0: 0.67986
	loss_value_0: 0.15112
	loss_policy_1: 0.03804
	accuracy_policy_1: 0.61359
	loss_value_1: 0.03169
	loss_reward_1: 0.00542
	loss_policy_2: 0.0412
	accuracy_policy_2: 0.58172
	loss_value_2: 0.0331
	loss_reward_2: 0.0051
	loss_policy_3: 0.04408
	accuracy_policy_3: 0.56092
	loss_value_3: 0.03421
	loss_reward_3: 0.0054
	loss_policy_4: 0.04658
	accuracy_policy_4: 0.54217
	loss_value_4: 0.03526
	loss_reward_4: 0.00582
	loss_policy_5: 0.04882
	accuracy_policy_5: 0.52408
	loss_value_5: 0.03618
	loss_reward_5: 0.00662
	loss_policy: 0.37086
	loss_value: 0.32155
	loss_reward: 0.02836
[2024-05-07 05:11:31] nn step 19200, lr: 0.06.
	loss_policy_0: 0.13783
	accuracy_policy_0: 0.72215
	loss_value_0: 0.1582
	loss_policy_1: 0.03672
	accuracy_policy_1: 0.63785
	loss_value_1: 0.03352
	loss_reward_1: 0.00573
	loss_policy_2: 0.04046
	accuracy_policy_2: 0.60756
	loss_value_2: 0.03482
	loss_reward_2: 0.0053
	loss_policy_3: 0.04387
	accuracy_policy_3: 0.58307
	loss_value_3: 0.03605
	loss_reward_3: 0.00567
	loss_policy_4: 0.04616
	accuracy_policy_4: 0.56982
	loss_value_4: 0.03735
	loss_reward_4: 0.00606
	loss_policy_5: 0.04885
	accuracy_policy_5: 0.55
	loss_value_5: 0.03833
	loss_reward_5: 0.00709
	loss_policy: 0.35388
	loss_value: 0.33828
	loss_reward: 0.02986
Optimization_Done 19200
[2024-05-07 05:13:42] [command] train weight_iter_19200.pkl 95 97
[2024-05-07 05:14:19] nn step 19300, lr: 0.06.
	loss_policy_0: 0.16392
	accuracy_policy_0: 0.67643
	loss_value_0: 0.18163
	loss_policy_1: 0.04062
	accuracy_policy_1: 0.60572
	loss_value_1: 0.03781
	loss_reward_1: 0.00609
	loss_policy_2: 0.04483
	accuracy_policy_2: 0.57307
	loss_value_2: 0.03935
	loss_reward_2: 0.00572
	loss_policy_3: 0.04775
	accuracy_policy_3: 0.54691
	loss_value_3: 0.04059
	loss_reward_3: 0.00638
	loss_policy_4: 0.05048
	accuracy_policy_4: 0.51889
	loss_value_4: 0.04195
	loss_reward_4: 0.00674
	loss_policy_5: 0.05286
	accuracy_policy_5: 0.49895
	loss_value_5: 0.04307
	loss_reward_5: 0.00774
	loss_policy: 0.40047
	loss_value: 0.38439
	loss_reward: 0.03267
[2024-05-07 05:14:54] nn step 19400, lr: 0.06.
	loss_policy_0: 0.13839
	accuracy_policy_0: 0.71807
	loss_value_0: 0.16542
	loss_policy_1: 0.0363
	accuracy_policy_1: 0.63576
	loss_value_1: 0.03457
	loss_reward_1: 0.00583
	loss_policy_2: 0.04046
	accuracy_policy_2: 0.59982
	loss_value_2: 0.03602
	loss_reward_2: 0.00561
	loss_policy_3: 0.04337
	accuracy_policy_3: 0.57244
	loss_value_3: 0.03737
	loss_reward_3: 0.00606
	loss_policy_4: 0.046
	accuracy_policy_4: 0.55562
	loss_value_4: 0.03868
	loss_reward_4: 0.0065
	loss_policy_5: 0.0488
	accuracy_policy_5: 0.53295
	loss_value_5: 0.03974
	loss_reward_5: 0.00743
	loss_policy: 0.35332
	loss_value: 0.35181
	loss_reward: 0.03143
Optimization_Done 19400
[2024-05-07 05:17:04] [command] train weight_iter_19400.pkl 96 98
[2024-05-07 05:17:41] nn step 19500, lr: 0.06.
	loss_policy_0: 0.15495
	accuracy_policy_0: 0.71529
	loss_value_0: 0.1679
	loss_policy_1: 0.03872
	accuracy_policy_1: 0.65033
	loss_value_1: 0.0351
	loss_reward_1: 0.00662
	loss_policy_2: 0.04262
	accuracy_policy_2: 0.61994
	loss_value_2: 0.03667
	loss_reward_2: 0.00624
	loss_policy_3: 0.04537
	accuracy_policy_3: 0.59648
	loss_value_3: 0.03808
	loss_reward_3: 0.00692
	loss_policy_4: 0.04819
	accuracy_policy_4: 0.5757
	loss_value_4: 0.0394
	loss_reward_4: 0.00744
	loss_policy_5: 0.05076
	accuracy_policy_5: 0.55609
	loss_value_5: 0.04056
	loss_reward_5: 0.00843
	loss_policy: 0.3806
	loss_value: 0.35772
	loss_reward: 0.03564
[2024-05-07 05:18:17] nn step 19600, lr: 0.06.
	loss_policy_0: 0.13126
	accuracy_policy_0: 0.74342
	loss_value_0: 0.16178
	loss_policy_1: 0.03521
	accuracy_policy_1: 0.66441
	loss_value_1: 0.03398
	loss_reward_1: 0.00644
	loss_policy_2: 0.03907
	accuracy_policy_2: 0.63096
	loss_value_2: 0.03527
	loss_reward_2: 0.00596
	loss_policy_3: 0.04213
	accuracy_policy_3: 0.61225
	loss_value_3: 0.03667
	loss_reward_3: 0.00642
	loss_policy_4: 0.04473
	accuracy_policy_4: 0.59391
	loss_value_4: 0.03784
	loss_reward_4: 0.00716
	loss_policy_5: 0.04693
	accuracy_policy_5: 0.57568
	loss_value_5: 0.03894
	loss_reward_5: 0.00796
	loss_policy: 0.33934
	loss_value: 0.34447
	loss_reward: 0.03393
Optimization_Done 19600
[2024-05-07 05:20:18] [command] train weight_iter_19600.pkl 97 99
[2024-05-07 05:20:55] nn step 19700, lr: 0.06.
	loss_policy_0: 0.14538
	accuracy_policy_0: 0.71236
	loss_value_0: 0.15859
	loss_policy_1: 0.03653
	accuracy_policy_1: 0.64445
	loss_value_1: 0.03306
	loss_reward_1: 0.00589
	loss_policy_2: 0.04003
	accuracy_policy_2: 0.61512
	loss_value_2: 0.03442
	loss_reward_2: 0.0058
	loss_policy_3: 0.04225
	accuracy_policy_3: 0.5983
	loss_value_3: 0.03562
	loss_reward_3: 0.00629
	loss_policy_4: 0.04504
	accuracy_policy_4: 0.57582
	loss_value_4: 0.03663
	loss_reward_4: 0.00663
	loss_policy_5: 0.04714
	accuracy_policy_5: 0.55316
	loss_value_5: 0.03746
	loss_reward_5: 0.00755
	loss_policy: 0.35637
	loss_value: 0.33578
	loss_reward: 0.03216
[2024-05-07 05:21:30] nn step 19800, lr: 0.06.
	loss_policy_0: 0.12513
	accuracy_policy_0: 0.74695
	loss_value_0: 0.15462
	loss_policy_1: 0.03368
	accuracy_policy_1: 0.66232
	loss_value_1: 0.03238
	loss_reward_1: 0.00591
	loss_policy_2: 0.03765
	accuracy_policy_2: 0.63123
	loss_value_2: 0.03389
	loss_reward_2: 0.00582
	loss_policy_3: 0.03966
	accuracy_policy_3: 0.61416
	loss_value_3: 0.03506
	loss_reward_3: 0.0061
	loss_policy_4: 0.04222
	accuracy_policy_4: 0.59258
	loss_value_4: 0.03607
	loss_reward_4: 0.00671
	loss_policy_5: 0.04438
	accuracy_policy_5: 0.57576
	loss_value_5: 0.03705
	loss_reward_5: 0.00754
	loss_policy: 0.32273
	loss_value: 0.32907
	loss_reward: 0.03208
Optimization_Done 19800
[2024-05-07 05:23:42] [command] train weight_iter_19800.pkl 98 100
[2024-05-07 05:24:18] nn step 19900, lr: 0.06.
	loss_policy_0: 0.14522
	accuracy_policy_0: 0.706
	loss_value_0: 0.15684
	loss_policy_1: 0.03644
	accuracy_policy_1: 0.64389
	loss_value_1: 0.03254
	loss_reward_1: 0.00585
	loss_policy_2: 0.04016
	accuracy_policy_2: 0.61377
	loss_value_2: 0.03403
	loss_reward_2: 0.00575
	loss_policy_3: 0.04313
	accuracy_policy_3: 0.58949
	loss_value_3: 0.03535
	loss_reward_3: 0.00606
	loss_policy_4: 0.04551
	accuracy_policy_4: 0.57145
	loss_value_4: 0.03642
	loss_reward_4: 0.00658
	loss_policy_5: 0.04775
	accuracy_policy_5: 0.55816
	loss_value_5: 0.03743
	loss_reward_5: 0.00738
	loss_policy: 0.35822
	loss_value: 0.33261
	loss_reward: 0.03162
[2024-05-07 05:24:54] nn step 20000, lr: 0.06.
	loss_policy_0: 0.12671
	accuracy_policy_0: 0.75137
	loss_value_0: 0.16085
	loss_policy_1: 0.03423
	accuracy_policy_1: 0.67436
	loss_value_1: 0.03356
	loss_reward_1: 0.00606
	loss_policy_2: 0.03851
	accuracy_policy_2: 0.63807
	loss_value_2: 0.03497
	loss_reward_2: 0.00581
	loss_policy_3: 0.04086
	accuracy_policy_3: 0.62275
	loss_value_3: 0.03637
	loss_reward_3: 0.00611
	loss_policy_4: 0.04317
	accuracy_policy_4: 0.60938
	loss_value_4: 0.03741
	loss_reward_4: 0.00662
	loss_policy_5: 0.04624
	accuracy_policy_5: 0.58518
	loss_value_5: 0.03846
	loss_reward_5: 0.00761
	loss_policy: 0.32972
	loss_value: 0.34162
	loss_reward: 0.03221
Optimization_Done 20000
[2024-05-07 05:26:45] [command] train weight_iter_20000.pkl 99 101
[2024-05-07 05:27:22] nn step 20100, lr: 0.06.
	loss_policy_0: 0.15935
	accuracy_policy_0: 0.68816
	loss_value_0: 0.15896
	loss_policy_1: 0.0392
	accuracy_policy_1: 0.62248
	loss_value_1: 0.03331
	loss_reward_1: 0.00577
	loss_policy_2: 0.04349
	accuracy_policy_2: 0.58842
	loss_value_2: 0.03471
	loss_reward_2: 0.00553
	loss_policy_3: 0.04637
	accuracy_policy_3: 0.56381
	loss_value_3: 0.03588
	loss_reward_3: 0.00595
	loss_policy_4: 0.04933
	accuracy_policy_4: 0.54398
	loss_value_4: 0.0371
	loss_reward_4: 0.0065
	loss_policy_5: 0.05221
	accuracy_policy_5: 0.5252
	loss_value_5: 0.03839
	loss_reward_5: 0.00719
	loss_policy: 0.38996
	loss_value: 0.33836
	loss_reward: 0.03094
[2024-05-07 05:27:57] nn step 20200, lr: 0.06.
	loss_policy_0: 0.13959
	accuracy_policy_0: 0.71895
	loss_value_0: 0.15473
	loss_policy_1: 0.0355
	accuracy_policy_1: 0.64891
	loss_value_1: 0.03257
	loss_reward_1: 0.00564
	loss_policy_2: 0.0397
	accuracy_policy_2: 0.61406
	loss_value_2: 0.03379
	loss_reward_2: 0.00545
	loss_policy_3: 0.04299
	accuracy_policy_3: 0.58752
	loss_value_3: 0.03516
	loss_reward_3: 0.00581
	loss_policy_4: 0.04553
	accuracy_policy_4: 0.57102
	loss_value_4: 0.03645
	loss_reward_4: 0.00615
	loss_policy_5: 0.04826
	accuracy_policy_5: 0.55373
	loss_value_5: 0.0377
	loss_reward_5: 0.00704
	loss_policy: 0.35158
	loss_value: 0.3304
	loss_reward: 0.03008
Optimization_Done 20200
[2024-05-07 05:30:10] [command] train weight_iter_20200.pkl 100 102
[2024-05-07 05:30:47] nn step 20300, lr: 0.06.
	loss_policy_0: 0.17146
	accuracy_policy_0: 0.66189
	loss_value_0: 0.1579
	loss_policy_1: 0.04213
	accuracy_policy_1: 0.59709
	loss_value_1: 0.03313
	loss_reward_1: 0.00657
	loss_policy_2: 0.04649
	accuracy_policy_2: 0.56215
	loss_value_2: 0.03472
	loss_reward_2: 0.00588
	loss_policy_3: 0.0501
	accuracy_policy_3: 0.53512
	loss_value_3: 0.03611
	loss_reward_3: 0.00639
	loss_policy_4: 0.05273
	accuracy_policy_4: 0.52086
	loss_value_4: 0.03736
	loss_reward_4: 0.00694
	loss_policy_5: 0.05536
	accuracy_policy_5: 0.50342
	loss_value_5: 0.03856
	loss_reward_5: 0.00818
	loss_policy: 0.41826
	loss_value: 0.33779
	loss_reward: 0.03396
[2024-05-07 05:31:22] nn step 20400, lr: 0.06.
	loss_policy_0: 0.15433
	accuracy_policy_0: 0.70182
	loss_value_0: 0.16483
	loss_policy_1: 0.04002
	accuracy_policy_1: 0.62492
	loss_value_1: 0.03464
	loss_reward_1: 0.00669
	loss_policy_2: 0.0445
	accuracy_policy_2: 0.59146
	loss_value_2: 0.03616
	loss_reward_2: 0.00609
	loss_policy_3: 0.04877
	accuracy_policy_3: 0.56361
	loss_value_3: 0.03784
	loss_reward_3: 0.00661
	loss_policy_4: 0.05133
	accuracy_policy_4: 0.54672
	loss_value_4: 0.03897
	loss_reward_4: 0.00708
	loss_policy_5: 0.05462
	accuracy_policy_5: 0.52535
	loss_value_5: 0.04036
	loss_reward_5: 0.0083
	loss_policy: 0.39357
	loss_value: 0.35279
	loss_reward: 0.03476
Optimization_Done 20400
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 05:53:45] [command] train weight_iter_20400.pkl 102 103
[2024-05-07 05:54:28] nn step 20500, lr: 0.06.
	loss_policy_0: 0.17391
	accuracy_policy_0: 0.66346
	loss_value_0: 0.155
	loss_policy_1: 0.04472
	accuracy_policy_1: 0.58611
	loss_value_1: 0.03268
	loss_reward_1: 0.00623
	loss_policy_2: 0.04936
	accuracy_policy_2: 0.54793
	loss_value_2: 0.03437
	loss_reward_2: 0.00579
	loss_policy_3: 0.05365
	accuracy_policy_3: 0.51453
	loss_value_3: 0.03591
	loss_reward_3: 0.0062
	loss_policy_4: 0.05709
	accuracy_policy_4: 0.49262
	loss_value_4: 0.03731
	loss_reward_4: 0.00671
	loss_policy_5: 0.06113
	accuracy_policy_5: 0.45916
	loss_value_5: 0.03857
	loss_reward_5: 0.00778
	loss_policy: 0.43987
	loss_value: 0.33385
	loss_reward: 0.0327
[2024-05-07 05:55:03] nn step 20600, lr: 0.06.
	loss_policy_0: 0.12871
	accuracy_policy_0: 0.71904
	loss_value_0: 0.15056
	loss_policy_1: 0.0368
	accuracy_policy_1: 0.61975
	loss_value_1: 0.03175
	loss_reward_1: 0.00565
	loss_policy_2: 0.04178
	accuracy_policy_2: 0.58281
	loss_value_2: 0.03321
	loss_reward_2: 0.00504
	loss_policy_3: 0.04512
	accuracy_policy_3: 0.55881
	loss_value_3: 0.03464
	loss_reward_3: 0.00556
	loss_policy_4: 0.04891
	accuracy_policy_4: 0.53004
	loss_value_4: 0.03583
	loss_reward_4: 0.00609
	loss_policy_5: 0.05251
	accuracy_policy_5: 0.49867
	loss_value_5: 0.03696
	loss_reward_5: 0.00723
	loss_policy: 0.35383
	loss_value: 0.32295
	loss_reward: 0.02956
Optimization_Done 20600
[2024-05-07 05:57:20] [command] train weight_iter_20600.pkl 103 104
[2024-05-07 05:57:57] nn step 20700, lr: 0.06.
	loss_policy_0: 0.1487
	accuracy_policy_0: 0.70492
	loss_value_0: 0.15513
	loss_policy_1: 0.0371
	accuracy_policy_1: 0.64969
	loss_value_1: 0.03275
	loss_reward_1: 0.00459
	loss_policy_2: 0.04139
	accuracy_policy_2: 0.61355
	loss_value_2: 0.03419
	loss_reward_2: 0.00437
	loss_policy_3: 0.04495
	accuracy_policy_3: 0.5867
	loss_value_3: 0.03558
	loss_reward_3: 0.00459
	loss_policy_4: 0.04796
	accuracy_policy_4: 0.56428
	loss_value_4: 0.03679
	loss_reward_4: 0.00495
	loss_policy_5: 0.05124
	accuracy_policy_5: 0.5365
	loss_value_5: 0.03808
	loss_reward_5: 0.00575
	loss_policy: 0.37134
	loss_value: 0.33252
	loss_reward: 0.02424
[2024-05-07 05:58:32] nn step 20800, lr: 0.06.
	loss_policy_0: 0.11627
	accuracy_policy_0: 0.75645
	loss_value_0: 0.15269
	loss_policy_1: 0.03222
	accuracy_policy_1: 0.68256
	loss_value_1: 0.03224
	loss_reward_1: 0.00457
	loss_policy_2: 0.0368
	accuracy_policy_2: 0.64656
	loss_value_2: 0.03368
	loss_reward_2: 0.0043
	loss_policy_3: 0.04006
	accuracy_policy_3: 0.62348
	loss_value_3: 0.03506
	loss_reward_3: 0.00456
	loss_policy_4: 0.04348
	accuracy_policy_4: 0.60045
	loss_value_4: 0.0364
	loss_reward_4: 0.00492
	loss_policy_5: 0.04688
	accuracy_policy_5: 0.57074
	loss_value_5: 0.03768
	loss_reward_5: 0.00586
	loss_policy: 0.31571
	loss_value: 0.32775
	loss_reward: 0.02422
Optimization_Done 20800
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 06:03:27] [command] train weight_iter_20800.pkl 104 105
[2024-05-07 06:04:10] nn step 20900, lr: 0.06561.
	loss_policy_0: 0.14455
	accuracy_policy_0: 0.72002
	loss_value_0: 0.16982
	loss_policy_1: 0.03767
	accuracy_policy_1: 0.65355
	loss_value_1: 0.03582
	loss_reward_1: 0.00512
	loss_policy_2: 0.04246
	accuracy_policy_2: 0.6149
	loss_value_2: 0.03759
	loss_reward_2: 0.00489
	loss_policy_3: 0.04651
	accuracy_policy_3: 0.58791
	loss_value_3: 0.03901
	loss_reward_3: 0.00506
	loss_policy_4: 0.05001
	accuracy_policy_4: 0.56229
	loss_value_4: 0.04037
	loss_reward_4: 0.00553
	loss_policy_5: 0.05309
	accuracy_policy_5: 0.53885
	loss_value_5: 0.04164
	loss_reward_5: 0.00622
	loss_policy: 0.37429
	loss_value: 0.36426
	loss_reward: 0.02681
[2024-05-07 06:04:45] nn step 21000, lr: 0.06561.
	loss_policy_0: 0.1159
	accuracy_policy_0: 0.76113
	loss_value_0: 0.1619
	loss_policy_1: 0.03295
	accuracy_policy_1: 0.68059
	loss_value_1: 0.03423
	loss_reward_1: 0.00495
	loss_policy_2: 0.03759
	accuracy_policy_2: 0.64736
	loss_value_2: 0.03569
	loss_reward_2: 0.00463
	loss_policy_3: 0.0416
	accuracy_policy_3: 0.62166
	loss_value_3: 0.03719
	loss_reward_3: 0.00499
	loss_policy_4: 0.04506
	accuracy_policy_4: 0.59682
	loss_value_4: 0.03854
	loss_reward_4: 0.00524
	loss_policy_5: 0.04816
	accuracy_policy_5: 0.57418
	loss_value_5: 0.03992
	loss_reward_5: 0.00627
	loss_policy: 0.32127
	loss_value: 0.34747
	loss_reward: 0.02608
Optimization_Done 21000
[2024-05-07 06:07:01] [command] train weight_iter_21000.pkl 105 106
[2024-05-07 06:07:38] nn step 21100, lr: 0.06561.
	loss_policy_0: 0.15015
	accuracy_policy_0: 0.68371
	loss_value_0: 0.15265
	loss_policy_1: 0.03819
	accuracy_policy_1: 0.61564
	loss_value_1: 0.03224
	loss_reward_1: 0.00511
	loss_policy_2: 0.04291
	accuracy_policy_2: 0.58053
	loss_value_2: 0.03386
	loss_reward_2: 0.00488
	loss_policy_3: 0.04636
	accuracy_policy_3: 0.55027
	loss_value_3: 0.03532
	loss_reward_3: 0.00509
	loss_policy_4: 0.04984
	accuracy_policy_4: 0.52652
	loss_value_4: 0.03667
	loss_reward_4: 0.00559
	loss_policy_5: 0.0531
	accuracy_policy_5: 0.50492
	loss_value_5: 0.03801
	loss_reward_5: 0.00647
	loss_policy: 0.38055
	loss_value: 0.32875
	loss_reward: 0.02713
[2024-05-07 06:08:14] nn step 21200, lr: 0.06561.
	loss_policy_0: 0.121
	accuracy_policy_0: 0.73268
	loss_value_0: 0.15333
	loss_policy_1: 0.03418
	accuracy_policy_1: 0.64904
	loss_value_1: 0.03252
	loss_reward_1: 0.00527
	loss_policy_2: 0.03909
	accuracy_policy_2: 0.61506
	loss_value_2: 0.03406
	loss_reward_2: 0.0048
	loss_policy_3: 0.0432
	accuracy_policy_3: 0.57971
	loss_value_3: 0.03551
	loss_reward_3: 0.00516
	loss_policy_4: 0.04665
	accuracy_policy_4: 0.55736
	loss_value_4: 0.03671
	loss_reward_4: 0.00565
	loss_policy_5: 0.04977
	accuracy_policy_5: 0.53959
	loss_value_5: 0.03804
	loss_reward_5: 0.00676
	loss_policy: 0.3339
	loss_value: 0.33017
	loss_reward: 0.02764
Optimization_Done 21200
[2024-05-07 06:10:23] [command] train weight_iter_21200.pkl 106 107
[2024-05-07 06:11:00] nn step 21300, lr: 0.06561.
	loss_policy_0: 0.15309
	accuracy_policy_0: 0.69113
	loss_value_0: 0.16035
	loss_policy_1: 0.03871
	accuracy_policy_1: 0.63037
	loss_value_1: 0.03375
	loss_reward_1: 0.00646
	loss_policy_2: 0.04288
	accuracy_policy_2: 0.59387
	loss_value_2: 0.03514
	loss_reward_2: 0.00591
	loss_policy_3: 0.04602
	accuracy_policy_3: 0.57121
	loss_value_3: 0.03656
	loss_reward_3: 0.00644
	loss_policy_4: 0.04918
	accuracy_policy_4: 0.5507
	loss_value_4: 0.03784
	loss_reward_4: 0.00683
	loss_policy_5: 0.05161
	accuracy_policy_5: 0.53316
	loss_value_5: 0.03904
	loss_reward_5: 0.00788
	loss_policy: 0.38149
	loss_value: 0.34268
	loss_reward: 0.03353
[2024-05-07 06:11:35] nn step 21400, lr: 0.06561.
	loss_policy_0: 0.12093
	accuracy_policy_0: 0.74232
	loss_value_0: 0.15639
	loss_policy_1: 0.03307
	accuracy_policy_1: 0.66336
	loss_value_1: 0.03283
	loss_reward_1: 0.006
	loss_policy_2: 0.0376
	accuracy_policy_2: 0.62549
	loss_value_2: 0.03404
	loss_reward_2: 0.00543
	loss_policy_3: 0.04099
	accuracy_policy_3: 0.60201
	loss_value_3: 0.03521
	loss_reward_3: 0.00566
	loss_policy_4: 0.04399
	accuracy_policy_4: 0.57969
	loss_value_4: 0.03633
	loss_reward_4: 0.00635
	loss_policy_5: 0.04654
	accuracy_policy_5: 0.5585
	loss_value_5: 0.03738
	loss_reward_5: 0.00747
	loss_policy: 0.32312
	loss_value: 0.33218
	loss_reward: 0.0309
Optimization_Done 21400
[2024-05-07 06:13:51] [command] train weight_iter_21400.pkl 107 108
[2024-05-07 06:14:28] nn step 21500, lr: 0.06561.
	loss_policy_0: 0.13961
	accuracy_policy_0: 0.73537
	loss_value_0: 0.18578
	loss_policy_1: 0.03644
	accuracy_policy_1: 0.66146
	loss_value_1: 0.03887
	loss_reward_1: 0.0059
	loss_policy_2: 0.04026
	accuracy_policy_2: 0.63197
	loss_value_2: 0.04018
	loss_reward_2: 0.00545
	loss_policy_3: 0.04323
	accuracy_policy_3: 0.61238
	loss_value_3: 0.04133
	loss_reward_3: 0.00576
	loss_policy_4: 0.04592
	accuracy_policy_4: 0.589
	loss_value_4: 0.04239
	loss_reward_4: 0.00641
	loss_policy_5: 0.04824
	accuracy_policy_5: 0.57152
	loss_value_5: 0.04336
	loss_reward_5: 0.00719
	loss_policy: 0.35369
	loss_value: 0.39191
	loss_reward: 0.0307
[2024-05-07 06:15:03] nn step 21600, lr: 0.06561.
	loss_policy_0: 0.11052
	accuracy_policy_0: 0.77219
	loss_value_0: 0.17646
	loss_policy_1: 0.03184
	accuracy_policy_1: 0.68447
	loss_value_1: 0.0369
	loss_reward_1: 0.00546
	loss_policy_2: 0.03585
	accuracy_policy_2: 0.6509
	loss_value_2: 0.038
	loss_reward_2: 0.00496
	loss_policy_3: 0.03866
	accuracy_policy_3: 0.63082
	loss_value_3: 0.03907
	loss_reward_3: 0.00546
	loss_policy_4: 0.0411
	accuracy_policy_4: 0.61477
	loss_value_4: 0.04007
	loss_reward_4: 0.00592
	loss_policy_5: 0.04334
	accuracy_policy_5: 0.59922
	loss_value_5: 0.04099
	loss_reward_5: 0.00692
	loss_policy: 0.30131
	loss_value: 0.37149
	loss_reward: 0.02871
Optimization_Done 21600
[2024-05-07 06:17:18] [command] train weight_iter_21600.pkl 108 109
[2024-05-07 06:17:55] nn step 21700, lr: 0.06561.
	loss_policy_0: 0.13693
	accuracy_policy_0: 0.68318
	loss_value_0: 0.16096
	loss_policy_1: 0.03469
	accuracy_policy_1: 0.61879
	loss_value_1: 0.03403
	loss_reward_1: 0.00464
	loss_policy_2: 0.03889
	accuracy_policy_2: 0.57873
	loss_value_2: 0.03568
	loss_reward_2: 0.00439
	loss_policy_3: 0.04154
	accuracy_policy_3: 0.55777
	loss_value_3: 0.03702
	loss_reward_3: 0.0046
	loss_policy_4: 0.04443
	accuracy_policy_4: 0.52834
	loss_value_4: 0.03815
	loss_reward_4: 0.00493
	loss_policy_5: 0.04698
	accuracy_policy_5: 0.50734
	loss_value_5: 0.03932
	loss_reward_5: 0.00563
	loss_policy: 0.34346
	loss_value: 0.34516
	loss_reward: 0.02419
[2024-05-07 06:18:31] nn step 21800, lr: 0.06561.
	loss_policy_0: 0.10705
	accuracy_policy_0: 0.74898
	loss_value_0: 0.15639
	loss_policy_1: 0.03002
	accuracy_policy_1: 0.66553
	loss_value_1: 0.03319
	loss_reward_1: 0.00439
	loss_policy_2: 0.03435
	accuracy_policy_2: 0.62322
	loss_value_2: 0.03453
	loss_reward_2: 0.00414
	loss_policy_3: 0.03758
	accuracy_policy_3: 0.59932
	loss_value_3: 0.03582
	loss_reward_3: 0.00449
	loss_policy_4: 0.0402
	accuracy_policy_4: 0.57633
	loss_value_4: 0.03688
	loss_reward_4: 0.00475
	loss_policy_5: 0.04282
	accuracy_policy_5: 0.55654
	loss_value_5: 0.03796
	loss_reward_5: 0.00564
	loss_policy: 0.29202
	loss_value: 0.33478
	loss_reward: 0.02341
Optimization_Done 21800
[2024-05-07 06:20:43] [command] train weight_iter_21800.pkl 109 110
[2024-05-07 06:21:21] nn step 21900, lr: 0.06561.
	loss_policy_0: 0.17277
	accuracy_policy_0: 0.65035
	loss_value_0: 0.17044
	loss_policy_1: 0.04169
	accuracy_policy_1: 0.59031
	loss_value_1: 0.03586
	loss_reward_1: 0.00603
	loss_policy_2: 0.04545
	accuracy_policy_2: 0.559
	loss_value_2: 0.03756
	loss_reward_2: 0.006
	loss_policy_3: 0.04905
	accuracy_policy_3: 0.53318
	loss_value_3: 0.03907
	loss_reward_3: 0.00633
	loss_policy_4: 0.05193
	accuracy_policy_4: 0.50943
	loss_value_4: 0.04045
	loss_reward_4: 0.0069
	loss_policy_5: 0.05463
	accuracy_policy_5: 0.48949
	loss_value_5: 0.04181
	loss_reward_5: 0.00745
	loss_policy: 0.41552
	loss_value: 0.36519
	loss_reward: 0.03272
[2024-05-07 06:21:57] nn step 22000, lr: 0.06561.
	loss_policy_0: 0.14652
	accuracy_policy_0: 0.699
	loss_value_0: 0.17108
	loss_policy_1: 0.03774
	accuracy_policy_1: 0.61865
	loss_value_1: 0.03589
	loss_reward_1: 0.00596
	loss_policy_2: 0.0421
	accuracy_policy_2: 0.58449
	loss_value_2: 0.03761
	loss_reward_2: 0.00578
	loss_policy_3: 0.04562
	accuracy_policy_3: 0.56
	loss_value_3: 0.03897
	loss_reward_3: 0.00617
	loss_policy_4: 0.0489
	accuracy_policy_4: 0.53521
	loss_value_4: 0.04021
	loss_reward_4: 0.0064
	loss_policy_5: 0.0516
	accuracy_policy_5: 0.51396
	loss_value_5: 0.04134
	loss_reward_5: 0.00746
	loss_policy: 0.37248
	loss_value: 0.36509
	loss_reward: 0.03178
Optimization_Done 22000
[2024-05-07 06:24:12] [command] train weight_iter_22000.pkl 110 111
[2024-05-07 06:24:50] nn step 22100, lr: 0.06561.
	loss_policy_0: 0.17479
	accuracy_policy_0: 0.6732
	loss_value_0: 0.18258
	loss_policy_1: 0.04442
	accuracy_policy_1: 0.5967
	loss_value_1: 0.03832
	loss_reward_1: 0.00697
	loss_policy_2: 0.04923
	accuracy_policy_2: 0.5598
	loss_value_2: 0.03992
	loss_reward_2: 0.00668
	loss_policy_3: 0.05233
	accuracy_policy_3: 0.53631
	loss_value_3: 0.04134
	loss_reward_3: 0.00717
	loss_policy_4: 0.05537
	accuracy_policy_4: 0.51398
	loss_value_4: 0.04264
	loss_reward_4: 0.00772
	loss_policy_5: 0.05808
	accuracy_policy_5: 0.4932
	loss_value_5: 0.04395
	loss_reward_5: 0.0087
	loss_policy: 0.4342
	loss_value: 0.38873
	loss_reward: 0.03724
[2024-05-07 06:25:26] nn step 22200, lr: 0.06561.
	loss_policy_0: 0.14704
	accuracy_policy_0: 0.71012
	loss_value_0: 0.17841
	loss_policy_1: 0.03929
	accuracy_policy_1: 0.62867
	loss_value_1: 0.03752
	loss_reward_1: 0.00685
	loss_policy_2: 0.04445
	accuracy_policy_2: 0.58852
	loss_value_2: 0.03912
	loss_reward_2: 0.00639
	loss_policy_3: 0.04809
	accuracy_policy_3: 0.56506
	loss_value_3: 0.04044
	loss_reward_3: 0.00687
	loss_policy_4: 0.0512
	accuracy_policy_4: 0.54301
	loss_value_4: 0.0418
	loss_reward_4: 0.00757
	loss_policy_5: 0.05375
	accuracy_policy_5: 0.52023
	loss_value_5: 0.04309
	loss_reward_5: 0.00836
	loss_policy: 0.38382
	loss_value: 0.38038
	loss_reward: 0.03605
Optimization_Done 22200
[2024-05-07 06:27:41] [command] train weight_iter_22200.pkl 111 112
[2024-05-07 06:28:18] nn step 22300, lr: 0.06561.
	loss_policy_0: 0.14512
	accuracy_policy_0: 0.71762
	loss_value_0: 0.18519
	loss_policy_1: 0.03931
	accuracy_policy_1: 0.62879
	loss_value_1: 0.03861
	loss_reward_1: 0.00722
	loss_policy_2: 0.04404
	accuracy_policy_2: 0.58914
	loss_value_2: 0.04016
	loss_reward_2: 0.00656
	loss_policy_3: 0.04721
	accuracy_policy_3: 0.56754
	loss_value_3: 0.04137
	loss_reward_3: 0.00718
	loss_policy_4: 0.0498
	accuracy_policy_4: 0.54822
	loss_value_4: 0.04244
	loss_reward_4: 0.00778
	loss_policy_5: 0.05234
	accuracy_policy_5: 0.52768
	loss_value_5: 0.04355
	loss_reward_5: 0.00889
	loss_policy: 0.37782
	loss_value: 0.39131
	loss_reward: 0.03762
[2024-05-07 06:28:54] nn step 22400, lr: 0.06561.
	loss_policy_0: 0.12032
	accuracy_policy_0: 0.75789
	loss_value_0: 0.18286
	loss_policy_1: 0.03576
	accuracy_policy_1: 0.6577
	loss_value_1: 0.03814
	loss_reward_1: 0.00694
	loss_policy_2: 0.04061
	accuracy_policy_2: 0.61568
	loss_value_2: 0.03962
	loss_reward_2: 0.00639
	loss_policy_3: 0.04379
	accuracy_policy_3: 0.59371
	loss_value_3: 0.04094
	loss_reward_3: 0.00703
	loss_policy_4: 0.04683
	accuracy_policy_4: 0.57396
	loss_value_4: 0.04206
	loss_reward_4: 0.00742
	loss_policy_5: 0.04957
	accuracy_policy_5: 0.55258
	loss_value_5: 0.04312
	loss_reward_5: 0.00872
	loss_policy: 0.33687
	loss_value: 0.38674
	loss_reward: 0.0365
Optimization_Done 22400
[2024-05-07 06:30:48] [command] train weight_iter_22400.pkl 112 113
[2024-05-07 06:31:26] nn step 22500, lr: 0.06561.
	loss_policy_0: 0.12718
	accuracy_policy_0: 0.74564
	loss_value_0: 0.18846
	loss_policy_1: 0.03503
	accuracy_policy_1: 0.66025
	loss_value_1: 0.03921
	loss_reward_1: 0.00646
	loss_policy_2: 0.0392
	accuracy_policy_2: 0.62273
	loss_value_2: 0.04034
	loss_reward_2: 0.00607
	loss_policy_3: 0.04227
	accuracy_policy_3: 0.59529
	loss_value_3: 0.04126
	loss_reward_3: 0.00684
	loss_policy_4: 0.04473
	accuracy_policy_4: 0.57154
	loss_value_4: 0.04211
	loss_reward_4: 0.00742
	loss_policy_5: 0.04709
	accuracy_policy_5: 0.54301
	loss_value_5: 0.04288
	loss_reward_5: 0.0086
	loss_policy: 0.33551
	loss_value: 0.39427
	loss_reward: 0.0354
[2024-05-07 06:32:01] nn step 22600, lr: 0.06561.
	loss_policy_0: 0.10909
	accuracy_policy_0: 0.78463
	loss_value_0: 0.18395
	loss_policy_1: 0.03319
	accuracy_policy_1: 0.67842
	loss_value_1: 0.03865
	loss_reward_1: 0.00666
	loss_policy_2: 0.03755
	accuracy_policy_2: 0.6434
	loss_value_2: 0.04
	loss_reward_2: 0.00612
	loss_policy_3: 0.04046
	accuracy_policy_3: 0.6215
	loss_value_3: 0.04089
	loss_reward_3: 0.00698
	loss_policy_4: 0.04318
	accuracy_policy_4: 0.59783
	loss_value_4: 0.04194
	loss_reward_4: 0.00748
	loss_policy_5: 0.04614
	accuracy_policy_5: 0.56869
	loss_value_5: 0.04277
	loss_reward_5: 0.00863
	loss_policy: 0.30961
	loss_value: 0.3882
	loss_reward: 0.03587
Optimization_Done 22600
[2024-05-07 06:34:20] [command] train weight_iter_22600.pkl 113 114
[2024-05-07 06:34:57] nn step 22700, lr: 0.06561.
	loss_policy_0: 0.14937
	accuracy_policy_0: 0.69574
	loss_value_0: 0.16659
	loss_policy_1: 0.03845
	accuracy_policy_1: 0.62373
	loss_value_1: 0.03504
	loss_reward_1: 0.00592
	loss_policy_2: 0.04314
	accuracy_policy_2: 0.5859
	loss_value_2: 0.0367
	loss_reward_2: 0.00573
	loss_policy_3: 0.04677
	accuracy_policy_3: 0.55184
	loss_value_3: 0.03826
	loss_reward_3: 0.00627
	loss_policy_4: 0.04986
	accuracy_policy_4: 0.52547
	loss_value_4: 0.03956
	loss_reward_4: 0.00679
	loss_policy_5: 0.05312
	accuracy_policy_5: 0.50135
	loss_value_5: 0.04046
	loss_reward_5: 0.00769
	loss_policy: 0.38072
	loss_value: 0.35661
	loss_reward: 0.0324
[2024-05-07 06:35:33] nn step 22800, lr: 0.06561.
	loss_policy_0: 0.12458
	accuracy_policy_0: 0.74051
	loss_value_0: 0.16321
	loss_policy_1: 0.03507
	accuracy_policy_1: 0.65086
	loss_value_1: 0.0345
	loss_reward_1: 0.00579
	loss_policy_2: 0.03973
	accuracy_policy_2: 0.61074
	loss_value_2: 0.03602
	loss_reward_2: 0.00577
	loss_policy_3: 0.0434
	accuracy_policy_3: 0.58221
	loss_value_3: 0.03755
	loss_reward_3: 0.00614
	loss_policy_4: 0.04666
	accuracy_policy_4: 0.55211
	loss_value_4: 0.03873
	loss_reward_4: 0.00663
	loss_policy_5: 0.04922
	accuracy_policy_5: 0.52729
	loss_value_5: 0.04002
	loss_reward_5: 0.00789
	loss_policy: 0.33866
	loss_value: 0.35002
	loss_reward: 0.03223
Optimization_Done 22800
[2024-05-07 06:37:42] [command] train weight_iter_22800.pkl 114 115
[2024-05-07 06:38:20] nn step 22900, lr: 0.06561.
	loss_policy_0: 0.16912
	accuracy_policy_0: 0.66369
	loss_value_0: 0.15555
	loss_policy_1: 0.04196
	accuracy_policy_1: 0.59584
	loss_value_1: 0.03285
	loss_reward_1: 0.00567
	loss_policy_2: 0.04608
	accuracy_policy_2: 0.55799
	loss_value_2: 0.03429
	loss_reward_2: 0.00555
	loss_policy_3: 0.04965
	accuracy_policy_3: 0.53
	loss_value_3: 0.036
	loss_reward_3: 0.00598
	loss_policy_4: 0.05312
	accuracy_policy_4: 0.5065
	loss_value_4: 0.03733
	loss_reward_4: 0.00653
	loss_policy_5: 0.05578
	accuracy_policy_5: 0.48717
	loss_value_5: 0.03862
	loss_reward_5: 0.00742
	loss_policy: 0.41571
	loss_value: 0.33464
	loss_reward: 0.03115
[2024-05-07 06:38:56] nn step 23000, lr: 0.06561.
	loss_policy_0: 0.14036
	accuracy_policy_0: 0.71451
	loss_value_0: 0.15694
	loss_policy_1: 0.03767
	accuracy_policy_1: 0.63115
	loss_value_1: 0.03316
	loss_reward_1: 0.00556
	loss_policy_2: 0.04219
	accuracy_policy_2: 0.59205
	loss_value_2: 0.03464
	loss_reward_2: 0.00545
	loss_policy_3: 0.04605
	accuracy_policy_3: 0.56523
	loss_value_3: 0.03599
	loss_reward_3: 0.00572
	loss_policy_4: 0.04954
	accuracy_policy_4: 0.53906
	loss_value_4: 0.0373
	loss_reward_4: 0.00626
	loss_policy_5: 0.05296
	accuracy_policy_5: 0.51482
	loss_value_5: 0.03856
	loss_reward_5: 0.00735
	loss_policy: 0.36876
	loss_value: 0.33658
	loss_reward: 0.03035
Optimization_Done 23000
[2024-05-07 06:41:11] [command] train weight_iter_23000.pkl 115 116
[2024-05-07 06:41:48] nn step 23100, lr: 0.06561.
	loss_policy_0: 0.16228
	accuracy_policy_0: 0.66986
	loss_value_0: 0.15559
	loss_policy_1: 0.04041
	accuracy_policy_1: 0.59926
	loss_value_1: 0.03264
	loss_reward_1: 0.00602
	loss_policy_2: 0.04399
	accuracy_policy_2: 0.57016
	loss_value_2: 0.03403
	loss_reward_2: 0.00579
	loss_policy_3: 0.04739
	accuracy_policy_3: 0.54273
	loss_value_3: 0.03532
	loss_reward_3: 0.00618
	loss_policy_4: 0.05029
	accuracy_policy_4: 0.51369
	loss_value_4: 0.03647
	loss_reward_4: 0.00676
	loss_policy_5: 0.05303
	accuracy_policy_5: 0.49633
	loss_value_5: 0.03752
	loss_reward_5: 0.00764
	loss_policy: 0.39739
	loss_value: 0.33158
	loss_reward: 0.0324
[2024-05-07 06:42:24] nn step 23200, lr: 0.06561.
	loss_policy_0: 0.13629
	accuracy_policy_0: 0.71641
	loss_value_0: 0.15941
	loss_policy_1: 0.03606
	accuracy_policy_1: 0.63736
	loss_value_1: 0.03354
	loss_reward_1: 0.00573
	loss_policy_2: 0.04046
	accuracy_policy_2: 0.59918
	loss_value_2: 0.03509
	loss_reward_2: 0.00553
	loss_policy_3: 0.04403
	accuracy_policy_3: 0.56484
	loss_value_3: 0.03629
	loss_reward_3: 0.00577
	loss_policy_4: 0.04699
	accuracy_policy_4: 0.54529
	loss_value_4: 0.03736
	loss_reward_4: 0.00638
	loss_policy_5: 0.04976
	accuracy_policy_5: 0.52318
	loss_value_5: 0.03856
	loss_reward_5: 0.00758
	loss_policy: 0.35358
	loss_value: 0.34025
	loss_reward: 0.03099
Optimization_Done 23200
[2024-05-07 06:44:26] [command] train weight_iter_23200.pkl 116 117
[2024-05-07 06:45:03] nn step 23300, lr: 0.06561.
	loss_policy_0: 0.14412
	accuracy_policy_0: 0.6943
	loss_value_0: 0.15337
	loss_policy_1: 0.03647
	accuracy_policy_1: 0.62203
	loss_value_1: 0.03223
	loss_reward_1: 0.00522
	loss_policy_2: 0.03962
	accuracy_policy_2: 0.58984
	loss_value_2: 0.03362
	loss_reward_2: 0.00498
	loss_policy_3: 0.04299
	accuracy_policy_3: 0.56035
	loss_value_3: 0.03475
	loss_reward_3: 0.00541
	loss_policy_4: 0.04518
	accuracy_policy_4: 0.53691
	loss_value_4: 0.03584
	loss_reward_4: 0.00562
	loss_policy_5: 0.0474
	accuracy_policy_5: 0.52201
	loss_value_5: 0.03687
	loss_reward_5: 0.00638
	loss_policy: 0.35579
	loss_value: 0.32669
	loss_reward: 0.02761
[2024-05-07 06:45:40] nn step 23400, lr: 0.06561.
	loss_policy_0: 0.11308
	accuracy_policy_0: 0.741
	loss_value_0: 0.14269
	loss_policy_1: 0.03144
	accuracy_policy_1: 0.64891
	loss_value_1: 0.03011
	loss_reward_1: 0.00478
	loss_policy_2: 0.03486
	accuracy_policy_2: 0.61588
	loss_value_2: 0.03136
	loss_reward_2: 0.00466
	loss_policy_3: 0.0373
	accuracy_policy_3: 0.59219
	loss_value_3: 0.03241
	loss_reward_3: 0.0049
	loss_policy_4: 0.03995
	accuracy_policy_4: 0.56586
	loss_value_4: 0.03347
	loss_reward_4: 0.00531
	loss_policy_5: 0.04207
	accuracy_policy_5: 0.54643
	loss_value_5: 0.03459
	loss_reward_5: 0.00612
	loss_policy: 0.29871
	loss_value: 0.30462
	loss_reward: 0.02577
Optimization_Done 23400
[2024-05-07 06:48:02] [command] train weight_iter_23400.pkl 117 118
[2024-05-07 06:48:39] nn step 23500, lr: 0.06561.
	loss_policy_0: 0.1958
	accuracy_policy_0: 0.60986
	loss_value_0: 0.13583
	loss_policy_1: 0.04423
	accuracy_policy_1: 0.55539
	loss_value_1: 0.02878
	loss_reward_1: 0.00418
	loss_policy_2: 0.04776
	accuracy_policy_2: 0.526
	loss_value_2: 0.02997
	loss_reward_2: 0.00418
	loss_policy_3: 0.05029
	accuracy_policy_3: 0.50775
	loss_value_3: 0.0311
	loss_reward_3: 0.00444
	loss_policy_4: 0.05235
	accuracy_policy_4: 0.48781
	loss_value_4: 0.03222
	loss_reward_4: 0.00474
	loss_policy_5: 0.05455
	accuracy_policy_5: 0.47287
	loss_value_5: 0.03317
	loss_reward_5: 0.00523
	loss_policy: 0.44498
	loss_value: 0.29107
	loss_reward: 0.02277
[2024-05-07 06:49:15] nn step 23600, lr: 0.06561.
	loss_policy_0: 0.15081
	accuracy_policy_0: 0.69443
	loss_value_0: 0.13765
	loss_policy_1: 0.03824
	accuracy_policy_1: 0.61162
	loss_value_1: 0.02896
	loss_reward_1: 0.00412
	loss_policy_2: 0.04207
	accuracy_policy_2: 0.58189
	loss_value_2: 0.03019
	loss_reward_2: 0.00408
	loss_policy_3: 0.0448
	accuracy_policy_3: 0.55689
	loss_value_3: 0.03136
	loss_reward_3: 0.0044
	loss_policy_4: 0.0472
	accuracy_policy_4: 0.53859
	loss_value_4: 0.03249
	loss_reward_4: 0.00459
	loss_policy_5: 0.04974
	accuracy_policy_5: 0.5183
	loss_value_5: 0.03358
	loss_reward_5: 0.00528
	loss_policy: 0.37287
	loss_value: 0.29423
	loss_reward: 0.02247
Optimization_Done 23600
[2024-05-07 06:51:35] [command] train weight_iter_23600.pkl 118 119
[2024-05-07 06:52:12] nn step 23700, lr: 0.06561.
	loss_policy_0: 0.21893
	accuracy_policy_0: 0.62244
	loss_value_0: 0.17514
	loss_policy_1: 0.04915
	accuracy_policy_1: 0.57887
	loss_value_1: 0.03643
	loss_reward_1: 0.00564
	loss_policy_2: 0.05271
	accuracy_policy_2: 0.55451
	loss_value_2: 0.03792
	loss_reward_2: 0.00538
	loss_policy_3: 0.05569
	accuracy_policy_3: 0.53395
	loss_value_3: 0.0391
	loss_reward_3: 0.00574
	loss_policy_4: 0.05811
	accuracy_policy_4: 0.51135
	loss_value_4: 0.04043
	loss_reward_4: 0.00623
	loss_policy_5: 0.06053
	accuracy_policy_5: 0.49959
	loss_value_5: 0.04153
	loss_reward_5: 0.00674
	loss_policy: 0.49512
	loss_value: 0.37056
	loss_reward: 0.02973
[2024-05-07 06:52:48] nn step 23800, lr: 0.06561.
	loss_policy_0: 0.18731
	accuracy_policy_0: 0.67283
	loss_value_0: 0.17727
	loss_policy_1: 0.04475
	accuracy_policy_1: 0.61252
	loss_value_1: 0.03683
	loss_reward_1: 0.00553
	loss_policy_2: 0.04937
	accuracy_policy_2: 0.58033
	loss_value_2: 0.03824
	loss_reward_2: 0.00521
	loss_policy_3: 0.05251
	accuracy_policy_3: 0.55883
	loss_value_3: 0.03947
	loss_reward_3: 0.00537
	loss_policy_4: 0.05508
	accuracy_policy_4: 0.53994
	loss_value_4: 0.04055
	loss_reward_4: 0.00582
	loss_policy_5: 0.05765
	accuracy_policy_5: 0.52627
	loss_value_5: 0.04185
	loss_reward_5: 0.00672
	loss_policy: 0.44669
	loss_value: 0.37421
	loss_reward: 0.02866
Optimization_Done 23800
[2024-05-07 06:55:10] [command] train weight_iter_23800.pkl 119 120
[2024-05-07 06:55:48] nn step 23900, lr: 0.06561.
	loss_policy_0: 0.17549
	accuracy_policy_0: 0.66205
	loss_value_0: 0.17376
	loss_policy_1: 0.04209
	accuracy_policy_1: 0.60326
	loss_value_1: 0.0362
	loss_reward_1: 0.00636
	loss_policy_2: 0.04666
	accuracy_policy_2: 0.56732
	loss_value_2: 0.03753
	loss_reward_2: 0.00606
	loss_policy_3: 0.04942
	accuracy_policy_3: 0.5457
	loss_value_3: 0.03876
	loss_reward_3: 0.00644
	loss_policy_4: 0.05238
	accuracy_policy_4: 0.52482
	loss_value_4: 0.03973
	loss_reward_4: 0.00715
	loss_policy_5: 0.05468
	accuracy_policy_5: 0.50643
	loss_value_5: 0.04086
	loss_reward_5: 0.0079
	loss_policy: 0.42071
	loss_value: 0.36684
	loss_reward: 0.03391
[2024-05-07 06:56:24] nn step 24000, lr: 0.06561.
	loss_policy_0: 0.14851
	accuracy_policy_0: 0.71883
	loss_value_0: 0.17678
	loss_policy_1: 0.03849
	accuracy_policy_1: 0.64254
	loss_value_1: 0.03677
	loss_reward_1: 0.00616
	loss_policy_2: 0.04298
	accuracy_policy_2: 0.60232
	loss_value_2: 0.03822
	loss_reward_2: 0.00567
	loss_policy_3: 0.04617
	accuracy_policy_3: 0.5809
	loss_value_3: 0.0395
	loss_reward_3: 0.00632
	loss_policy_4: 0.04894
	accuracy_policy_4: 0.56223
	loss_value_4: 0.04065
	loss_reward_4: 0.00673
	loss_policy_5: 0.05156
	accuracy_policy_5: 0.54045
	loss_value_5: 0.04176
	loss_reward_5: 0.0079
	loss_policy: 0.37666
	loss_value: 0.37368
	loss_reward: 0.03278
Optimization_Done 24000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 07:06:51] [command] train weight_iter_24000.pkl 120 121
[2024-05-07 07:07:35] nn step 24100, lr: 0.059049.
	loss_policy_0: 0.17242
	accuracy_policy_0: 0.68611
	loss_value_0: 0.1693
	loss_policy_1: 0.04407
	accuracy_policy_1: 0.61145
	loss_value_1: 0.03532
	loss_reward_1: 0.00613
	loss_policy_2: 0.04973
	accuracy_policy_2: 0.56697
	loss_value_2: 0.03683
	loss_reward_2: 0.00579
	loss_policy_3: 0.05358
	accuracy_policy_3: 0.54127
	loss_value_3: 0.03802
	loss_reward_3: 0.00632
	loss_policy_4: 0.05691
	accuracy_policy_4: 0.51457
	loss_value_4: 0.03931
	loss_reward_4: 0.00682
	loss_policy_5: 0.06033
	accuracy_policy_5: 0.49195
	loss_value_5: 0.04042
	loss_reward_5: 0.00781
	loss_policy: 0.43704
	loss_value: 0.3592
	loss_reward: 0.03286
[2024-05-07 07:08:10] nn step 24200, lr: 0.059049.
	loss_policy_0: 0.14192
	accuracy_policy_0: 0.73477
	loss_value_0: 0.17261
	loss_policy_1: 0.04045
	accuracy_policy_1: 0.64057
	loss_value_1: 0.03604
	loss_reward_1: 0.00608
	loss_policy_2: 0.04635
	accuracy_policy_2: 0.59572
	loss_value_2: 0.03763
	loss_reward_2: 0.0057
	loss_policy_3: 0.05076
	accuracy_policy_3: 0.57227
	loss_value_3: 0.03905
	loss_reward_3: 0.00607
	loss_policy_4: 0.05419
	accuracy_policy_4: 0.54695
	loss_value_4: 0.04026
	loss_reward_4: 0.00668
	loss_policy_5: 0.05772
	accuracy_policy_5: 0.52445
	loss_value_5: 0.04164
	loss_reward_5: 0.00789
	loss_policy: 0.39138
	loss_value: 0.36724
	loss_reward: 0.03243
Optimization_Done 24200
[2024-05-07 07:10:30] [command] train weight_iter_24200.pkl 121 122
[2024-05-07 07:11:07] nn step 24300, lr: 0.059049.
	loss_policy_0: 0.16565
	accuracy_policy_0: 0.67447
	loss_value_0: 0.16052
	loss_policy_1: 0.04127
	accuracy_policy_1: 0.61059
	loss_value_1: 0.03356
	loss_reward_1: 0.0048
	loss_policy_2: 0.04588
	accuracy_policy_2: 0.57078
	loss_value_2: 0.03497
	loss_reward_2: 0.00444
	loss_policy_3: 0.04997
	accuracy_policy_3: 0.5435
	loss_value_3: 0.03639
	loss_reward_3: 0.00456
	loss_policy_4: 0.05319
	accuracy_policy_4: 0.51975
	loss_value_4: 0.03749
	loss_reward_4: 0.00506
	loss_policy_5: 0.0564
	accuracy_policy_5: 0.49635
	loss_value_5: 0.03876
	loss_reward_5: 0.00583
	loss_policy: 0.41235
	loss_value: 0.34169
	loss_reward: 0.0247
[2024-05-07 07:11:42] nn step 24400, lr: 0.059049.
	loss_policy_0: 0.12444
	accuracy_policy_0: 0.74088
	loss_value_0: 0.15379
	loss_policy_1: 0.03516
	accuracy_policy_1: 0.65354
	loss_value_1: 0.0322
	loss_reward_1: 0.00452
	loss_policy_2: 0.04031
	accuracy_policy_2: 0.61607
	loss_value_2: 0.03351
	loss_reward_2: 0.00429
	loss_policy_3: 0.04448
	accuracy_policy_3: 0.58424
	loss_value_3: 0.03476
	loss_reward_3: 0.00449
	loss_policy_4: 0.04755
	accuracy_policy_4: 0.55646
	loss_value_4: 0.03591
	loss_reward_4: 0.00483
	loss_policy_5: 0.05104
	accuracy_policy_5: 0.53291
	loss_value_5: 0.03705
	loss_reward_5: 0.0056
	loss_policy: 0.34298
	loss_value: 0.32722
	loss_reward: 0.02373
Optimization_Done 24400
[2024-05-07 07:14:02] [command] train weight_iter_24400.pkl 122 123
[2024-05-07 07:14:39] nn step 24500, lr: 0.059049.
	loss_policy_0: 0.15439
	accuracy_policy_0: 0.66566
	loss_value_0: 0.16232
	loss_policy_1: 0.0399
	accuracy_policy_1: 0.59291
	loss_value_1: 0.03371
	loss_reward_1: 0.00599
	loss_policy_2: 0.04488
	accuracy_policy_2: 0.55342
	loss_value_2: 0.03479
	loss_reward_2: 0.00561
	loss_policy_3: 0.04808
	accuracy_policy_3: 0.53045
	loss_value_3: 0.03585
	loss_reward_3: 0.00595
	loss_policy_4: 0.05134
	accuracy_policy_4: 0.50283
	loss_value_4: 0.03698
	loss_reward_4: 0.00643
	loss_policy_5: 0.05433
	accuracy_policy_5: 0.48184
	loss_value_5: 0.03798
	loss_reward_5: 0.00758
	loss_policy: 0.39292
	loss_value: 0.34163
	loss_reward: 0.03156
[2024-05-07 07:15:15] nn step 24600, lr: 0.059049.
	loss_policy_0: 0.12701
	accuracy_policy_0: 0.72365
	loss_value_0: 0.16887
	loss_policy_1: 0.03664
	accuracy_policy_1: 0.62818
	loss_value_1: 0.03519
	loss_reward_1: 0.00593
	loss_policy_2: 0.04162
	accuracy_policy_2: 0.59002
	loss_value_2: 0.03652
	loss_reward_2: 0.00525
	loss_policy_3: 0.04529
	accuracy_policy_3: 0.56889
	loss_value_3: 0.03768
	loss_reward_3: 0.00581
	loss_policy_4: 0.04846
	accuracy_policy_4: 0.54057
	loss_value_4: 0.03894
	loss_reward_4: 0.00627
	loss_policy_5: 0.05204
	accuracy_policy_5: 0.51938
	loss_value_5: 0.04005
	loss_reward_5: 0.00747
	loss_policy: 0.35106
	loss_value: 0.35725
	loss_reward: 0.03074
Optimization_Done 24600
[2024-05-07 07:17:30] [command] train weight_iter_24600.pkl 123 124
[2024-05-07 07:18:06] nn step 24700, lr: 0.059049.
	loss_policy_0: 0.16593
	accuracy_policy_0: 0.6675
	loss_value_0: 0.15617
	loss_policy_1: 0.04325
	accuracy_policy_1: 0.58168
	loss_value_1: 0.03275
	loss_reward_1: 0.00581
	loss_policy_2: 0.04761
	accuracy_policy_2: 0.54619
	loss_value_2: 0.03403
	loss_reward_2: 0.00521
	loss_policy_3: 0.05072
	accuracy_policy_3: 0.52299
	loss_value_3: 0.03533
	loss_reward_3: 0.00568
	loss_policy_4: 0.05334
	accuracy_policy_4: 0.50623
	loss_value_4: 0.03652
	loss_reward_4: 0.00602
	loss_policy_5: 0.05598
	accuracy_policy_5: 0.48334
	loss_value_5: 0.03764
	loss_reward_5: 0.00699
	loss_policy: 0.41682
	loss_value: 0.33245
	loss_reward: 0.02972
[2024-05-07 07:18:42] nn step 24800, lr: 0.059049.
	loss_policy_0: 0.13936
	accuracy_policy_0: 0.71965
	loss_value_0: 0.15625
	loss_policy_1: 0.04005
	accuracy_policy_1: 0.61367
	loss_value_1: 0.03282
	loss_reward_1: 0.00589
	loss_policy_2: 0.0452
	accuracy_policy_2: 0.57441
	loss_value_2: 0.03429
	loss_reward_2: 0.00532
	loss_policy_3: 0.04883
	accuracy_policy_3: 0.54844
	loss_value_3: 0.0359
	loss_reward_3: 0.00572
	loss_policy_4: 0.05209
	accuracy_policy_4: 0.52855
	loss_value_4: 0.03716
	loss_reward_4: 0.00623
	loss_policy_5: 0.05483
	accuracy_policy_5: 0.506
	loss_value_5: 0.03849
	loss_reward_5: 0.00752
	loss_policy: 0.38036
	loss_value: 0.33491
	loss_reward: 0.03069
Optimization_Done 24800
[2024-05-07 07:20:57] [command] train weight_iter_24800.pkl 124 125
[2024-05-07 07:21:34] nn step 24900, lr: 0.059049.
	loss_policy_0: 0.18465
	accuracy_policy_0: 0.62672
	loss_value_0: 0.15005
	loss_policy_1: 0.04571
	accuracy_policy_1: 0.55512
	loss_value_1: 0.03136
	loss_reward_1: 0.00447
	loss_policy_2: 0.04998
	accuracy_policy_2: 0.52203
	loss_value_2: 0.03294
	loss_reward_2: 0.00413
	loss_policy_3: 0.05331
	accuracy_policy_3: 0.49975
	loss_value_3: 0.03423
	loss_reward_3: 0.0045
	loss_policy_4: 0.05572
	accuracy_policy_4: 0.47971
	loss_value_4: 0.03533
	loss_reward_4: 0.0048
	loss_policy_5: 0.0577
	accuracy_policy_5: 0.46412
	loss_value_5: 0.03645
	loss_reward_5: 0.00533
	loss_policy: 0.44707
	loss_value: 0.32036
	loss_reward: 0.02323
[2024-05-07 07:22:11] nn step 25000, lr: 0.059049.
	loss_policy_0: 0.14008
	accuracy_policy_0: 0.70346
	loss_value_0: 0.13377
	loss_policy_1: 0.03835
	accuracy_policy_1: 0.61088
	loss_value_1: 0.0283
	loss_reward_1: 0.00447
	loss_policy_2: 0.04344
	accuracy_policy_2: 0.56668
	loss_value_2: 0.02973
	loss_reward_2: 0.00402
	loss_policy_3: 0.0469
	accuracy_policy_3: 0.5399
	loss_value_3: 0.03094
	loss_reward_3: 0.00428
	loss_policy_4: 0.04923
	accuracy_policy_4: 0.52535
	loss_value_4: 0.03212
	loss_reward_4: 0.00467
	loss_policy_5: 0.05187
	accuracy_policy_5: 0.49732
	loss_value_5: 0.03326
	loss_reward_5: 0.00539
	loss_policy: 0.36987
	loss_value: 0.28812
	loss_reward: 0.02283
Optimization_Done 25000
[2024-05-07 07:24:24] [command] train weight_iter_25000.pkl 125 126
[2024-05-07 07:25:02] nn step 25100, lr: 0.059049.
	loss_policy_0: 0.2189
	accuracy_policy_0: 0.61238
	loss_value_0: 0.17769
	loss_policy_1: 0.05063
	accuracy_policy_1: 0.56143
	loss_value_1: 0.03668
	loss_reward_1: 0.00462
	loss_policy_2: 0.05483
	accuracy_policy_2: 0.52988
	loss_value_2: 0.0377
	loss_reward_2: 0.00466
	loss_policy_3: 0.05777
	accuracy_policy_3: 0.50855
	loss_value_3: 0.03876
	loss_reward_3: 0.00464
	loss_policy_4: 0.06007
	accuracy_policy_4: 0.49316
	loss_value_4: 0.03958
	loss_reward_4: 0.00487
	loss_policy_5: 0.06212
	accuracy_policy_5: 0.47395
	loss_value_5: 0.04033
	loss_reward_5: 0.00524
	loss_policy: 0.50431
	loss_value: 0.37073
	loss_reward: 0.02402
[2024-05-07 07:25:38] nn step 25200, lr: 0.059049.
	loss_policy_0: 0.16134
	accuracy_policy_0: 0.6809
	loss_value_0: 0.1568
	loss_policy_1: 0.04051
	accuracy_policy_1: 0.60936
	loss_value_1: 0.03254
	loss_reward_1: 0.00398
	loss_policy_2: 0.04563
	accuracy_policy_2: 0.5684
	loss_value_2: 0.03344
	loss_reward_2: 0.00408
	loss_policy_3: 0.04869
	accuracy_policy_3: 0.54004
	loss_value_3: 0.0344
	loss_reward_3: 0.00419
	loss_policy_4: 0.05127
	accuracy_policy_4: 0.52035
	loss_value_4: 0.03544
	loss_reward_4: 0.00435
	loss_policy_5: 0.05359
	accuracy_policy_5: 0.50076
	loss_value_5: 0.03629
	loss_reward_5: 0.00481
	loss_policy: 0.40104
	loss_value: 0.3289
	loss_reward: 0.0214
Optimization_Done 25200
[2024-05-07 07:27:51] [command] train weight_iter_25200.pkl 126 127
[2024-05-07 07:28:29] nn step 25300, lr: 0.059049.
	loss_policy_0: 0.21929
	accuracy_policy_0: 0.60738
	loss_value_0: 0.1677
	loss_policy_1: 0.04985
	accuracy_policy_1: 0.56277
	loss_value_1: 0.03486
	loss_reward_1: 0.00429
	loss_policy_2: 0.05398
	accuracy_policy_2: 0.5258
	loss_value_2: 0.03605
	loss_reward_2: 0.00414
	loss_policy_3: 0.05711
	accuracy_policy_3: 0.50129
	loss_value_3: 0.03722
	loss_reward_3: 0.00425
	loss_policy_4: 0.05945
	accuracy_policy_4: 0.48482
	loss_value_4: 0.03841
	loss_reward_4: 0.00457
	loss_policy_5: 0.06133
	accuracy_policy_5: 0.47156
	loss_value_5: 0.03953
	loss_reward_5: 0.00476
	loss_policy: 0.50102
	loss_value: 0.35378
	loss_reward: 0.02202
[2024-05-07 07:29:05] nn step 25400, lr: 0.059049.
	loss_policy_0: 0.18196
	accuracy_policy_0: 0.67564
	loss_value_0: 0.17246
	loss_policy_1: 0.04544
	accuracy_policy_1: 0.61174
	loss_value_1: 0.03575
	loss_reward_1: 0.00438
	loss_policy_2: 0.0505
	accuracy_policy_2: 0.57232
	loss_value_2: 0.03696
	loss_reward_2: 0.00423
	loss_policy_3: 0.05403
	accuracy_policy_3: 0.54539
	loss_value_3: 0.03812
	loss_reward_3: 0.00436
	loss_policy_4: 0.05717
	accuracy_policy_4: 0.52418
	loss_value_4: 0.0393
	loss_reward_4: 0.00464
	loss_policy_5: 0.05955
	accuracy_policy_5: 0.50635
	loss_value_5: 0.0403
	loss_reward_5: 0.00501
	loss_policy: 0.44866
	loss_value: 0.36287
	loss_reward: 0.02263
Optimization_Done 25400
[2024-05-07 07:31:21] [command] train weight_iter_25400.pkl 127 128
[2024-05-07 07:31:59] nn step 25500, lr: 0.059049.
	loss_policy_0: 0.2006
	accuracy_policy_0: 0.63945
	loss_value_0: 0.16555
	loss_policy_1: 0.04716
	accuracy_policy_1: 0.58459
	loss_value_1: 0.0342
	loss_reward_1: 0.00482
	loss_policy_2: 0.0517
	accuracy_policy_2: 0.55262
	loss_value_2: 0.03545
	loss_reward_2: 0.00447
	loss_policy_3: 0.05535
	accuracy_policy_3: 0.52457
	loss_value_3: 0.03664
	loss_reward_3: 0.00449
	loss_policy_4: 0.05834
	accuracy_policy_4: 0.50496
	loss_value_4: 0.03774
	loss_reward_4: 0.0048
	loss_policy_5: 0.06035
	accuracy_policy_5: 0.49367
	loss_value_5: 0.03868
	loss_reward_5: 0.00541
	loss_policy: 0.4735
	loss_value: 0.34826
	loss_reward: 0.02399
[2024-05-07 07:32:35] nn step 25600, lr: 0.059049.
	loss_policy_0: 0.1612
	accuracy_policy_0: 0.69639
	loss_value_0: 0.15697
	loss_policy_1: 0.04064
	accuracy_policy_1: 0.6308
	loss_value_1: 0.03269
	loss_reward_1: 0.00467
	loss_policy_2: 0.04573
	accuracy_policy_2: 0.5933
	loss_value_2: 0.0337
	loss_reward_2: 0.00424
	loss_policy_3: 0.04943
	accuracy_policy_3: 0.56135
	loss_value_3: 0.03483
	loss_reward_3: 0.00426
	loss_policy_4: 0.05244
	accuracy_policy_4: 0.54004
	loss_value_4: 0.036
	loss_reward_4: 0.0047
	loss_policy_5: 0.05523
	accuracy_policy_5: 0.52609
	loss_value_5: 0.03721
	loss_reward_5: 0.00518
	loss_policy: 0.40468
	loss_value: 0.33139
	loss_reward: 0.02305
Optimization_Done 25600
[2024-05-07 07:34:29] [command] train weight_iter_25600.pkl 128 129
[2024-05-07 07:35:07] nn step 25700, lr: 0.059049.
	loss_policy_0: 0.18357
	accuracy_policy_0: 0.65584
	loss_value_0: 0.17225
	loss_policy_1: 0.04506
	accuracy_policy_1: 0.59891
	loss_value_1: 0.03571
	loss_reward_1: 0.00553
	loss_policy_2: 0.04956
	accuracy_policy_2: 0.56021
	loss_value_2: 0.03667
	loss_reward_2: 0.00517
	loss_policy_3: 0.05303
	accuracy_policy_3: 0.53443
	loss_value_3: 0.03737
	loss_reward_3: 0.00536
	loss_policy_4: 0.05555
	accuracy_policy_4: 0.51174
	loss_value_4: 0.03809
	loss_reward_4: 0.00598
	loss_policy_5: 0.05797
	accuracy_policy_5: 0.49254
	loss_value_5: 0.0388
	loss_reward_5: 0.00687
	loss_policy: 0.44474
	loss_value: 0.35889
	loss_reward: 0.02892
[2024-05-07 07:35:42] nn step 25800, lr: 0.059049.
	loss_policy_0: 0.14786
	accuracy_policy_0: 0.71543
	loss_value_0: 0.1664
	loss_policy_1: 0.03942
	accuracy_policy_1: 0.635
	loss_value_1: 0.03455
	loss_reward_1: 0.00534
	loss_policy_2: 0.04457
	accuracy_policy_2: 0.59295
	loss_value_2: 0.03559
	loss_reward_2: 0.00504
	loss_policy_3: 0.0483
	accuracy_policy_3: 0.56637
	loss_value_3: 0.03664
	loss_reward_3: 0.00514
	loss_policy_4: 0.05089
	accuracy_policy_4: 0.54488
	loss_value_4: 0.03754
	loss_reward_4: 0.00578
	loss_policy_5: 0.05359
	accuracy_policy_5: 0.52246
	loss_value_5: 0.03859
	loss_reward_5: 0.00671
	loss_policy: 0.38464
	loss_value: 0.34931
	loss_reward: 0.02801
Optimization_Done 25800
[2024-05-07 07:38:00] [command] train weight_iter_25800.pkl 129 130
[2024-05-07 07:38:37] nn step 25900, lr: 0.059049.
	loss_policy_0: 0.17616
	accuracy_policy_0: 0.66939
	loss_value_0: 0.1731
	loss_policy_1: 0.04159
	accuracy_policy_1: 0.62143
	loss_value_1: 0.03588
	loss_reward_1: 0.005
	loss_policy_2: 0.04649
	accuracy_policy_2: 0.58053
	loss_value_2: 0.03705
	loss_reward_2: 0.00479
	loss_policy_3: 0.05027
	accuracy_policy_3: 0.54688
	loss_value_3: 0.03817
	loss_reward_3: 0.00515
	loss_policy_4: 0.05341
	accuracy_policy_4: 0.52574
	loss_value_4: 0.03924
	loss_reward_4: 0.0058
	loss_policy_5: 0.05595
	accuracy_policy_5: 0.50537
	loss_value_5: 0.04026
	loss_reward_5: 0.00681
	loss_policy: 0.42387
	loss_value: 0.3637
	loss_reward: 0.02756
[2024-05-07 07:39:13] nn step 26000, lr: 0.059049.
	loss_policy_0: 0.13117
	accuracy_policy_0: 0.72568
	loss_value_0: 0.15829
	loss_policy_1: 0.03423
	accuracy_policy_1: 0.66014
	loss_value_1: 0.03288
	loss_reward_1: 0.00456
	loss_policy_2: 0.0388
	accuracy_policy_2: 0.62008
	loss_value_2: 0.03404
	loss_reward_2: 0.00432
	loss_policy_3: 0.04227
	accuracy_policy_3: 0.58775
	loss_value_3: 0.03502
	loss_reward_3: 0.00484
	loss_policy_4: 0.0452
	accuracy_policy_4: 0.56469
	loss_value_4: 0.03605
	loss_reward_4: 0.00542
	loss_policy_5: 0.04835
	accuracy_policy_5: 0.53848
	loss_value_5: 0.03717
	loss_reward_5: 0.00643
	loss_policy: 0.34001
	loss_value: 0.33345
	loss_reward: 0.02557
Optimization_Done 26000
[2024-05-07 07:40:59] [command] train weight_iter_26000.pkl 130 131
[2024-05-07 07:41:36] nn step 26100, lr: 0.059049.
	loss_policy_0: 0.1671
	accuracy_policy_0: 0.64396
	loss_value_0: 0.15764
	loss_policy_1: 0.03873
	accuracy_policy_1: 0.59723
	loss_value_1: 0.03283
	loss_reward_1: 0.00457
	loss_policy_2: 0.04253
	accuracy_policy_2: 0.566
	loss_value_2: 0.03401
	loss_reward_2: 0.00432
	loss_policy_3: 0.04584
	accuracy_policy_3: 0.54107
	loss_value_3: 0.03512
	loss_reward_3: 0.00466
	loss_policy_4: 0.04832
	accuracy_policy_4: 0.51752
	loss_value_4: 0.03606
	loss_reward_4: 0.00527
	loss_policy_5: 0.05117
	accuracy_policy_5: 0.49863
	loss_value_5: 0.03704
	loss_reward_5: 0.00618
	loss_policy: 0.39369
	loss_value: 0.33271
	loss_reward: 0.02499
[2024-05-07 07:42:12] nn step 26200, lr: 0.059049.
	loss_policy_0: 0.14338
	accuracy_policy_0: 0.70383
	loss_value_0: 0.16063
	loss_policy_1: 0.03569
	accuracy_policy_1: 0.64078
	loss_value_1: 0.03339
	loss_reward_1: 0.00497
	loss_policy_2: 0.04026
	accuracy_policy_2: 0.60625
	loss_value_2: 0.03448
	loss_reward_2: 0.00452
	loss_policy_3: 0.0435
	accuracy_policy_3: 0.58008
	loss_value_3: 0.03563
	loss_reward_3: 0.00503
	loss_policy_4: 0.04678
	accuracy_policy_4: 0.55816
	loss_value_4: 0.03659
	loss_reward_4: 0.00536
	loss_policy_5: 0.04938
	accuracy_policy_5: 0.53859
	loss_value_5: 0.03766
	loss_reward_5: 0.00647
	loss_policy: 0.359
	loss_value: 0.33838
	loss_reward: 0.02635
Optimization_Done 26200
[2024-05-07 07:44:31] [command] train weight_iter_26200.pkl 131 132
[2024-05-07 07:45:08] nn step 26300, lr: 0.059049.
	loss_policy_0: 0.17793
	accuracy_policy_0: 0.66826
	loss_value_0: 0.17252
	loss_policy_1: 0.04223
	accuracy_policy_1: 0.61652
	loss_value_1: 0.03572
	loss_reward_1: 0.00541
	loss_policy_2: 0.04698
	accuracy_policy_2: 0.5815
	loss_value_2: 0.03703
	loss_reward_2: 0.00491
	loss_policy_3: 0.05024
	accuracy_policy_3: 0.55668
	loss_value_3: 0.03816
	loss_reward_3: 0.00511
	loss_policy_4: 0.0533
	accuracy_policy_4: 0.53408
	loss_value_4: 0.03926
	loss_reward_4: 0.00563
	loss_policy_5: 0.05608
	accuracy_policy_5: 0.51184
	loss_value_5: 0.04038
	loss_reward_5: 0.00693
	loss_policy: 0.42676
	loss_value: 0.36307
	loss_reward: 0.02798
[2024-05-07 07:45:44] nn step 26400, lr: 0.059049.
	loss_policy_0: 0.12836
	accuracy_policy_0: 0.72551
	loss_value_0: 0.15429
	loss_policy_1: 0.03396
	accuracy_policy_1: 0.65318
	loss_value_1: 0.0321
	loss_reward_1: 0.00481
	loss_policy_2: 0.03852
	accuracy_policy_2: 0.61586
	loss_value_2: 0.03322
	loss_reward_2: 0.00441
	loss_policy_3: 0.04204
	accuracy_policy_3: 0.59111
	loss_value_3: 0.03427
	loss_reward_3: 0.00471
	loss_policy_4: 0.04481
	accuracy_policy_4: 0.56648
	loss_value_4: 0.03539
	loss_reward_4: 0.00523
	loss_policy_5: 0.04751
	accuracy_policy_5: 0.54473
	loss_value_5: 0.03651
	loss_reward_5: 0.00629
	loss_policy: 0.33519
	loss_value: 0.3258
	loss_reward: 0.02546
Optimization_Done 26400
[2024-05-07 07:48:04] [command] train weight_iter_26400.pkl 132 133
[2024-05-07 07:48:41] nn step 26500, lr: 0.059049.
	loss_policy_0: 0.17938
	accuracy_policy_0: 0.66672
	loss_value_0: 0.15987
	loss_policy_1: 0.04422
	accuracy_policy_1: 0.60186
	loss_value_1: 0.03321
	loss_reward_1: 0.00539
	loss_policy_2: 0.04835
	accuracy_policy_2: 0.57107
	loss_value_2: 0.03431
	loss_reward_2: 0.00501
	loss_policy_3: 0.05155
	accuracy_policy_3: 0.54533
	loss_value_3: 0.0354
	loss_reward_3: 0.00518
	loss_policy_4: 0.05402
	accuracy_policy_4: 0.52301
	loss_value_4: 0.03621
	loss_reward_4: 0.00573
	loss_policy_5: 0.05653
	accuracy_policy_5: 0.50455
	loss_value_5: 0.03717
	loss_reward_5: 0.00669
	loss_policy: 0.43405
	loss_value: 0.33617
	loss_reward: 0.02799
[2024-05-07 07:49:18] nn step 26600, lr: 0.059049.
	loss_policy_0: 0.14132
	accuracy_policy_0: 0.71568
	loss_value_0: 0.14634
	loss_policy_1: 0.03736
	accuracy_policy_1: 0.63961
	loss_value_1: 0.03061
	loss_reward_1: 0.00514
	loss_policy_2: 0.04197
	accuracy_policy_2: 0.6027
	loss_value_2: 0.03166
	loss_reward_2: 0.00457
	loss_policy_3: 0.04537
	accuracy_policy_3: 0.57641
	loss_value_3: 0.03277
	loss_reward_3: 0.00477
	loss_policy_4: 0.04796
	accuracy_policy_4: 0.55785
	loss_value_4: 0.03392
	loss_reward_4: 0.00541
	loss_policy_5: 0.05077
	accuracy_policy_5: 0.54059
	loss_value_5: 0.03494
	loss_reward_5: 0.00649
	loss_policy: 0.36475
	loss_value: 0.31025
	loss_reward: 0.02636
Optimization_Done 26600
[2024-05-07 07:51:40] [command] train weight_iter_26600.pkl 133 134
[2024-05-07 07:52:16] nn step 26700, lr: 0.059049.
	loss_policy_0: 0.15663
	accuracy_policy_0: 0.69035
	loss_value_0: 0.15397
	loss_policy_1: 0.03904
	accuracy_policy_1: 0.62193
	loss_value_1: 0.03215
	loss_reward_1: 0.00499
	loss_policy_2: 0.04362
	accuracy_policy_2: 0.58551
	loss_value_2: 0.03336
	loss_reward_2: 0.00475
	loss_policy_3: 0.04672
	accuracy_policy_3: 0.56217
	loss_value_3: 0.03455
	loss_reward_3: 0.00487
	loss_policy_4: 0.04942
	accuracy_policy_4: 0.54113
	loss_value_4: 0.03561
	loss_reward_4: 0.00551
	loss_policy_5: 0.05157
	accuracy_policy_5: 0.52578
	loss_value_5: 0.03659
	loss_reward_5: 0.00643
	loss_policy: 0.387
	loss_value: 0.32622
	loss_reward: 0.02655
[2024-05-07 07:52:52] nn step 26800, lr: 0.059049.
	loss_policy_0: 0.12293
	accuracy_policy_0: 0.74383
	loss_value_0: 0.14883
	loss_policy_1: 0.03396
	accuracy_policy_1: 0.65846
	loss_value_1: 0.03117
	loss_reward_1: 0.00495
	loss_policy_2: 0.03911
	accuracy_policy_2: 0.61689
	loss_value_2: 0.03233
	loss_reward_2: 0.00449
	loss_policy_3: 0.04264
	accuracy_policy_3: 0.59207
	loss_value_3: 0.03357
	loss_reward_3: 0.0048
	loss_policy_4: 0.04558
	accuracy_policy_4: 0.56533
	loss_value_4: 0.03465
	loss_reward_4: 0.00552
	loss_policy_5: 0.04833
	accuracy_policy_5: 0.54701
	loss_value_5: 0.03568
	loss_reward_5: 0.0065
	loss_policy: 0.33256
	loss_value: 0.31623
	loss_reward: 0.02626
Optimization_Done 26800
[2024-05-07 07:55:09] [command] train weight_iter_26800.pkl 134 135
[2024-05-07 07:55:46] nn step 26900, lr: 0.059049.
	loss_policy_0: 0.16411
	accuracy_policy_0: 0.68148
	loss_value_0: 0.17251
	loss_policy_1: 0.03955
	accuracy_policy_1: 0.62197
	loss_value_1: 0.03609
	loss_reward_1: 0.00577
	loss_policy_2: 0.04337
	accuracy_policy_2: 0.58859
	loss_value_2: 0.03758
	loss_reward_2: 0.00537
	loss_policy_3: 0.0466
	accuracy_policy_3: 0.56619
	loss_value_3: 0.03876
	loss_reward_3: 0.00587
	loss_policy_4: 0.04894
	accuracy_policy_4: 0.54734
	loss_value_4: 0.03985
	loss_reward_4: 0.00636
	loss_policy_5: 0.05113
	accuracy_policy_5: 0.53475
	loss_value_5: 0.0408
	loss_reward_5: 0.00724
	loss_policy: 0.39369
	loss_value: 0.36558
	loss_reward: 0.03061
[2024-05-07 07:56:22] nn step 27000, lr: 0.059049.
	loss_policy_0: 0.13369
	accuracy_policy_0: 0.73393
	loss_value_0: 0.17109
	loss_policy_1: 0.03458
	accuracy_policy_1: 0.66301
	loss_value_1: 0.03557
	loss_reward_1: 0.00561
	loss_policy_2: 0.03885
	accuracy_policy_2: 0.62682
	loss_value_2: 0.03692
	loss_reward_2: 0.00535
	loss_policy_3: 0.04238
	accuracy_policy_3: 0.60408
	loss_value_3: 0.03826
	loss_reward_3: 0.00567
	loss_policy_4: 0.04506
	accuracy_policy_4: 0.58541
	loss_value_4: 0.03952
	loss_reward_4: 0.00615
	loss_policy_5: 0.04728
	accuracy_policy_5: 0.56719
	loss_value_5: 0.04062
	loss_reward_5: 0.00707
	loss_policy: 0.34184
	loss_value: 0.36198
	loss_reward: 0.02985
Optimization_Done 27000
[2024-05-07 07:58:36] [command] train weight_iter_27000.pkl 135 136
[2024-05-07 07:59:13] nn step 27100, lr: 0.059049.
	loss_policy_0: 0.15858
	accuracy_policy_0: 0.70807
	loss_value_0: 0.16603
	loss_policy_1: 0.03869
	accuracy_policy_1: 0.65209
	loss_value_1: 0.03452
	loss_reward_1: 0.00685
	loss_policy_2: 0.043
	accuracy_policy_2: 0.61951
	loss_value_2: 0.03611
	loss_reward_2: 0.00654
	loss_policy_3: 0.04638
	accuracy_policy_3: 0.59979
	loss_value_3: 0.03737
	loss_reward_3: 0.00724
	loss_policy_4: 0.04933
	accuracy_policy_4: 0.57623
	loss_value_4: 0.03854
	loss_reward_4: 0.00775
	loss_policy_5: 0.05144
	accuracy_policy_5: 0.56113
	loss_value_5: 0.03971
	loss_reward_5: 0.00896
	loss_policy: 0.38742
	loss_value: 0.35229
	loss_reward: 0.03734
[2024-05-07 07:59:49] nn step 27200, lr: 0.059049.
	loss_policy_0: 0.13728
	accuracy_policy_0: 0.74549
	loss_value_0: 0.17803
	loss_policy_1: 0.03598
	accuracy_policy_1: 0.67959
	loss_value_1: 0.03714
	loss_reward_1: 0.00695
	loss_policy_2: 0.04087
	accuracy_policy_2: 0.64307
	loss_value_2: 0.03867
	loss_reward_2: 0.00665
	loss_policy_3: 0.04436
	accuracy_policy_3: 0.61746
	loss_value_3: 0.04004
	loss_reward_3: 0.00729
	loss_policy_4: 0.04736
	accuracy_policy_4: 0.60186
	loss_value_4: 0.04132
	loss_reward_4: 0.00775
	loss_policy_5: 0.05004
	accuracy_policy_5: 0.58213
	loss_value_5: 0.04247
	loss_reward_5: 0.00892
	loss_policy: 0.3559
	loss_value: 0.37767
	loss_reward: 0.03757
Optimization_Done 27200
[2024-05-07 08:01:49] [command] train weight_iter_27200.pkl 136 137
[2024-05-07 08:02:26] nn step 27300, lr: 0.059049.
	loss_policy_0: 0.14379
	accuracy_policy_0: 0.71016
	loss_value_0: 0.15376
	loss_policy_1: 0.03752
	accuracy_policy_1: 0.63486
	loss_value_1: 0.0322
	loss_reward_1: 0.00651
	loss_policy_2: 0.04194
	accuracy_policy_2: 0.60436
	loss_value_2: 0.03365
	loss_reward_2: 0.00609
	loss_policy_3: 0.04534
	accuracy_policy_3: 0.57582
	loss_value_3: 0.0348
	loss_reward_3: 0.00679
	loss_policy_4: 0.0482
	accuracy_policy_4: 0.55271
	loss_value_4: 0.03592
	loss_reward_4: 0.00734
	loss_policy_5: 0.05065
	accuracy_policy_5: 0.53514
	loss_value_5: 0.03688
	loss_reward_5: 0.00851
	loss_policy: 0.36743
	loss_value: 0.3272
	loss_reward: 0.03523
[2024-05-07 08:03:02] nn step 27400, lr: 0.059049.
	loss_policy_0: 0.1195
	accuracy_policy_0: 0.7624
	loss_value_0: 0.16338
	loss_policy_1: 0.03436
	accuracy_policy_1: 0.67488
	loss_value_1: 0.03427
	loss_reward_1: 0.00672
	loss_policy_2: 0.03952
	accuracy_policy_2: 0.63787
	loss_value_2: 0.03571
	loss_reward_2: 0.00603
	loss_policy_3: 0.04275
	accuracy_policy_3: 0.60932
	loss_value_3: 0.03702
	loss_reward_3: 0.00676
	loss_policy_4: 0.04593
	accuracy_policy_4: 0.58771
	loss_value_4: 0.0383
	loss_reward_4: 0.00747
	loss_policy_5: 0.04893
	accuracy_policy_5: 0.56773
	loss_value_5: 0.03929
	loss_reward_5: 0.00884
	loss_policy: 0.33099
	loss_value: 0.34798
	loss_reward: 0.03581
Optimization_Done 27400
[2024-05-07 08:05:05] [command] train weight_iter_27400.pkl 137 138
[2024-05-07 08:05:41] nn step 27500, lr: 0.059049.
	loss_policy_0: 0.15815
	accuracy_policy_0: 0.6792
	loss_value_0: 0.1554
	loss_policy_1: 0.04016
	accuracy_policy_1: 0.61342
	loss_value_1: 0.03262
	loss_reward_1: 0.00528
	loss_policy_2: 0.04433
	accuracy_policy_2: 0.57932
	loss_value_2: 0.03387
	loss_reward_2: 0.00505
	loss_policy_3: 0.04721
	accuracy_policy_3: 0.55273
	loss_value_3: 0.03497
	loss_reward_3: 0.00563
	loss_policy_4: 0.04992
	accuracy_policy_4: 0.52891
	loss_value_4: 0.03582
	loss_reward_4: 0.00606
	loss_policy_5: 0.05281
	accuracy_policy_5: 0.50123
	loss_value_5: 0.03668
	loss_reward_5: 0.00704
	loss_policy: 0.39259
	loss_value: 0.32936
	loss_reward: 0.02906
[2024-05-07 08:06:16] nn step 27600, lr: 0.059049.
	loss_policy_0: 0.12483
	accuracy_policy_0: 0.73689
	loss_value_0: 0.15131
	loss_policy_1: 0.03512
	accuracy_policy_1: 0.64793
	loss_value_1: 0.03191
	loss_reward_1: 0.00527
	loss_policy_2: 0.0393
	accuracy_policy_2: 0.61451
	loss_value_2: 0.03314
	loss_reward_2: 0.00504
	loss_policy_3: 0.04247
	accuracy_policy_3: 0.59135
	loss_value_3: 0.03413
	loss_reward_3: 0.00543
	loss_policy_4: 0.04554
	accuracy_policy_4: 0.5659
	loss_value_4: 0.03521
	loss_reward_4: 0.00586
	loss_policy_5: 0.04872
	accuracy_policy_5: 0.5368
	loss_value_5: 0.03623
	loss_reward_5: 0.00692
	loss_policy: 0.33599
	loss_value: 0.32192
	loss_reward: 0.02851
Optimization_Done 27600
[2024-05-07 08:08:29] [command] train weight_iter_27600.pkl 138 139
[2024-05-07 08:09:05] nn step 27700, lr: 0.059049.
	loss_policy_0: 0.18519
	accuracy_policy_0: 0.65326
	loss_value_0: 0.16868
	loss_policy_1: 0.04279
	accuracy_policy_1: 0.60115
	loss_value_1: 0.0352
	loss_reward_1: 0.00602
	loss_policy_2: 0.04649
	accuracy_policy_2: 0.57643
	loss_value_2: 0.03636
	loss_reward_2: 0.0059
	loss_policy_3: 0.04974
	accuracy_policy_3: 0.55574
	loss_value_3: 0.03746
	loss_reward_3: 0.00639
	loss_policy_4: 0.05232
	accuracy_policy_4: 0.54133
	loss_value_4: 0.03847
	loss_reward_4: 0.00672
	loss_policy_5: 0.05411
	accuracy_policy_5: 0.52496
	loss_value_5: 0.03946
	loss_reward_5: 0.00752
	loss_policy: 0.43065
	loss_value: 0.35563
	loss_reward: 0.03256
[2024-05-07 08:09:41] nn step 27800, lr: 0.059049.
	loss_policy_0: 0.15892
	accuracy_policy_0: 0.70883
	loss_value_0: 0.17475
	loss_policy_1: 0.03981
	accuracy_policy_1: 0.64346
	loss_value_1: 0.03653
	loss_reward_1: 0.00604
	loss_policy_2: 0.04449
	accuracy_policy_2: 0.61168
	loss_value_2: 0.03771
	loss_reward_2: 0.00611
	loss_policy_3: 0.04761
	accuracy_policy_3: 0.58523
	loss_value_3: 0.03898
	loss_reward_3: 0.00661
	loss_policy_4: 0.05125
	accuracy_policy_4: 0.56701
	loss_value_4: 0.04015
	loss_reward_4: 0.00695
	loss_policy_5: 0.05343
	accuracy_policy_5: 0.55732
	loss_value_5: 0.04132
	loss_reward_5: 0.00779
	loss_policy: 0.39551
	loss_value: 0.36944
	loss_reward: 0.0335
Optimization_Done 27800
[2024-05-07 08:11:54] [command] train weight_iter_27800.pkl 139 140
[2024-05-07 08:12:31] nn step 27900, lr: 0.059049.
	loss_policy_0: 0.2135
	accuracy_policy_0: 0.64672
	loss_value_0: 0.1992
	loss_policy_1: 0.04954
	accuracy_policy_1: 0.59771
	loss_value_1: 0.04156
	loss_reward_1: 0.00663
	loss_policy_2: 0.05382
	accuracy_policy_2: 0.56586
	loss_value_2: 0.04306
	loss_reward_2: 0.0065
	loss_policy_3: 0.05721
	accuracy_policy_3: 0.54352
	loss_value_3: 0.04462
	loss_reward_3: 0.00679
	loss_policy_4: 0.06014
	accuracy_policy_4: 0.52865
	loss_value_4: 0.04618
	loss_reward_4: 0.0072
	loss_policy_5: 0.06238
	accuracy_policy_5: 0.51291
	loss_value_5: 0.04746
	loss_reward_5: 0.00809
	loss_policy: 0.49659
	loss_value: 0.42207
	loss_reward: 0.03521
[2024-05-07 08:13:07] nn step 28000, lr: 0.059049.
	loss_policy_0: 0.16947
	accuracy_policy_0: 0.7043
	loss_value_0: 0.18921
	loss_policy_1: 0.04175
	accuracy_policy_1: 0.64082
	loss_value_1: 0.03942
	loss_reward_1: 0.00619
	loss_policy_2: 0.04661
	accuracy_policy_2: 0.60836
	loss_value_2: 0.04085
	loss_reward_2: 0.00601
	loss_policy_3: 0.05031
	accuracy_policy_3: 0.58141
	loss_value_3: 0.04247
	loss_reward_3: 0.0064
	loss_policy_4: 0.05306
	accuracy_policy_4: 0.56387
	loss_value_4: 0.0438
	loss_reward_4: 0.00692
	loss_policy_5: 0.05599
	accuracy_policy_5: 0.54711
	loss_value_5: 0.04533
	loss_reward_5: 0.0077
	loss_policy: 0.41719
	loss_value: 0.40107
	loss_reward: 0.03323
Optimization_Done 28000
[2024-05-07 08:15:19] [command] train weight_iter_28000.pkl 140 141
[2024-05-07 08:15:56] nn step 28100, lr: 0.059049.
	loss_policy_0: 0.19118
	accuracy_policy_0: 0.67592
	loss_value_0: 0.18195
	loss_policy_1: 0.04639
	accuracy_policy_1: 0.6107
	loss_value_1: 0.03783
	loss_reward_1: 0.00614
	loss_policy_2: 0.05084
	accuracy_policy_2: 0.5741
	loss_value_2: 0.03944
	loss_reward_2: 0.00577
	loss_policy_3: 0.05445
	accuracy_policy_3: 0.54826
	loss_value_3: 0.04092
	loss_reward_3: 0.00612
	loss_policy_4: 0.05816
	accuracy_policy_4: 0.51867
	loss_value_4: 0.04228
	loss_reward_4: 0.00684
	loss_policy_5: 0.06024
	accuracy_policy_5: 0.50363
	loss_value_5: 0.04352
	loss_reward_5: 0.00764
	loss_policy: 0.46125
	loss_value: 0.38594
	loss_reward: 0.03251
[2024-05-07 08:16:32] nn step 28200, lr: 0.059049.
	loss_policy_0: 0.15065
	accuracy_policy_0: 0.72414
	loss_value_0: 0.17154
	loss_policy_1: 0.03967
	accuracy_policy_1: 0.64428
	loss_value_1: 0.0357
	loss_reward_1: 0.00589
	loss_policy_2: 0.04458
	accuracy_policy_2: 0.60467
	loss_value_2: 0.03725
	loss_reward_2: 0.00541
	loss_policy_3: 0.04814
	accuracy_policy_3: 0.57656
	loss_value_3: 0.03855
	loss_reward_3: 0.00587
	loss_policy_4: 0.05105
	accuracy_policy_4: 0.55596
	loss_value_4: 0.03979
	loss_reward_4: 0.0066
	loss_policy_5: 0.0542
	accuracy_policy_5: 0.53434
	loss_value_5: 0.04099
	loss_reward_5: 0.00736
	loss_policy: 0.3883
	loss_value: 0.36383
	loss_reward: 0.03114
Optimization_Done 28200
[2024-05-07 08:18:17] [command] train weight_iter_28200.pkl 141 142
[2024-05-07 08:18:54] nn step 28300, lr: 0.059049.
	loss_policy_0: 0.14091
	accuracy_policy_0: 0.70994
	loss_value_0: 0.15366
	loss_policy_1: 0.03555
	accuracy_policy_1: 0.63932
	loss_value_1: 0.03216
	loss_reward_1: 0.00482
	loss_policy_2: 0.03969
	accuracy_policy_2: 0.6024
	loss_value_2: 0.03356
	loss_reward_2: 0.00468
	loss_policy_3: 0.0429
	accuracy_policy_3: 0.56795
	loss_value_3: 0.03477
	loss_reward_3: 0.00505
	loss_policy_4: 0.04521
	accuracy_policy_4: 0.54381
	loss_value_4: 0.03587
	loss_reward_4: 0.00549
	loss_policy_5: 0.04715
	accuracy_policy_5: 0.52494
	loss_value_5: 0.03687
	loss_reward_5: 0.0063
	loss_policy: 0.3514
	loss_value: 0.3269
	loss_reward: 0.02634
[2024-05-07 08:19:29] nn step 28400, lr: 0.059049.
	loss_policy_0: 0.11596
	accuracy_policy_0: 0.75359
	loss_value_0: 0.15033
	loss_policy_1: 0.03208
	accuracy_policy_1: 0.66771
	loss_value_1: 0.03158
	loss_reward_1: 0.00495
	loss_policy_2: 0.03636
	accuracy_policy_2: 0.62949
	loss_value_2: 0.03293
	loss_reward_2: 0.00458
	loss_policy_3: 0.03945
	accuracy_policy_3: 0.5977
	loss_value_3: 0.03398
	loss_reward_3: 0.00481
	loss_policy_4: 0.04226
	accuracy_policy_4: 0.57014
	loss_value_4: 0.03504
	loss_reward_4: 0.00526
	loss_policy_5: 0.04451
	accuracy_policy_5: 0.54785
	loss_value_5: 0.03614
	loss_reward_5: 0.00644
	loss_policy: 0.31061
	loss_value: 0.32
	loss_reward: 0.02604
Optimization_Done 28400
[2024-05-07 08:21:44] [command] train weight_iter_28400.pkl 142 143
[2024-05-07 08:22:21] nn step 28500, lr: 0.059049.
	loss_policy_0: 0.16243
	accuracy_policy_0: 0.69135
	loss_value_0: 0.16472
	loss_policy_1: 0.03853
	accuracy_policy_1: 0.63447
	loss_value_1: 0.03429
	loss_reward_1: 0.00541
	loss_policy_2: 0.04216
	accuracy_policy_2: 0.60578
	loss_value_2: 0.03559
	loss_reward_2: 0.00528
	loss_policy_3: 0.04523
	accuracy_policy_3: 0.58027
	loss_value_3: 0.03674
	loss_reward_3: 0.00566
	loss_policy_4: 0.04788
	accuracy_policy_4: 0.55598
	loss_value_4: 0.03776
	loss_reward_4: 0.0061
	loss_policy_5: 0.04984
	accuracy_policy_5: 0.54027
	loss_value_5: 0.03875
	loss_reward_5: 0.00691
	loss_policy: 0.38608
	loss_value: 0.34785
	loss_reward: 0.02936
[2024-05-07 08:22:56] nn step 28600, lr: 0.059049.
	loss_policy_0: 0.12913
	accuracy_policy_0: 0.74309
	loss_value_0: 0.16122
	loss_policy_1: 0.03357
	accuracy_policy_1: 0.67191
	loss_value_1: 0.03371
	loss_reward_1: 0.00498
	loss_policy_2: 0.03767
	accuracy_policy_2: 0.63873
	loss_value_2: 0.03509
	loss_reward_2: 0.00495
	loss_policy_3: 0.04132
	accuracy_policy_3: 0.60791
	loss_value_3: 0.0364
	loss_reward_3: 0.00541
	loss_policy_4: 0.0438
	accuracy_policy_4: 0.58469
	loss_value_4: 0.03758
	loss_reward_4: 0.00569
	loss_policy_5: 0.04598
	accuracy_policy_5: 0.57088
	loss_value_5: 0.0388
	loss_reward_5: 0.00663
	loss_policy: 0.33148
	loss_value: 0.34279
	loss_reward: 0.02767
Optimization_Done 28600
[2024-05-07 08:25:09] [command] train weight_iter_28600.pkl 143 144
[2024-05-07 08:25:46] nn step 28700, lr: 0.059049.
	loss_policy_0: 0.21522
	accuracy_policy_0: 0.62594
	loss_value_0: 0.1972
	loss_policy_1: 0.04908
	accuracy_policy_1: 0.58252
	loss_value_1: 0.04098
	loss_reward_1: 0.00606
	loss_policy_2: 0.05347
	accuracy_policy_2: 0.54922
	loss_value_2: 0.04254
	loss_reward_2: 0.00601
	loss_policy_3: 0.05653
	accuracy_policy_3: 0.52756
	loss_value_3: 0.04403
	loss_reward_3: 0.00634
	loss_policy_4: 0.05901
	accuracy_policy_4: 0.50787
	loss_value_4: 0.04566
	loss_reward_4: 0.00684
	loss_policy_5: 0.06194
	accuracy_policy_5: 0.48926
	loss_value_5: 0.04712
	loss_reward_5: 0.00778
	loss_policy: 0.49524
	loss_value: 0.41753
	loss_reward: 0.03302
[2024-05-07 08:26:22] nn step 28800, lr: 0.059049.
	loss_policy_0: 0.16609
	accuracy_policy_0: 0.69754
	loss_value_0: 0.18175
	loss_policy_1: 0.04062
	accuracy_policy_1: 0.63174
	loss_value_1: 0.03805
	loss_reward_1: 0.00561
	loss_policy_2: 0.04493
	accuracy_policy_2: 0.59557
	loss_value_2: 0.03951
	loss_reward_2: 0.0055
	loss_policy_3: 0.04797
	accuracy_policy_3: 0.57432
	loss_value_3: 0.04104
	loss_reward_3: 0.00594
	loss_policy_4: 0.05096
	accuracy_policy_4: 0.55539
	loss_value_4: 0.04247
	loss_reward_4: 0.00639
	loss_policy_5: 0.05405
	accuracy_policy_5: 0.53129
	loss_value_5: 0.04385
	loss_reward_5: 0.00734
	loss_policy: 0.40463
	loss_value: 0.38667
	loss_reward: 0.03078
Optimization_Done 28800
[2024-05-07 08:28:34] [command] train weight_iter_28800.pkl 144 145
[2024-05-07 08:29:10] nn step 28900, lr: 0.059049.
	loss_policy_0: 0.19463
	accuracy_policy_0: 0.6507
	loss_value_0: 0.17326
	loss_policy_1: 0.04634
	accuracy_policy_1: 0.58766
	loss_value_1: 0.03631
	loss_reward_1: 0.00621
	loss_policy_2: 0.05047
	accuracy_policy_2: 0.55254
	loss_value_2: 0.0378
	loss_reward_2: 0.00592
	loss_policy_3: 0.05402
	accuracy_policy_3: 0.52287
	loss_value_3: 0.03932
	loss_reward_3: 0.00651
	loss_policy_4: 0.05695
	accuracy_policy_4: 0.49891
	loss_value_4: 0.04071
	loss_reward_4: 0.00704
	loss_policy_5: 0.05926
	accuracy_policy_5: 0.48199
	loss_value_5: 0.04204
	loss_reward_5: 0.00786
	loss_policy: 0.46169
	loss_value: 0.36943
	loss_reward: 0.03354
[2024-05-07 08:29:46] nn step 29000, lr: 0.059049.
	loss_policy_0: 0.16542
	accuracy_policy_0: 0.69555
	loss_value_0: 0.17665
	loss_policy_1: 0.04218
	accuracy_policy_1: 0.6227
	loss_value_1: 0.03689
	loss_reward_1: 0.00613
	loss_policy_2: 0.0466
	accuracy_policy_2: 0.58498
	loss_value_2: 0.03845
	loss_reward_2: 0.00588
	loss_policy_3: 0.05031
	accuracy_policy_3: 0.55773
	loss_value_3: 0.03985
	loss_reward_3: 0.0064
	loss_policy_4: 0.05363
	accuracy_policy_4: 0.53297
	loss_value_4: 0.04118
	loss_reward_4: 0.00698
	loss_policy_5: 0.05683
	accuracy_policy_5: 0.51133
	loss_value_5: 0.04249
	loss_reward_5: 0.00797
	loss_policy: 0.41497
	loss_value: 0.37552
	loss_reward: 0.03336
Optimization_Done 29000
[2024-05-07 08:31:48] [command] train weight_iter_29000.pkl 145 146
[2024-05-07 08:32:24] nn step 29100, lr: 0.059049.
	loss_policy_0: 0.17105
	accuracy_policy_0: 0.64973
	loss_value_0: 0.15389
	loss_policy_1: 0.04179
	accuracy_policy_1: 0.58176
	loss_value_1: 0.03204
	loss_reward_1: 0.00534
	loss_policy_2: 0.04581
	accuracy_policy_2: 0.54316
	loss_value_2: 0.03327
	loss_reward_2: 0.00515
	loss_policy_3: 0.04894
	accuracy_policy_3: 0.51758
	loss_value_3: 0.03446
	loss_reward_3: 0.00555
	loss_policy_4: 0.05144
	accuracy_policy_4: 0.48773
	loss_value_4: 0.03555
	loss_reward_4: 0.00606
	loss_policy_5: 0.05407
	accuracy_policy_5: 0.46807
	loss_value_5: 0.03652
	loss_reward_5: 0.00696
	loss_policy: 0.4131
	loss_value: 0.32573
	loss_reward: 0.02906
[2024-05-07 08:33:00] nn step 29200, lr: 0.059049.
	loss_policy_0: 0.14979
	accuracy_policy_0: 0.70219
	loss_value_0: 0.16032
	loss_policy_1: 0.03945
	accuracy_policy_1: 0.61887
	loss_value_1: 0.03355
	loss_reward_1: 0.00568
	loss_policy_2: 0.04412
	accuracy_policy_2: 0.57643
	loss_value_2: 0.03489
	loss_reward_2: 0.00519
	loss_policy_3: 0.04725
	accuracy_policy_3: 0.55203
	loss_value_3: 0.03597
	loss_reward_3: 0.00566
	loss_policy_4: 0.05024
	accuracy_policy_4: 0.5232
	loss_value_4: 0.03708
	loss_reward_4: 0.00626
	loss_policy_5: 0.05304
	accuracy_policy_5: 0.50186
	loss_value_5: 0.0383
	loss_reward_5: 0.00732
	loss_policy: 0.38389
	loss_value: 0.34012
	loss_reward: 0.03011
Optimization_Done 29200
[2024-05-07 08:35:01] [command] train weight_iter_29200.pkl 146 147
[2024-05-07 08:35:38] nn step 29300, lr: 0.059049.
	loss_policy_0: 0.154
	accuracy_policy_0: 0.67488
	loss_value_0: 0.15542
	loss_policy_1: 0.03718
	accuracy_policy_1: 0.60918
	loss_value_1: 0.03251
	loss_reward_1: 0.00487
	loss_policy_2: 0.04095
	accuracy_policy_2: 0.57635
	loss_value_2: 0.03388
	loss_reward_2: 0.00465
	loss_policy_3: 0.04396
	accuracy_policy_3: 0.55303
	loss_value_3: 0.0351
	loss_reward_3: 0.00517
	loss_policy_4: 0.04643
	accuracy_policy_4: 0.53031
	loss_value_4: 0.03618
	loss_reward_4: 0.00563
	loss_policy_5: 0.04844
	accuracy_policy_5: 0.51305
	loss_value_5: 0.03729
	loss_reward_5: 0.00657
	loss_policy: 0.37097
	loss_value: 0.33037
	loss_reward: 0.02688
[2024-05-07 08:36:13] nn step 29400, lr: 0.059049.
	loss_policy_0: 0.12557
	accuracy_policy_0: 0.73578
	loss_value_0: 0.15927
	loss_policy_1: 0.03355
	accuracy_policy_1: 0.656
	loss_value_1: 0.03344
	loss_reward_1: 0.00488
	loss_policy_2: 0.03784
	accuracy_policy_2: 0.61896
	loss_value_2: 0.03452
	loss_reward_2: 0.00472
	loss_policy_3: 0.04117
	accuracy_policy_3: 0.58908
	loss_value_3: 0.03576
	loss_reward_3: 0.00525
	loss_policy_4: 0.04447
	accuracy_policy_4: 0.56174
	loss_value_4: 0.03693
	loss_reward_4: 0.00569
	loss_policy_5: 0.04684
	accuracy_policy_5: 0.54445
	loss_value_5: 0.03797
	loss_reward_5: 0.00675
	loss_policy: 0.32944
	loss_value: 0.33789
	loss_reward: 0.02729
Optimization_Done 29400
[2024-05-07 08:38:26] [command] train weight_iter_29400.pkl 147 148
[2024-05-07 08:39:03] nn step 29500, lr: 0.059049.
	loss_policy_0: 0.17507
	accuracy_policy_0: 0.68172
	loss_value_0: 0.18578
	loss_policy_1: 0.04119
	accuracy_policy_1: 0.62805
	loss_value_1: 0.03872
	loss_reward_1: 0.00562
	loss_policy_2: 0.04492
	accuracy_policy_2: 0.59473
	loss_value_2: 0.04031
	loss_reward_2: 0.00557
	loss_policy_3: 0.04822
	accuracy_policy_3: 0.57715
	loss_value_3: 0.04175
	loss_reward_3: 0.00579
	loss_policy_4: 0.05109
	accuracy_policy_4: 0.55678
	loss_value_4: 0.04317
	loss_reward_4: 0.00628
	loss_policy_5: 0.05352
	accuracy_policy_5: 0.53729
	loss_value_5: 0.04433
	loss_reward_5: 0.00709
	loss_policy: 0.414
	loss_value: 0.39405
	loss_reward: 0.03035
[2024-05-07 08:39:39] nn step 29600, lr: 0.059049.
	loss_policy_0: 0.14114
	accuracy_policy_0: 0.73561
	loss_value_0: 0.17909
	loss_policy_1: 0.03629
	accuracy_policy_1: 0.66467
	loss_value_1: 0.03757
	loss_reward_1: 0.00536
	loss_policy_2: 0.0403
	accuracy_policy_2: 0.6323
	loss_value_2: 0.03919
	loss_reward_2: 0.00534
	loss_policy_3: 0.04404
	accuracy_policy_3: 0.6067
	loss_value_3: 0.04067
	loss_reward_3: 0.00582
	loss_policy_4: 0.04688
	accuracy_policy_4: 0.58451
	loss_value_4: 0.04202
	loss_reward_4: 0.00598
	loss_policy_5: 0.04977
	accuracy_policy_5: 0.56318
	loss_value_5: 0.04339
	loss_reward_5: 0.00706
	loss_policy: 0.35843
	loss_value: 0.38193
	loss_reward: 0.02956
Optimization_Done 29600
[2024-05-07 08:41:57] [command] train weight_iter_29600.pkl 148 149
[2024-05-07 08:42:35] nn step 29700, lr: 0.059049.
	loss_policy_0: 0.17441
	accuracy_policy_0: 0.67879
	loss_value_0: 0.17558
	loss_policy_1: 0.04117
	accuracy_policy_1: 0.62449
	loss_value_1: 0.03647
	loss_reward_1: 0.00559
	loss_policy_2: 0.04533
	accuracy_policy_2: 0.59543
	loss_value_2: 0.03791
	loss_reward_2: 0.00535
	loss_policy_3: 0.04862
	accuracy_policy_3: 0.57166
	loss_value_3: 0.03944
	loss_reward_3: 0.00583
	loss_policy_4: 0.05169
	accuracy_policy_4: 0.54764
	loss_value_4: 0.04065
	loss_reward_4: 0.00615
	loss_policy_5: 0.05458
	accuracy_policy_5: 0.53166
	loss_value_5: 0.04187
	loss_reward_5: 0.00687
	loss_policy: 0.41578
	loss_value: 0.37192
	loss_reward: 0.0298
[2024-05-07 08:43:11] nn step 29800, lr: 0.059049.
	loss_policy_0: 0.14766
	accuracy_policy_0: 0.73004
	loss_value_0: 0.18104
	loss_policy_1: 0.03804
	accuracy_policy_1: 0.65887
	loss_value_1: 0.03771
	loss_reward_1: 0.00577
	loss_policy_2: 0.04255
	accuracy_policy_2: 0.62533
	loss_value_2: 0.03916
	loss_reward_2: 0.00543
	loss_policy_3: 0.04637
	accuracy_policy_3: 0.59631
	loss_value_3: 0.04048
	loss_reward_3: 0.00588
	loss_policy_4: 0.04966
	accuracy_policy_4: 0.57396
	loss_value_4: 0.04174
	loss_reward_4: 0.00624
	loss_policy_5: 0.05279
	accuracy_policy_5: 0.55855
	loss_value_5: 0.04308
	loss_reward_5: 0.00725
	loss_policy: 0.37708
	loss_value: 0.38322
	loss_reward: 0.03057
Optimization_Done 29800
[2024-05-07 08:45:27] [command] train weight_iter_29800.pkl 149 150
[2024-05-07 08:46:05] nn step 29900, lr: 0.059049.
	loss_policy_0: 0.19163
	accuracy_policy_0: 0.65725
	loss_value_0: 0.18155
	loss_policy_1: 0.04607
	accuracy_policy_1: 0.59756
	loss_value_1: 0.03771
	loss_reward_1: 0.00621
	loss_policy_2: 0.0504
	accuracy_policy_2: 0.5617
	loss_value_2: 0.03901
	loss_reward_2: 0.00593
	loss_policy_3: 0.05365
	accuracy_policy_3: 0.5358
	loss_value_3: 0.04006
	loss_reward_3: 0.00653
	loss_policy_4: 0.05635
	accuracy_policy_4: 0.51352
	loss_value_4: 0.04106
	loss_reward_4: 0.00709
	loss_policy_5: 0.05883
	accuracy_policy_5: 0.49779
	loss_value_5: 0.04219
	loss_reward_5: 0.00784
	loss_policy: 0.45693
	loss_value: 0.38157
	loss_reward: 0.0336
[2024-05-07 08:46:40] nn step 30000, lr: 0.059049.
	loss_policy_0: 0.14827
	accuracy_policy_0: 0.71426
	loss_value_0: 0.17772
	loss_policy_1: 0.03892
	accuracy_policy_1: 0.63494
	loss_value_1: 0.03671
	loss_reward_1: 0.00563
	loss_policy_2: 0.04361
	accuracy_policy_2: 0.59646
	loss_value_2: 0.03782
	loss_reward_2: 0.00552
	loss_policy_3: 0.04735
	accuracy_policy_3: 0.5625
	loss_value_3: 0.03886
	loss_reward_3: 0.00605
	loss_policy_4: 0.05038
	accuracy_policy_4: 0.54277
	loss_value_4: 0.03989
	loss_reward_4: 0.00641
	loss_policy_5: 0.05281
	accuracy_policy_5: 0.5268
	loss_value_5: 0.04104
	loss_reward_5: 0.00744
	loss_policy: 0.38134
	loss_value: 0.37205
	loss_reward: 0.03106
Optimization_Done 30000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 09:19:11] [command] train weight_iter_30000.pkl 150 151
[2024-05-07 09:19:55] nn step 30100, lr: 0.053144.
	loss_policy_0: 0.17162
	accuracy_policy_0: 0.69424
	loss_value_0: 0.18437
	loss_policy_1: 0.04242
	accuracy_policy_1: 0.63277
	loss_value_1: 0.03852
	loss_reward_1: 0.00578
	loss_policy_2: 0.04735
	accuracy_policy_2: 0.59111
	loss_value_2: 0.04002
	loss_reward_2: 0.00579
	loss_policy_3: 0.05125
	accuracy_policy_3: 0.55801
	loss_value_3: 0.04125
	loss_reward_3: 0.00648
	loss_policy_4: 0.05432
	accuracy_policy_4: 0.53373
	loss_value_4: 0.04236
	loss_reward_4: 0.00687
	loss_policy_5: 0.05747
	accuracy_policy_5: 0.50889
	loss_value_5: 0.04333
	loss_reward_5: 0.00765
	loss_policy: 0.42443
	loss_value: 0.38985
	loss_reward: 0.03258
[2024-05-07 09:20:30] nn step 30200, lr: 0.053144.
	loss_policy_0: 0.14012
	accuracy_policy_0: 0.74566
	loss_value_0: 0.18825
	loss_policy_1: 0.03781
	accuracy_policy_1: 0.66938
	loss_value_1: 0.03919
	loss_reward_1: 0.00588
	loss_policy_2: 0.0436
	accuracy_policy_2: 0.62832
	loss_value_2: 0.04062
	loss_reward_2: 0.00571
	loss_policy_3: 0.04753
	accuracy_policy_3: 0.59301
	loss_value_3: 0.04168
	loss_reward_3: 0.0062
	loss_policy_4: 0.05108
	accuracy_policy_4: 0.57168
	loss_value_4: 0.04267
	loss_reward_4: 0.00695
	loss_policy_5: 0.05438
	accuracy_policy_5: 0.5459
	loss_value_5: 0.0437
	loss_reward_5: 0.00757
	loss_policy: 0.37452
	loss_value: 0.39611
	loss_reward: 0.03231
Optimization_Done 30200
[2024-05-07 09:22:45] [command] train weight_iter_30200.pkl 151 152
[2024-05-07 09:23:23] nn step 30300, lr: 0.053144.
	loss_policy_0: 0.15941
	accuracy_policy_0: 0.71271
	loss_value_0: 0.18446
	loss_policy_1: 0.03888
	accuracy_policy_1: 0.65934
	loss_value_1: 0.0385
	loss_reward_1: 0.005
	loss_policy_2: 0.0433
	accuracy_policy_2: 0.62381
	loss_value_2: 0.04018
	loss_reward_2: 0.00463
	loss_policy_3: 0.04711
	accuracy_policy_3: 0.59191
	loss_value_3: 0.04171
	loss_reward_3: 0.00519
	loss_policy_4: 0.0502
	accuracy_policy_4: 0.56604
	loss_value_4: 0.04294
	loss_reward_4: 0.0056
	loss_policy_5: 0.05339
	accuracy_policy_5: 0.54295
	loss_value_5: 0.04407
	loss_reward_5: 0.00642
	loss_policy: 0.3923
	loss_value: 0.39186
	loss_reward: 0.02683
[2024-05-07 09:23:59] nn step 30400, lr: 0.053144.
	loss_policy_0: 0.12748
	accuracy_policy_0: 0.7582
	loss_value_0: 0.17501
	loss_policy_1: 0.03317
	accuracy_policy_1: 0.69143
	loss_value_1: 0.03676
	loss_reward_1: 0.00479
	loss_policy_2: 0.0383
	accuracy_policy_2: 0.64586
	loss_value_2: 0.03841
	loss_reward_2: 0.00455
	loss_policy_3: 0.04206
	accuracy_policy_3: 0.6235
	loss_value_3: 0.03977
	loss_reward_3: 0.00506
	loss_policy_4: 0.0454
	accuracy_policy_4: 0.60041
	loss_value_4: 0.04098
	loss_reward_4: 0.00538
	loss_policy_5: 0.04856
	accuracy_policy_5: 0.57613
	loss_value_5: 0.04219
	loss_reward_5: 0.00632
	loss_policy: 0.33497
	loss_value: 0.37312
	loss_reward: 0.0261
Optimization_Done 30400
[2024-05-07 09:26:12] [command] train weight_iter_30400.pkl 152 153
[2024-05-07 09:26:49] nn step 30500, lr: 0.053144.
	loss_policy_0: 0.15086
	accuracy_policy_0: 0.69287
	loss_value_0: 0.1752
	loss_policy_1: 0.03747
	accuracy_policy_1: 0.63516
	loss_value_1: 0.03671
	loss_reward_1: 0.00612
	loss_policy_2: 0.0418
	accuracy_policy_2: 0.60141
	loss_value_2: 0.03816
	loss_reward_2: 0.00567
	loss_policy_3: 0.04537
	accuracy_policy_3: 0.5765
	loss_value_3: 0.03941
	loss_reward_3: 0.0062
	loss_policy_4: 0.04822
	accuracy_policy_4: 0.55645
	loss_value_4: 0.04083
	loss_reward_4: 0.0067
	loss_policy_5: 0.05094
	accuracy_policy_5: 0.53463
	loss_value_5: 0.04208
	loss_reward_5: 0.0076
	loss_policy: 0.37466
	loss_value: 0.37239
	loss_reward: 0.03228
[2024-05-07 09:27:25] nn step 30600, lr: 0.053144.
	loss_policy_0: 0.12979
	accuracy_policy_0: 0.74488
	loss_value_0: 0.18378
	loss_policy_1: 0.03496
	accuracy_policy_1: 0.67383
	loss_value_1: 0.03866
	loss_reward_1: 0.00648
	loss_policy_2: 0.04015
	accuracy_policy_2: 0.63504
	loss_value_2: 0.04019
	loss_reward_2: 0.00563
	loss_policy_3: 0.04356
	accuracy_policy_3: 0.61424
	loss_value_3: 0.04158
	loss_reward_3: 0.00617
	loss_policy_4: 0.04694
	accuracy_policy_4: 0.59227
	loss_value_4: 0.04289
	loss_reward_4: 0.0067
	loss_policy_5: 0.0503
	accuracy_policy_5: 0.57311
	loss_value_5: 0.04418
	loss_reward_5: 0.00796
	loss_policy: 0.3457
	loss_value: 0.39129
	loss_reward: 0.03294
Optimization_Done 30600
[2024-05-07 09:29:41] [command] train weight_iter_30600.pkl 153 154
[2024-05-07 09:30:18] nn step 30700, lr: 0.053144.
	loss_policy_0: 0.12167
	accuracy_policy_0: 0.70336
	loss_value_0: 0.13728
	loss_policy_1: 0.03207
	accuracy_policy_1: 0.6192
	loss_value_1: 0.02876
	loss_reward_1: 0.00507
	loss_policy_2: 0.0363
	accuracy_policy_2: 0.57887
	loss_value_2: 0.02987
	loss_reward_2: 0.00448
	loss_policy_3: 0.03912
	accuracy_policy_3: 0.55729
	loss_value_3: 0.03094
	loss_reward_3: 0.00497
	loss_policy_4: 0.04165
	accuracy_policy_4: 0.53797
	loss_value_4: 0.03186
	loss_reward_4: 0.00534
	loss_policy_5: 0.04398
	accuracy_policy_5: 0.52166
	loss_value_5: 0.03284
	loss_reward_5: 0.00623
	loss_policy: 0.31479
	loss_value: 0.29154
	loss_reward: 0.0261
[2024-05-07 09:30:54] nn step 30800, lr: 0.053144.
	loss_policy_0: 0.09861
	accuracy_policy_0: 0.74986
	loss_value_0: 0.1369
	loss_policy_1: 0.02856
	accuracy_policy_1: 0.65871
	loss_value_1: 0.02861
	loss_reward_1: 0.00503
	loss_policy_2: 0.03293
	accuracy_policy_2: 0.61648
	loss_value_2: 0.02968
	loss_reward_2: 0.00447
	loss_policy_3: 0.03625
	accuracy_policy_3: 0.58932
	loss_value_3: 0.03061
	loss_reward_3: 0.005
	loss_policy_4: 0.03895
	accuracy_policy_4: 0.56502
	loss_value_4: 0.03157
	loss_reward_4: 0.00521
	loss_policy_5: 0.04174
	accuracy_policy_5: 0.54787
	loss_value_5: 0.03265
	loss_reward_5: 0.00643
	loss_policy: 0.27704
	loss_value: 0.29002
	loss_reward: 0.02615
Optimization_Done 30800
[2024-05-07 09:33:14] [command] train weight_iter_30800.pkl 154 155
[2024-05-07 09:33:51] nn step 30900, lr: 0.053144.
	loss_policy_0: 0.16135
	accuracy_policy_0: 0.6348
	loss_value_0: 0.13222
	loss_policy_1: 0.03765
	accuracy_policy_1: 0.57525
	loss_value_1: 0.02779
	loss_reward_1: 0.00316
	loss_policy_2: 0.0409
	accuracy_policy_2: 0.54004
	loss_value_2: 0.02907
	loss_reward_2: 0.00314
	loss_policy_3: 0.04364
	accuracy_policy_3: 0.51395
	loss_value_3: 0.03007
	loss_reward_3: 0.00343
	loss_policy_4: 0.04559
	accuracy_policy_4: 0.49594
	loss_value_4: 0.03102
	loss_reward_4: 0.00361
	loss_policy_5: 0.04724
	accuracy_policy_5: 0.48008
	loss_value_5: 0.032
	loss_reward_5: 0.00402
	loss_policy: 0.37638
	loss_value: 0.28217
	loss_reward: 0.01737
[2024-05-07 09:34:27] nn step 31000, lr: 0.053144.
	loss_policy_0: 0.11291
	accuracy_policy_0: 0.72105
	loss_value_0: 0.12636
	loss_policy_1: 0.02991
	accuracy_policy_1: 0.63629
	loss_value_1: 0.02641
	loss_reward_1: 0.00283
	loss_policy_2: 0.03377
	accuracy_policy_2: 0.59537
	loss_value_2: 0.0275
	loss_reward_2: 0.00277
	loss_policy_3: 0.03703
	accuracy_policy_3: 0.56162
	loss_value_3: 0.02841
	loss_reward_3: 0.00299
	loss_policy_4: 0.03888
	accuracy_policy_4: 0.53979
	loss_value_4: 0.02926
	loss_reward_4: 0.00321
	loss_policy_5: 0.04102
	accuracy_policy_5: 0.52016
	loss_value_5: 0.03001
	loss_reward_5: 0.00378
	loss_policy: 0.29352
	loss_value: 0.26795
	loss_reward: 0.01558
Optimization_Done 31000
[2024-05-07 09:36:44] [command] train weight_iter_31000.pkl 155 156
[2024-05-07 09:37:22] nn step 31100, lr: 0.053144.
	loss_policy_0: 0.19422
	accuracy_policy_0: 0.64621
	loss_value_0: 0.17821
	loss_policy_1: 0.04459
	accuracy_policy_1: 0.59658
	loss_value_1: 0.03671
	loss_reward_1: 0.00557
	loss_policy_2: 0.04797
	accuracy_policy_2: 0.56543
	loss_value_2: 0.03777
	loss_reward_2: 0.00571
	loss_policy_3: 0.05099
	accuracy_policy_3: 0.54646
	loss_value_3: 0.03864
	loss_reward_3: 0.00592
	loss_policy_4: 0.05275
	accuracy_policy_4: 0.52973
	loss_value_4: 0.03948
	loss_reward_4: 0.00599
	loss_policy_5: 0.05421
	accuracy_policy_5: 0.52213
	loss_value_5: 0.04032
	loss_reward_5: 0.00665
	loss_policy: 0.44473
	loss_value: 0.37113
	loss_reward: 0.02985
[2024-05-07 09:37:59] nn step 31200, lr: 0.053144.
	loss_policy_0: 0.15551
	accuracy_policy_0: 0.70738
	loss_value_0: 0.16321
	loss_policy_1: 0.03837
	accuracy_policy_1: 0.63965
	loss_value_1: 0.03402
	loss_reward_1: 0.00524
	loss_policy_2: 0.04275
	accuracy_policy_2: 0.60492
	loss_value_2: 0.03526
	loss_reward_2: 0.00538
	loss_policy_3: 0.04572
	accuracy_policy_3: 0.5835
	loss_value_3: 0.03627
	loss_reward_3: 0.00569
	loss_policy_4: 0.04798
	accuracy_policy_4: 0.56643
	loss_value_4: 0.03725
	loss_reward_4: 0.00584
	loss_policy_5: 0.04973
	accuracy_policy_5: 0.55533
	loss_value_5: 0.03818
	loss_reward_5: 0.00662
	loss_policy: 0.38006
	loss_value: 0.34419
	loss_reward: 0.02878
Optimization_Done 31200
[2024-05-07 09:40:19] [command] train weight_iter_31200.pkl 156 157
[2024-05-07 09:40:57] nn step 31300, lr: 0.053144.
	loss_policy_0: 0.20288
	accuracy_policy_0: 0.66799
	loss_value_0: 0.1971
	loss_policy_1: 0.04702
	accuracy_policy_1: 0.61742
	loss_value_1: 0.04066
	loss_reward_1: 0.00689
	loss_policy_2: 0.05161
	accuracy_policy_2: 0.58586
	loss_value_2: 0.04206
	loss_reward_2: 0.00711
	loss_policy_3: 0.05523
	accuracy_policy_3: 0.55861
	loss_value_3: 0.04356
	loss_reward_3: 0.00741
	loss_policy_4: 0.05789
	accuracy_policy_4: 0.53812
	loss_value_4: 0.04481
	loss_reward_4: 0.0077
	loss_policy_5: 0.05999
	accuracy_policy_5: 0.51846
	loss_value_5: 0.04615
	loss_reward_5: 0.00853
	loss_policy: 0.47462
	loss_value: 0.41434
	loss_reward: 0.03764
[2024-05-07 09:41:34] nn step 31400, lr: 0.053144.
	loss_policy_0: 0.15854
	accuracy_policy_0: 0.71744
	loss_value_0: 0.18883
	loss_policy_1: 0.04012
	accuracy_policy_1: 0.64969
	loss_value_1: 0.03924
	loss_reward_1: 0.00622
	loss_policy_2: 0.04472
	accuracy_policy_2: 0.61473
	loss_value_2: 0.04053
	loss_reward_2: 0.00612
	loss_policy_3: 0.04773
	accuracy_policy_3: 0.5907
	loss_value_3: 0.0418
	loss_reward_3: 0.00648
	loss_policy_4: 0.05049
	accuracy_policy_4: 0.57051
	loss_value_4: 0.04315
	loss_reward_4: 0.00699
	loss_policy_5: 0.05322
	accuracy_policy_5: 0.55
	loss_value_5: 0.04434
	loss_reward_5: 0.00773
	loss_policy: 0.39482
	loss_value: 0.3979
	loss_reward: 0.03353
Optimization_Done 31400
[2024-05-07 09:43:52] [command] train weight_iter_31400.pkl 157 158
[2024-05-07 09:44:30] nn step 31500, lr: 0.053144.
	loss_policy_0: 0.1704
	accuracy_policy_0: 0.69535
	loss_value_0: 0.18786
	loss_policy_1: 0.0442
	accuracy_policy_1: 0.61018
	loss_value_1: 0.03928
	loss_reward_1: 0.00795
	loss_policy_2: 0.04905
	accuracy_policy_2: 0.57359
	loss_value_2: 0.04084
	loss_reward_2: 0.00745
	loss_policy_3: 0.05248
	accuracy_policy_3: 0.54088
	loss_value_3: 0.04213
	loss_reward_3: 0.00825
	loss_policy_4: 0.05542
	accuracy_policy_4: 0.51869
	loss_value_4: 0.04334
	loss_reward_4: 0.00877
	loss_policy_5: 0.0584
	accuracy_policy_5: 0.49514
	loss_value_5: 0.0444
	loss_reward_5: 0.00987
	loss_policy: 0.42996
	loss_value: 0.39786
	loss_reward: 0.04229
[2024-05-07 09:45:07] nn step 31600, lr: 0.053144.
	loss_policy_0: 0.13915
	accuracy_policy_0: 0.74652
	loss_value_0: 0.18479
	loss_policy_1: 0.03928
	accuracy_policy_1: 0.64463
	loss_value_1: 0.03851
	loss_reward_1: 0.00733
	loss_policy_2: 0.04422
	accuracy_policy_2: 0.60229
	loss_value_2: 0.03994
	loss_reward_2: 0.00704
	loss_policy_3: 0.04838
	accuracy_policy_3: 0.57389
	loss_value_3: 0.04108
	loss_reward_3: 0.00764
	loss_policy_4: 0.0515
	accuracy_policy_4: 0.5475
	loss_value_4: 0.04239
	loss_reward_4: 0.00812
	loss_policy_5: 0.05418
	accuracy_policy_5: 0.526
	loss_value_5: 0.04339
	loss_reward_5: 0.00954
	loss_policy: 0.37671
	loss_value: 0.39009
	loss_reward: 0.03967
Optimization_Done 31600
[2024-05-07 09:47:03] [command] train weight_iter_31600.pkl 158 159
[2024-05-07 09:47:40] nn step 31700, lr: 0.053144.
	loss_policy_0: 0.14362
	accuracy_policy_0: 0.74818
	loss_value_0: 0.19368
	loss_policy_1: 0.03972
	accuracy_policy_1: 0.66051
	loss_value_1: 0.04025
	loss_reward_1: 0.00773
	loss_policy_2: 0.04458
	accuracy_policy_2: 0.61814
	loss_value_2: 0.04165
	loss_reward_2: 0.00733
	loss_policy_3: 0.04809
	accuracy_policy_3: 0.58535
	loss_value_3: 0.04253
	loss_reward_3: 0.00783
	loss_policy_4: 0.05093
	accuracy_policy_4: 0.56348
	loss_value_4: 0.04325
	loss_reward_4: 0.00882
	loss_policy_5: 0.05324
	accuracy_policy_5: 0.54613
	loss_value_5: 0.0441
	loss_reward_5: 0.00986
	loss_policy: 0.38018
	loss_value: 0.40546
	loss_reward: 0.04157
[2024-05-07 09:48:16] nn step 31800, lr: 0.053144.
	loss_policy_0: 0.11325
	accuracy_policy_0: 0.78689
	loss_value_0: 0.18377
	loss_policy_1: 0.03484
	accuracy_policy_1: 0.68453
	loss_value_1: 0.03835
	loss_reward_1: 0.00746
	loss_policy_2: 0.03956
	accuracy_policy_2: 0.64074
	loss_value_2: 0.03948
	loss_reward_2: 0.00691
	loss_policy_3: 0.04271
	accuracy_policy_3: 0.61428
	loss_value_3: 0.04047
	loss_reward_3: 0.00759
	loss_policy_4: 0.04573
	accuracy_policy_4: 0.58912
	loss_value_4: 0.04146
	loss_reward_4: 0.00804
	loss_policy_5: 0.04797
	accuracy_policy_5: 0.57455
	loss_value_5: 0.0424
	loss_reward_5: 0.00947
	loss_policy: 0.32406
	loss_value: 0.38592
	loss_reward: 0.03947
Optimization_Done 31800
[2024-05-07 09:50:35] [command] train weight_iter_31800.pkl 159 160
[2024-05-07 09:51:12] nn step 31900, lr: 0.053144.
	loss_policy_0: 0.16434
	accuracy_policy_0: 0.72576
	loss_value_0: 0.17874
	loss_policy_1: 0.03916
	accuracy_policy_1: 0.6725
	loss_value_1: 0.03709
	loss_reward_1: 0.00512
	loss_policy_2: 0.04299
	accuracy_policy_2: 0.63961
	loss_value_2: 0.0382
	loss_reward_2: 0.00512
	loss_policy_3: 0.04594
	accuracy_policy_3: 0.61826
	loss_value_3: 0.03922
	loss_reward_3: 0.00559
	loss_policy_4: 0.04784
	accuracy_policy_4: 0.60162
	loss_value_4: 0.04017
	loss_reward_4: 0.00604
	loss_policy_5: 0.05044
	accuracy_policy_5: 0.5835
	loss_value_5: 0.0412
	loss_reward_5: 0.00701
	loss_policy: 0.3907
	loss_value: 0.37464
	loss_reward: 0.02888
[2024-05-07 09:51:49] nn step 32000, lr: 0.053144.
	loss_policy_0: 0.12128
	accuracy_policy_0: 0.78033
	loss_value_0: 0.1653
	loss_policy_1: 0.03207
	accuracy_policy_1: 0.71369
	loss_value_1: 0.03442
	loss_reward_1: 0.00493
	loss_policy_2: 0.03612
	accuracy_policy_2: 0.67521
	loss_value_2: 0.0354
	loss_reward_2: 0.00486
	loss_policy_3: 0.03924
	accuracy_policy_3: 0.65434
	loss_value_3: 0.03667
	loss_reward_3: 0.0053
	loss_policy_4: 0.04179
	accuracy_policy_4: 0.63545
	loss_value_4: 0.03763
	loss_reward_4: 0.00585
	loss_policy_5: 0.04407
	accuracy_policy_5: 0.61834
	loss_value_5: 0.03845
	loss_reward_5: 0.00667
	loss_policy: 0.31457
	loss_value: 0.34787
	loss_reward: 0.02762
Optimization_Done 32000
[2024-05-07 09:53:35] [command] train weight_iter_32000.pkl 160 161
[2024-05-07 09:54:13] nn step 32100, lr: 0.053144.
	loss_policy_0: 0.16204
	accuracy_policy_0: 0.71314
	loss_value_0: 0.16306
	loss_policy_1: 0.03805
	accuracy_policy_1: 0.67193
	loss_value_1: 0.03415
	loss_reward_1: 0.00508
	loss_policy_2: 0.04213
	accuracy_policy_2: 0.63734
	loss_value_2: 0.03566
	loss_reward_2: 0.00503
	loss_policy_3: 0.04556
	accuracy_policy_3: 0.60908
	loss_value_3: 0.0369
	loss_reward_3: 0.00533
	loss_policy_4: 0.04872
	accuracy_policy_4: 0.58404
	loss_value_4: 0.03802
	loss_reward_4: 0.00565
	loss_policy_5: 0.05186
	accuracy_policy_5: 0.55779
	loss_value_5: 0.03911
	loss_reward_5: 0.00643
	loss_policy: 0.38836
	loss_value: 0.3469
	loss_reward: 0.02753
[2024-05-07 09:54:49] nn step 32200, lr: 0.053144.
	loss_policy_0: 0.13418
	accuracy_policy_0: 0.75984
	loss_value_0: 0.15905
	loss_policy_1: 0.03376
	accuracy_policy_1: 0.7018
	loss_value_1: 0.03311
	loss_reward_1: 0.00517
	loss_policy_2: 0.03814
	accuracy_policy_2: 0.67029
	loss_value_2: 0.03418
	loss_reward_2: 0.00473
	loss_policy_3: 0.04104
	accuracy_policy_3: 0.64812
	loss_value_3: 0.03533
	loss_reward_3: 0.00521
	loss_policy_4: 0.04429
	accuracy_policy_4: 0.6208
	loss_value_4: 0.03641
	loss_reward_4: 0.00549
	loss_policy_5: 0.0478
	accuracy_policy_5: 0.59627
	loss_value_5: 0.0375
	loss_reward_5: 0.00628
	loss_policy: 0.33921
	loss_value: 0.33559
	loss_reward: 0.02687
Optimization_Done 32200
[2024-05-07 09:57:08] [command] train weight_iter_32200.pkl 161 162
[2024-05-07 09:57:45] nn step 32300, lr: 0.053144.
	loss_policy_0: 0.18329
	accuracy_policy_0: 0.68283
	loss_value_0: 0.16166
	loss_policy_1: 0.0432
	accuracy_policy_1: 0.63131
	loss_value_1: 0.03359
	loss_reward_1: 0.00608
	loss_policy_2: 0.0474
	accuracy_policy_2: 0.6042
	loss_value_2: 0.03485
	loss_reward_2: 0.00582
	loss_policy_3: 0.05084
	accuracy_policy_3: 0.5809
	loss_value_3: 0.03615
	loss_reward_3: 0.0063
	loss_policy_4: 0.05416
	accuracy_policy_4: 0.56068
	loss_value_4: 0.03732
	loss_reward_4: 0.00697
	loss_policy_5: 0.05736
	accuracy_policy_5: 0.53432
	loss_value_5: 0.03856
	loss_reward_5: 0.00797
	loss_policy: 0.43625
	loss_value: 0.34213
	loss_reward: 0.03315
[2024-05-07 09:58:21] nn step 32400, lr: 0.053144.
	loss_policy_0: 0.15044
	accuracy_policy_0: 0.7476
	loss_value_0: 0.15945
	loss_policy_1: 0.03946
	accuracy_policy_1: 0.67697
	loss_value_1: 0.03346
	loss_reward_1: 0.00639
	loss_policy_2: 0.04418
	accuracy_policy_2: 0.64127
	loss_value_2: 0.03488
	loss_reward_2: 0.00589
	loss_policy_3: 0.04836
	accuracy_policy_3: 0.61684
	loss_value_3: 0.03633
	loss_reward_3: 0.00627
	loss_policy_4: 0.05207
	accuracy_policy_4: 0.59119
	loss_value_4: 0.03763
	loss_reward_4: 0.00702
	loss_policy_5: 0.05552
	accuracy_policy_5: 0.56924
	loss_value_5: 0.03883
	loss_reward_5: 0.00824
	loss_policy: 0.39002
	loss_value: 0.34057
	loss_reward: 0.03381
Optimization_Done 32400
[2024-05-07 10:00:38] [command] train weight_iter_32400.pkl 162 163
[2024-05-07 10:01:15] nn step 32500, lr: 0.053144.
	loss_policy_0: 0.1427
	accuracy_policy_0: 0.69428
	loss_value_0: 0.13802
	loss_policy_1: 0.03579
	accuracy_policy_1: 0.62609
	loss_value_1: 0.02875
	loss_reward_1: 0.00536
	loss_policy_2: 0.03959
	accuracy_policy_2: 0.5883
	loss_value_2: 0.02987
	loss_reward_2: 0.00497
	loss_policy_3: 0.04288
	accuracy_policy_3: 0.56254
	loss_value_3: 0.03084
	loss_reward_3: 0.00553
	loss_policy_4: 0.04515
	accuracy_policy_4: 0.54693
	loss_value_4: 0.03173
	loss_reward_4: 0.00593
	loss_policy_5: 0.04739
	accuracy_policy_5: 0.52951
	loss_value_5: 0.03246
	loss_reward_5: 0.00696
	loss_policy: 0.35351
	loss_value: 0.29167
	loss_reward: 0.02874
[2024-05-07 10:01:52] nn step 32600, lr: 0.053144.
	loss_policy_0: 0.11431
	accuracy_policy_0: 0.75021
	loss_value_0: 0.14097
	loss_policy_1: 0.0319
	accuracy_policy_1: 0.65771
	loss_value_1: 0.02927
	loss_reward_1: 0.00521
	loss_policy_2: 0.03647
	accuracy_policy_2: 0.6208
	loss_value_2: 0.03022
	loss_reward_2: 0.00481
	loss_policy_3: 0.0395
	accuracy_policy_3: 0.59287
	loss_value_3: 0.03127
	loss_reward_3: 0.00536
	loss_policy_4: 0.04175
	accuracy_policy_4: 0.57523
	loss_value_4: 0.03217
	loss_reward_4: 0.00578
	loss_policy_5: 0.04403
	accuracy_policy_5: 0.5616
	loss_value_5: 0.03305
	loss_reward_5: 0.00678
	loss_policy: 0.30796
	loss_value: 0.29695
	loss_reward: 0.02794
Optimization_Done 32600
[2024-05-07 10:04:12] [command] train weight_iter_32600.pkl 163 164
[2024-05-07 10:04:50] nn step 32700, lr: 0.053144.
	loss_policy_0: 0.14285
	accuracy_policy_0: 0.66891
	loss_value_0: 0.14628
	loss_policy_1: 0.03337
	accuracy_policy_1: 0.6126
	loss_value_1: 0.03022
	loss_reward_1: 0.00481
	loss_policy_2: 0.03704
	accuracy_policy_2: 0.57744
	loss_value_2: 0.0314
	loss_reward_2: 0.00444
	loss_policy_3: 0.03976
	accuracy_policy_3: 0.54396
	loss_value_3: 0.03239
	loss_reward_3: 0.00497
	loss_policy_4: 0.04199
	accuracy_policy_4: 0.52984
	loss_value_4: 0.03325
	loss_reward_4: 0.00526
	loss_policy_5: 0.04387
	accuracy_policy_5: 0.51193
	loss_value_5: 0.03412
	loss_reward_5: 0.0062
	loss_policy: 0.33889
	loss_value: 0.30766
	loss_reward: 0.02567
[2024-05-07 10:05:26] nn step 32800, lr: 0.053144.
	loss_policy_0: 0.10987
	accuracy_policy_0: 0.74229
	loss_value_0: 0.14647
	loss_policy_1: 0.02967
	accuracy_policy_1: 0.65537
	loss_value_1: 0.03059
	loss_reward_1: 0.00474
	loss_policy_2: 0.03367
	accuracy_policy_2: 0.61521
	loss_value_2: 0.0317
	loss_reward_2: 0.00447
	loss_policy_3: 0.03672
	accuracy_policy_3: 0.5859
	loss_value_3: 0.03266
	loss_reward_3: 0.00489
	loss_policy_4: 0.03927
	accuracy_policy_4: 0.56809
	loss_value_4: 0.03364
	loss_reward_4: 0.00516
	loss_policy_5: 0.04163
	accuracy_policy_5: 0.5484
	loss_value_5: 0.0346
	loss_reward_5: 0.00611
	loss_policy: 0.29083
	loss_value: 0.30965
	loss_reward: 0.02537
Optimization_Done 32800
[2024-05-07 10:07:45] [command] train weight_iter_32800.pkl 164 165
[2024-05-07 10:08:23] nn step 32900, lr: 0.053144.
	loss_policy_0: 0.19935
	accuracy_policy_0: 0.62719
	loss_value_0: 0.1655
	loss_policy_1: 0.04465
	accuracy_policy_1: 0.58422
	loss_value_1: 0.0343
	loss_reward_1: 0.00526
	loss_policy_2: 0.04795
	accuracy_policy_2: 0.55434
	loss_value_2: 0.03561
	loss_reward_2: 0.00501
	loss_policy_3: 0.05048
	accuracy_policy_3: 0.53447
	loss_value_3: 0.03688
	loss_reward_3: 0.00524
	loss_policy_4: 0.05299
	accuracy_policy_4: 0.5173
	loss_value_4: 0.03807
	loss_reward_4: 0.00567
	loss_policy_5: 0.05512
	accuracy_policy_5: 0.49689
	loss_value_5: 0.03927
	loss_reward_5: 0.00646
	loss_policy: 0.45055
	loss_value: 0.34964
	loss_reward: 0.02763
[2024-05-07 10:08:59] nn step 33000, lr: 0.053144.
	loss_policy_0: 0.1577
	accuracy_policy_0: 0.69105
	loss_value_0: 0.1572
	loss_policy_1: 0.03828
	accuracy_policy_1: 0.63029
	loss_value_1: 0.03285
	loss_reward_1: 0.00491
	loss_policy_2: 0.04218
	accuracy_policy_2: 0.59664
	loss_value_2: 0.034
	loss_reward_2: 0.00478
	loss_policy_3: 0.04505
	accuracy_policy_3: 0.57383
	loss_value_3: 0.03524
	loss_reward_3: 0.00502
	loss_policy_4: 0.04738
	accuracy_policy_4: 0.55424
	loss_value_4: 0.03644
	loss_reward_4: 0.00567
	loss_policy_5: 0.04973
	accuracy_policy_5: 0.53363
	loss_value_5: 0.0375
	loss_reward_5: 0.00645
	loss_policy: 0.38032
	loss_value: 0.33324
	loss_reward: 0.02683
Optimization_Done 33000
[2024-05-07 10:11:18] [command] train weight_iter_33000.pkl 165 166
[2024-05-07 10:11:56] nn step 33100, lr: 0.053144.
	loss_policy_0: 0.18014
	accuracy_policy_0: 0.69674
	loss_value_0: 0.18047
	loss_policy_1: 0.04234
	accuracy_policy_1: 0.64615
	loss_value_1: 0.03754
	loss_reward_1: 0.00634
	loss_policy_2: 0.04563
	accuracy_policy_2: 0.61645
	loss_value_2: 0.03872
	loss_reward_2: 0.0063
	loss_policy_3: 0.04839
	accuracy_policy_3: 0.59729
	loss_value_3: 0.03991
	loss_reward_3: 0.00664
	loss_policy_4: 0.05086
	accuracy_policy_4: 0.57598
	loss_value_4: 0.04109
	loss_reward_4: 0.00724
	loss_policy_5: 0.05283
	accuracy_policy_5: 0.56387
	loss_value_5: 0.04224
	loss_reward_5: 0.00801
	loss_policy: 0.42019
	loss_value: 0.37997
	loss_reward: 0.03453
[2024-05-07 10:12:32] nn step 33200, lr: 0.053144.
	loss_policy_0: 0.14079
	accuracy_policy_0: 0.74666
	loss_value_0: 0.16833
	loss_policy_1: 0.03515
	accuracy_policy_1: 0.68342
	loss_value_1: 0.03499
	loss_reward_1: 0.00579
	loss_policy_2: 0.03874
	accuracy_policy_2: 0.65512
	loss_value_2: 0.03612
	loss_reward_2: 0.00555
	loss_policy_3: 0.0423
	accuracy_policy_3: 0.62697
	loss_value_3: 0.0373
	loss_reward_3: 0.00604
	loss_policy_4: 0.04486
	accuracy_policy_4: 0.60693
	loss_value_4: 0.03848
	loss_reward_4: 0.00669
	loss_policy_5: 0.04694
	accuracy_policy_5: 0.58836
	loss_value_5: 0.03965
	loss_reward_5: 0.00755
	loss_policy: 0.34876
	loss_value: 0.35486
	loss_reward: 0.03162
Optimization_Done 33200
[2024-05-07 10:14:52] [command] train weight_iter_33200.pkl 166 167
[2024-05-07 10:15:30] nn step 33300, lr: 0.053144.
	loss_policy_0: 0.17967
	accuracy_policy_0: 0.69582
	loss_value_0: 0.18921
	loss_policy_1: 0.04199
	accuracy_policy_1: 0.64182
	loss_value_1: 0.039
	loss_reward_1: 0.00722
	loss_policy_2: 0.04594
	accuracy_policy_2: 0.61445
	loss_value_2: 0.04039
	loss_reward_2: 0.00689
	loss_policy_3: 0.04864
	accuracy_policy_3: 0.59338
	loss_value_3: 0.04159
	loss_reward_3: 0.00765
	loss_policy_4: 0.05055
	accuracy_policy_4: 0.57967
	loss_value_4: 0.0427
	loss_reward_4: 0.0084
	loss_policy_5: 0.05318
	accuracy_policy_5: 0.55902
	loss_value_5: 0.04369
	loss_reward_5: 0.00929
	loss_policy: 0.41997
	loss_value: 0.39658
	loss_reward: 0.03945
[2024-05-07 10:16:06] nn step 33400, lr: 0.053144.
	loss_policy_0: 0.13367
	accuracy_policy_0: 0.75277
	loss_value_0: 0.17716
	loss_policy_1: 0.03404
	accuracy_policy_1: 0.68785
	loss_value_1: 0.0367
	loss_reward_1: 0.0067
	loss_policy_2: 0.03801
	accuracy_policy_2: 0.65992
	loss_value_2: 0.03804
	loss_reward_2: 0.00655
	loss_policy_3: 0.04048
	accuracy_policy_3: 0.63971
	loss_value_3: 0.03936
	loss_reward_3: 0.00682
	loss_policy_4: 0.04342
	accuracy_policy_4: 0.61902
	loss_value_4: 0.04037
	loss_reward_4: 0.00772
	loss_policy_5: 0.04544
	accuracy_policy_5: 0.60549
	loss_value_5: 0.04139
	loss_reward_5: 0.00876
	loss_policy: 0.33507
	loss_value: 0.37302
	loss_reward: 0.03654
Optimization_Done 33400
[2024-05-07 10:18:05] [command] train weight_iter_33400.pkl 167 168
[2024-05-07 10:18:42] nn step 33500, lr: 0.053144.
	loss_policy_0: 0.15052
	accuracy_policy_0: 0.70172
	loss_value_0: 0.17383
	loss_policy_1: 0.03727
	accuracy_policy_1: 0.64199
	loss_value_1: 0.036
	loss_reward_1: 0.00598
	loss_policy_2: 0.04149
	accuracy_policy_2: 0.60619
	loss_value_2: 0.0374
	loss_reward_2: 0.00596
	loss_policy_3: 0.04395
	accuracy_policy_3: 0.58779
	loss_value_3: 0.0383
	loss_reward_3: 0.00643
	loss_policy_4: 0.04625
	accuracy_policy_4: 0.56225
	loss_value_4: 0.03932
	loss_reward_4: 0.00698
	loss_policy_5: 0.04878
	accuracy_policy_5: 0.5408
	loss_value_5: 0.04015
	loss_reward_5: 0.0079
	loss_policy: 0.36826
	loss_value: 0.36499
	loss_reward: 0.03326
[2024-05-07 10:19:18] nn step 33600, lr: 0.053144.
	loss_policy_0: 0.12511
	accuracy_policy_0: 0.74244
	loss_value_0: 0.16348
	loss_policy_1: 0.03312
	accuracy_policy_1: 0.67072
	loss_value_1: 0.03425
	loss_reward_1: 0.00584
	loss_policy_2: 0.03704
	accuracy_policy_2: 0.6416
	loss_value_2: 0.03561
	loss_reward_2: 0.00564
	loss_policy_3: 0.03978
	accuracy_policy_3: 0.61584
	loss_value_3: 0.03668
	loss_reward_3: 0.00618
	loss_policy_4: 0.04199
	accuracy_policy_4: 0.59639
	loss_value_4: 0.03783
	loss_reward_4: 0.00666
	loss_policy_5: 0.0448
	accuracy_policy_5: 0.5732
	loss_value_5: 0.03889
	loss_reward_5: 0.00781
	loss_policy: 0.32185
	loss_value: 0.34675
	loss_reward: 0.03213
Optimization_Done 33600
[2024-05-07 10:21:40] [command] train weight_iter_33600.pkl 168 169
[2024-05-07 10:22:18] nn step 33700, lr: 0.053144.
	loss_policy_0: 0.17232
	accuracy_policy_0: 0.69266
	loss_value_0: 0.1764
	loss_policy_1: 0.04017
	accuracy_policy_1: 0.64777
	loss_value_1: 0.03677
	loss_reward_1: 0.00529
	loss_policy_2: 0.04428
	accuracy_policy_2: 0.61709
	loss_value_2: 0.03834
	loss_reward_2: 0.00523
	loss_policy_3: 0.04762
	accuracy_policy_3: 0.59443
	loss_value_3: 0.03965
	loss_reward_3: 0.00556
	loss_policy_4: 0.05037
	accuracy_policy_4: 0.57242
	loss_value_4: 0.04109
	loss_reward_4: 0.00607
	loss_policy_5: 0.05357
	accuracy_policy_5: 0.54992
	loss_value_5: 0.04238
	loss_reward_5: 0.00708
	loss_policy: 0.40833
	loss_value: 0.37463
	loss_reward: 0.02923
[2024-05-07 10:22:54] nn step 33800, lr: 0.053144.
	loss_policy_0: 0.1344
	accuracy_policy_0: 0.74838
	loss_value_0: 0.16649
	loss_policy_1: 0.03498
	accuracy_policy_1: 0.68387
	loss_value_1: 0.03484
	loss_reward_1: 0.00507
	loss_policy_2: 0.0393
	accuracy_policy_2: 0.64582
	loss_value_2: 0.03624
	loss_reward_2: 0.00499
	loss_policy_3: 0.0424
	accuracy_policy_3: 0.62598
	loss_value_3: 0.03756
	loss_reward_3: 0.00527
	loss_policy_4: 0.04525
	accuracy_policy_4: 0.60361
	loss_value_4: 0.03892
	loss_reward_4: 0.00567
	loss_policy_5: 0.0484
	accuracy_policy_5: 0.57572
	loss_value_5: 0.04027
	loss_reward_5: 0.00672
	loss_policy: 0.34473
	loss_value: 0.35432
	loss_reward: 0.02772
Optimization_Done 33800
[2024-05-07 10:25:09] [command] train weight_iter_33800.pkl 169 170
[2024-05-07 10:25:46] nn step 33900, lr: 0.053144.
	loss_policy_0: 0.15602
	accuracy_policy_0: 0.69336
	loss_value_0: 0.16495
	loss_policy_1: 0.03712
	accuracy_policy_1: 0.64131
	loss_value_1: 0.03472
	loss_reward_1: 0.00537
	loss_policy_2: 0.04058
	accuracy_policy_2: 0.61627
	loss_value_2: 0.03614
	loss_reward_2: 0.00497
	loss_policy_3: 0.04375
	accuracy_policy_3: 0.59104
	loss_value_3: 0.03762
	loss_reward_3: 0.00553
	loss_policy_4: 0.04653
	accuracy_policy_4: 0.56875
	loss_value_4: 0.03895
	loss_reward_4: 0.00588
	loss_policy_5: 0.04964
	accuracy_policy_5: 0.54508
	loss_value_5: 0.04024
	loss_reward_5: 0.0068
	loss_policy: 0.37364
	loss_value: 0.35263
	loss_reward: 0.02855
[2024-05-07 10:26:21] nn step 34000, lr: 0.053144.
	loss_policy_0: 0.13584
	accuracy_policy_0: 0.74275
	loss_value_0: 0.17008
	loss_policy_1: 0.03446
	accuracy_policy_1: 0.68252
	loss_value_1: 0.03563
	loss_reward_1: 0.0056
	loss_policy_2: 0.03891
	accuracy_policy_2: 0.6502
	loss_value_2: 0.03716
	loss_reward_2: 0.00506
	loss_policy_3: 0.04215
	accuracy_policy_3: 0.62547
	loss_value_3: 0.0386
	loss_reward_3: 0.00545
	loss_policy_4: 0.0458
	accuracy_policy_4: 0.59811
	loss_value_4: 0.03995
	loss_reward_4: 0.00603
	loss_policy_5: 0.04874
	accuracy_policy_5: 0.57689
	loss_value_5: 0.04137
	loss_reward_5: 0.0072
	loss_policy: 0.3459
	loss_value: 0.36279
	loss_reward: 0.02935
Optimization_Done 34000
[2024-05-07 10:28:36] [command] train weight_iter_34000.pkl 170 171
[2024-05-07 10:29:12] nn step 34100, lr: 0.053144.
	loss_policy_0: 0.15048
	accuracy_policy_0: 0.67207
	loss_value_0: 0.15725
	loss_policy_1: 0.03709
	accuracy_policy_1: 0.60904
	loss_value_1: 0.03265
	loss_reward_1: 0.00578
	loss_policy_2: 0.04119
	accuracy_policy_2: 0.57607
	loss_value_2: 0.03421
	loss_reward_2: 0.00523
	loss_policy_3: 0.04415
	accuracy_policy_3: 0.5516
	loss_value_3: 0.03533
	loss_reward_3: 0.0058
	loss_policy_4: 0.047
	accuracy_policy_4: 0.52658
	loss_value_4: 0.0366
	loss_reward_4: 0.00628
	loss_policy_5: 0.04954
	accuracy_policy_5: 0.50686
	loss_value_5: 0.0377
	loss_reward_5: 0.00751
	loss_policy: 0.36945
	loss_value: 0.33374
	loss_reward: 0.0306
[2024-05-07 10:29:48] nn step 34200, lr: 0.053144.
	loss_policy_0: 0.12609
	accuracy_policy_0: 0.72395
	loss_value_0: 0.16087
	loss_policy_1: 0.03354
	accuracy_policy_1: 0.64607
	loss_value_1: 0.03378
	loss_reward_1: 0.00571
	loss_policy_2: 0.03796
	accuracy_policy_2: 0.60602
	loss_value_2: 0.03507
	loss_reward_2: 0.00528
	loss_policy_3: 0.04123
	accuracy_policy_3: 0.58496
	loss_value_3: 0.03615
	loss_reward_3: 0.00586
	loss_policy_4: 0.04425
	accuracy_policy_4: 0.56328
	loss_value_4: 0.03737
	loss_reward_4: 0.00625
	loss_policy_5: 0.04745
	accuracy_policy_5: 0.53527
	loss_value_5: 0.03847
	loss_reward_5: 0.00756
	loss_policy: 0.33052
	loss_value: 0.34172
	loss_reward: 0.03066
Optimization_Done 34200
[2024-05-07 10:31:32] [command] train weight_iter_34200.pkl 171 172
[2024-05-07 10:32:08] nn step 34300, lr: 0.053144.
	loss_policy_0: 0.12822
	accuracy_policy_0: 0.69062
	loss_value_0: 0.14547
	loss_policy_1: 0.03199
	accuracy_policy_1: 0.61713
	loss_value_1: 0.03048
	loss_reward_1: 0.00442
	loss_policy_2: 0.03597
	accuracy_policy_2: 0.58158
	loss_value_2: 0.03169
	loss_reward_2: 0.00419
	loss_policy_3: 0.03848
	accuracy_policy_3: 0.55654
	loss_value_3: 0.03268
	loss_reward_3: 0.00462
	loss_policy_4: 0.04098
	accuracy_policy_4: 0.53848
	loss_value_4: 0.03383
	loss_reward_4: 0.00496
	loss_policy_5: 0.04342
	accuracy_policy_5: 0.51537
	loss_value_5: 0.03475
	loss_reward_5: 0.00587
	loss_policy: 0.31905
	loss_value: 0.30891
	loss_reward: 0.02406
[2024-05-07 10:32:44] nn step 34400, lr: 0.053144.
	loss_policy_0: 0.09696
	accuracy_policy_0: 0.74881
	loss_value_0: 0.13795
	loss_policy_1: 0.02733
	accuracy_policy_1: 0.66068
	loss_value_1: 0.02881
	loss_reward_1: 0.00437
	loss_policy_2: 0.03087
	accuracy_policy_2: 0.62625
	loss_value_2: 0.03009
	loss_reward_2: 0.00396
	loss_policy_3: 0.03383
	accuracy_policy_3: 0.59439
	loss_value_3: 0.03114
	loss_reward_3: 0.00437
	loss_policy_4: 0.03658
	accuracy_policy_4: 0.57162
	loss_value_4: 0.03226
	loss_reward_4: 0.00465
	loss_policy_5: 0.03931
	accuracy_policy_5: 0.54531
	loss_value_5: 0.03338
	loss_reward_5: 0.00555
	loss_policy: 0.26489
	loss_value: 0.29363
	loss_reward: 0.0229
Optimization_Done 34400
[2024-05-07 10:35:00] [command] train weight_iter_34400.pkl 172 173
[2024-05-07 10:35:37] nn step 34500, lr: 0.053144.
	loss_policy_0: 0.17699
	accuracy_policy_0: 0.60986
	loss_value_0: 0.15124
	loss_policy_1: 0.03936
	accuracy_policy_1: 0.5657
	loss_value_1: 0.03169
	loss_reward_1: 0.00492
	loss_policy_2: 0.04231
	accuracy_policy_2: 0.53686
	loss_value_2: 0.03297
	loss_reward_2: 0.00481
	loss_policy_3: 0.04486
	accuracy_policy_3: 0.51607
	loss_value_3: 0.03415
	loss_reward_3: 0.00516
	loss_policy_4: 0.0469
	accuracy_policy_4: 0.49535
	loss_value_4: 0.03523
	loss_reward_4: 0.00551
	loss_policy_5: 0.0492
	accuracy_policy_5: 0.47615
	loss_value_5: 0.03628
	loss_reward_5: 0.00635
	loss_policy: 0.39962
	loss_value: 0.32156
	loss_reward: 0.02676
[2024-05-07 10:36:13] nn step 34600, lr: 0.053144.
	loss_policy_0: 0.13851
	accuracy_policy_0: 0.68668
	loss_value_0: 0.15113
	loss_policy_1: 0.03364
	accuracy_policy_1: 0.61756
	loss_value_1: 0.03169
	loss_reward_1: 0.00464
	loss_policy_2: 0.03743
	accuracy_policy_2: 0.58035
	loss_value_2: 0.03289
	loss_reward_2: 0.00463
	loss_policy_3: 0.04017
	accuracy_policy_3: 0.55691
	loss_value_3: 0.03399
	loss_reward_3: 0.005
	loss_policy_4: 0.04286
	accuracy_policy_4: 0.53438
	loss_value_4: 0.0352
	loss_reward_4: 0.00535
	loss_policy_5: 0.04534
	accuracy_policy_5: 0.51076
	loss_value_5: 0.0362
	loss_reward_5: 0.00601
	loss_policy: 0.33795
	loss_value: 0.3211
	loss_reward: 0.02563
Optimization_Done 34600
[2024-05-07 10:38:30] [command] train weight_iter_34600.pkl 173 174
[2024-05-07 10:39:08] nn step 34700, lr: 0.053144.
	loss_policy_0: 0.22787
	accuracy_policy_0: 0.5977
	loss_value_0: 0.17968
	loss_policy_1: 0.05039
	accuracy_policy_1: 0.55439
	loss_value_1: 0.03754
	loss_reward_1: 0.00579
	loss_policy_2: 0.05418
	accuracy_policy_2: 0.52662
	loss_value_2: 0.03908
	loss_reward_2: 0.00563
	loss_policy_3: 0.05691
	accuracy_policy_3: 0.50211
	loss_value_3: 0.04041
	loss_reward_3: 0.00596
	loss_policy_4: 0.05994
	accuracy_policy_4: 0.48766
	loss_value_4: 0.04177
	loss_reward_4: 0.00631
	loss_policy_5: 0.06188
	accuracy_policy_5: 0.4723
	loss_value_5: 0.04296
	loss_reward_5: 0.00708
	loss_policy: 0.51117
	loss_value: 0.38143
	loss_reward: 0.03077
[2024-05-07 10:39:44] nn step 34800, lr: 0.053144.
	loss_policy_0: 0.18593
	accuracy_policy_0: 0.66037
	loss_value_0: 0.17654
	loss_policy_1: 0.04374
	accuracy_policy_1: 0.60326
	loss_value_1: 0.03681
	loss_reward_1: 0.00562
	loss_policy_2: 0.04777
	accuracy_policy_2: 0.5718
	loss_value_2: 0.03813
	loss_reward_2: 0.00555
	loss_policy_3: 0.051
	accuracy_policy_3: 0.5482
	loss_value_3: 0.0394
	loss_reward_3: 0.0059
	loss_policy_4: 0.05438
	accuracy_policy_4: 0.5248
	loss_value_4: 0.04067
	loss_reward_4: 0.00623
	loss_policy_5: 0.05693
	accuracy_policy_5: 0.50453
	loss_value_5: 0.04202
	loss_reward_5: 0.00697
	loss_policy: 0.43975
	loss_value: 0.37357
	loss_reward: 0.03028
Optimization_Done 34800
[2024-05-07 10:41:59] [command] train weight_iter_34800.pkl 174 175
[2024-05-07 10:42:37] nn step 34900, lr: 0.053144.
	loss_policy_0: 0.1771
	accuracy_policy_0: 0.67111
	loss_value_0: 0.16628
	loss_policy_1: 0.04167
	accuracy_policy_1: 0.61711
	loss_value_1: 0.03466
	loss_reward_1: 0.00597
	loss_policy_2: 0.04546
	accuracy_policy_2: 0.59189
	loss_value_2: 0.03613
	loss_reward_2: 0.00546
	loss_policy_3: 0.04837
	accuracy_policy_3: 0.57477
	loss_value_3: 0.03737
	loss_reward_3: 0.00607
	loss_policy_4: 0.05062
	accuracy_policy_4: 0.56123
	loss_value_4: 0.03857
	loss_reward_4: 0.00681
	loss_policy_5: 0.05288
	accuracy_policy_5: 0.5477
	loss_value_5: 0.03975
	loss_reward_5: 0.0076
	loss_policy: 0.4161
	loss_value: 0.35275
	loss_reward: 0.0319
[2024-05-07 10:43:12] nn step 35000, lr: 0.053144.
	loss_policy_0: 0.1585
	accuracy_policy_0: 0.71621
	loss_value_0: 0.17471
	loss_policy_1: 0.03932
	accuracy_policy_1: 0.64883
	loss_value_1: 0.03638
	loss_reward_1: 0.00621
	loss_policy_2: 0.04353
	accuracy_policy_2: 0.62215
	loss_value_2: 0.03787
	loss_reward_2: 0.00558
	loss_policy_3: 0.04751
	accuracy_policy_3: 0.59918
	loss_value_3: 0.03941
	loss_reward_3: 0.00619
	loss_policy_4: 0.04972
	accuracy_policy_4: 0.5883
	loss_value_4: 0.04073
	loss_reward_4: 0.00665
	loss_policy_5: 0.05247
	accuracy_policy_5: 0.56871
	loss_value_5: 0.04183
	loss_reward_5: 0.00777
	loss_policy: 0.39105
	loss_value: 0.37092
	loss_reward: 0.03239
Optimization_Done 35000
[2024-05-07 10:45:04] [command] train weight_iter_35000.pkl 175 176
[2024-05-07 10:45:41] nn step 35100, lr: 0.053144.
	loss_policy_0: 0.15805
	accuracy_policy_0: 0.70662
	loss_value_0: 0.18098
	loss_policy_1: 0.03888
	accuracy_policy_1: 0.65197
	loss_value_1: 0.03753
	loss_reward_1: 0.00641
	loss_policy_2: 0.04295
	accuracy_policy_2: 0.62357
	loss_value_2: 0.03915
	loss_reward_2: 0.00604
	loss_policy_3: 0.04579
	accuracy_policy_3: 0.60033
	loss_value_3: 0.0403
	loss_reward_3: 0.00674
	loss_policy_4: 0.04777
	accuracy_policy_4: 0.58568
	loss_value_4: 0.04129
	loss_reward_4: 0.00743
	loss_policy_5: 0.04991
	accuracy_policy_5: 0.5683
	loss_value_5: 0.042
	loss_reward_5: 0.0086
	loss_policy: 0.38335
	loss_value: 0.38125
	loss_reward: 0.03521
[2024-05-07 10:46:17] nn step 35200, lr: 0.053144.
	loss_policy_0: 0.13604
	accuracy_policy_0: 0.75031
	loss_value_0: 0.17807
	loss_policy_1: 0.03575
	accuracy_policy_1: 0.68436
	loss_value_1: 0.03728
	loss_reward_1: 0.0065
	loss_policy_2: 0.04007
	accuracy_policy_2: 0.65049
	loss_value_2: 0.03865
	loss_reward_2: 0.00634
	loss_policy_3: 0.04284
	accuracy_policy_3: 0.62936
	loss_value_3: 0.03996
	loss_reward_3: 0.00661
	loss_policy_4: 0.04551
	accuracy_policy_4: 0.61156
	loss_value_4: 0.04113
	loss_reward_4: 0.00723
	loss_policy_5: 0.04807
	accuracy_policy_5: 0.59795
	loss_value_5: 0.04234
	loss_reward_5: 0.00863
	loss_policy: 0.34829
	loss_value: 0.37743
	loss_reward: 0.03532
Optimization_Done 35200
[2024-05-07 10:48:35] [command] train weight_iter_35200.pkl 176 177
[2024-05-07 10:49:12] nn step 35300, lr: 0.053144.
	loss_policy_0: 0.16621
	accuracy_policy_0: 0.68869
	loss_value_0: 0.16926
	loss_policy_1: 0.04019
	accuracy_policy_1: 0.6308
	loss_value_1: 0.03552
	loss_reward_1: 0.00451
	loss_policy_2: 0.04427
	accuracy_policy_2: 0.59713
	loss_value_2: 0.03678
	loss_reward_2: 0.00421
	loss_policy_3: 0.04732
	accuracy_policy_3: 0.57576
	loss_value_3: 0.03798
	loss_reward_3: 0.0046
	loss_policy_4: 0.04987
	accuracy_policy_4: 0.55477
	loss_value_4: 0.0389
	loss_reward_4: 0.00514
	loss_policy_5: 0.0528
	accuracy_policy_5: 0.53504
	loss_value_5: 0.03995
	loss_reward_5: 0.00579
	loss_policy: 0.40066
	loss_value: 0.35839
	loss_reward: 0.02426
[2024-05-07 10:49:47] nn step 35400, lr: 0.053144.
	loss_policy_0: 0.13077
	accuracy_policy_0: 0.74797
	loss_value_0: 0.16623
	loss_policy_1: 0.03457
	accuracy_policy_1: 0.67604
	loss_value_1: 0.03474
	loss_reward_1: 0.00459
	loss_policy_2: 0.03892
	accuracy_policy_2: 0.64404
	loss_value_2: 0.03604
	loss_reward_2: 0.00427
	loss_policy_3: 0.04255
	accuracy_policy_3: 0.61389
	loss_value_3: 0.03725
	loss_reward_3: 0.00454
	loss_policy_4: 0.04529
	accuracy_policy_4: 0.59623
	loss_value_4: 0.03831
	loss_reward_4: 0.00529
	loss_policy_5: 0.04795
	accuracy_policy_5: 0.5757
	loss_value_5: 0.03948
	loss_reward_5: 0.00592
	loss_policy: 0.34004
	loss_value: 0.35205
	loss_reward: 0.02461
Optimization_Done 35400
[2024-05-07 10:52:03] [command] train weight_iter_35400.pkl 177 178
[2024-05-07 10:52:39] nn step 35500, lr: 0.053144.
	loss_policy_0: 0.18561
	accuracy_policy_0: 0.64939
	loss_value_0: 0.17561
	loss_policy_1: 0.0432
	accuracy_policy_1: 0.60088
	loss_value_1: 0.03641
	loss_reward_1: 0.00431
	loss_policy_2: 0.04744
	accuracy_policy_2: 0.57
	loss_value_2: 0.03755
	loss_reward_2: 0.00415
	loss_policy_3: 0.05087
	accuracy_policy_3: 0.54297
	loss_value_3: 0.03875
	loss_reward_3: 0.0042
	loss_policy_4: 0.05334
	accuracy_policy_4: 0.52355
	loss_value_4: 0.0398
	loss_reward_4: 0.00457
	loss_policy_5: 0.0562
	accuracy_policy_5: 0.50414
	loss_value_5: 0.04086
	loss_reward_5: 0.00517
	loss_policy: 0.43666
	loss_value: 0.36897
	loss_reward: 0.02239
[2024-05-07 10:53:14] nn step 35600, lr: 0.053144.
	loss_policy_0: 0.14654
	accuracy_policy_0: 0.7067
	loss_value_0: 0.16516
	loss_policy_1: 0.03617
	accuracy_policy_1: 0.64623
	loss_value_1: 0.03432
	loss_reward_1: 0.00404
	loss_policy_2: 0.04103
	accuracy_policy_2: 0.61004
	loss_value_2: 0.03548
	loss_reward_2: 0.00384
	loss_policy_3: 0.04456
	accuracy_policy_3: 0.58055
	loss_value_3: 0.0366
	loss_reward_3: 0.00404
	loss_policy_4: 0.04743
	accuracy_policy_4: 0.56225
	loss_value_4: 0.03762
	loss_reward_4: 0.00433
	loss_policy_5: 0.05012
	accuracy_policy_5: 0.5425
	loss_value_5: 0.03858
	loss_reward_5: 0.00483
	loss_policy: 0.36585
	loss_value: 0.34776
	loss_reward: 0.02106
Optimization_Done 35600
[2024-05-07 10:55:29] [command] train weight_iter_35600.pkl 178 179
[2024-05-07 10:56:06] nn step 35700, lr: 0.053144.
	loss_policy_0: 0.16144
	accuracy_policy_0: 0.68076
	loss_value_0: 0.17306
	loss_policy_1: 0.03775
	accuracy_policy_1: 0.63543
	loss_value_1: 0.03587
	loss_reward_1: 0.00584
	loss_policy_2: 0.04147
	accuracy_policy_2: 0.60256
	loss_value_2: 0.03721
	loss_reward_2: 0.00564
	loss_policy_3: 0.04453
	accuracy_policy_3: 0.5757
	loss_value_3: 0.03837
	loss_reward_3: 0.00625
	loss_policy_4: 0.04705
	accuracy_policy_4: 0.55408
	loss_value_4: 0.03958
	loss_reward_4: 0.00665
	loss_policy_5: 0.04918
	accuracy_policy_5: 0.53295
	loss_value_5: 0.04064
	loss_reward_5: 0.00746
	loss_policy: 0.38142
	loss_value: 0.36473
	loss_reward: 0.03184
[2024-05-07 10:56:41] nn step 35800, lr: 0.053144.
	loss_policy_0: 0.12998
	accuracy_policy_0: 0.72824
	loss_value_0: 0.17356
	loss_policy_1: 0.03336
	accuracy_policy_1: 0.66422
	loss_value_1: 0.03605
	loss_reward_1: 0.00555
	loss_policy_2: 0.03745
	accuracy_policy_2: 0.62846
	loss_value_2: 0.03726
	loss_reward_2: 0.0054
	loss_policy_3: 0.04041
	accuracy_policy_3: 0.6077
	loss_value_3: 0.03852
	loss_reward_3: 0.00568
	loss_policy_4: 0.04292
	accuracy_policy_4: 0.58725
	loss_value_4: 0.03957
	loss_reward_4: 0.00617
	loss_policy_5: 0.04583
	accuracy_policy_5: 0.56281
	loss_value_5: 0.04063
	loss_reward_5: 0.00721
	loss_policy: 0.32995
	loss_value: 0.36558
	loss_reward: 0.03001
Optimization_Done 35800
[2024-05-07 10:58:56] [command] train weight_iter_35800.pkl 179 180
[2024-05-07 10:59:32] nn step 35900, lr: 0.053144.
	loss_policy_0: 0.14642
	accuracy_policy_0: 0.71648
	loss_value_0: 0.19164
	loss_policy_1: 0.03697
	accuracy_policy_1: 0.65307
	loss_value_1: 0.03998
	loss_reward_1: 0.00705
	loss_policy_2: 0.04083
	accuracy_policy_2: 0.61652
	loss_value_2: 0.04142
	loss_reward_2: 0.0067
	loss_policy_3: 0.04423
	accuracy_policy_3: 0.58746
	loss_value_3: 0.04277
	loss_reward_3: 0.00748
	loss_policy_4: 0.0474
	accuracy_policy_4: 0.56414
	loss_value_4: 0.04419
	loss_reward_4: 0.00808
	loss_policy_5: 0.05003
	accuracy_policy_5: 0.54123
	loss_value_5: 0.04537
	loss_reward_5: 0.0095
	loss_policy: 0.36588
	loss_value: 0.40536
	loss_reward: 0.03881
[2024-05-07 11:00:08] nn step 36000, lr: 0.053144.
	loss_policy_0: 0.11652
	accuracy_policy_0: 0.75209
	loss_value_0: 0.1784
	loss_policy_1: 0.03123
	accuracy_policy_1: 0.67848
	loss_value_1: 0.0373
	loss_reward_1: 0.00652
	loss_policy_2: 0.03532
	accuracy_policy_2: 0.6392
	loss_value_2: 0.03856
	loss_reward_2: 0.00623
	loss_policy_3: 0.03856
	accuracy_policy_3: 0.61164
	loss_value_3: 0.03982
	loss_reward_3: 0.00685
	loss_policy_4: 0.04126
	accuracy_policy_4: 0.59303
	loss_value_4: 0.04102
	loss_reward_4: 0.00732
	loss_policy_5: 0.04384
	accuracy_policy_5: 0.57113
	loss_value_5: 0.04201
	loss_reward_5: 0.00864
	loss_policy: 0.30672
	loss_value: 0.3771
	loss_reward: 0.03555
Optimization_Done 36000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 11:14:51] [command] train weight_iter_36000.pkl 180 181
[2024-05-07 11:15:35] nn step 36100, lr: 0.04783.
	loss_policy_0: 0.16155
	accuracy_policy_0: 0.69928
	loss_value_0: 0.17791
	loss_policy_1: 0.04016
	accuracy_policy_1: 0.63807
	loss_value_1: 0.03719
	loss_reward_1: 0.00559
	loss_policy_2: 0.0447
	accuracy_policy_2: 0.59793
	loss_value_2: 0.03868
	loss_reward_2: 0.00544
	loss_policy_3: 0.04791
	accuracy_policy_3: 0.57242
	loss_value_3: 0.04009
	loss_reward_3: 0.00587
	loss_policy_4: 0.05112
	accuracy_policy_4: 0.54512
	loss_value_4: 0.04138
	loss_reward_4: 0.00627
	loss_policy_5: 0.05438
	accuracy_policy_5: 0.52191
	loss_value_5: 0.04248
	loss_reward_5: 0.00738
	loss_policy: 0.39982
	loss_value: 0.37772
	loss_reward: 0.03055
[2024-05-07 11:16:10] nn step 36200, lr: 0.04783.
	loss_policy_0: 0.12591
	accuracy_policy_0: 0.74352
	loss_value_0: 0.16796
	loss_policy_1: 0.03399
	accuracy_policy_1: 0.66836
	loss_value_1: 0.03511
	loss_reward_1: 0.00542
	loss_policy_2: 0.03836
	accuracy_policy_2: 0.63035
	loss_value_2: 0.03647
	loss_reward_2: 0.00505
	loss_policy_3: 0.04201
	accuracy_policy_3: 0.60695
	loss_value_3: 0.0378
	loss_reward_3: 0.00569
	loss_policy_4: 0.04535
	accuracy_policy_4: 0.58027
	loss_value_4: 0.03895
	loss_reward_4: 0.00599
	loss_policy_5: 0.04858
	accuracy_policy_5: 0.55572
	loss_value_5: 0.04024
	loss_reward_5: 0.00707
	loss_policy: 0.3342
	loss_value: 0.35653
	loss_reward: 0.02923
Optimization_Done 36200
[2024-05-07 11:18:24] [command] train weight_iter_36200.pkl 181 182
[2024-05-07 11:19:01] nn step 36300, lr: 0.04783.
	loss_policy_0: 0.16885
	accuracy_policy_0: 0.66791
	loss_value_0: 0.1474
	loss_policy_1: 0.03941
	accuracy_policy_1: 0.61475
	loss_value_1: 0.03087
	loss_reward_1: 0.00349
	loss_policy_2: 0.04354
	accuracy_policy_2: 0.58047
	loss_value_2: 0.03208
	loss_reward_2: 0.00335
	loss_policy_3: 0.04746
	accuracy_policy_3: 0.55041
	loss_value_3: 0.03327
	loss_reward_3: 0.00357
	loss_policy_4: 0.0499
	accuracy_policy_4: 0.53207
	loss_value_4: 0.0344
	loss_reward_4: 0.00387
	loss_policy_5: 0.05294
	accuracy_policy_5: 0.5127
	loss_value_5: 0.03541
	loss_reward_5: 0.00433
	loss_policy: 0.40211
	loss_value: 0.31344
	loss_reward: 0.01861
[2024-05-07 11:19:36] nn step 36400, lr: 0.04783.
	loss_policy_0: 0.13859
	accuracy_policy_0: 0.72576
	loss_value_0: 0.14759
	loss_policy_1: 0.03536
	accuracy_policy_1: 0.65881
	loss_value_1: 0.03088
	loss_reward_1: 0.00371
	loss_policy_2: 0.04066
	accuracy_policy_2: 0.61699
	loss_value_2: 0.03198
	loss_reward_2: 0.00339
	loss_policy_3: 0.04446
	accuracy_policy_3: 0.5874
	loss_value_3: 0.03325
	loss_reward_3: 0.00356
	loss_policy_4: 0.04762
	accuracy_policy_4: 0.56947
	loss_value_4: 0.03449
	loss_reward_4: 0.00381
	loss_policy_5: 0.0504
	accuracy_policy_5: 0.55072
	loss_value_5: 0.03562
	loss_reward_5: 0.00458
	loss_policy: 0.35709
	loss_value: 0.31381
	loss_reward: 0.01905
Optimization_Done 36400
[2024-05-07 11:21:45] [command] train weight_iter_36400.pkl 182 183
[2024-05-07 11:22:22] nn step 36500, lr: 0.04783.
	loss_policy_0: 0.14872
	accuracy_policy_0: 0.70824
	loss_value_0: 0.16714
	loss_policy_1: 0.03629
	accuracy_policy_1: 0.65217
	loss_value_1: 0.03483
	loss_reward_1: 0.0055
	loss_policy_2: 0.04095
	accuracy_policy_2: 0.61174
	loss_value_2: 0.03617
	loss_reward_2: 0.00512
	loss_policy_3: 0.0442
	accuracy_policy_3: 0.58293
	loss_value_3: 0.03731
	loss_reward_3: 0.00559
	loss_policy_4: 0.04743
	accuracy_policy_4: 0.56002
	loss_value_4: 0.03839
	loss_reward_4: 0.00605
	loss_policy_5: 0.05029
	accuracy_policy_5: 0.53709
	loss_value_5: 0.03939
	loss_reward_5: 0.0069
	loss_policy: 0.36787
	loss_value: 0.35323
	loss_reward: 0.02917
[2024-05-07 11:22:58] nn step 36600, lr: 0.04783.
	loss_policy_0: 0.12422
	accuracy_policy_0: 0.74984
	loss_value_0: 0.16903
	loss_policy_1: 0.03325
	accuracy_policy_1: 0.6793
	loss_value_1: 0.0352
	loss_reward_1: 0.00526
	loss_policy_2: 0.03834
	accuracy_policy_2: 0.63641
	loss_value_2: 0.03649
	loss_reward_2: 0.00492
	loss_policy_3: 0.04194
	accuracy_policy_3: 0.60836
	loss_value_3: 0.03763
	loss_reward_3: 0.00529
	loss_policy_4: 0.04522
	accuracy_policy_4: 0.58207
	loss_value_4: 0.03871
	loss_reward_4: 0.00578
	loss_policy_5: 0.04828
	accuracy_policy_5: 0.56234
	loss_value_5: 0.03982
	loss_reward_5: 0.00686
	loss_policy: 0.33125
	loss_value: 0.35688
	loss_reward: 0.0281
Optimization_Done 36600
[2024-05-07 11:25:09] [command] train weight_iter_36600.pkl 183 184
[2024-05-07 11:25:46] nn step 36700, lr: 0.04783.
	loss_policy_0: 0.15213
	accuracy_policy_0: 0.6873
	loss_value_0: 0.16744
	loss_policy_1: 0.03794
	accuracy_policy_1: 0.6215
	loss_value_1: 0.03484
	loss_reward_1: 0.0053
	loss_policy_2: 0.04267
	accuracy_policy_2: 0.58121
	loss_value_2: 0.03613
	loss_reward_2: 0.00517
	loss_policy_3: 0.04572
	accuracy_policy_3: 0.55693
	loss_value_3: 0.03728
	loss_reward_3: 0.00554
	loss_policy_4: 0.04855
	accuracy_policy_4: 0.53836
	loss_value_4: 0.03819
	loss_reward_4: 0.00621
	loss_policy_5: 0.05086
	accuracy_policy_5: 0.51834
	loss_value_5: 0.03923
	loss_reward_5: 0.00713
	loss_policy: 0.37787
	loss_value: 0.35309
	loss_reward: 0.02935
[2024-05-07 11:26:21] nn step 36800, lr: 0.04783.
	loss_policy_0: 0.12742
	accuracy_policy_0: 0.74482
	loss_value_0: 0.16943
	loss_policy_1: 0.03526
	accuracy_policy_1: 0.65744
	loss_value_1: 0.03551
	loss_reward_1: 0.0057
	loss_policy_2: 0.04021
	accuracy_policy_2: 0.61842
	loss_value_2: 0.03689
	loss_reward_2: 0.00552
	loss_policy_3: 0.04353
	accuracy_policy_3: 0.59324
	loss_value_3: 0.0382
	loss_reward_3: 0.00609
	loss_policy_4: 0.04674
	accuracy_policy_4: 0.56857
	loss_value_4: 0.0393
	loss_reward_4: 0.00646
	loss_policy_5: 0.04901
	accuracy_policy_5: 0.55576
	loss_value_5: 0.04052
	loss_reward_5: 0.00771
	loss_policy: 0.34217
	loss_value: 0.35986
	loss_reward: 0.03149
Optimization_Done 36800
[2024-05-07 11:28:34] [command] train weight_iter_36800.pkl 184 185
[2024-05-07 11:29:10] nn step 36900, lr: 0.04783.
	loss_policy_0: 0.15519
	accuracy_policy_0: 0.67158
	loss_value_0: 0.15303
	loss_policy_1: 0.03839
	accuracy_policy_1: 0.60943
	loss_value_1: 0.03215
	loss_reward_1: 0.00483
	loss_policy_2: 0.04258
	accuracy_policy_2: 0.56832
	loss_value_2: 0.03358
	loss_reward_2: 0.00458
	loss_policy_3: 0.04578
	accuracy_policy_3: 0.54025
	loss_value_3: 0.03475
	loss_reward_3: 0.00497
	loss_policy_4: 0.04823
	accuracy_policy_4: 0.52385
	loss_value_4: 0.0359
	loss_reward_4: 0.00547
	loss_policy_5: 0.05064
	accuracy_policy_5: 0.50285
	loss_value_5: 0.03676
	loss_reward_5: 0.00633
	loss_policy: 0.38082
	loss_value: 0.32616
	loss_reward: 0.02618
[2024-05-07 11:29:46] nn step 37000, lr: 0.04783.
	loss_policy_0: 0.12986
	accuracy_policy_0: 0.72916
	loss_value_0: 0.15548
	loss_policy_1: 0.03551
	accuracy_policy_1: 0.64287
	loss_value_1: 0.03291
	loss_reward_1: 0.00491
	loss_policy_2: 0.04066
	accuracy_policy_2: 0.59865
	loss_value_2: 0.03431
	loss_reward_2: 0.00471
	loss_policy_3: 0.04402
	accuracy_policy_3: 0.57291
	loss_value_3: 0.03574
	loss_reward_3: 0.0051
	loss_policy_4: 0.04696
	accuracy_policy_4: 0.54971
	loss_value_4: 0.0369
	loss_reward_4: 0.00561
	loss_policy_5: 0.0492
	accuracy_policy_5: 0.53414
	loss_value_5: 0.0379
	loss_reward_5: 0.00657
	loss_policy: 0.34621
	loss_value: 0.33325
	loss_reward: 0.02691
Optimization_Done 37000
[2024-05-07 11:31:57] [command] train weight_iter_37000.pkl 185 186
[2024-05-07 11:32:34] nn step 37100, lr: 0.04783.
	loss_policy_0: 0.1823
	accuracy_policy_0: 0.64537
	loss_value_0: 0.16555
	loss_policy_1: 0.04183
	accuracy_policy_1: 0.60004
	loss_value_1: 0.03458
	loss_reward_1: 0.00584
	loss_policy_2: 0.0459
	accuracy_policy_2: 0.56533
	loss_value_2: 0.03578
	loss_reward_2: 0.00582
	loss_policy_3: 0.04873
	accuracy_policy_3: 0.54613
	loss_value_3: 0.03675
	loss_reward_3: 0.00613
	loss_policy_4: 0.05144
	accuracy_policy_4: 0.5285
	loss_value_4: 0.03775
	loss_reward_4: 0.00667
	loss_policy_5: 0.05337
	accuracy_policy_5: 0.50934
	loss_value_5: 0.03862
	loss_reward_5: 0.00746
	loss_policy: 0.42357
	loss_value: 0.34904
	loss_reward: 0.03192
[2024-05-07 11:33:10] nn step 37200, lr: 0.04783.
	loss_policy_0: 0.14947
	accuracy_policy_0: 0.70916
	loss_value_0: 0.16677
	loss_policy_1: 0.03726
	accuracy_policy_1: 0.64301
	loss_value_1: 0.03499
	loss_reward_1: 0.00596
	loss_policy_2: 0.04201
	accuracy_policy_2: 0.60648
	loss_value_2: 0.03612
	loss_reward_2: 0.00597
	loss_policy_3: 0.04512
	accuracy_policy_3: 0.58719
	loss_value_3: 0.03735
	loss_reward_3: 0.00628
	loss_policy_4: 0.0483
	accuracy_policy_4: 0.56158
	loss_value_4: 0.03845
	loss_reward_4: 0.00672
	loss_policy_5: 0.05038
	accuracy_policy_5: 0.54389
	loss_value_5: 0.0395
	loss_reward_5: 0.00766
	loss_policy: 0.37254
	loss_value: 0.35318
	loss_reward: 0.03259
Optimization_Done 37200
[2024-05-07 11:35:23] [command] train weight_iter_37200.pkl 186 187
[2024-05-07 11:36:00] nn step 37300, lr: 0.04783.
	loss_policy_0: 0.18585
	accuracy_policy_0: 0.65779
	loss_value_0: 0.18438
	loss_policy_1: 0.04428
	accuracy_policy_1: 0.60496
	loss_value_1: 0.03834
	loss_reward_1: 0.00623
	loss_policy_2: 0.04812
	accuracy_policy_2: 0.57346
	loss_value_2: 0.03969
	loss_reward_2: 0.00608
	loss_policy_3: 0.05106
	accuracy_policy_3: 0.55473
	loss_value_3: 0.04104
	loss_reward_3: 0.00663
	loss_policy_4: 0.05379
	accuracy_policy_4: 0.53785
	loss_value_4: 0.04217
	loss_reward_4: 0.00726
	loss_policy_5: 0.05642
	accuracy_policy_5: 0.52482
	loss_value_5: 0.04328
	loss_reward_5: 0.0081
	loss_policy: 0.43953
	loss_value: 0.3889
	loss_reward: 0.0343
[2024-05-07 11:36:35] nn step 37400, lr: 0.04783.
	loss_policy_0: 0.14964
	accuracy_policy_0: 0.72105
	loss_value_0: 0.17912
	loss_policy_1: 0.03799
	accuracy_policy_1: 0.6524
	loss_value_1: 0.03722
	loss_reward_1: 0.00627
	loss_policy_2: 0.04281
	accuracy_policy_2: 0.6158
	loss_value_2: 0.03854
	loss_reward_2: 0.00603
	loss_policy_3: 0.04558
	accuracy_policy_3: 0.59771
	loss_value_3: 0.03985
	loss_reward_3: 0.0064
	loss_policy_4: 0.0487
	accuracy_policy_4: 0.57492
	loss_value_4: 0.04112
	loss_reward_4: 0.00701
	loss_policy_5: 0.05126
	accuracy_policy_5: 0.55883
	loss_value_5: 0.04241
	loss_reward_5: 0.00792
	loss_policy: 0.37598
	loss_value: 0.37825
	loss_reward: 0.03363
Optimization_Done 37400
[2024-05-07 11:38:47] [command] train weight_iter_37400.pkl 187 188
[2024-05-07 11:39:24] nn step 37500, lr: 0.04783.
	loss_policy_0: 0.16483
	accuracy_policy_0: 0.68074
	loss_value_0: 0.17156
	loss_policy_1: 0.04128
	accuracy_policy_1: 0.61344
	loss_value_1: 0.03576
	loss_reward_1: 0.00616
	loss_policy_2: 0.04592
	accuracy_policy_2: 0.57307
	loss_value_2: 0.03712
	loss_reward_2: 0.00571
	loss_policy_3: 0.04893
	accuracy_policy_3: 0.55402
	loss_value_3: 0.03853
	loss_reward_3: 0.0061
	loss_policy_4: 0.05196
	accuracy_policy_4: 0.53596
	loss_value_4: 0.03986
	loss_reward_4: 0.00723
	loss_policy_5: 0.05478
	accuracy_policy_5: 0.51803
	loss_value_5: 0.04104
	loss_reward_5: 0.00804
	loss_policy: 0.40771
	loss_value: 0.36386
	loss_reward: 0.03324
[2024-05-07 11:40:00] nn step 37600, lr: 0.04783.
	loss_policy_0: 0.13299
	accuracy_policy_0: 0.73521
	loss_value_0: 0.1707
	loss_policy_1: 0.03675
	accuracy_policy_1: 0.64994
	loss_value_1: 0.03579
	loss_reward_1: 0.00598
	loss_policy_2: 0.04157
	accuracy_policy_2: 0.60758
	loss_value_2: 0.03713
	loss_reward_2: 0.00552
	loss_policy_3: 0.04489
	accuracy_policy_3: 0.58752
	loss_value_3: 0.03846
	loss_reward_3: 0.00591
	loss_policy_4: 0.04809
	accuracy_policy_4: 0.56576
	loss_value_4: 0.03989
	loss_reward_4: 0.00669
	loss_policy_5: 0.05083
	accuracy_policy_5: 0.54473
	loss_value_5: 0.04109
	loss_reward_5: 0.00799
	loss_policy: 0.35512
	loss_value: 0.36306
	loss_reward: 0.03208
Optimization_Done 37600
[2024-05-07 11:41:51] [command] train weight_iter_37600.pkl 188 189
[2024-05-07 11:42:28] nn step 37700, lr: 0.04783.
	loss_policy_0: 0.14624
	accuracy_policy_0: 0.69791
	loss_value_0: 0.17713
	loss_policy_1: 0.0385
	accuracy_policy_1: 0.62197
	loss_value_1: 0.03705
	loss_reward_1: 0.00625
	loss_policy_2: 0.0428
	accuracy_policy_2: 0.58391
	loss_value_2: 0.0383
	loss_reward_2: 0.00612
	loss_policy_3: 0.04644
	accuracy_policy_3: 0.54906
	loss_value_3: 0.0394
	loss_reward_3: 0.00654
	loss_policy_4: 0.0494
	accuracy_policy_4: 0.52693
	loss_value_4: 0.04035
	loss_reward_4: 0.00706
	loss_policy_5: 0.05206
	accuracy_policy_5: 0.50158
	loss_value_5: 0.04112
	loss_reward_5: 0.00824
	loss_policy: 0.37544
	loss_value: 0.37336
	loss_reward: 0.0342
[2024-05-07 11:43:03] nn step 37800, lr: 0.04783.
	loss_policy_0: 0.11812
	accuracy_policy_0: 0.75055
	loss_value_0: 0.16992
	loss_policy_1: 0.03415
	accuracy_policy_1: 0.65869
	loss_value_1: 0.03559
	loss_reward_1: 0.00615
	loss_policy_2: 0.0388
	accuracy_policy_2: 0.62023
	loss_value_2: 0.03698
	loss_reward_2: 0.00586
	loss_policy_3: 0.04251
	accuracy_policy_3: 0.58707
	loss_value_3: 0.03825
	loss_reward_3: 0.00628
	loss_policy_4: 0.04526
	accuracy_policy_4: 0.56312
	loss_value_4: 0.03948
	loss_reward_4: 0.00687
	loss_policy_5: 0.04834
	accuracy_policy_5: 0.53662
	loss_value_5: 0.04066
	loss_reward_5: 0.00813
	loss_policy: 0.32718
	loss_value: 0.36088
	loss_reward: 0.03328
Optimization_Done 37800
[2024-05-07 11:45:17] [command] train weight_iter_37800.pkl 189 190
[2024-05-07 11:45:54] nn step 37900, lr: 0.04783.
	loss_policy_0: 0.17081
	accuracy_policy_0: 0.6673
	loss_value_0: 0.16779
	loss_policy_1: 0.04
	accuracy_policy_1: 0.61662
	loss_value_1: 0.03514
	loss_reward_1: 0.00515
	loss_policy_2: 0.04389
	accuracy_policy_2: 0.58217
	loss_value_2: 0.03663
	loss_reward_2: 0.00502
	loss_policy_3: 0.04712
	accuracy_policy_3: 0.55762
	loss_value_3: 0.03805
	loss_reward_3: 0.00545
	loss_policy_4: 0.04944
	accuracy_policy_4: 0.53908
	loss_value_4: 0.03922
	loss_reward_4: 0.00574
	loss_policy_5: 0.0519
	accuracy_policy_5: 0.52166
	loss_value_5: 0.04039
	loss_reward_5: 0.00663
	loss_policy: 0.40317
	loss_value: 0.35722
	loss_reward: 0.02799
[2024-05-07 11:46:29] nn step 38000, lr: 0.04783.
	loss_policy_0: 0.13146
	accuracy_policy_0: 0.72871
	loss_value_0: 0.16187
	loss_policy_1: 0.03426
	accuracy_policy_1: 0.65736
	loss_value_1: 0.03404
	loss_reward_1: 0.00502
	loss_policy_2: 0.03843
	accuracy_policy_2: 0.62297
	loss_value_2: 0.03546
	loss_reward_2: 0.00495
	loss_policy_3: 0.04135
	accuracy_policy_3: 0.59902
	loss_value_3: 0.03663
	loss_reward_3: 0.00542
	loss_policy_4: 0.04405
	accuracy_policy_4: 0.5782
	loss_value_4: 0.03788
	loss_reward_4: 0.00577
	loss_policy_5: 0.04691
	accuracy_policy_5: 0.55682
	loss_value_5: 0.03917
	loss_reward_5: 0.00671
	loss_policy: 0.33644
	loss_value: 0.34505
	loss_reward: 0.02788
Optimization_Done 38000
[2024-05-07 11:48:22] [command] train weight_iter_38000.pkl 190 191
[2024-05-07 11:48:59] nn step 38100, lr: 0.04783.
	loss_policy_0: 0.18674
	accuracy_policy_0: 0.63768
	loss_value_0: 0.16405
	loss_policy_1: 0.04274
	accuracy_policy_1: 0.59277
	loss_value_1: 0.03445
	loss_reward_1: 0.00454
	loss_policy_2: 0.04625
	accuracy_policy_2: 0.56506
	loss_value_2: 0.03598
	loss_reward_2: 0.00418
	loss_policy_3: 0.04986
	accuracy_policy_3: 0.53771
	loss_value_3: 0.03738
	loss_reward_3: 0.00445
	loss_policy_4: 0.05298
	accuracy_policy_4: 0.51766
	loss_value_4: 0.03863
	loss_reward_4: 0.00488
	loss_policy_5: 0.05544
	accuracy_policy_5: 0.49689
	loss_value_5: 0.04003
	loss_reward_5: 0.00575
	loss_policy: 0.43401
	loss_value: 0.35052
	loss_reward: 0.0238
[2024-05-07 11:49:35] nn step 38200, lr: 0.04783.
	loss_policy_0: 0.15077
	accuracy_policy_0: 0.69586
	loss_value_0: 0.15804
	loss_policy_1: 0.03658
	accuracy_policy_1: 0.64328
	loss_value_1: 0.03307
	loss_reward_1: 0.0043
	loss_policy_2: 0.0408
	accuracy_policy_2: 0.6068
	loss_value_2: 0.03433
	loss_reward_2: 0.00414
	loss_policy_3: 0.04459
	accuracy_policy_3: 0.58033
	loss_value_3: 0.03557
	loss_reward_3: 0.00452
	loss_policy_4: 0.04769
	accuracy_policy_4: 0.55572
	loss_value_4: 0.03672
	loss_reward_4: 0.00478
	loss_policy_5: 0.05037
	accuracy_policy_5: 0.53475
	loss_value_5: 0.03784
	loss_reward_5: 0.00568
	loss_policy: 0.37079
	loss_value: 0.33557
	loss_reward: 0.02343
Optimization_Done 38200
[2024-05-07 11:51:37] [command] train weight_iter_38200.pkl 191 192
[2024-05-07 11:52:13] nn step 38300, lr: 0.04783.
	loss_policy_0: 0.18238
	accuracy_policy_0: 0.63379
	loss_value_0: 0.16991
	loss_policy_1: 0.04134
	accuracy_policy_1: 0.59172
	loss_value_1: 0.03501
	loss_reward_1: 0.00601
	loss_policy_2: 0.04487
	accuracy_policy_2: 0.56047
	loss_value_2: 0.03601
	loss_reward_2: 0.00586
	loss_policy_3: 0.0474
	accuracy_policy_3: 0.53789
	loss_value_3: 0.03682
	loss_reward_3: 0.00625
	loss_policy_4: 0.04972
	accuracy_policy_4: 0.5199
	loss_value_4: 0.0377
	loss_reward_4: 0.0068
	loss_policy_5: 0.05208
	accuracy_policy_5: 0.50199
	loss_value_5: 0.03855
	loss_reward_5: 0.00762
	loss_policy: 0.4178
	loss_value: 0.354
	loss_reward: 0.03253
[2024-05-07 11:52:48] nn step 38400, lr: 0.04783.
	loss_policy_0: 0.14276
	accuracy_policy_0: 0.70402
	loss_value_0: 0.17384
	loss_policy_1: 0.03564
	accuracy_policy_1: 0.64043
	loss_value_1: 0.03596
	loss_reward_1: 0.00592
	loss_policy_2: 0.03979
	accuracy_policy_2: 0.59855
	loss_value_2: 0.03679
	loss_reward_2: 0.00586
	loss_policy_3: 0.0428
	accuracy_policy_3: 0.57465
	loss_value_3: 0.03768
	loss_reward_3: 0.00611
	loss_policy_4: 0.04551
	accuracy_policy_4: 0.5577
	loss_value_4: 0.03852
	loss_reward_4: 0.00645
	loss_policy_5: 0.04836
	accuracy_policy_5: 0.54021
	loss_value_5: 0.03937
	loss_reward_5: 0.00737
	loss_policy: 0.35485
	loss_value: 0.36217
	loss_reward: 0.03172
Optimization_Done 38400
[2024-05-07 11:54:50] [command] train weight_iter_38400.pkl 192 193
[2024-05-07 11:55:27] nn step 38500, lr: 0.04783.
	loss_policy_0: 0.1805
	accuracy_policy_0: 0.63418
	loss_value_0: 0.20085
	loss_policy_1: 0.04182
	accuracy_policy_1: 0.5882
	loss_value_1: 0.04086
	loss_reward_1: 0.00622
	loss_policy_2: 0.04507
	accuracy_policy_2: 0.5515
	loss_value_2: 0.04153
	loss_reward_2: 0.00625
	loss_policy_3: 0.04778
	accuracy_policy_3: 0.52688
	loss_value_3: 0.04235
	loss_reward_3: 0.00661
	loss_policy_4: 0.04975
	accuracy_policy_4: 0.51092
	loss_value_4: 0.04301
	loss_reward_4: 0.00689
	loss_policy_5: 0.05155
	accuracy_policy_5: 0.49559
	loss_value_5: 0.04382
	loss_reward_5: 0.00762
	loss_policy: 0.41648
	loss_value: 0.41243
	loss_reward: 0.03358
[2024-05-07 11:56:04] nn step 38600, lr: 0.04783.
	loss_policy_0: 0.13957
	accuracy_policy_0: 0.69938
	loss_value_0: 0.19442
	loss_policy_1: 0.03493
	accuracy_policy_1: 0.63469
	loss_value_1: 0.03981
	loss_reward_1: 0.00583
	loss_policy_2: 0.03835
	accuracy_policy_2: 0.59961
	loss_value_2: 0.04059
	loss_reward_2: 0.00594
	loss_policy_3: 0.04107
	accuracy_policy_3: 0.57518
	loss_value_3: 0.04137
	loss_reward_3: 0.00643
	loss_policy_4: 0.04335
	accuracy_policy_4: 0.55297
	loss_value_4: 0.04215
	loss_reward_4: 0.00674
	loss_policy_5: 0.04564
	accuracy_policy_5: 0.54094
	loss_value_5: 0.04317
	loss_reward_5: 0.00743
	loss_policy: 0.34291
	loss_value: 0.4015
	loss_reward: 0.03236
Optimization_Done 38600
[2024-05-07 11:58:20] [command] train weight_iter_38600.pkl 193 194
[2024-05-07 11:58:56] nn step 38700, lr: 0.04783.
	loss_policy_0: 0.14944
	accuracy_policy_0: 0.68926
	loss_value_0: 0.18391
	loss_policy_1: 0.03695
	accuracy_policy_1: 0.62881
	loss_value_1: 0.03787
	loss_reward_1: 0.00535
	loss_policy_2: 0.04115
	accuracy_policy_2: 0.59033
	loss_value_2: 0.03921
	loss_reward_2: 0.00518
	loss_policy_3: 0.04497
	accuracy_policy_3: 0.55822
	loss_value_3: 0.04021
	loss_reward_3: 0.00562
	loss_policy_4: 0.04789
	accuracy_policy_4: 0.53297
	loss_value_4: 0.04122
	loss_reward_4: 0.00613
	loss_policy_5: 0.05025
	accuracy_policy_5: 0.51305
	loss_value_5: 0.04226
	loss_reward_5: 0.00694
	loss_policy: 0.37064
	loss_value: 0.38467
	loss_reward: 0.02922
[2024-05-07 11:59:31] nn step 38800, lr: 0.04783.
	loss_policy_0: 0.11856
	accuracy_policy_0: 0.73975
	loss_value_0: 0.17777
	loss_policy_1: 0.03242
	accuracy_policy_1: 0.66574
	loss_value_1: 0.03678
	loss_reward_1: 0.0052
	loss_policy_2: 0.0373
	accuracy_policy_2: 0.62391
	loss_value_2: 0.038
	loss_reward_2: 0.00505
	loss_policy_3: 0.04102
	accuracy_policy_3: 0.59078
	loss_value_3: 0.03909
	loss_reward_3: 0.00558
	loss_policy_4: 0.04406
	accuracy_policy_4: 0.56924
	loss_value_4: 0.04024
	loss_reward_4: 0.00618
	loss_policy_5: 0.0469
	accuracy_policy_5: 0.54398
	loss_value_5: 0.04139
	loss_reward_5: 0.00693
	loss_policy: 0.32026
	loss_value: 0.37327
	loss_reward: 0.02894
Optimization_Done 38800
[2024-05-07 12:01:45] [command] train weight_iter_38800.pkl 194 195
[2024-05-07 12:02:21] nn step 38900, lr: 0.04783.
	loss_policy_0: 0.15918
	accuracy_policy_0: 0.68521
	loss_value_0: 0.16507
	loss_policy_1: 0.03929
	accuracy_policy_1: 0.62266
	loss_value_1: 0.03459
	loss_reward_1: 0.00472
	loss_policy_2: 0.04435
	accuracy_policy_2: 0.58697
	loss_value_2: 0.03596
	loss_reward_2: 0.00461
	loss_policy_3: 0.04821
	accuracy_policy_3: 0.55436
	loss_value_3: 0.03732
	loss_reward_3: 0.00517
	loss_policy_4: 0.05113
	accuracy_policy_4: 0.53557
	loss_value_4: 0.0387
	loss_reward_4: 0.00552
	loss_policy_5: 0.05446
	accuracy_policy_5: 0.51092
	loss_value_5: 0.03982
	loss_reward_5: 0.0063
	loss_policy: 0.39662
	loss_value: 0.35146
	loss_reward: 0.02631
[2024-05-07 12:02:57] nn step 39000, lr: 0.04783.
	loss_policy_0: 0.12109
	accuracy_policy_0: 0.74156
	loss_value_0: 0.15711
	loss_policy_1: 0.03329
	accuracy_policy_1: 0.66645
	loss_value_1: 0.03295
	loss_reward_1: 0.00486
	loss_policy_2: 0.03835
	accuracy_policy_2: 0.62693
	loss_value_2: 0.03424
	loss_reward_2: 0.00458
	loss_policy_3: 0.04267
	accuracy_policy_3: 0.59605
	loss_value_3: 0.03556
	loss_reward_3: 0.0049
	loss_policy_4: 0.04618
	accuracy_policy_4: 0.56914
	loss_value_4: 0.03696
	loss_reward_4: 0.00531
	loss_policy_5: 0.04928
	accuracy_policy_5: 0.54469
	loss_value_5: 0.0383
	loss_reward_5: 0.00641
	loss_policy: 0.33086
	loss_value: 0.33511
	loss_reward: 0.02606
Optimization_Done 39000
[2024-05-07 12:05:10] [command] train weight_iter_39000.pkl 195 196
[2024-05-07 12:05:47] nn step 39100, lr: 0.04783.
	loss_policy_0: 0.19879
	accuracy_policy_0: 0.63361
	loss_value_0: 0.16719
	loss_policy_1: 0.04668
	accuracy_policy_1: 0.58082
	loss_value_1: 0.03483
	loss_reward_1: 0.00541
	loss_policy_2: 0.05076
	accuracy_policy_2: 0.55494
	loss_value_2: 0.03603
	loss_reward_2: 0.00498
	loss_policy_3: 0.05456
	accuracy_policy_3: 0.52197
	loss_value_3: 0.03739
	loss_reward_3: 0.00524
	loss_policy_4: 0.05794
	accuracy_policy_4: 0.49848
	loss_value_4: 0.03857
	loss_reward_4: 0.00573
	loss_policy_5: 0.06097
	accuracy_policy_5: 0.47598
	loss_value_5: 0.03978
	loss_reward_5: 0.00649
	loss_policy: 0.46969
	loss_value: 0.3538
	loss_reward: 0.02785
[2024-05-07 12:06:22] nn step 39200, lr: 0.04783.
	loss_policy_0: 0.14982
	accuracy_policy_0: 0.71045
	loss_value_0: 0.15627
	loss_policy_1: 0.0386
	accuracy_policy_1: 0.6376
	loss_value_1: 0.0327
	loss_reward_1: 0.00502
	loss_policy_2: 0.04376
	accuracy_policy_2: 0.59965
	loss_value_2: 0.03414
	loss_reward_2: 0.00464
	loss_policy_3: 0.04833
	accuracy_policy_3: 0.56686
	loss_value_3: 0.03522
	loss_reward_3: 0.00493
	loss_policy_4: 0.05175
	accuracy_policy_4: 0.54045
	loss_value_4: 0.0365
	loss_reward_4: 0.00528
	loss_policy_5: 0.05515
	accuracy_policy_5: 0.51654
	loss_value_5: 0.03763
	loss_reward_5: 0.00639
	loss_policy: 0.3874
	loss_value: 0.33246
	loss_reward: 0.02627
Optimization_Done 39200
[2024-05-07 12:08:35] [command] train weight_iter_39200.pkl 196 197
[2024-05-07 12:09:12] nn step 39300, lr: 0.04783.
	loss_policy_0: 0.16578
	accuracy_policy_0: 0.66668
	loss_value_0: 0.15112
	loss_policy_1: 0.04178
	accuracy_policy_1: 0.59477
	loss_value_1: 0.03152
	loss_reward_1: 0.00506
	loss_policy_2: 0.04668
	accuracy_policy_2: 0.55531
	loss_value_2: 0.03295
	loss_reward_2: 0.00472
	loss_policy_3: 0.05034
	accuracy_policy_3: 0.52643
	loss_value_3: 0.03407
	loss_reward_3: 0.00507
	loss_policy_4: 0.05324
	accuracy_policy_4: 0.50701
	loss_value_4: 0.03527
	loss_reward_4: 0.00557
	loss_policy_5: 0.05595
	accuracy_policy_5: 0.48285
	loss_value_5: 0.03638
	loss_reward_5: 0.00638
	loss_policy: 0.41378
	loss_value: 0.32131
	loss_reward: 0.0268
[2024-05-07 12:09:47] nn step 39400, lr: 0.04783.
	loss_policy_0: 0.14872
	accuracy_policy_0: 0.7183
	loss_value_0: 0.15238
	loss_policy_1: 0.03996
	accuracy_policy_1: 0.62971
	loss_value_1: 0.0319
	loss_reward_1: 0.00532
	loss_policy_2: 0.04509
	accuracy_policy_2: 0.59145
	loss_value_2: 0.03331
	loss_reward_2: 0.0048
	loss_policy_3: 0.04903
	accuracy_policy_3: 0.56273
	loss_value_3: 0.03459
	loss_reward_3: 0.00534
	loss_policy_4: 0.05255
	accuracy_policy_4: 0.53932
	loss_value_4: 0.03582
	loss_reward_4: 0.00572
	loss_policy_5: 0.05608
	accuracy_policy_5: 0.51695
	loss_value_5: 0.03688
	loss_reward_5: 0.00687
	loss_policy: 0.39144
	loss_value: 0.32488
	loss_reward: 0.02804
Optimization_Done 39400
[2024-05-07 12:11:58] [command] train weight_iter_39400.pkl 197 198
[2024-05-07 12:12:35] nn step 39500, lr: 0.04783.
	loss_policy_0: 0.13906
	accuracy_policy_0: 0.69926
	loss_value_0: 0.15507
	loss_policy_1: 0.03633
	accuracy_policy_1: 0.62129
	loss_value_1: 0.03244
	loss_reward_1: 0.00528
	loss_policy_2: 0.04093
	accuracy_policy_2: 0.57791
	loss_value_2: 0.03401
	loss_reward_2: 0.00514
	loss_policy_3: 0.04399
	accuracy_policy_3: 0.55689
	loss_value_3: 0.03534
	loss_reward_3: 0.00573
	loss_policy_4: 0.04709
	accuracy_policy_4: 0.5352
	loss_value_4: 0.03648
	loss_reward_4: 0.00615
	loss_policy_5: 0.04977
	accuracy_policy_5: 0.51484
	loss_value_5: 0.03761
	loss_reward_5: 0.00713
	loss_policy: 0.35718
	loss_value: 0.33096
	loss_reward: 0.02943
[2024-05-07 12:13:11] nn step 39600, lr: 0.04783.
	loss_policy_0: 0.1154
	accuracy_policy_0: 0.74998
	loss_value_0: 0.15225
	loss_policy_1: 0.03256
	accuracy_policy_1: 0.65828
	loss_value_1: 0.03223
	loss_reward_1: 0.00526
	loss_policy_2: 0.03764
	accuracy_policy_2: 0.61758
	loss_value_2: 0.03364
	loss_reward_2: 0.00504
	loss_policy_3: 0.04117
	accuracy_policy_3: 0.59156
	loss_value_3: 0.03495
	loss_reward_3: 0.00573
	loss_policy_4: 0.04399
	accuracy_policy_4: 0.56947
	loss_value_4: 0.03615
	loss_reward_4: 0.00617
	loss_policy_5: 0.04683
	accuracy_policy_5: 0.54596
	loss_value_5: 0.03715
	loss_reward_5: 0.00736
	loss_policy: 0.31759
	loss_value: 0.32637
	loss_reward: 0.02955
Optimization_Done 39600
[2024-05-07 12:15:20] [command] train weight_iter_39600.pkl 198 199
[2024-05-07 12:15:56] nn step 39700, lr: 0.04783.
	loss_policy_0: 0.14636
	accuracy_policy_0: 0.69242
	loss_value_0: 0.16643
	loss_policy_1: 0.03511
	accuracy_policy_1: 0.63719
	loss_value_1: 0.03514
	loss_reward_1: 0.00683
	loss_policy_2: 0.03953
	accuracy_policy_2: 0.59637
	loss_value_2: 0.03671
	loss_reward_2: 0.00668
	loss_policy_3: 0.04253
	accuracy_policy_3: 0.57266
	loss_value_3: 0.03819
	loss_reward_3: 0.00747
	loss_policy_4: 0.04499
	accuracy_policy_4: 0.54951
	loss_value_4: 0.03961
	loss_reward_4: 0.00796
	loss_policy_5: 0.04774
	accuracy_policy_5: 0.52943
	loss_value_5: 0.04066
	loss_reward_5: 0.00888
	loss_policy: 0.35626
	loss_value: 0.35674
	loss_reward: 0.03782
[2024-05-07 12:16:32] nn step 39800, lr: 0.04783.
	loss_policy_0: 0.11736
	accuracy_policy_0: 0.7443
	loss_value_0: 0.16795
	loss_policy_1: 0.03082
	accuracy_policy_1: 0.67363
	loss_value_1: 0.03549
	loss_reward_1: 0.0067
	loss_policy_2: 0.03521
	accuracy_policy_2: 0.63221
	loss_value_2: 0.03712
	loss_reward_2: 0.00628
	loss_policy_3: 0.03884
	accuracy_policy_3: 0.60135
	loss_value_3: 0.03854
	loss_reward_3: 0.00698
	loss_policy_4: 0.04155
	accuracy_policy_4: 0.58123
	loss_value_4: 0.04005
	loss_reward_4: 0.00749
	loss_policy_5: 0.04428
	accuracy_policy_5: 0.56199
	loss_value_5: 0.04139
	loss_reward_5: 0.00865
	loss_policy: 0.30805
	loss_value: 0.36054
	loss_reward: 0.0361
Optimization_Done 39800
[2024-05-07 12:18:41] [command] train weight_iter_39800.pkl 199 200
[2024-05-07 12:19:18] nn step 39900, lr: 0.04783.
	loss_policy_0: 0.15649
	accuracy_policy_0: 0.69543
	loss_value_0: 0.18966
	loss_policy_1: 0.03806
	accuracy_policy_1: 0.64146
	loss_value_1: 0.0398
	loss_reward_1: 0.00756
	loss_policy_2: 0.04248
	accuracy_policy_2: 0.6093
	loss_value_2: 0.04155
	loss_reward_2: 0.00755
	loss_policy_3: 0.0459
	accuracy_policy_3: 0.57928
	loss_value_3: 0.04301
	loss_reward_3: 0.00828
	loss_policy_4: 0.04877
	accuracy_policy_4: 0.56
	loss_value_4: 0.04438
	loss_reward_4: 0.00879
	loss_policy_5: 0.05124
	accuracy_policy_5: 0.54691
	loss_value_5: 0.04592
	loss_reward_5: 0.00991
	loss_policy: 0.38293
	loss_value: 0.40432
	loss_reward: 0.04209
[2024-05-07 12:19:54] nn step 40000, lr: 0.04783.
	loss_policy_0: 0.12219
	accuracy_policy_0: 0.74785
	loss_value_0: 0.18192
	loss_policy_1: 0.03201
	accuracy_policy_1: 0.68236
	loss_value_1: 0.03812
	loss_reward_1: 0.00707
	loss_policy_2: 0.03681
	accuracy_policy_2: 0.64051
	loss_value_2: 0.03971
	loss_reward_2: 0.00699
	loss_policy_3: 0.04054
	accuracy_policy_3: 0.61279
	loss_value_3: 0.04142
	loss_reward_3: 0.00762
	loss_policy_4: 0.04348
	accuracy_policy_4: 0.58965
	loss_value_4: 0.04268
	loss_reward_4: 0.00841
	loss_policy_5: 0.04612
	accuracy_policy_5: 0.57174
	loss_value_5: 0.04406
	loss_reward_5: 0.00963
	loss_policy: 0.32116
	loss_value: 0.38792
	loss_reward: 0.03972
Optimization_Done 40000
[2024-05-07 12:21:53] [command] train weight_iter_40000.pkl 200 201
[2024-05-07 12:22:31] nn step 40100, lr: 0.04783.
	loss_policy_0: 0.12325
	accuracy_policy_0: 0.7542
	loss_value_0: 0.18989
	loss_policy_1: 0.03201
	accuracy_policy_1: 0.69477
	loss_value_1: 0.03945
	loss_reward_1: 0.0072
	loss_policy_2: 0.03576
	accuracy_policy_2: 0.66451
	loss_value_2: 0.04087
	loss_reward_2: 0.0068
	loss_policy_3: 0.03821
	accuracy_policy_3: 0.64889
	loss_value_3: 0.04194
	loss_reward_3: 0.00755
	loss_policy_4: 0.04075
	accuracy_policy_4: 0.63469
	loss_value_4: 0.04314
	loss_reward_4: 0.00821
	loss_policy_5: 0.04303
	accuracy_policy_5: 0.61857
	loss_value_5: 0.04411
	loss_reward_5: 0.00926
	loss_policy: 0.31301
	loss_value: 0.3994
	loss_reward: 0.03901
[2024-05-07 12:23:06] nn step 40200, lr: 0.04783.
	loss_policy_0: 0.10624
	accuracy_policy_0: 0.79248
	loss_value_0: 0.19323
	loss_policy_1: 0.03026
	accuracy_policy_1: 0.71789
	loss_value_1: 0.0404
	loss_reward_1: 0.00737
	loss_policy_2: 0.03479
	accuracy_policy_2: 0.68383
	loss_value_2: 0.04183
	loss_reward_2: 0.00701
	loss_policy_3: 0.03748
	accuracy_policy_3: 0.66463
	loss_value_3: 0.04313
	loss_reward_3: 0.00761
	loss_policy_4: 0.03999
	accuracy_policy_4: 0.64936
	loss_value_4: 0.04426
	loss_reward_4: 0.00832
	loss_policy_5: 0.04235
	accuracy_policy_5: 0.64002
	loss_value_5: 0.04543
	loss_reward_5: 0.00981
	loss_policy: 0.29111
	loss_value: 0.40828
	loss_reward: 0.04012
Optimization_Done 40200
[2024-05-07 12:24:59] [command] train weight_iter_40200.pkl 201 202
[2024-05-07 12:25:36] nn step 40300, lr: 0.04783.
	loss_policy_0: 0.14685
	accuracy_policy_0: 0.71531
	loss_value_0: 0.19092
	loss_policy_1: 0.03663
	accuracy_policy_1: 0.65627
	loss_value_1: 0.03974
	loss_reward_1: 0.00634
	loss_policy_2: 0.04028
	accuracy_policy_2: 0.62439
	loss_value_2: 0.04081
	loss_reward_2: 0.00621
	loss_policy_3: 0.04307
	accuracy_policy_3: 0.59873
	loss_value_3: 0.04176
	loss_reward_3: 0.00662
	loss_policy_4: 0.0455
	accuracy_policy_4: 0.58299
	loss_value_4: 0.04257
	loss_reward_4: 0.00742
	loss_policy_5: 0.04764
	accuracy_policy_5: 0.56437
	loss_value_5: 0.04342
	loss_reward_5: 0.00849
	loss_policy: 0.35998
	loss_value: 0.39923
	loss_reward: 0.03508
[2024-05-07 12:26:12] nn step 40400, lr: 0.04783.
	loss_policy_0: 0.1077
	accuracy_policy_0: 0.76984
	loss_value_0: 0.16509
	loss_policy_1: 0.02952
	accuracy_policy_1: 0.69713
	loss_value_1: 0.03437
	loss_reward_1: 0.0059
	loss_policy_2: 0.03357
	accuracy_policy_2: 0.65951
	loss_value_2: 0.03567
	loss_reward_2: 0.0057
	loss_policy_3: 0.03595
	accuracy_policy_3: 0.63734
	loss_value_3: 0.03676
	loss_reward_3: 0.0063
	loss_policy_4: 0.03786
	accuracy_policy_4: 0.6216
	loss_value_4: 0.03767
	loss_reward_4: 0.00674
	loss_policy_5: 0.04008
	accuracy_policy_5: 0.6048
	loss_value_5: 0.03861
	loss_reward_5: 0.00782
	loss_policy: 0.28467
	loss_value: 0.34817
	loss_reward: 0.03246
Optimization_Done 40400
[2024-05-07 12:28:23] [command] train weight_iter_40400.pkl 202 203
[2024-05-07 12:29:00] nn step 40500, lr: 0.04783.
	loss_policy_0: 0.15875
	accuracy_policy_0: 0.69061
	loss_value_0: 0.17116
	loss_policy_1: 0.03813
	accuracy_policy_1: 0.6357
	loss_value_1: 0.03604
	loss_reward_1: 0.00549
	loss_policy_2: 0.04174
	accuracy_policy_2: 0.60979
	loss_value_2: 0.03765
	loss_reward_2: 0.00553
	loss_policy_3: 0.04452
	accuracy_policy_3: 0.58379
	loss_value_3: 0.03907
	loss_reward_3: 0.00617
	loss_policy_4: 0.04643
	accuracy_policy_4: 0.57076
	loss_value_4: 0.0403
	loss_reward_4: 0.00664
	loss_policy_5: 0.0488
	accuracy_policy_5: 0.56053
	loss_value_5: 0.04127
	loss_reward_5: 0.00773
	loss_policy: 0.37837
	loss_value: 0.3655
	loss_reward: 0.03157
[2024-05-07 12:29:36] nn step 40600, lr: 0.04783.
	loss_policy_0: 0.13163
	accuracy_policy_0: 0.73627
	loss_value_0: 0.16496
	loss_policy_1: 0.03355
	accuracy_policy_1: 0.67363
	loss_value_1: 0.03467
	loss_reward_1: 0.00549
	loss_policy_2: 0.03764
	accuracy_policy_2: 0.63992
	loss_value_2: 0.03609
	loss_reward_2: 0.00553
	loss_policy_3: 0.04051
	accuracy_policy_3: 0.61906
	loss_value_3: 0.03762
	loss_reward_3: 0.00616
	loss_policy_4: 0.0434
	accuracy_policy_4: 0.60242
	loss_value_4: 0.03876
	loss_reward_4: 0.00666
	loss_policy_5: 0.04589
	accuracy_policy_5: 0.58422
	loss_value_5: 0.03998
	loss_reward_5: 0.00785
	loss_policy: 0.33261
	loss_value: 0.35208
	loss_reward: 0.0317
Optimization_Done 40600
[2024-05-07 12:31:47] [command] train weight_iter_40600.pkl 203 204
[2024-05-07 12:32:24] nn step 40700, lr: 0.04783.
	loss_policy_0: 0.14063
	accuracy_policy_0: 0.71295
	loss_value_0: 0.15356
	loss_policy_1: 0.0326
	accuracy_policy_1: 0.67344
	loss_value_1: 0.03243
	loss_reward_1: 0.00514
	loss_policy_2: 0.03599
	accuracy_policy_2: 0.64471
	loss_value_2: 0.0338
	loss_reward_2: 0.00504
	loss_policy_3: 0.03896
	accuracy_policy_3: 0.61994
	loss_value_3: 0.03504
	loss_reward_3: 0.00543
	loss_policy_4: 0.04122
	accuracy_policy_4: 0.60658
	loss_value_4: 0.03621
	loss_reward_4: 0.00591
	loss_policy_5: 0.04302
	accuracy_policy_5: 0.59619
	loss_value_5: 0.03746
	loss_reward_5: 0.00678
	loss_policy: 0.3324
	loss_value: 0.32851
	loss_reward: 0.0283
[2024-05-07 12:33:00] nn step 40800, lr: 0.04783.
	loss_policy_0: 0.12566
	accuracy_policy_0: 0.75344
	loss_value_0: 0.16208
	loss_policy_1: 0.03138
	accuracy_policy_1: 0.70287
	loss_value_1: 0.03403
	loss_reward_1: 0.00548
	loss_policy_2: 0.03527
	accuracy_policy_2: 0.67139
	loss_value_2: 0.0355
	loss_reward_2: 0.00542
	loss_policy_3: 0.03875
	accuracy_policy_3: 0.64584
	loss_value_3: 0.03695
	loss_reward_3: 0.00578
	loss_policy_4: 0.04125
	accuracy_policy_4: 0.63305
	loss_value_4: 0.03828
	loss_reward_4: 0.00645
	loss_policy_5: 0.04349
	accuracy_policy_5: 0.61861
	loss_value_5: 0.03976
	loss_reward_5: 0.00724
	loss_policy: 0.31579
	loss_value: 0.3466
	loss_reward: 0.03036
Optimization_Done 40800
[2024-05-07 12:34:46] [command] train weight_iter_40800.pkl 204 205
[2024-05-07 12:35:23] nn step 40900, lr: 0.04783.
	loss_policy_0: 0.17348
	accuracy_policy_0: 0.67725
	loss_value_0: 0.15266
	loss_policy_1: 0.04033
	accuracy_policy_1: 0.62807
	loss_value_1: 0.03214
	loss_reward_1: 0.00552
	loss_policy_2: 0.04416
	accuracy_policy_2: 0.59729
	loss_value_2: 0.0336
	loss_reward_2: 0.00525
	loss_policy_3: 0.04702
	accuracy_policy_3: 0.57871
	loss_value_3: 0.03504
	loss_reward_3: 0.00565
	loss_policy_4: 0.04987
	accuracy_policy_4: 0.56232
	loss_value_4: 0.0363
	loss_reward_4: 0.00608
	loss_policy_5: 0.0521
	accuracy_policy_5: 0.54256
	loss_value_5: 0.03745
	loss_reward_5: 0.00689
	loss_policy: 0.40697
	loss_value: 0.32718
	loss_reward: 0.02938
[2024-05-07 12:35:58] nn step 41000, lr: 0.04783.
	loss_policy_0: 0.13597
	accuracy_policy_0: 0.73496
	loss_value_0: 0.14911
	loss_policy_1: 0.03422
	accuracy_policy_1: 0.67174
	loss_value_1: 0.03117
	loss_reward_1: 0.00511
	loss_policy_2: 0.0382
	accuracy_policy_2: 0.63979
	loss_value_2: 0.03252
	loss_reward_2: 0.00503
	loss_policy_3: 0.04125
	accuracy_policy_3: 0.61678
	loss_value_3: 0.03396
	loss_reward_3: 0.00532
	loss_policy_4: 0.04376
	accuracy_policy_4: 0.60018
	loss_value_4: 0.03521
	loss_reward_4: 0.00577
	loss_policy_5: 0.04655
	accuracy_policy_5: 0.57957
	loss_value_5: 0.03651
	loss_reward_5: 0.0067
	loss_policy: 0.33996
	loss_value: 0.31848
	loss_reward: 0.02794
Optimization_Done 41000
[2024-05-07 12:38:10] [command] train weight_iter_41000.pkl 205 206
[2024-05-07 12:38:47] nn step 41100, lr: 0.04783.
	loss_policy_0: 0.16383
	accuracy_policy_0: 0.65422
	loss_value_0: 0.15009
	loss_policy_1: 0.03906
	accuracy_policy_1: 0.59863
	loss_value_1: 0.03136
	loss_reward_1: 0.00484
	loss_policy_2: 0.04265
	accuracy_policy_2: 0.56646
	loss_value_2: 0.03263
	loss_reward_2: 0.0046
	loss_policy_3: 0.04558
	accuracy_policy_3: 0.54162
	loss_value_3: 0.03386
	loss_reward_3: 0.00503
	loss_policy_4: 0.04753
	accuracy_policy_4: 0.53152
	loss_value_4: 0.03476
	loss_reward_4: 0.00552
	loss_policy_5: 0.04979
	accuracy_policy_5: 0.51188
	loss_value_5: 0.03565
	loss_reward_5: 0.00636
	loss_policy: 0.38845
	loss_value: 0.31835
	loss_reward: 0.02634
[2024-05-07 12:39:23] nn step 41200, lr: 0.04783.
	loss_policy_0: 0.14338
	accuracy_policy_0: 0.71713
	loss_value_0: 0.15502
	loss_policy_1: 0.03695
	accuracy_policy_1: 0.64084
	loss_value_1: 0.03261
	loss_reward_1: 0.00516
	loss_policy_2: 0.04103
	accuracy_policy_2: 0.60885
	loss_value_2: 0.03391
	loss_reward_2: 0.00484
	loss_policy_3: 0.04423
	accuracy_policy_3: 0.5841
	loss_value_3: 0.03514
	loss_reward_3: 0.00509
	loss_policy_4: 0.04681
	accuracy_policy_4: 0.56281
	loss_value_4: 0.03632
	loss_reward_4: 0.00565
	loss_policy_5: 0.04928
	accuracy_policy_5: 0.54605
	loss_value_5: 0.03742
	loss_reward_5: 0.00689
	loss_policy: 0.36168
	loss_value: 0.33041
	loss_reward: 0.02763
Optimization_Done 41200
[2024-05-07 12:41:37] [command] train weight_iter_41200.pkl 206 207
[2024-05-07 12:42:14] nn step 41300, lr: 0.04783.
	loss_policy_0: 0.15868
	accuracy_policy_0: 0.65559
	loss_value_0: 0.14625
	loss_policy_1: 0.0368
	accuracy_policy_1: 0.61072
	loss_value_1: 0.03076
	loss_reward_1: 0.00442
	loss_policy_2: 0.03981
	accuracy_policy_2: 0.58852
	loss_value_2: 0.03214
	loss_reward_2: 0.00447
	loss_policy_3: 0.04269
	accuracy_policy_3: 0.56506
	loss_value_3: 0.03342
	loss_reward_3: 0.00495
	loss_policy_4: 0.04511
	accuracy_policy_4: 0.54812
	loss_value_4: 0.0345
	loss_reward_4: 0.00535
	loss_policy_5: 0.04728
	accuracy_policy_5: 0.5275
	loss_value_5: 0.03562
	loss_reward_5: 0.0063
	loss_policy: 0.37037
	loss_value: 0.3127
	loss_reward: 0.02549
[2024-05-07 12:42:49] nn step 41400, lr: 0.04783.
	loss_policy_0: 0.12004
	accuracy_policy_0: 0.72918
	loss_value_0: 0.14371
	loss_policy_1: 0.03089
	accuracy_policy_1: 0.6624
	loss_value_1: 0.03037
	loss_reward_1: 0.0045
	loss_policy_2: 0.03517
	accuracy_policy_2: 0.62791
	loss_value_2: 0.03185
	loss_reward_2: 0.00428
	loss_policy_3: 0.03814
	accuracy_policy_3: 0.60443
	loss_value_3: 0.03294
	loss_reward_3: 0.00487
	loss_policy_4: 0.04052
	accuracy_policy_4: 0.58564
	loss_value_4: 0.03401
	loss_reward_4: 0.00513
	loss_policy_5: 0.04266
	accuracy_policy_5: 0.56902
	loss_value_5: 0.03508
	loss_reward_5: 0.00607
	loss_policy: 0.30742
	loss_value: 0.30796
	loss_reward: 0.02486
Optimization_Done 41400
[2024-05-07 12:45:02] [command] train weight_iter_41400.pkl 207 208
[2024-05-07 12:45:39] nn step 41500, lr: 0.04783.
	loss_policy_0: 0.19624
	accuracy_policy_0: 0.63998
	loss_value_0: 0.16883
	loss_policy_1: 0.04386
	accuracy_policy_1: 0.60227
	loss_value_1: 0.03533
	loss_reward_1: 0.00521
	loss_policy_2: 0.04723
	accuracy_policy_2: 0.57877
	loss_value_2: 0.03707
	loss_reward_2: 0.005
	loss_policy_3: 0.05026
	accuracy_policy_3: 0.55895
	loss_value_3: 0.03828
	loss_reward_3: 0.00538
	loss_policy_4: 0.05296
	accuracy_policy_4: 0.54066
	loss_value_4: 0.03946
	loss_reward_4: 0.00591
	loss_policy_5: 0.05536
	accuracy_policy_5: 0.52346
	loss_value_5: 0.0408
	loss_reward_5: 0.00663
	loss_policy: 0.44591
	loss_value: 0.35977
	loss_reward: 0.02813
[2024-05-07 12:46:14] nn step 41600, lr: 0.04783.
	loss_policy_0: 0.16584
	accuracy_policy_0: 0.69879
	loss_value_0: 0.17326
	loss_policy_1: 0.03957
	accuracy_policy_1: 0.65146
	loss_value_1: 0.03648
	loss_reward_1: 0.0053
	loss_policy_2: 0.0438
	accuracy_policy_2: 0.61732
	loss_value_2: 0.03805
	loss_reward_2: 0.00524
	loss_policy_3: 0.04714
	accuracy_policy_3: 0.59795
	loss_value_3: 0.03959
	loss_reward_3: 0.00536
	loss_policy_4: 0.05025
	accuracy_policy_4: 0.57646
	loss_value_4: 0.04085
	loss_reward_4: 0.00597
	loss_policy_5: 0.05245
	accuracy_policy_5: 0.56346
	loss_value_5: 0.04196
	loss_reward_5: 0.00674
	loss_policy: 0.39905
	loss_value: 0.37018
	loss_reward: 0.0286
Optimization_Done 41600
[2024-05-07 12:48:26] [command] train weight_iter_41600.pkl 208 209
[2024-05-07 12:49:02] nn step 41700, lr: 0.04783.
	loss_policy_0: 0.196
	accuracy_policy_0: 0.64547
	loss_value_0: 0.17788
	loss_policy_1: 0.04521
	accuracy_policy_1: 0.59922
	loss_value_1: 0.03723
	loss_reward_1: 0.00621
	loss_policy_2: 0.04913
	accuracy_policy_2: 0.56668
	loss_value_2: 0.03882
	loss_reward_2: 0.00564
	loss_policy_3: 0.05214
	accuracy_policy_3: 0.54881
	loss_value_3: 0.0404
	loss_reward_3: 0.00615
	loss_policy_4: 0.05475
	accuracy_policy_4: 0.53428
	loss_value_4: 0.04181
	loss_reward_4: 0.00674
	loss_policy_5: 0.05712
	accuracy_policy_5: 0.51812
	loss_value_5: 0.04315
	loss_reward_5: 0.00764
	loss_policy: 0.45435
	loss_value: 0.37928
	loss_reward: 0.03237
[2024-05-07 12:49:38] nn step 41800, lr: 0.04783.
	loss_policy_0: 0.16131
	accuracy_policy_0: 0.70473
	loss_value_0: 0.17493
	loss_policy_1: 0.03965
	accuracy_policy_1: 0.64328
	loss_value_1: 0.03674
	loss_reward_1: 0.00599
	loss_policy_2: 0.04371
	accuracy_policy_2: 0.61281
	loss_value_2: 0.0381
	loss_reward_2: 0.00552
	loss_policy_3: 0.0466
	accuracy_policy_3: 0.59098
	loss_value_3: 0.03938
	loss_reward_3: 0.0058
	loss_policy_4: 0.04997
	accuracy_policy_4: 0.56662
	loss_value_4: 0.04075
	loss_reward_4: 0.00635
	loss_policy_5: 0.05256
	accuracy_policy_5: 0.55266
	loss_value_5: 0.04197
	loss_reward_5: 0.00758
	loss_policy: 0.39382
	loss_value: 0.37188
	loss_reward: 0.03124
Optimization_Done 41800
[2024-05-07 12:51:47] [command] train weight_iter_41800.pkl 209 210
[2024-05-07 12:52:25] nn step 41900, lr: 0.04783.
	loss_policy_0: 0.15924
	accuracy_policy_0: 0.68586
	loss_value_0: 0.16987
	loss_policy_1: 0.03795
	accuracy_policy_1: 0.63184
	loss_value_1: 0.03524
	loss_reward_1: 0.00582
	loss_policy_2: 0.04183
	accuracy_policy_2: 0.60045
	loss_value_2: 0.03661
	loss_reward_2: 0.00543
	loss_policy_3: 0.04451
	accuracy_policy_3: 0.58166
	loss_value_3: 0.038
	loss_reward_3: 0.00583
	loss_policy_4: 0.04676
	accuracy_policy_4: 0.56777
	loss_value_4: 0.0391
	loss_reward_4: 0.00651
	loss_policy_5: 0.0489
	accuracy_policy_5: 0.55715
	loss_value_5: 0.04025
	loss_reward_5: 0.0074
	loss_policy: 0.37921
	loss_value: 0.35906
	loss_reward: 0.03099
[2024-05-07 12:53:02] nn step 42000, lr: 0.04783.
	loss_policy_0: 0.13794
	accuracy_policy_0: 0.72586
	loss_value_0: 0.17047
	loss_policy_1: 0.03561
	accuracy_policy_1: 0.65123
	loss_value_1: 0.0355
	loss_reward_1: 0.00582
	loss_policy_2: 0.03932
	accuracy_policy_2: 0.62641
	loss_value_2: 0.03679
	loss_reward_2: 0.00537
	loss_policy_3: 0.04254
	accuracy_policy_3: 0.60455
	loss_value_3: 0.03792
	loss_reward_3: 0.00571
	loss_policy_4: 0.04471
	accuracy_policy_4: 0.59123
	loss_value_4: 0.03914
	loss_reward_4: 0.00645
	loss_policy_5: 0.04774
	accuracy_policy_5: 0.57021
	loss_value_5: 0.04026
	loss_reward_5: 0.00773
	loss_policy: 0.34786
	loss_value: 0.36008
	loss_reward: 0.03108
Optimization_Done 42000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 12:57:10] [command] train weight_iter_42000.pkl 210 211
[2024-05-07 12:57:54] nn step 42100, lr: 0.043047.
	loss_policy_0: 0.12365
	accuracy_policy_0: 0.74168
	loss_value_0: 0.16617
	loss_policy_1: 0.03113
	accuracy_policy_1: 0.68369
	loss_value_1: 0.03476
	loss_reward_1: 0.00511
	loss_policy_2: 0.03493
	accuracy_policy_2: 0.65768
	loss_value_2: 0.03618
	loss_reward_2: 0.00478
	loss_policy_3: 0.03787
	accuracy_policy_3: 0.63551
	loss_value_3: 0.03743
	loss_reward_3: 0.00501
	loss_policy_4: 0.04047
	accuracy_policy_4: 0.61836
	loss_value_4: 0.03847
	loss_reward_4: 0.00552
	loss_policy_5: 0.04297
	accuracy_policy_5: 0.60562
	loss_value_5: 0.03955
	loss_reward_5: 0.00624
	loss_policy: 0.31102
	loss_value: 0.35256
	loss_reward: 0.02665
[2024-05-07 12:58:30] nn step 42200, lr: 0.043047.
	loss_policy_0: 0.09987
	accuracy_policy_0: 0.7858
	loss_value_0: 0.16378
	loss_policy_1: 0.02767
	accuracy_policy_1: 0.71871
	loss_value_1: 0.03439
	loss_reward_1: 0.00498
	loss_policy_2: 0.03143
	accuracy_policy_2: 0.69
	loss_value_2: 0.03577
	loss_reward_2: 0.00473
	loss_policy_3: 0.03419
	accuracy_policy_3: 0.67385
	loss_value_3: 0.03696
	loss_reward_3: 0.00503
	loss_policy_4: 0.03702
	accuracy_policy_4: 0.65453
	loss_value_4: 0.03809
	loss_reward_4: 0.0055
	loss_policy_5: 0.03954
	accuracy_policy_5: 0.63875
	loss_value_5: 0.03927
	loss_reward_5: 0.0063
	loss_policy: 0.26973
	loss_value: 0.34825
	loss_reward: 0.02654
Optimization_Done 42200
[2024-05-07 13:00:40] [command] train weight_iter_42200.pkl 211 212
[2024-05-07 13:01:17] nn step 42300, lr: 0.043047.
	loss_policy_0: 0.13781
	accuracy_policy_0: 0.72277
	loss_value_0: 0.15596
	loss_policy_1: 0.03361
	accuracy_policy_1: 0.67564
	loss_value_1: 0.03282
	loss_reward_1: 0.00508
	loss_policy_2: 0.03839
	accuracy_policy_2: 0.64316
	loss_value_2: 0.03439
	loss_reward_2: 0.00478
	loss_policy_3: 0.04173
	accuracy_policy_3: 0.62105
	loss_value_3: 0.03579
	loss_reward_3: 0.00513
	loss_policy_4: 0.04566
	accuracy_policy_4: 0.59453
	loss_value_4: 0.03704
	loss_reward_4: 0.0055
	loss_policy_5: 0.04799
	accuracy_policy_5: 0.57873
	loss_value_5: 0.03816
	loss_reward_5: 0.0064
	loss_policy: 0.34519
	loss_value: 0.33417
	loss_reward: 0.02689
[2024-05-07 13:01:53] nn step 42400, lr: 0.043047.
	loss_policy_0: 0.10392
	accuracy_policy_0: 0.78246
	loss_value_0: 0.15434
	loss_policy_1: 0.02834
	accuracy_policy_1: 0.71898
	loss_value_1: 0.03271
	loss_reward_1: 0.00488
	loss_policy_2: 0.03296
	accuracy_policy_2: 0.68643
	loss_value_2: 0.03431
	loss_reward_2: 0.0046
	loss_policy_3: 0.03664
	accuracy_policy_3: 0.6642
	loss_value_3: 0.03556
	loss_reward_3: 0.00495
	loss_policy_4: 0.03986
	accuracy_policy_4: 0.63994
	loss_value_4: 0.03668
	loss_reward_4: 0.00537
	loss_policy_5: 0.04328
	accuracy_policy_5: 0.61699
	loss_value_5: 0.038
	loss_reward_5: 0.00658
	loss_policy: 0.28499
	loss_value: 0.3316
	loss_reward: 0.02637
Optimization_Done 42400
[2024-05-07 13:04:03] [command] train weight_iter_42400.pkl 212 213
[2024-05-07 13:04:39] nn step 42500, lr: 0.043047.
	loss_policy_0: 0.13878
	accuracy_policy_0: 0.71984
	loss_value_0: 0.15377
	loss_policy_1: 0.03492
	accuracy_policy_1: 0.65699
	loss_value_1: 0.03233
	loss_reward_1: 0.00587
	loss_policy_2: 0.03856
	accuracy_policy_2: 0.63059
	loss_value_2: 0.03396
	loss_reward_2: 0.00545
	loss_policy_3: 0.04233
	accuracy_policy_3: 0.60559
	loss_value_3: 0.03533
	loss_reward_3: 0.0059
	loss_policy_4: 0.04542
	accuracy_policy_4: 0.58266
	loss_value_4: 0.03673
	loss_reward_4: 0.00633
	loss_policy_5: 0.04841
	accuracy_policy_5: 0.56447
	loss_value_5: 0.03786
	loss_reward_5: 0.00741
	loss_policy: 0.34842
	loss_value: 0.32998
	loss_reward: 0.03094
[2024-05-07 13:05:15] nn step 42600, lr: 0.043047.
	loss_policy_0: 0.11049
	accuracy_policy_0: 0.76838
	loss_value_0: 0.1527
	loss_policy_1: 0.03093
	accuracy_policy_1: 0.69195
	loss_value_1: 0.03208
	loss_reward_1: 0.0056
	loss_policy_2: 0.03512
	accuracy_policy_2: 0.65596
	loss_value_2: 0.03354
	loss_reward_2: 0.00521
	loss_policy_3: 0.0386
	accuracy_policy_3: 0.63465
	loss_value_3: 0.035
	loss_reward_3: 0.00592
	loss_policy_4: 0.04164
	accuracy_policy_4: 0.61514
	loss_value_4: 0.03615
	loss_reward_4: 0.00616
	loss_policy_5: 0.04449
	accuracy_policy_5: 0.59727
	loss_value_5: 0.03742
	loss_reward_5: 0.0074
	loss_policy: 0.30126
	loss_value: 0.32689
	loss_reward: 0.03029
Optimization_Done 42600
[2024-05-07 13:07:28] [command] train weight_iter_42600.pkl 213 214
[2024-05-07 13:08:04] nn step 42700, lr: 0.043047.
	loss_policy_0: 0.10719
	accuracy_policy_0: 0.74406
	loss_value_0: 0.13038
	loss_policy_1: 0.02808
	accuracy_policy_1: 0.67428
	loss_value_1: 0.02736
	loss_reward_1: 0.00459
	loss_policy_2: 0.03112
	accuracy_policy_2: 0.64945
	loss_value_2: 0.02859
	loss_reward_2: 0.00429
	loss_policy_3: 0.03372
	accuracy_policy_3: 0.6298
	loss_value_3: 0.02976
	loss_reward_3: 0.00473
	loss_policy_4: 0.03634
	accuracy_policy_4: 0.61029
	loss_value_4: 0.03085
	loss_reward_4: 0.00495
	loss_policy_5: 0.03864
	accuracy_policy_5: 0.59449
	loss_value_5: 0.03195
	loss_reward_5: 0.00596
	loss_policy: 0.27509
	loss_value: 0.27888
	loss_reward: 0.02453
[2024-05-07 13:08:40] nn step 42800, lr: 0.043047.
	loss_policy_0: 0.0867
	accuracy_policy_0: 0.79686
	loss_value_0: 0.13665
	loss_policy_1: 0.02529
	accuracy_policy_1: 0.71436
	loss_value_1: 0.02859
	loss_reward_1: 0.00469
	loss_policy_2: 0.02846
	accuracy_policy_2: 0.68359
	loss_value_2: 0.02979
	loss_reward_2: 0.00444
	loss_policy_3: 0.03127
	accuracy_policy_3: 0.66055
	loss_value_3: 0.03078
	loss_reward_3: 0.00476
	loss_policy_4: 0.03367
	accuracy_policy_4: 0.644
	loss_value_4: 0.03178
	loss_reward_4: 0.00498
	loss_policy_5: 0.03642
	accuracy_policy_5: 0.62217
	loss_value_5: 0.03288
	loss_reward_5: 0.00602
	loss_policy: 0.24183
	loss_value: 0.29047
	loss_reward: 0.02489
Optimization_Done 42800
[2024-05-07 13:10:51] [command] train weight_iter_42800.pkl 214 215
[2024-05-07 13:11:28] nn step 42900, lr: 0.043047.
	loss_policy_0: 0.11783
	accuracy_policy_0: 0.71463
	loss_value_0: 0.14131
	loss_policy_1: 0.02959
	accuracy_policy_1: 0.65082
	loss_value_1: 0.02995
	loss_reward_1: 0.00482
	loss_policy_2: 0.03282
	accuracy_policy_2: 0.62375
	loss_value_2: 0.03126
	loss_reward_2: 0.0048
	loss_policy_3: 0.03539
	accuracy_policy_3: 0.60199
	loss_value_3: 0.03249
	loss_reward_3: 0.00513
	loss_policy_4: 0.0372
	accuracy_policy_4: 0.58844
	loss_value_4: 0.0336
	loss_reward_4: 0.00549
	loss_policy_5: 0.03924
	accuracy_policy_5: 0.5715
	loss_value_5: 0.03483
	loss_reward_5: 0.00613
	loss_policy: 0.29207
	loss_value: 0.30345
	loss_reward: 0.02636
[2024-05-07 13:12:04] nn step 43000, lr: 0.043047.
	loss_policy_0: 0.08824
	accuracy_policy_0: 0.78422
	loss_value_0: 0.14269
	loss_policy_1: 0.0255
	accuracy_policy_1: 0.69973
	loss_value_1: 0.03018
	loss_reward_1: 0.00452
	loss_policy_2: 0.0291
	accuracy_policy_2: 0.66561
	loss_value_2: 0.0315
	loss_reward_2: 0.00451
	loss_policy_3: 0.03193
	accuracy_policy_3: 0.6416
	loss_value_3: 0.03273
	loss_reward_3: 0.00495
	loss_policy_4: 0.0341
	accuracy_policy_4: 0.62586
	loss_value_4: 0.03391
	loss_reward_4: 0.00532
	loss_policy_5: 0.03616
	accuracy_policy_5: 0.60629
	loss_value_5: 0.03503
	loss_reward_5: 0.00611
	loss_policy: 0.24502
	loss_value: 0.30605
	loss_reward: 0.0254
Optimization_Done 43000
[2024-05-07 13:14:13] [command] train weight_iter_43000.pkl 215 216
[2024-05-07 13:14:50] nn step 43100, lr: 0.043047.
	loss_policy_0: 0.15682
	accuracy_policy_0: 0.68035
	loss_value_0: 0.17397
	loss_policy_1: 0.03696
	accuracy_policy_1: 0.6298
	loss_value_1: 0.0365
	loss_reward_1: 0.00645
	loss_policy_2: 0.04068
	accuracy_policy_2: 0.5943
	loss_value_2: 0.038
	loss_reward_2: 0.00657
	loss_policy_3: 0.04342
	accuracy_policy_3: 0.57348
	loss_value_3: 0.03934
	loss_reward_3: 0.00728
	loss_policy_4: 0.04558
	accuracy_policy_4: 0.55748
	loss_value_4: 0.04055
	loss_reward_4: 0.00757
	loss_policy_5: 0.0475
	accuracy_policy_5: 0.53898
	loss_value_5: 0.04173
	loss_reward_5: 0.00848
	loss_policy: 0.37096
	loss_value: 0.37009
	loss_reward: 0.03634
[2024-05-07 13:15:26] nn step 43200, lr: 0.043047.
	loss_policy_0: 0.12626
	accuracy_policy_0: 0.73605
	loss_value_0: 0.17056
	loss_policy_1: 0.03336
	accuracy_policy_1: 0.66166
	loss_value_1: 0.03595
	loss_reward_1: 0.00646
	loss_policy_2: 0.03729
	accuracy_policy_2: 0.62635
	loss_value_2: 0.03748
	loss_reward_2: 0.00654
	loss_policy_3: 0.03996
	accuracy_policy_3: 0.6076
	loss_value_3: 0.03895
	loss_reward_3: 0.00703
	loss_policy_4: 0.04187
	accuracy_policy_4: 0.59131
	loss_value_4: 0.0401
	loss_reward_4: 0.00748
	loss_policy_5: 0.04436
	accuracy_policy_5: 0.57285
	loss_value_5: 0.0414
	loss_reward_5: 0.00852
	loss_policy: 0.3231
	loss_value: 0.36445
	loss_reward: 0.03603
Optimization_Done 43200
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 13:21:10] [command] train weight_iter_43200.pkl 216 217
[2024-05-07 13:21:54] nn step 43300, lr: 0.043047.
	loss_policy_0: 0.15447
	accuracy_policy_0: 0.71309
	loss_value_0: 0.18528
	loss_policy_1: 0.03835
	accuracy_policy_1: 0.64861
	loss_value_1: 0.03904
	loss_reward_1: 0.00644
	loss_policy_2: 0.04251
	accuracy_policy_2: 0.61566
	loss_value_2: 0.04056
	loss_reward_2: 0.00649
	loss_policy_3: 0.04577
	accuracy_policy_3: 0.58891
	loss_value_3: 0.04188
	loss_reward_3: 0.00712
	loss_policy_4: 0.04859
	accuracy_policy_4: 0.56734
	loss_value_4: 0.04323
	loss_reward_4: 0.00777
	loss_policy_5: 0.05096
	accuracy_policy_5: 0.5492
	loss_value_5: 0.04439
	loss_reward_5: 0.00867
	loss_policy: 0.38066
	loss_value: 0.39438
	loss_reward: 0.0365
[2024-05-07 13:22:29] nn step 43400, lr: 0.043047.
	loss_policy_0: 0.12882
	accuracy_policy_0: 0.75826
	loss_value_0: 0.18312
	loss_policy_1: 0.03475
	accuracy_policy_1: 0.67998
	loss_value_1: 0.03848
	loss_reward_1: 0.00634
	loss_policy_2: 0.03922
	accuracy_policy_2: 0.6442
	loss_value_2: 0.04007
	loss_reward_2: 0.00642
	loss_policy_3: 0.04235
	accuracy_policy_3: 0.61793
	loss_value_3: 0.04129
	loss_reward_3: 0.00712
	loss_policy_4: 0.0451
	accuracy_policy_4: 0.5985
	loss_value_4: 0.04256
	loss_reward_4: 0.00752
	loss_policy_5: 0.04792
	accuracy_policy_5: 0.57943
	loss_value_5: 0.04393
	loss_reward_5: 0.00844
	loss_policy: 0.33817
	loss_value: 0.38945
	loss_reward: 0.03584
Optimization_Done 43400
[2024-05-07 13:24:41] [command] train weight_iter_43400.pkl 217 218
[2024-05-07 13:25:17] nn step 43500, lr: 0.043047.
	loss_policy_0: 0.14249
	accuracy_policy_0: 0.72551
	loss_value_0: 0.15538
	loss_policy_1: 0.0361
	accuracy_policy_1: 0.65938
	loss_value_1: 0.03275
	loss_reward_1: 0.00485
	loss_policy_2: 0.04047
	accuracy_policy_2: 0.62391
	loss_value_2: 0.0344
	loss_reward_2: 0.00479
	loss_policy_3: 0.04377
	accuracy_policy_3: 0.60025
	loss_value_3: 0.03584
	loss_reward_3: 0.00526
	loss_policy_4: 0.04664
	accuracy_policy_4: 0.5777
	loss_value_4: 0.03713
	loss_reward_4: 0.00559
	loss_policy_5: 0.04941
	accuracy_policy_5: 0.5558
	loss_value_5: 0.03828
	loss_reward_5: 0.00675
	loss_policy: 0.35887
	loss_value: 0.33378
	loss_reward: 0.02724
[2024-05-07 13:25:53] nn step 43600, lr: 0.043047.
	loss_policy_0: 0.11279
	accuracy_policy_0: 0.77586
	loss_value_0: 0.15591
	loss_policy_1: 0.0314
	accuracy_policy_1: 0.69832
	loss_value_1: 0.03306
	loss_reward_1: 0.00497
	loss_policy_2: 0.03628
	accuracy_policy_2: 0.661
	loss_value_2: 0.03448
	loss_reward_2: 0.00479
	loss_policy_3: 0.03932
	accuracy_policy_3: 0.63838
	loss_value_3: 0.03592
	loss_reward_3: 0.00522
	loss_policy_4: 0.0428
	accuracy_policy_4: 0.61221
	loss_value_4: 0.03717
	loss_reward_4: 0.0057
	loss_policy_5: 0.04592
	accuracy_policy_5: 0.59127
	loss_value_5: 0.03834
	loss_reward_5: 0.00662
	loss_policy: 0.3085
	loss_value: 0.33488
	loss_reward: 0.02729
Optimization_Done 43600
[2024-05-07 13:28:07] [command] train weight_iter_43600.pkl 218 219
[2024-05-07 13:28:44] nn step 43700, lr: 0.043047.
	loss_policy_0: 0.16878
	accuracy_policy_0: 0.67477
	loss_value_0: 0.1525
	loss_policy_1: 0.04197
	accuracy_policy_1: 0.60461
	loss_value_1: 0.03262
	loss_reward_1: 0.00651
	loss_policy_2: 0.04604
	accuracy_policy_2: 0.57314
	loss_value_2: 0.03429
	loss_reward_2: 0.00615
	loss_policy_3: 0.04934
	accuracy_policy_3: 0.54678
	loss_value_3: 0.03566
	loss_reward_3: 0.00671
	loss_policy_4: 0.05239
	accuracy_policy_4: 0.52643
	loss_value_4: 0.03692
	loss_reward_4: 0.00728
	loss_policy_5: 0.05564
	accuracy_policy_5: 0.49975
	loss_value_5: 0.03823
	loss_reward_5: 0.00818
	loss_policy: 0.41416
	loss_value: 0.33022
	loss_reward: 0.03483
[2024-05-07 13:29:19] nn step 43800, lr: 0.043047.
	loss_policy_0: 0.13151
	accuracy_policy_0: 0.73725
	loss_value_0: 0.1485
	loss_policy_1: 0.03585
	accuracy_policy_1: 0.66029
	loss_value_1: 0.03161
	loss_reward_1: 0.00631
	loss_policy_2: 0.04053
	accuracy_policy_2: 0.62207
	loss_value_2: 0.03312
	loss_reward_2: 0.00589
	loss_policy_3: 0.04376
	accuracy_policy_3: 0.59846
	loss_value_3: 0.03454
	loss_reward_3: 0.00643
	loss_policy_4: 0.04737
	accuracy_policy_4: 0.5717
	loss_value_4: 0.03601
	loss_reward_4: 0.00698
	loss_policy_5: 0.05073
	accuracy_policy_5: 0.54193
	loss_value_5: 0.03735
	loss_reward_5: 0.00807
	loss_policy: 0.34975
	loss_value: 0.32111
	loss_reward: 0.03367
Optimization_Done 43800
[2024-05-07 13:31:38] [command] train weight_iter_43800.pkl 219 220
[2024-05-07 13:32:14] nn step 43900, lr: 0.043047.
	loss_policy_0: 0.11013
	accuracy_policy_0: 0.72492
	loss_value_0: 0.1049
	loss_policy_1: 0.02959
	accuracy_policy_1: 0.63537
	loss_value_1: 0.02257
	loss_reward_1: 0.00466
	loss_policy_2: 0.03278
	accuracy_policy_2: 0.59945
	loss_value_2: 0.02375
	loss_reward_2: 0.0044
	loss_policy_3: 0.03486
	accuracy_policy_3: 0.57832
	loss_value_3: 0.02499
	loss_reward_3: 0.00482
	loss_policy_4: 0.03728
	accuracy_policy_4: 0.55166
	loss_value_4: 0.02592
	loss_reward_4: 0.00521
	loss_policy_5: 0.03939
	accuracy_policy_5: 0.53018
	loss_value_5: 0.02701
	loss_reward_5: 0.00605
	loss_policy: 0.28402
	loss_value: 0.22914
	loss_reward: 0.02515
[2024-05-07 13:32:50] nn step 44000, lr: 0.043047.
	loss_policy_0: 0.08935
	accuracy_policy_0: 0.7674
	loss_value_0: 0.09593
	loss_policy_1: 0.02638
	accuracy_policy_1: 0.66283
	loss_value_1: 0.02058
	loss_reward_1: 0.00448
	loss_policy_2: 0.02932
	accuracy_policy_2: 0.62857
	loss_value_2: 0.02172
	loss_reward_2: 0.00423
	loss_policy_3: 0.03193
	accuracy_policy_3: 0.60207
	loss_value_3: 0.02291
	loss_reward_3: 0.00459
	loss_policy_4: 0.03413
	accuracy_policy_4: 0.58078
	loss_value_4: 0.02397
	loss_reward_4: 0.00486
	loss_policy_5: 0.03649
	accuracy_policy_5: 0.55535
	loss_value_5: 0.02509
	loss_reward_5: 0.00588
	loss_policy: 0.2476
	loss_value: 0.2102
	loss_reward: 0.02404
Optimization_Done 44000
[2024-05-07 13:35:03] [command] train weight_iter_44000.pkl 220 221
[2024-05-07 13:35:41] nn step 44100, lr: 0.043047.
	loss_policy_0: 0.11601
	accuracy_policy_0: 0.69689
	loss_value_0: 0.10445
	loss_policy_1: 0.02891
	accuracy_policy_1: 0.63137
	loss_value_1: 0.0225
	loss_reward_1: 0.00396
	loss_policy_2: 0.03172
	accuracy_policy_2: 0.60307
	loss_value_2: 0.02409
	loss_reward_2: 0.00425
	loss_policy_3: 0.03343
	accuracy_policy_3: 0.58631
	loss_value_3: 0.02546
	loss_reward_3: 0.00457
	loss_policy_4: 0.03518
	accuracy_policy_4: 0.56426
	loss_value_4: 0.0267
	loss_reward_4: 0.00495
	loss_policy_5: 0.0364
	accuracy_policy_5: 0.55178
	loss_value_5: 0.0279
	loss_reward_5: 0.00524
	loss_policy: 0.28165
	loss_value: 0.2311
	loss_reward: 0.02298
[2024-05-07 13:36:16] nn step 44200, lr: 0.043047.
	loss_policy_0: 0.08265
	accuracy_policy_0: 0.77385
	loss_value_0: 0.10487
	loss_policy_1: 0.02386
	accuracy_policy_1: 0.67971
	loss_value_1: 0.02221
	loss_reward_1: 0.00362
	loss_policy_2: 0.02657
	accuracy_policy_2: 0.65146
	loss_value_2: 0.02348
	loss_reward_2: 0.00376
	loss_policy_3: 0.02863
	accuracy_policy_3: 0.62947
	loss_value_3: 0.02462
	loss_reward_3: 0.00408
	loss_policy_4: 0.03058
	accuracy_policy_4: 0.61012
	loss_value_4: 0.02562
	loss_reward_4: 0.00434
	loss_policy_5: 0.03215
	accuracy_policy_5: 0.59092
	loss_value_5: 0.02674
	loss_reward_5: 0.00501
	loss_policy: 0.22445
	loss_value: 0.22755
	loss_reward: 0.02081
Optimization_Done 44200
[2024-05-07 13:38:28] [command] train weight_iter_44200.pkl 221 222
[2024-05-07 13:39:05] nn step 44300, lr: 0.043047.
	loss_policy_0: 0.15988
	accuracy_policy_0: 0.66885
	loss_value_0: 0.15333
	loss_policy_1: 0.03796
	accuracy_policy_1: 0.61305
	loss_value_1: 0.03226
	loss_reward_1: 0.00627
	loss_policy_2: 0.04141
	accuracy_policy_2: 0.5833
	loss_value_2: 0.03375
	loss_reward_2: 0.00634
	loss_policy_3: 0.04404
	accuracy_policy_3: 0.56158
	loss_value_3: 0.03535
	loss_reward_3: 0.00693
	loss_policy_4: 0.04595
	accuracy_policy_4: 0.54428
	loss_value_4: 0.03671
	loss_reward_4: 0.00728
	loss_policy_5: 0.04813
	accuracy_policy_5: 0.52791
	loss_value_5: 0.03783
	loss_reward_5: 0.00834
	loss_policy: 0.37738
	loss_value: 0.32925
	loss_reward: 0.03516
[2024-05-07 13:39:41] nn step 44400, lr: 0.043047.
	loss_policy_0: 0.12833
	accuracy_policy_0: 0.7268
	loss_value_0: 0.1624
	loss_policy_1: 0.03383
	accuracy_policy_1: 0.65607
	loss_value_1: 0.03421
	loss_reward_1: 0.00618
	loss_policy_2: 0.03765
	accuracy_policy_2: 0.62408
	loss_value_2: 0.03577
	loss_reward_2: 0.00627
	loss_policy_3: 0.04057
	accuracy_policy_3: 0.59697
	loss_value_3: 0.03725
	loss_reward_3: 0.00701
	loss_policy_4: 0.04266
	accuracy_policy_4: 0.57723
	loss_value_4: 0.03842
	loss_reward_4: 0.00724
	loss_policy_5: 0.04486
	accuracy_policy_5: 0.5626
	loss_value_5: 0.03959
	loss_reward_5: 0.00807
	loss_policy: 0.3279
	loss_value: 0.34763
	loss_reward: 0.03477
Optimization_Done 44400
[2024-05-07 13:41:52] [command] train weight_iter_44400.pkl 222 223
[2024-05-07 13:42:29] nn step 44500, lr: 0.043047.
	loss_policy_0: 0.17001
	accuracy_policy_0: 0.67562
	loss_value_0: 0.18443
	loss_policy_1: 0.04086
	accuracy_policy_1: 0.62066
	loss_value_1: 0.03844
	loss_reward_1: 0.00769
	loss_policy_2: 0.04516
	accuracy_policy_2: 0.58818
	loss_value_2: 0.03995
	loss_reward_2: 0.00766
	loss_policy_3: 0.04765
	accuracy_policy_3: 0.57129
	loss_value_3: 0.04153
	loss_reward_3: 0.00837
	loss_policy_4: 0.0501
	accuracy_policy_4: 0.55463
	loss_value_4: 0.04294
	loss_reward_4: 0.0089
	loss_policy_5: 0.05223
	accuracy_policy_5: 0.54139
	loss_value_5: 0.04438
	loss_reward_5: 0.00995
	loss_policy: 0.40601
	loss_value: 0.39167
	loss_reward: 0.04257
[2024-05-07 13:43:05] nn step 44600, lr: 0.043047.
	loss_policy_0: 0.14074
	accuracy_policy_0: 0.72877
	loss_value_0: 0.18259
	loss_policy_1: 0.03646
	accuracy_policy_1: 0.66002
	loss_value_1: 0.03818
	loss_reward_1: 0.00751
	loss_policy_2: 0.04075
	accuracy_policy_2: 0.62807
	loss_value_2: 0.03958
	loss_reward_2: 0.00742
	loss_policy_3: 0.04375
	accuracy_policy_3: 0.60082
	loss_value_3: 0.04115
	loss_reward_3: 0.00798
	loss_policy_4: 0.04697
	accuracy_policy_4: 0.57939
	loss_value_4: 0.04252
	loss_reward_4: 0.0087
	loss_policy_5: 0.04905
	accuracy_policy_5: 0.56777
	loss_value_5: 0.04379
	loss_reward_5: 0.00975
	loss_policy: 0.35772
	loss_value: 0.38781
	loss_reward: 0.04137
Optimization_Done 44600
[2024-05-07 13:45:16] [command] train weight_iter_44600.pkl 223 224
[2024-05-07 13:45:53] nn step 44700, lr: 0.043047.
	loss_policy_0: 0.14982
	accuracy_policy_0: 0.70543
	loss_value_0: 0.17661
	loss_policy_1: 0.03886
	accuracy_policy_1: 0.62633
	loss_value_1: 0.03698
	loss_reward_1: 0.0079
	loss_policy_2: 0.04335
	accuracy_policy_2: 0.59256
	loss_value_2: 0.03828
	loss_reward_2: 0.00759
	loss_policy_3: 0.04626
	accuracy_policy_3: 0.57408
	loss_value_3: 0.03984
	loss_reward_3: 0.00829
	loss_policy_4: 0.04955
	accuracy_policy_4: 0.55232
	loss_value_4: 0.04107
	loss_reward_4: 0.00908
	loss_policy_5: 0.05212
	accuracy_policy_5: 0.53838
	loss_value_5: 0.04231
	loss_reward_5: 0.01043
	loss_policy: 0.37997
	loss_value: 0.3751
	loss_reward: 0.0433
[2024-05-07 13:46:29] nn step 44800, lr: 0.043047.
	loss_policy_0: 0.12945
	accuracy_policy_0: 0.7501
	loss_value_0: 0.177
	loss_policy_1: 0.0369
	accuracy_policy_1: 0.65801
	loss_value_1: 0.03705
	loss_reward_1: 0.00841
	loss_policy_2: 0.04167
	accuracy_policy_2: 0.62182
	loss_value_2: 0.03864
	loss_reward_2: 0.00782
	loss_policy_3: 0.04479
	accuracy_policy_3: 0.60707
	loss_value_3: 0.03994
	loss_reward_3: 0.00836
	loss_policy_4: 0.04815
	accuracy_policy_4: 0.58568
	loss_value_4: 0.04137
	loss_reward_4: 0.00911
	loss_policy_5: 0.05117
	accuracy_policy_5: 0.56982
	loss_value_5: 0.04278
	loss_reward_5: 0.01062
	loss_policy: 0.35212
	loss_value: 0.37678
	loss_reward: 0.04432
Optimization_Done 44800
[2024-05-07 13:48:19] [command] train weight_iter_44800.pkl 224 225
[2024-05-07 13:48:56] nn step 44900, lr: 0.043047.
	loss_policy_0: 0.13806
	accuracy_policy_0: 0.69861
	loss_value_0: 0.16045
	loss_policy_1: 0.03665
	accuracy_policy_1: 0.61506
	loss_value_1: 0.03381
	loss_reward_1: 0.00658
	loss_policy_2: 0.04041
	accuracy_policy_2: 0.58367
	loss_value_2: 0.03524
	loss_reward_2: 0.0063
	loss_policy_3: 0.04314
	accuracy_policy_3: 0.5592
	loss_value_3: 0.03653
	loss_reward_3: 0.00704
	loss_policy_4: 0.04613
	accuracy_policy_4: 0.53525
	loss_value_4: 0.03763
	loss_reward_4: 0.00746
	loss_policy_5: 0.04831
	accuracy_policy_5: 0.51334
	loss_value_5: 0.03881
	loss_reward_5: 0.00854
	loss_policy: 0.35271
	loss_value: 0.34248
	loss_reward: 0.03591
[2024-05-07 13:49:32] nn step 45000, lr: 0.043047.
	loss_policy_0: 0.11393
	accuracy_policy_0: 0.75189
	loss_value_0: 0.16074
	loss_policy_1: 0.03405
	accuracy_policy_1: 0.65062
	loss_value_1: 0.03397
	loss_reward_1: 0.00685
	loss_policy_2: 0.03751
	accuracy_policy_2: 0.62363
	loss_value_2: 0.03543
	loss_reward_2: 0.00652
	loss_policy_3: 0.04122
	accuracy_policy_3: 0.59293
	loss_value_3: 0.03673
	loss_reward_3: 0.00717
	loss_policy_4: 0.04455
	accuracy_policy_4: 0.5615
	loss_value_4: 0.03784
	loss_reward_4: 0.00767
	loss_policy_5: 0.04687
	accuracy_policy_5: 0.54586
	loss_value_5: 0.03892
	loss_reward_5: 0.00893
	loss_policy: 0.31814
	loss_value: 0.34363
	loss_reward: 0.03713
Optimization_Done 45000
[2024-05-07 13:51:48] [command] train weight_iter_45000.pkl 225 226
[2024-05-07 13:52:25] nn step 45100, lr: 0.043047.
	loss_policy_0: 0.17369
	accuracy_policy_0: 0.64854
	loss_value_0: 0.16051
	loss_policy_1: 0.04019
	accuracy_policy_1: 0.59861
	loss_value_1: 0.03353
	loss_reward_1: 0.00511
	loss_policy_2: 0.04343
	accuracy_policy_2: 0.56977
	loss_value_2: 0.03521
	loss_reward_2: 0.00519
	loss_policy_3: 0.0458
	accuracy_policy_3: 0.55297
	loss_value_3: 0.03669
	loss_reward_3: 0.00558
	loss_policy_4: 0.04808
	accuracy_policy_4: 0.53246
	loss_value_4: 0.03792
	loss_reward_4: 0.0063
	loss_policy_5: 0.05026
	accuracy_policy_5: 0.51771
	loss_value_5: 0.03913
	loss_reward_5: 0.00677
	loss_policy: 0.40145
	loss_value: 0.34299
	loss_reward: 0.02895
[2024-05-07 13:53:01] nn step 45200, lr: 0.043047.
	loss_policy_0: 0.13573
	accuracy_policy_0: 0.71127
	loss_value_0: 0.1541
	loss_policy_1: 0.03446
	accuracy_policy_1: 0.64449
	loss_value_1: 0.0324
	loss_reward_1: 0.00515
	loss_policy_2: 0.03817
	accuracy_policy_2: 0.61393
	loss_value_2: 0.0338
	loss_reward_2: 0.00501
	loss_policy_3: 0.04084
	accuracy_policy_3: 0.589
	loss_value_3: 0.03521
	loss_reward_3: 0.00535
	loss_policy_4: 0.04346
	accuracy_policy_4: 0.56672
	loss_value_4: 0.03653
	loss_reward_4: 0.00584
	loss_policy_5: 0.04569
	accuracy_policy_5: 0.55066
	loss_value_5: 0.03771
	loss_reward_5: 0.00673
	loss_policy: 0.33834
	loss_value: 0.32975
	loss_reward: 0.02808
Optimization_Done 45200
[2024-05-07 13:55:11] [command] train weight_iter_45200.pkl 226 227
[2024-05-07 13:55:48] nn step 45300, lr: 0.043047.
	loss_policy_0: 0.20106
	accuracy_policy_0: 0.63404
	loss_value_0: 0.18182
	loss_policy_1: 0.04664
	accuracy_policy_1: 0.5825
	loss_value_1: 0.03795
	loss_reward_1: 0.0059
	loss_policy_2: 0.04955
	accuracy_policy_2: 0.55947
	loss_value_2: 0.03944
	loss_reward_2: 0.00584
	loss_policy_3: 0.05297
	accuracy_policy_3: 0.536
	loss_value_3: 0.04105
	loss_reward_3: 0.00633
	loss_policy_4: 0.05541
	accuracy_policy_4: 0.52256
	loss_value_4: 0.04235
	loss_reward_4: 0.00672
	loss_policy_5: 0.0581
	accuracy_policy_5: 0.50373
	loss_value_5: 0.04366
	loss_reward_5: 0.00785
	loss_policy: 0.46372
	loss_value: 0.38628
	loss_reward: 0.03264
[2024-05-07 13:56:25] nn step 45400, lr: 0.043047.
	loss_policy_0: 0.1672
	accuracy_policy_0: 0.69621
	loss_value_0: 0.18224
	loss_policy_1: 0.04196
	accuracy_policy_1: 0.62498
	loss_value_1: 0.03823
	loss_reward_1: 0.00586
	loss_policy_2: 0.04602
	accuracy_policy_2: 0.59303
	loss_value_2: 0.03967
	loss_reward_2: 0.00579
	loss_policy_3: 0.04971
	accuracy_policy_3: 0.57043
	loss_value_3: 0.04098
	loss_reward_3: 0.00631
	loss_policy_4: 0.05254
	accuracy_policy_4: 0.55242
	loss_value_4: 0.04242
	loss_reward_4: 0.00684
	loss_policy_5: 0.05492
	accuracy_policy_5: 0.53793
	loss_value_5: 0.04402
	loss_reward_5: 0.00785
	loss_policy: 0.41235
	loss_value: 0.38755
	loss_reward: 0.03264
Optimization_Done 45400
[2024-05-07 13:58:39] [command] train weight_iter_45400.pkl 227 228
[2024-05-07 13:59:16] nn step 45500, lr: 0.043047.
	loss_policy_0: 0.17768
	accuracy_policy_0: 0.65855
	loss_value_0: 0.19398
	loss_policy_1: 0.04534
	accuracy_policy_1: 0.57102
	loss_value_1: 0.04016
	loss_reward_1: 0.00772
	loss_policy_2: 0.04966
	accuracy_policy_2: 0.53854
	loss_value_2: 0.04165
	loss_reward_2: 0.00725
	loss_policy_3: 0.05342
	accuracy_policy_3: 0.5152
	loss_value_3: 0.04295
	loss_reward_3: 0.00825
	loss_policy_4: 0.05636
	accuracy_policy_4: 0.49039
	loss_value_4: 0.04417
	loss_reward_4: 0.00899
	loss_policy_5: 0.059
	accuracy_policy_5: 0.47379
	loss_value_5: 0.04541
	loss_reward_5: 0.01034
	loss_policy: 0.44146
	loss_value: 0.40832
	loss_reward: 0.04255
[2024-05-07 13:59:53] nn step 45600, lr: 0.043047.
	loss_policy_0: 0.13731
	accuracy_policy_0: 0.71654
	loss_value_0: 0.17814
	loss_policy_1: 0.03849
	accuracy_policy_1: 0.60883
	loss_value_1: 0.03699
	loss_reward_1: 0.00697
	loss_policy_2: 0.04312
	accuracy_policy_2: 0.57367
	loss_value_2: 0.03846
	loss_reward_2: 0.00668
	loss_policy_3: 0.04649
	accuracy_policy_3: 0.54957
	loss_value_3: 0.03969
	loss_reward_3: 0.0074
	loss_policy_4: 0.0496
	accuracy_policy_4: 0.52473
	loss_value_4: 0.04091
	loss_reward_4: 0.00803
	loss_policy_5: 0.05195
	accuracy_policy_5: 0.50645
	loss_value_5: 0.04204
	loss_reward_5: 0.0093
	loss_policy: 0.36696
	loss_value: 0.37622
	loss_reward: 0.03838
Optimization_Done 45600
[2024-05-07 14:02:05] [command] train weight_iter_45600.pkl 228 229
[2024-05-07 14:02:42] nn step 45700, lr: 0.043047.
	loss_policy_0: 0.13164
	accuracy_policy_0: 0.7
	loss_value_0: 0.16962
	loss_policy_1: 0.0357
	accuracy_policy_1: 0.60791
	loss_value_1: 0.0351
	loss_reward_1: 0.00672
	loss_policy_2: 0.0399
	accuracy_policy_2: 0.56963
	loss_value_2: 0.03624
	loss_reward_2: 0.00651
	loss_policy_3: 0.04285
	accuracy_policy_3: 0.54127
	loss_value_3: 0.03709
	loss_reward_3: 0.00721
	loss_policy_4: 0.04556
	accuracy_policy_4: 0.51969
	loss_value_4: 0.038
	loss_reward_4: 0.00776
	loss_policy_5: 0.04754
	accuracy_policy_5: 0.50207
	loss_value_5: 0.03876
	loss_reward_5: 0.00883
	loss_policy: 0.3432
	loss_value: 0.35482
	loss_reward: 0.03703
[2024-05-07 14:03:18] nn step 45800, lr: 0.043047.
	loss_policy_0: 0.10923
	accuracy_policy_0: 0.75229
	loss_value_0: 0.1643
	loss_policy_1: 0.03252
	accuracy_policy_1: 0.64588
	loss_value_1: 0.0344
	loss_reward_1: 0.00682
	loss_policy_2: 0.03675
	accuracy_policy_2: 0.60383
	loss_value_2: 0.0356
	loss_reward_2: 0.00647
	loss_policy_3: 0.03994
	accuracy_policy_3: 0.57594
	loss_value_3: 0.03665
	loss_reward_3: 0.00704
	loss_policy_4: 0.04288
	accuracy_policy_4: 0.55398
	loss_value_4: 0.0376
	loss_reward_4: 0.00767
	loss_policy_5: 0.04542
	accuracy_policy_5: 0.53434
	loss_value_5: 0.03843
	loss_reward_5: 0.00891
	loss_policy: 0.30674
	loss_value: 0.34698
	loss_reward: 0.03691
Optimization_Done 45800
[2024-05-07 14:05:34] [command] train weight_iter_45800.pkl 229 230
[2024-05-07 14:06:11] nn step 45900, lr: 0.043047.
	loss_policy_0: 0.15307
	accuracy_policy_0: 0.67064
	loss_value_0: 0.17287
	loss_policy_1: 0.03799
	accuracy_policy_1: 0.60299
	loss_value_1: 0.03608
	loss_reward_1: 0.0058
	loss_policy_2: 0.04167
	accuracy_policy_2: 0.56752
	loss_value_2: 0.0375
	loss_reward_2: 0.00592
	loss_policy_3: 0.0448
	accuracy_policy_3: 0.54316
	loss_value_3: 0.03867
	loss_reward_3: 0.00659
	loss_policy_4: 0.04778
	accuracy_policy_4: 0.5235
	loss_value_4: 0.03967
	loss_reward_4: 0.00714
	loss_policy_5: 0.0501
	accuracy_policy_5: 0.50439
	loss_value_5: 0.04052
	loss_reward_5: 0.00823
	loss_policy: 0.37541
	loss_value: 0.36531
	loss_reward: 0.03367
[2024-05-07 14:06:47] nn step 46000, lr: 0.043047.
	loss_policy_0: 0.1187
	accuracy_policy_0: 0.7374
	loss_value_0: 0.17006
	loss_policy_1: 0.03341
	accuracy_policy_1: 0.65033
	loss_value_1: 0.03567
	loss_reward_1: 0.00595
	loss_policy_2: 0.03794
	accuracy_policy_2: 0.60773
	loss_value_2: 0.037
	loss_reward_2: 0.00591
	loss_policy_3: 0.0412
	accuracy_policy_3: 0.58453
	loss_value_3: 0.03835
	loss_reward_3: 0.00659
	loss_policy_4: 0.044
	accuracy_policy_4: 0.56336
	loss_value_4: 0.03949
	loss_reward_4: 0.00708
	loss_policy_5: 0.04637
	accuracy_policy_5: 0.54615
	loss_value_5: 0.04066
	loss_reward_5: 0.00839
	loss_policy: 0.32162
	loss_value: 0.36124
	loss_reward: 0.03391
Optimization_Done 46000
[2024-05-07 14:09:03] [command] train weight_iter_46000.pkl 230 231
[2024-05-07 14:09:41] nn step 46100, lr: 0.043047.
	loss_policy_0: 0.1897
	accuracy_policy_0: 0.64133
	loss_value_0: 0.18685
	loss_policy_1: 0.04566
	accuracy_policy_1: 0.58379
	loss_value_1: 0.03903
	loss_reward_1: 0.00641
	loss_policy_2: 0.04994
	accuracy_policy_2: 0.55004
	loss_value_2: 0.04061
	loss_reward_2: 0.00646
	loss_policy_3: 0.05342
	accuracy_policy_3: 0.52742
	loss_value_3: 0.04219
	loss_reward_3: 0.00699
	loss_policy_4: 0.05647
	accuracy_policy_4: 0.50727
	loss_value_4: 0.04363
	loss_reward_4: 0.00741
	loss_policy_5: 0.05859
	accuracy_policy_5: 0.49682
	loss_value_5: 0.04509
	loss_reward_5: 0.00869
	loss_policy: 0.45378
	loss_value: 0.39738
	loss_reward: 0.03597
[2024-05-07 14:10:17] nn step 46200, lr: 0.043047.
	loss_policy_0: 0.15871
	accuracy_policy_0: 0.69285
	loss_value_0: 0.18137
	loss_policy_1: 0.04087
	accuracy_policy_1: 0.62066
	loss_value_1: 0.03776
	loss_reward_1: 0.0063
	loss_policy_2: 0.04531
	accuracy_policy_2: 0.58537
	loss_value_2: 0.03945
	loss_reward_2: 0.0064
	loss_policy_3: 0.04839
	accuracy_policy_3: 0.56301
	loss_value_3: 0.0409
	loss_reward_3: 0.00702
	loss_policy_4: 0.05175
	accuracy_policy_4: 0.5415
	loss_value_4: 0.04227
	loss_reward_4: 0.00748
	loss_policy_5: 0.05489
	accuracy_policy_5: 0.52477
	loss_value_5: 0.0437
	loss_reward_5: 0.00837
	loss_policy: 0.39991
	loss_value: 0.38545
	loss_reward: 0.03558
Optimization_Done 46200
[2024-05-07 14:12:30] [command] train weight_iter_46200.pkl 231 232
[2024-05-07 14:13:07] nn step 46300, lr: 0.043047.
	loss_policy_0: 0.15422
	accuracy_policy_0: 0.69709
	loss_value_0: 0.16056
	loss_policy_1: 0.03834
	accuracy_policy_1: 0.63141
	loss_value_1: 0.03374
	loss_reward_1: 0.00683
	loss_policy_2: 0.04176
	accuracy_policy_2: 0.60275
	loss_value_2: 0.03536
	loss_reward_2: 0.0069
	loss_policy_3: 0.04461
	accuracy_policy_3: 0.57979
	loss_value_3: 0.03672
	loss_reward_3: 0.0073
	loss_policy_4: 0.04704
	accuracy_policy_4: 0.56322
	loss_value_4: 0.03806
	loss_reward_4: 0.00772
	loss_policy_5: 0.04958
	accuracy_policy_5: 0.54232
	loss_value_5: 0.03936
	loss_reward_5: 0.00885
	loss_policy: 0.37554
	loss_value: 0.34379
	loss_reward: 0.03761
[2024-05-07 14:13:43] nn step 46400, lr: 0.043047.
	loss_policy_0: 0.13259
	accuracy_policy_0: 0.73725
	loss_value_0: 0.16464
	loss_policy_1: 0.03585
	accuracy_policy_1: 0.65566
	loss_value_1: 0.03473
	loss_reward_1: 0.00679
	loss_policy_2: 0.03991
	accuracy_policy_2: 0.62324
	loss_value_2: 0.03619
	loss_reward_2: 0.00673
	loss_policy_3: 0.04297
	accuracy_policy_3: 0.60076
	loss_value_3: 0.03764
	loss_reward_3: 0.00722
	loss_policy_4: 0.04579
	accuracy_policy_4: 0.58311
	loss_value_4: 0.03903
	loss_reward_4: 0.00763
	loss_policy_5: 0.04844
	accuracy_policy_5: 0.56045
	loss_value_5: 0.04031
	loss_reward_5: 0.00891
	loss_policy: 0.34555
	loss_value: 0.35254
	loss_reward: 0.03727
Optimization_Done 46400
[2024-05-07 14:15:44] [command] train weight_iter_46400.pkl 232 233
[2024-05-07 14:16:22] nn step 46500, lr: 0.043047.
	loss_policy_0: 0.10968
	accuracy_policy_0: 0.73586
	loss_value_0: 0.13898
	loss_policy_1: 0.02914
	accuracy_policy_1: 0.65012
	loss_value_1: 0.02912
	loss_reward_1: 0.00577
	loss_policy_2: 0.0319
	accuracy_policy_2: 0.62713
	loss_value_2: 0.03033
	loss_reward_2: 0.00559
	loss_policy_3: 0.03435
	accuracy_policy_3: 0.60299
	loss_value_3: 0.03146
	loss_reward_3: 0.00603
	loss_policy_4: 0.03639
	accuracy_policy_4: 0.58611
	loss_value_4: 0.03244
	loss_reward_4: 0.00637
	loss_policy_5: 0.03843
	accuracy_policy_5: 0.5683
	loss_value_5: 0.03341
	loss_reward_5: 0.00743
	loss_policy: 0.27989
	loss_value: 0.29574
	loss_reward: 0.03119
[2024-05-07 14:16:58] nn step 46600, lr: 0.043047.
	loss_policy_0: 0.08759
	accuracy_policy_0: 0.7884
	loss_value_0: 0.13813
	loss_policy_1: 0.0264
	accuracy_policy_1: 0.68711
	loss_value_1: 0.02901
	loss_reward_1: 0.00566
	loss_policy_2: 0.02906
	accuracy_policy_2: 0.65863
	loss_value_2: 0.03026
	loss_reward_2: 0.00557
	loss_policy_3: 0.03158
	accuracy_policy_3: 0.63932
	loss_value_3: 0.03139
	loss_reward_3: 0.00586
	loss_policy_4: 0.03373
	accuracy_policy_4: 0.61777
	loss_value_4: 0.03242
	loss_reward_4: 0.00622
	loss_policy_5: 0.03608
	accuracy_policy_5: 0.59377
	loss_value_5: 0.03352
	loss_reward_5: 0.0072
	loss_policy: 0.24443
	loss_value: 0.29473
	loss_reward: 0.03051
Optimization_Done 46600
[2024-05-07 14:19:08] [command] train weight_iter_46600.pkl 233 234
[2024-05-07 14:19:45] nn step 46700, lr: 0.043047.
	loss_policy_0: 0.13546
	accuracy_policy_0: 0.676
	loss_value_0: 0.14064
	loss_policy_1: 0.03236
	accuracy_policy_1: 0.61613
	loss_value_1: 0.02951
	loss_reward_1: 0.00488
	loss_policy_2: 0.03479
	accuracy_policy_2: 0.59102
	loss_value_2: 0.03084
	loss_reward_2: 0.00485
	loss_policy_3: 0.03716
	accuracy_policy_3: 0.57
	loss_value_3: 0.03205
	loss_reward_3: 0.00533
	loss_policy_4: 0.03919
	accuracy_policy_4: 0.54805
	loss_value_4: 0.0331
	loss_reward_4: 0.0058
	loss_policy_5: 0.04088
	accuracy_policy_5: 0.53326
	loss_value_5: 0.034
	loss_reward_5: 0.00644
	loss_policy: 0.31983
	loss_value: 0.30014
	loss_reward: 0.02732
[2024-05-07 14:20:21] nn step 46800, lr: 0.043047.
	loss_policy_0: 0.10449
	accuracy_policy_0: 0.7541
	loss_value_0: 0.14608
	loss_policy_1: 0.029
	accuracy_policy_1: 0.66277
	loss_value_1: 0.03055
	loss_reward_1: 0.00511
	loss_policy_2: 0.03234
	accuracy_policy_2: 0.63328
	loss_value_2: 0.03167
	loss_reward_2: 0.00501
	loss_policy_3: 0.03486
	accuracy_policy_3: 0.61148
	loss_value_3: 0.03283
	loss_reward_3: 0.00552
	loss_policy_4: 0.03695
	accuracy_policy_4: 0.59164
	loss_value_4: 0.03394
	loss_reward_4: 0.00606
	loss_policy_5: 0.03915
	accuracy_policy_5: 0.56908
	loss_value_5: 0.03498
	loss_reward_5: 0.00693
	loss_policy: 0.2768
	loss_value: 0.31004
	loss_reward: 0.02862
Optimization_Done 46800
[2024-05-07 14:22:40] [command] train weight_iter_46800.pkl 234 235
[2024-05-07 14:23:17] nn step 46900, lr: 0.043047.
	loss_policy_0: 0.17505
	accuracy_policy_0: 0.65236
	loss_value_0: 0.16518
	loss_policy_1: 0.04165
	accuracy_policy_1: 0.59605
	loss_value_1: 0.03482
	loss_reward_1: 0.00597
	loss_policy_2: 0.04488
	accuracy_policy_2: 0.56701
	loss_value_2: 0.03656
	loss_reward_2: 0.00612
	loss_policy_3: 0.04795
	accuracy_policy_3: 0.54311
	loss_value_3: 0.03804
	loss_reward_3: 0.00647
	loss_policy_4: 0.05039
	accuracy_policy_4: 0.52859
	loss_value_4: 0.03941
	loss_reward_4: 0.00684
	loss_policy_5: 0.05285
	accuracy_policy_5: 0.50275
	loss_value_5: 0.04067
	loss_reward_5: 0.00787
	loss_policy: 0.41278
	loss_value: 0.35467
	loss_reward: 0.03326
[2024-05-07 14:23:53] nn step 47000, lr: 0.043047.
	loss_policy_0: 0.14484
	accuracy_policy_0: 0.71422
	loss_value_0: 0.1632
	loss_policy_1: 0.03753
	accuracy_policy_1: 0.63617
	loss_value_1: 0.03464
	loss_reward_1: 0.00624
	loss_policy_2: 0.0407
	accuracy_policy_2: 0.6132
	loss_value_2: 0.03602
	loss_reward_2: 0.00608
	loss_policy_3: 0.04381
	accuracy_policy_3: 0.58826
	loss_value_3: 0.03746
	loss_reward_3: 0.00639
	loss_policy_4: 0.04675
	accuracy_policy_4: 0.56752
	loss_value_4: 0.03887
	loss_reward_4: 0.00706
	loss_policy_5: 0.04944
	accuracy_policy_5: 0.54443
	loss_value_5: 0.04016
	loss_reward_5: 0.00806
	loss_policy: 0.36308
	loss_value: 0.35036
	loss_reward: 0.03382
Optimization_Done 47000
[2024-05-07 14:26:13] [command] train weight_iter_47000.pkl 235 236
[2024-05-07 14:26:50] nn step 47100, lr: 0.043047.
	loss_policy_0: 0.16228
	accuracy_policy_0: 0.6999
	loss_value_0: 0.17713
	loss_policy_1: 0.04063
	accuracy_policy_1: 0.62777
	loss_value_1: 0.03724
	loss_reward_1: 0.00821
	loss_policy_2: 0.04409
	accuracy_policy_2: 0.60143
	loss_value_2: 0.03876
	loss_reward_2: 0.00801
	loss_policy_3: 0.04726
	accuracy_policy_3: 0.58146
	loss_value_3: 0.04014
	loss_reward_3: 0.00874
	loss_policy_4: 0.04997
	accuracy_policy_4: 0.56109
	loss_value_4: 0.04134
	loss_reward_4: 0.00925
	loss_policy_5: 0.05247
	accuracy_policy_5: 0.54201
	loss_value_5: 0.0425
	loss_reward_5: 0.01042
	loss_policy: 0.3967
	loss_value: 0.37712
	loss_reward: 0.04462
[2024-05-07 14:27:26] nn step 47200, lr: 0.043047.
	loss_policy_0: 0.14525
	accuracy_policy_0: 0.73625
	loss_value_0: 0.17592
	loss_policy_1: 0.03871
	accuracy_policy_1: 0.65361
	loss_value_1: 0.03696
	loss_reward_1: 0.008
	loss_policy_2: 0.0428
	accuracy_policy_2: 0.62486
	loss_value_2: 0.03843
	loss_reward_2: 0.00802
	loss_policy_3: 0.04634
	accuracy_policy_3: 0.59881
	loss_value_3: 0.03986
	loss_reward_3: 0.00869
	loss_policy_4: 0.04876
	accuracy_policy_4: 0.58348
	loss_value_4: 0.0412
	loss_reward_4: 0.00937
	loss_policy_5: 0.05151
	accuracy_policy_5: 0.56451
	loss_value_5: 0.04264
	loss_reward_5: 0.01082
	loss_policy: 0.37336
	loss_value: 0.375
	loss_reward: 0.0449
Optimization_Done 47200
[2024-05-07 14:29:32] [command] train weight_iter_47200.pkl 236 237
[2024-05-07 14:30:09] nn step 47300, lr: 0.043047.
	loss_policy_0: 0.13743
	accuracy_policy_0: 0.73848
	loss_value_0: 0.18227
	loss_policy_1: 0.03607
	accuracy_policy_1: 0.6585
	loss_value_1: 0.03799
	loss_reward_1: 0.00831
	loss_policy_2: 0.03925
	accuracy_policy_2: 0.63699
	loss_value_2: 0.03934
	loss_reward_2: 0.0082
	loss_policy_3: 0.04201
	accuracy_policy_3: 0.61691
	loss_value_3: 0.04056
	loss_reward_3: 0.00888
	loss_policy_4: 0.04482
	accuracy_policy_4: 0.59852
	loss_value_4: 0.0416
	loss_reward_4: 0.00956
	loss_policy_5: 0.04623
	accuracy_policy_5: 0.59254
	loss_value_5: 0.04257
	loss_reward_5: 0.01071
	loss_policy: 0.34581
	loss_value: 0.38433
	loss_reward: 0.04566
[2024-05-07 14:30:45] nn step 47400, lr: 0.043047.
	loss_policy_0: 0.11014
	accuracy_policy_0: 0.78564
	loss_value_0: 0.17472
	loss_policy_1: 0.03183
	accuracy_policy_1: 0.69092
	loss_value_1: 0.03671
	loss_reward_1: 0.00823
	loss_policy_2: 0.03545
	accuracy_policy_2: 0.6577
	loss_value_2: 0.03806
	loss_reward_2: 0.00788
	loss_policy_3: 0.03814
	accuracy_policy_3: 0.63826
	loss_value_3: 0.03929
	loss_reward_3: 0.00859
	loss_policy_4: 0.04045
	accuracy_policy_4: 0.62795
	loss_value_4: 0.04045
	loss_reward_4: 0.00933
	loss_policy_5: 0.04242
	accuracy_policy_5: 0.61533
	loss_value_5: 0.04179
	loss_reward_5: 0.01032
	loss_policy: 0.29843
	loss_value: 0.37102
	loss_reward: 0.04435
Optimization_Done 47400
[2024-05-07 14:33:01] [command] train weight_iter_47400.pkl 237 238
[2024-05-07 14:33:38] nn step 47500, lr: 0.043047.
	loss_policy_0: 0.12879
	accuracy_policy_0: 0.73465
	loss_value_0: 0.17145
	loss_policy_1: 0.03305
	accuracy_policy_1: 0.66744
	loss_value_1: 0.03578
	loss_reward_1: 0.00616
	loss_policy_2: 0.03613
	accuracy_policy_2: 0.63541
	loss_value_2: 0.03705
	loss_reward_2: 0.00616
	loss_policy_3: 0.03859
	accuracy_policy_3: 0.61719
	loss_value_3: 0.0382
	loss_reward_3: 0.00663
	loss_policy_4: 0.04058
	accuracy_policy_4: 0.59541
	loss_value_4: 0.03921
	loss_reward_4: 0.00705
	loss_policy_5: 0.04232
	accuracy_policy_5: 0.58215
	loss_value_5: 0.04011
	loss_reward_5: 0.00793
	loss_policy: 0.31947
	loss_value: 0.3618
	loss_reward: 0.03393
[2024-05-07 14:34:14] nn step 47600, lr: 0.043047.
	loss_policy_0: 0.10214
	accuracy_policy_0: 0.78139
	loss_value_0: 0.1627
	loss_policy_1: 0.02832
	accuracy_policy_1: 0.70266
	loss_value_1: 0.03396
	loss_reward_1: 0.00631
	loss_policy_2: 0.03194
	accuracy_policy_2: 0.66969
	loss_value_2: 0.03536
	loss_reward_2: 0.00601
	loss_policy_3: 0.03446
	accuracy_policy_3: 0.64723
	loss_value_3: 0.03658
	loss_reward_3: 0.00646
	loss_policy_4: 0.03696
	accuracy_policy_4: 0.62752
	loss_value_4: 0.03781
	loss_reward_4: 0.00683
	loss_policy_5: 0.03901
	accuracy_policy_5: 0.61242
	loss_value_5: 0.03893
	loss_reward_5: 0.00793
	loss_policy: 0.27284
	loss_value: 0.34533
	loss_reward: 0.03354
Optimization_Done 47600
[2024-05-07 14:36:31] [command] train weight_iter_47600.pkl 238 239
[2024-05-07 14:37:08] nn step 47700, lr: 0.043047.
	loss_policy_0: 0.15337
	accuracy_policy_0: 0.69695
	loss_value_0: 0.16731
	loss_policy_1: 0.03755
	accuracy_policy_1: 0.63781
	loss_value_1: 0.03513
	loss_reward_1: 0.00592
	loss_policy_2: 0.04025
	accuracy_policy_2: 0.61318
	loss_value_2: 0.03666
	loss_reward_2: 0.00584
	loss_policy_3: 0.0435
	accuracy_policy_3: 0.58496
	loss_value_3: 0.03799
	loss_reward_3: 0.00631
	loss_policy_4: 0.04533
	accuracy_policy_4: 0.57363
	loss_value_4: 0.0392
	loss_reward_4: 0.00677
	loss_policy_5: 0.04753
	accuracy_policy_5: 0.55434
	loss_value_5: 0.04042
	loss_reward_5: 0.00771
	loss_policy: 0.36753
	loss_value: 0.35671
	loss_reward: 0.03254
[2024-05-07 14:37:43] nn step 47800, lr: 0.043047.
	loss_policy_0: 0.12796
	accuracy_policy_0: 0.75664
	loss_value_0: 0.16841
	loss_policy_1: 0.03397
	accuracy_policy_1: 0.67969
	loss_value_1: 0.03554
	loss_reward_1: 0.00628
	loss_policy_2: 0.03812
	accuracy_policy_2: 0.64688
	loss_value_2: 0.03706
	loss_reward_2: 0.00621
	loss_policy_3: 0.04185
	accuracy_policy_3: 0.61691
	loss_value_3: 0.0384
	loss_reward_3: 0.0067
	loss_policy_4: 0.04406
	accuracy_policy_4: 0.59924
	loss_value_4: 0.03965
	loss_reward_4: 0.00703
	loss_policy_5: 0.04637
	accuracy_policy_5: 0.58486
	loss_value_5: 0.04096
	loss_reward_5: 0.00811
	loss_policy: 0.33234
	loss_value: 0.36002
	loss_reward: 0.03433
Optimization_Done 47800
[2024-05-07 14:39:52] [command] train weight_iter_47800.pkl 239 240
[2024-05-07 14:40:29] nn step 47900, lr: 0.043047.
	loss_policy_0: 0.18124
	accuracy_policy_0: 0.66377
	loss_value_0: 0.1832
	loss_policy_1: 0.04403
	accuracy_policy_1: 0.60363
	loss_value_1: 0.03851
	loss_reward_1: 0.00721
	loss_policy_2: 0.04807
	accuracy_policy_2: 0.57055
	loss_value_2: 0.04027
	loss_reward_2: 0.00693
	loss_policy_3: 0.05136
	accuracy_policy_3: 0.54658
	loss_value_3: 0.04179
	loss_reward_3: 0.00752
	loss_policy_4: 0.05397
	accuracy_policy_4: 0.5276
	loss_value_4: 0.0432
	loss_reward_4: 0.00801
	loss_policy_5: 0.05645
	accuracy_policy_5: 0.51371
	loss_value_5: 0.04429
	loss_reward_5: 0.00919
	loss_policy: 0.43512
	loss_value: 0.39126
	loss_reward: 0.03886
[2024-05-07 14:41:05] nn step 48000, lr: 0.043047.
	loss_policy_0: 0.1501
	accuracy_policy_0: 0.73084
	loss_value_0: 0.18307
	loss_policy_1: 0.03876
	accuracy_policy_1: 0.65404
	loss_value_1: 0.0384
	loss_reward_1: 0.00729
	loss_policy_2: 0.04369
	accuracy_policy_2: 0.62182
	loss_value_2: 0.04002
	loss_reward_2: 0.00701
	loss_policy_3: 0.04758
	accuracy_policy_3: 0.59346
	loss_value_3: 0.04151
	loss_reward_3: 0.00754
	loss_policy_4: 0.05051
	accuracy_policy_4: 0.57426
	loss_value_4: 0.04291
	loss_reward_4: 0.00837
	loss_policy_5: 0.05333
	accuracy_policy_5: 0.55117
	loss_value_5: 0.04437
	loss_reward_5: 0.00948
	loss_policy: 0.38396
	loss_value: 0.39028
	loss_reward: 0.03969
Optimization_Done 48000
[2024-05-07 14:43:12] [command] train weight_iter_48000.pkl 240 241
[2024-05-07 14:43:49] nn step 48100, lr: 0.043047.
	loss_policy_0: 0.1567
	accuracy_policy_0: 0.6875
	loss_value_0: 0.16358
	loss_policy_1: 0.03805
	accuracy_policy_1: 0.62725
	loss_value_1: 0.03409
	loss_reward_1: 0.00669
	loss_policy_2: 0.04146
	accuracy_policy_2: 0.59902
	loss_value_2: 0.03547
	loss_reward_2: 0.00616
	loss_policy_3: 0.04426
	accuracy_policy_3: 0.57842
	loss_value_3: 0.03674
	loss_reward_3: 0.00651
	loss_policy_4: 0.04684
	accuracy_policy_4: 0.55729
	loss_value_4: 0.0379
	loss_reward_4: 0.00716
	loss_policy_5: 0.04903
	accuracy_policy_5: 0.54361
	loss_value_5: 0.03902
	loss_reward_5: 0.00796
	loss_policy: 0.37634
	loss_value: 0.3468
	loss_reward: 0.03447
[2024-05-07 14:44:24] nn step 48200, lr: 0.043047.
	loss_policy_0: 0.13402
	accuracy_policy_0: 0.73369
	loss_value_0: 0.16663
	loss_policy_1: 0.03539
	accuracy_policy_1: 0.66252
	loss_value_1: 0.03487
	loss_reward_1: 0.00677
	loss_policy_2: 0.03952
	accuracy_policy_2: 0.62682
	loss_value_2: 0.03632
	loss_reward_2: 0.0064
	loss_policy_3: 0.04288
	accuracy_policy_3: 0.59906
	loss_value_3: 0.03763
	loss_reward_3: 0.00684
	loss_policy_4: 0.04571
	accuracy_policy_4: 0.57924
	loss_value_4: 0.03881
	loss_reward_4: 0.00745
	loss_policy_5: 0.0485
	accuracy_policy_5: 0.55939
	loss_value_5: 0.04008
	loss_reward_5: 0.00853
	loss_policy: 0.34601
	loss_value: 0.35435
	loss_reward: 0.03599
Optimization_Done 48200
[2024-05-07 14:46:39] [command] train weight_iter_48200.pkl 241 242
[2024-05-07 14:47:17] nn step 48300, lr: 0.043047.
	loss_policy_0: 0.13718
	accuracy_policy_0: 0.69531
	loss_value_0: 0.16251
	loss_policy_1: 0.03371
	accuracy_policy_1: 0.62299
	loss_value_1: 0.03358
	loss_reward_1: 0.00536
	loss_policy_2: 0.03679
	accuracy_policy_2: 0.59463
	loss_value_2: 0.03465
	loss_reward_2: 0.00518
	loss_policy_3: 0.03902
	accuracy_policy_3: 0.57352
	loss_value_3: 0.03574
	loss_reward_3: 0.00572
	loss_policy_4: 0.04093
	accuracy_policy_4: 0.54982
	loss_value_4: 0.03652
	loss_reward_4: 0.00611
	loss_policy_5: 0.04268
	accuracy_policy_5: 0.53383
	loss_value_5: 0.03734
	loss_reward_5: 0.00683
	loss_policy: 0.3303
	loss_value: 0.34034
	loss_reward: 0.02921
[2024-05-07 14:47:52] nn step 48400, lr: 0.043047.
	loss_policy_0: 0.11058
	accuracy_policy_0: 0.74539
	loss_value_0: 0.15547
	loss_policy_1: 0.02986
	accuracy_policy_1: 0.66322
	loss_value_1: 0.03246
	loss_reward_1: 0.00539
	loss_policy_2: 0.03306
	accuracy_policy_2: 0.63074
	loss_value_2: 0.03367
	loss_reward_2: 0.0051
	loss_policy_3: 0.03572
	accuracy_policy_3: 0.60992
	loss_value_3: 0.03489
	loss_reward_3: 0.00551
	loss_policy_4: 0.03776
	accuracy_policy_4: 0.59023
	loss_value_4: 0.03601
	loss_reward_4: 0.00591
	loss_policy_5: 0.03981
	accuracy_policy_5: 0.57367
	loss_value_5: 0.03704
	loss_reward_5: 0.00687
	loss_policy: 0.28679
	loss_value: 0.32954
	loss_reward: 0.02878
Optimization_Done 48400
[2024-05-07 14:50:11] [command] train weight_iter_48400.pkl 242 243
[2024-05-07 14:50:47] nn step 48500, lr: 0.043047.
	loss_policy_0: 0.16796
	accuracy_policy_0: 0.63986
	loss_value_0: 0.1537
	loss_policy_1: 0.03875
	accuracy_policy_1: 0.58711
	loss_value_1: 0.03219
	loss_reward_1: 0.00505
	loss_policy_2: 0.04168
	accuracy_policy_2: 0.55832
	loss_value_2: 0.03351
	loss_reward_2: 0.00483
	loss_policy_3: 0.04411
	accuracy_policy_3: 0.53555
	loss_value_3: 0.0346
	loss_reward_3: 0.00526
	loss_policy_4: 0.04623
	accuracy_policy_4: 0.51668
	loss_value_4: 0.03559
	loss_reward_4: 0.00567
	loss_policy_5: 0.04835
	accuracy_policy_5: 0.49797
	loss_value_5: 0.03668
	loss_reward_5: 0.0066
	loss_policy: 0.38708
	loss_value: 0.32626
	loss_reward: 0.02741
[2024-05-07 14:51:23] nn step 48600, lr: 0.043047.
	loss_policy_0: 0.13337
	accuracy_policy_0: 0.71623
	loss_value_0: 0.15452
	loss_policy_1: 0.03396
	accuracy_policy_1: 0.64363
	loss_value_1: 0.03239
	loss_reward_1: 0.00529
	loss_policy_2: 0.03807
	accuracy_policy_2: 0.60912
	loss_value_2: 0.03379
	loss_reward_2: 0.00505
	loss_policy_3: 0.04074
	accuracy_policy_3: 0.58549
	loss_value_3: 0.03511
	loss_reward_3: 0.00544
	loss_policy_4: 0.04332
	accuracy_policy_4: 0.56477
	loss_value_4: 0.03632
	loss_reward_4: 0.0059
	loss_policy_5: 0.04551
	accuracy_policy_5: 0.54475
	loss_value_5: 0.03743
	loss_reward_5: 0.00678
	loss_policy: 0.33496
	loss_value: 0.32957
	loss_reward: 0.02846
Optimization_Done 48600
[2024-05-07 14:53:39] [command] train weight_iter_48600.pkl 243 244
[2024-05-07 14:54:16] nn step 48700, lr: 0.043047.
	loss_policy_0: 0.22421
	accuracy_policy_0: 0.61293
	loss_value_0: 0.17865
	loss_policy_1: 0.05122
	accuracy_policy_1: 0.5643
	loss_value_1: 0.0375
	loss_reward_1: 0.00626
	loss_policy_2: 0.05464
	accuracy_policy_2: 0.54025
	loss_value_2: 0.03924
	loss_reward_2: 0.00602
	loss_policy_3: 0.05742
	accuracy_policy_3: 0.51799
	loss_value_3: 0.04084
	loss_reward_3: 0.00647
	loss_policy_4: 0.0602
	accuracy_policy_4: 0.50232
	loss_value_4: 0.04216
	loss_reward_4: 0.00707
	loss_policy_5: 0.06293
	accuracy_policy_5: 0.48592
	loss_value_5: 0.0435
	loss_reward_5: 0.00796
	loss_policy: 0.51062
	loss_value: 0.38189
	loss_reward: 0.03377
[2024-05-07 14:54:52] nn step 48800, lr: 0.043047.
	loss_policy_0: 0.18069
	accuracy_policy_0: 0.67395
	loss_value_0: 0.17131
	loss_policy_1: 0.04386
	accuracy_policy_1: 0.60926
	loss_value_1: 0.03589
	loss_reward_1: 0.00613
	loss_policy_2: 0.04804
	accuracy_policy_2: 0.57938
	loss_value_2: 0.03733
	loss_reward_2: 0.00574
	loss_policy_3: 0.05146
	accuracy_policy_3: 0.55434
	loss_value_3: 0.0389
	loss_reward_3: 0.00628
	loss_policy_4: 0.05426
	accuracy_policy_4: 0.53348
	loss_value_4: 0.04039
	loss_reward_4: 0.00672
	loss_policy_5: 0.05707
	accuracy_policy_5: 0.51617
	loss_value_5: 0.04161
	loss_reward_5: 0.00795
	loss_policy: 0.43538
	loss_value: 0.36542
	loss_reward: 0.03281
Optimization_Done 48800
[2024-05-07 14:56:57] [command] train weight_iter_48800.pkl 244 245
[2024-05-07 14:57:34] nn step 48900, lr: 0.043047.
	loss_policy_0: 0.18265
	accuracy_policy_0: 0.66082
	loss_value_0: 0.17709
	loss_policy_1: 0.04343
	accuracy_policy_1: 0.60875
	loss_value_1: 0.03717
	loss_reward_1: 0.0069
	loss_policy_2: 0.04698
	accuracy_policy_2: 0.5816
	loss_value_2: 0.03872
	loss_reward_2: 0.00663
	loss_policy_3: 0.05009
	accuracy_policy_3: 0.55607
	loss_value_3: 0.04022
	loss_reward_3: 0.00711
	loss_policy_4: 0.05308
	accuracy_policy_4: 0.53369
	loss_value_4: 0.04157
	loss_reward_4: 0.0077
	loss_policy_5: 0.05555
	accuracy_policy_5: 0.51773
	loss_value_5: 0.04267
	loss_reward_5: 0.00877
	loss_policy: 0.43177
	loss_value: 0.37742
	loss_reward: 0.0371
[2024-05-07 14:58:09] nn step 49000, lr: 0.043047.
	loss_policy_0: 0.15499
	accuracy_policy_0: 0.71076
	loss_value_0: 0.17695
	loss_policy_1: 0.03902
	accuracy_policy_1: 0.64598
	loss_value_1: 0.0371
	loss_reward_1: 0.00695
	loss_policy_2: 0.04284
	accuracy_policy_2: 0.61428
	loss_value_2: 0.03866
	loss_reward_2: 0.00658
	loss_policy_3: 0.04631
	accuracy_policy_3: 0.58918
	loss_value_3: 0.04018
	loss_reward_3: 0.00704
	loss_policy_4: 0.04936
	accuracy_policy_4: 0.56838
	loss_value_4: 0.04156
	loss_reward_4: 0.00762
	loss_policy_5: 0.05236
	accuracy_policy_5: 0.54551
	loss_value_5: 0.04302
	loss_reward_5: 0.00888
	loss_policy: 0.38488
	loss_value: 0.37747
	loss_reward: 0.03707
Optimization_Done 49000
[2024-05-07 15:00:18] [command] train weight_iter_49000.pkl 245 246
[2024-05-07 15:00:55] nn step 49100, lr: 0.043047.
	loss_policy_0: 0.15501
	accuracy_policy_0: 0.65588
	loss_value_0: 0.16408
	loss_policy_1: 0.03725
	accuracy_policy_1: 0.59791
	loss_value_1: 0.03411
	loss_reward_1: 0.00615
	loss_policy_2: 0.04055
	accuracy_policy_2: 0.56283
	loss_value_2: 0.0354
	loss_reward_2: 0.0059
	loss_policy_3: 0.0432
	accuracy_policy_3: 0.5392
	loss_value_3: 0.03654
	loss_reward_3: 0.00647
	loss_policy_4: 0.04527
	accuracy_policy_4: 0.51916
	loss_value_4: 0.03751
	loss_reward_4: 0.00671
	loss_policy_5: 0.04778
	accuracy_policy_5: 0.49988
	loss_value_5: 0.03857
	loss_reward_5: 0.00776
	loss_policy: 0.36906
	loss_value: 0.34621
	loss_reward: 0.033
[2024-05-07 15:01:30] nn step 49200, lr: 0.043047.
	loss_policy_0: 0.12723
	accuracy_policy_0: 0.71973
	loss_value_0: 0.16098
	loss_policy_1: 0.0334
	accuracy_policy_1: 0.64164
	loss_value_1: 0.03366
	loss_reward_1: 0.00625
	loss_policy_2: 0.03693
	accuracy_policy_2: 0.60953
	loss_value_2: 0.03501
	loss_reward_2: 0.00573
	loss_policy_3: 0.03956
	accuracy_policy_3: 0.58389
	loss_value_3: 0.03617
	loss_reward_3: 0.00629
	loss_policy_4: 0.04192
	accuracy_policy_4: 0.56414
	loss_value_4: 0.03736
	loss_reward_4: 0.00669
	loss_policy_5: 0.04466
	accuracy_policy_5: 0.54127
	loss_value_5: 0.03847
	loss_reward_5: 0.00784
	loss_policy: 0.3237
	loss_value: 0.34164
	loss_reward: 0.0328
Optimization_Done 49200
[2024-05-07 15:03:53] [command] train weight_iter_49200.pkl 246 247
[2024-05-07 15:04:30] nn step 49300, lr: 0.043047.
	loss_policy_0: 0.13762
	accuracy_policy_0: 0.68785
	loss_value_0: 0.16379
	loss_policy_1: 0.03341
	accuracy_policy_1: 0.62588
	loss_value_1: 0.03397
	loss_reward_1: 0.00512
	loss_policy_2: 0.03656
	accuracy_policy_2: 0.59779
	loss_value_2: 0.03518
	loss_reward_2: 0.00503
	loss_policy_3: 0.03954
	accuracy_policy_3: 0.56943
	loss_value_3: 0.03633
	loss_reward_3: 0.00542
	loss_policy_4: 0.04149
	accuracy_policy_4: 0.55027
	loss_value_4: 0.03747
	loss_reward_4: 0.00591
	loss_policy_5: 0.04349
	accuracy_policy_5: 0.52906
	loss_value_5: 0.0384
	loss_reward_5: 0.0067
	loss_policy: 0.3321
	loss_value: 0.34514
	loss_reward: 0.02818
[2024-05-07 15:05:05] nn step 49400, lr: 0.043047.
	loss_policy_0: 0.1078
	accuracy_policy_0: 0.75467
	loss_value_0: 0.16254
	loss_policy_1: 0.02939
	accuracy_policy_1: 0.67514
	loss_value_1: 0.03414
	loss_reward_1: 0.00534
	loss_policy_2: 0.03275
	accuracy_policy_2: 0.64191
	loss_value_2: 0.03538
	loss_reward_2: 0.00519
	loss_policy_3: 0.03548
	accuracy_policy_3: 0.61697
	loss_value_3: 0.03646
	loss_reward_3: 0.0057
	loss_policy_4: 0.03809
	accuracy_policy_4: 0.59242
	loss_value_4: 0.03771
	loss_reward_4: 0.00604
	loss_policy_5: 0.04008
	accuracy_policy_5: 0.57225
	loss_value_5: 0.03871
	loss_reward_5: 0.00695
	loss_policy: 0.28359
	loss_value: 0.34494
	loss_reward: 0.02922
Optimization_Done 49400
[2024-05-07 15:07:23] [command] train weight_iter_49400.pkl 247 248
[2024-05-07 15:07:59] nn step 49500, lr: 0.043047.
	loss_policy_0: 0.16902
	accuracy_policy_0: 0.68537
	loss_value_0: 0.18661
	loss_policy_1: 0.03974
	accuracy_policy_1: 0.6358
	loss_value_1: 0.039
	loss_reward_1: 0.00609
	loss_policy_2: 0.04356
	accuracy_policy_2: 0.60217
	loss_value_2: 0.04069
	loss_reward_2: 0.00619
	loss_policy_3: 0.04685
	accuracy_policy_3: 0.57451
	loss_value_3: 0.04214
	loss_reward_3: 0.00664
	loss_policy_4: 0.04931
	accuracy_policy_4: 0.5583
	loss_value_4: 0.0433
	loss_reward_4: 0.0069
	loss_policy_5: 0.05156
	accuracy_policy_5: 0.54127
	loss_value_5: 0.04463
	loss_reward_5: 0.00773
	loss_policy: 0.40004
	loss_value: 0.39637
	loss_reward: 0.03355
[2024-05-07 15:08:35] nn step 49600, lr: 0.043047.
	loss_policy_0: 0.12948
	accuracy_policy_0: 0.74498
	loss_value_0: 0.17794
	loss_policy_1: 0.03354
	accuracy_policy_1: 0.67748
	loss_value_1: 0.03712
	loss_reward_1: 0.00608
	loss_policy_2: 0.0375
	accuracy_policy_2: 0.6432
	loss_value_2: 0.03868
	loss_reward_2: 0.00587
	loss_policy_3: 0.04073
	accuracy_policy_3: 0.6193
	loss_value_3: 0.04011
	loss_reward_3: 0.00639
	loss_policy_4: 0.04371
	accuracy_policy_4: 0.59695
	loss_value_4: 0.04137
	loss_reward_4: 0.00673
	loss_policy_5: 0.04593
	accuracy_policy_5: 0.57521
	loss_value_5: 0.04251
	loss_reward_5: 0.0076
	loss_policy: 0.33089
	loss_value: 0.37773
	loss_reward: 0.03268
Optimization_Done 49600
[2024-05-07 15:10:57] [command] train weight_iter_49600.pkl 248 249
[2024-05-07 15:11:35] nn step 49700, lr: 0.043047.
	loss_policy_0: 0.16517
	accuracy_policy_0: 0.69018
	loss_value_0: 0.18051
	loss_policy_1: 0.04105
	accuracy_policy_1: 0.6252
	loss_value_1: 0.03781
	loss_reward_1: 0.00759
	loss_policy_2: 0.04514
	accuracy_policy_2: 0.59494
	loss_value_2: 0.03945
	loss_reward_2: 0.00726
	loss_policy_3: 0.04826
	accuracy_policy_3: 0.57174
	loss_value_3: 0.04092
	loss_reward_3: 0.00782
	loss_policy_4: 0.05053
	accuracy_policy_4: 0.55504
	loss_value_4: 0.04226
	loss_reward_4: 0.00858
	loss_policy_5: 0.05357
	accuracy_policy_5: 0.53047
	loss_value_5: 0.04369
	loss_reward_5: 0.00965
	loss_policy: 0.40372
	loss_value: 0.38464
	loss_reward: 0.04091
[2024-05-07 15:12:11] nn step 49800, lr: 0.043047.
	loss_policy_0: 0.1415
	accuracy_policy_0: 0.7332
	loss_value_0: 0.1806
	loss_policy_1: 0.0374
	accuracy_policy_1: 0.65615
	loss_value_1: 0.03768
	loss_reward_1: 0.00744
	loss_policy_2: 0.04178
	accuracy_policy_2: 0.62262
	loss_value_2: 0.03923
	loss_reward_2: 0.00724
	loss_policy_3: 0.04516
	accuracy_policy_3: 0.60061
	loss_value_3: 0.04067
	loss_reward_3: 0.00769
	loss_policy_4: 0.04801
	accuracy_policy_4: 0.57918
	loss_value_4: 0.04195
	loss_reward_4: 0.00822
	loss_policy_5: 0.05086
	accuracy_policy_5: 0.56047
	loss_value_5: 0.04342
	loss_reward_5: 0.00953
	loss_policy: 0.36471
	loss_value: 0.38354
	loss_reward: 0.04011
Optimization_Done 49800
[2024-05-07 15:14:30] [command] train weight_iter_49800.pkl 249 250
[2024-05-07 15:15:07] nn step 49900, lr: 0.043047.
	loss_policy_0: 0.14105
	accuracy_policy_0: 0.68977
	loss_value_0: 0.15773
	loss_policy_1: 0.03528
	accuracy_policy_1: 0.62539
	loss_value_1: 0.0329
	loss_reward_1: 0.00652
	loss_policy_2: 0.03849
	accuracy_policy_2: 0.5967
	loss_value_2: 0.03423
	loss_reward_2: 0.00638
	loss_policy_3: 0.04139
	accuracy_policy_3: 0.57035
	loss_value_3: 0.03537
	loss_reward_3: 0.00702
	loss_policy_4: 0.04366
	accuracy_policy_4: 0.54982
	loss_value_4: 0.03649
	loss_reward_4: 0.00748
	loss_policy_5: 0.04574
	accuracy_policy_5: 0.53314
	loss_value_5: 0.03763
	loss_reward_5: 0.00849
	loss_policy: 0.34561
	loss_value: 0.33435
	loss_reward: 0.03589
[2024-05-07 15:15:42] nn step 50000, lr: 0.043047.
	loss_policy_0: 0.11997
	accuracy_policy_0: 0.74709
	loss_value_0: 0.16194
	loss_policy_1: 0.03348
	accuracy_policy_1: 0.65562
	loss_value_1: 0.03391
	loss_reward_1: 0.00692
	loss_policy_2: 0.03694
	accuracy_policy_2: 0.62957
	loss_value_2: 0.03517
	loss_reward_2: 0.00667
	loss_policy_3: 0.03998
	accuracy_policy_3: 0.6032
	loss_value_3: 0.03641
	loss_reward_3: 0.00738
	loss_policy_4: 0.04242
	accuracy_policy_4: 0.58564
	loss_value_4: 0.03755
	loss_reward_4: 0.00781
	loss_policy_5: 0.04482
	accuracy_policy_5: 0.56545
	loss_value_5: 0.03877
	loss_reward_5: 0.00901
	loss_policy: 0.31761
	loss_value: 0.34375
	loss_reward: 0.0378
Optimization_Done 50000
[2024-05-07 15:17:49] [command] train weight_iter_50000.pkl 250 251
[2024-05-07 15:18:26] nn step 50100, lr: 0.043047.
	loss_policy_0: 0.13208
	accuracy_policy_0: 0.69322
	loss_value_0: 0.16008
	loss_policy_1: 0.03287
	accuracy_policy_1: 0.62418
	loss_value_1: 0.03341
	loss_reward_1: 0.00533
	loss_policy_2: 0.03569
	accuracy_policy_2: 0.59621
	loss_value_2: 0.03457
	loss_reward_2: 0.00539
	loss_policy_3: 0.03804
	accuracy_policy_3: 0.57113
	loss_value_3: 0.03556
	loss_reward_3: 0.00573
	loss_policy_4: 0.04027
	accuracy_policy_4: 0.55186
	loss_value_4: 0.0365
	loss_reward_4: 0.00616
	loss_policy_5: 0.04225
	accuracy_policy_5: 0.52902
	loss_value_5: 0.03721
	loss_reward_5: 0.00695
	loss_policy: 0.3212
	loss_value: 0.33733
	loss_reward: 0.02956
[2024-05-07 15:19:02] nn step 50200, lr: 0.043047.
	loss_policy_0: 0.10284
	accuracy_policy_0: 0.75193
	loss_value_0: 0.15714
	loss_policy_1: 0.02868
	accuracy_policy_1: 0.66244
	loss_value_1: 0.03278
	loss_reward_1: 0.00544
	loss_policy_2: 0.03191
	accuracy_policy_2: 0.63656
	loss_value_2: 0.03399
	loss_reward_2: 0.00527
	loss_policy_3: 0.03459
	accuracy_policy_3: 0.60951
	loss_value_3: 0.03512
	loss_reward_3: 0.00576
	loss_policy_4: 0.03705
	accuracy_policy_4: 0.58939
	loss_value_4: 0.03618
	loss_reward_4: 0.00617
	loss_policy_5: 0.03919
	accuracy_policy_5: 0.56717
	loss_value_5: 0.03721
	loss_reward_5: 0.00699
	loss_policy: 0.27424
	loss_value: 0.33242
	loss_reward: 0.02964
Optimization_Done 50200
[2024-05-07 15:21:11] [command] train weight_iter_50200.pkl 251 252
[2024-05-07 15:21:48] nn step 50300, lr: 0.043047.
	loss_policy_0: 0.16167
	accuracy_policy_0: 0.66447
	loss_value_0: 0.16955
	loss_policy_1: 0.03906
	accuracy_policy_1: 0.60076
	loss_value_1: 0.03545
	loss_reward_1: 0.00614
	loss_policy_2: 0.04222
	accuracy_policy_2: 0.56875
	loss_value_2: 0.03676
	loss_reward_2: 0.00589
	loss_policy_3: 0.04492
	accuracy_policy_3: 0.55164
	loss_value_3: 0.03803
	loss_reward_3: 0.00634
	loss_policy_4: 0.0476
	accuracy_policy_4: 0.52904
	loss_value_4: 0.03924
	loss_reward_4: 0.0068
	loss_policy_5: 0.04977
	accuracy_policy_5: 0.51053
	loss_value_5: 0.04039
	loss_reward_5: 0.00759
	loss_policy: 0.38523
	loss_value: 0.35942
	loss_reward: 0.03276
[2024-05-07 15:22:23] nn step 50400, lr: 0.043047.
	loss_policy_0: 0.1323
	accuracy_policy_0: 0.72596
	loss_value_0: 0.16968
	loss_policy_1: 0.03499
	accuracy_policy_1: 0.64646
	loss_value_1: 0.03575
	loss_reward_1: 0.00624
	loss_policy_2: 0.03861
	accuracy_policy_2: 0.61973
	loss_value_2: 0.03707
	loss_reward_2: 0.00603
	loss_policy_3: 0.04161
	accuracy_policy_3: 0.59545
	loss_value_3: 0.03835
	loss_reward_3: 0.00643
	loss_policy_4: 0.04449
	accuracy_policy_4: 0.57213
	loss_value_4: 0.03975
	loss_reward_4: 0.00682
	loss_policy_5: 0.04734
	accuracy_policy_5: 0.55031
	loss_value_5: 0.04094
	loss_reward_5: 0.00786
	loss_policy: 0.33933
	loss_value: 0.36155
	loss_reward: 0.03338
Optimization_Done 50400
[2024-05-07 15:24:31] [command] train weight_iter_50400.pkl 252 253
[2024-05-07 15:25:08] nn step 50500, lr: 0.043047.
	loss_policy_0: 0.18223
	accuracy_policy_0: 0.66562
	loss_value_0: 0.18513
	loss_policy_1: 0.04321
	accuracy_policy_1: 0.61266
	loss_value_1: 0.03882
	loss_reward_1: 0.00696
	loss_policy_2: 0.04699
	accuracy_policy_2: 0.58426
	loss_value_2: 0.04041
	loss_reward_2: 0.00684
	loss_policy_3: 0.05096
	accuracy_policy_3: 0.55902
	loss_value_3: 0.04193
	loss_reward_3: 0.00736
	loss_policy_4: 0.05434
	accuracy_policy_4: 0.534
	loss_value_4: 0.04333
	loss_reward_4: 0.00779
	loss_policy_5: 0.05735
	accuracy_policy_5: 0.51031
	loss_value_5: 0.04472
	loss_reward_5: 0.00888
	loss_policy: 0.43507
	loss_value: 0.39435
	loss_reward: 0.03782
[2024-05-07 15:25:44] nn step 50600, lr: 0.043047.
	loss_policy_0: 0.14606
	accuracy_policy_0: 0.7176
	loss_value_0: 0.17969
	loss_policy_1: 0.03755
	accuracy_policy_1: 0.65209
	loss_value_1: 0.03779
	loss_reward_1: 0.00694
	loss_policy_2: 0.04221
	accuracy_policy_2: 0.61609
	loss_value_2: 0.0394
	loss_reward_2: 0.00681
	loss_policy_3: 0.04588
	accuracy_policy_3: 0.58914
	loss_value_3: 0.04086
	loss_reward_3: 0.00702
	loss_policy_4: 0.04975
	accuracy_policy_4: 0.56129
	loss_value_4: 0.04233
	loss_reward_4: 0.00762
	loss_policy_5: 0.05265
	accuracy_policy_5: 0.54121
	loss_value_5: 0.04375
	loss_reward_5: 0.00888
	loss_policy: 0.3741
	loss_value: 0.38381
	loss_reward: 0.03727
Optimization_Done 50600
[2024-05-07 15:27:59] [command] train weight_iter_50600.pkl 253 254
[2024-05-07 15:28:37] nn step 50700, lr: 0.043047.
	loss_policy_0: 0.12455
	accuracy_policy_0: 0.69613
	loss_value_0: 0.1459
	loss_policy_1: 0.03116
	accuracy_policy_1: 0.63631
	loss_value_1: 0.03056
	loss_reward_1: 0.00584
	loss_policy_2: 0.03446
	accuracy_policy_2: 0.60643
	loss_value_2: 0.03193
	loss_reward_2: 0.00556
	loss_policy_3: 0.03717
	accuracy_policy_3: 0.58285
	loss_value_3: 0.03312
	loss_reward_3: 0.00605
	loss_policy_4: 0.03949
	accuracy_policy_4: 0.55859
	loss_value_4: 0.0342
	loss_reward_4: 0.00663
	loss_policy_5: 0.04185
	accuracy_policy_5: 0.53902
	loss_value_5: 0.03525
	loss_reward_5: 0.00767
	loss_policy: 0.30869
	loss_value: 0.31096
	loss_reward: 0.03175
[2024-05-07 15:29:12] nn step 50800, lr: 0.043047.
	loss_policy_0: 0.10398
	accuracy_policy_0: 0.7541
	loss_value_0: 0.14713
	loss_policy_1: 0.02842
	accuracy_policy_1: 0.67299
	loss_value_1: 0.03089
	loss_reward_1: 0.00586
	loss_policy_2: 0.032
	accuracy_policy_2: 0.64123
	loss_value_2: 0.0321
	loss_reward_2: 0.00553
	loss_policy_3: 0.03483
	accuracy_policy_3: 0.61318
	loss_value_3: 0.03335
	loss_reward_3: 0.00617
	loss_policy_4: 0.03763
	accuracy_policy_4: 0.59484
	loss_value_4: 0.03444
	loss_reward_4: 0.00652
	loss_policy_5: 0.04045
	accuracy_policy_5: 0.56857
	loss_value_5: 0.03553
	loss_reward_5: 0.00755
	loss_policy: 0.2773
	loss_value: 0.31345
	loss_reward: 0.03163
Optimization_Done 50800
[2024-05-07 15:31:31] [command] train weight_iter_50800.pkl 254 255
[2024-05-07 15:32:08] nn step 50900, lr: 0.043047.
	loss_policy_0: 0.1119
	accuracy_policy_0: 0.70465
	loss_value_0: 0.13695
	loss_policy_1: 0.02767
	accuracy_policy_1: 0.64568
	loss_value_1: 0.02877
	loss_reward_1: 0.00458
	loss_policy_2: 0.03042
	accuracy_policy_2: 0.61865
	loss_value_2: 0.02994
	loss_reward_2: 0.00436
	loss_policy_3: 0.03239
	accuracy_policy_3: 0.5959
	loss_value_3: 0.03093
	loss_reward_3: 0.00484
	loss_policy_4: 0.03467
	accuracy_policy_4: 0.57336
	loss_value_4: 0.03184
	loss_reward_4: 0.00523
	loss_policy_5: 0.03634
	accuracy_policy_5: 0.55709
	loss_value_5: 0.03281
	loss_reward_5: 0.00577
	loss_policy: 0.2734
	loss_value: 0.29125
	loss_reward: 0.02478
[2024-05-07 15:32:44] nn step 51000, lr: 0.043047.
	loss_policy_0: 0.08664
	accuracy_policy_0: 0.78312
	loss_value_0: 0.14409
	loss_policy_1: 0.02472
	accuracy_policy_1: 0.69311
	loss_value_1: 0.03018
	loss_reward_1: 0.00478
	loss_policy_2: 0.02787
	accuracy_policy_2: 0.66211
	loss_value_2: 0.03123
	loss_reward_2: 0.00454
	loss_policy_3: 0.03027
	accuracy_policy_3: 0.63904
	loss_value_3: 0.03204
	loss_reward_3: 0.00493
	loss_policy_4: 0.03247
	accuracy_policy_4: 0.62113
	loss_value_4: 0.03292
	loss_reward_4: 0.00545
	loss_policy_5: 0.0343
	accuracy_policy_5: 0.60371
	loss_value_5: 0.03403
	loss_reward_5: 0.00618
	loss_policy: 0.23627
	loss_value: 0.30448
	loss_reward: 0.02588
Optimization_Done 51000
[2024-05-07 15:35:01] [command] train weight_iter_51000.pkl 255 256
[2024-05-07 15:35:38] nn step 51100, lr: 0.043047.
	loss_policy_0: 0.15706
	accuracy_policy_0: 0.63625
	loss_value_0: 0.14936
	loss_policy_1: 0.03638
	accuracy_policy_1: 0.58357
	loss_value_1: 0.0311
	loss_reward_1: 0.00473
	loss_policy_2: 0.03888
	accuracy_policy_2: 0.55529
	loss_value_2: 0.03228
	loss_reward_2: 0.00464
	loss_policy_3: 0.04104
	accuracy_policy_3: 0.53619
	loss_value_3: 0.03328
	loss_reward_3: 0.00502
	loss_policy_4: 0.0428
	accuracy_policy_4: 0.51598
	loss_value_4: 0.03422
	loss_reward_4: 0.00533
	loss_policy_5: 0.04484
	accuracy_policy_5: 0.49652
	loss_value_5: 0.03505
	loss_reward_5: 0.00602
	loss_policy: 0.361
	loss_value: 0.31529
	loss_reward: 0.02574
[2024-05-07 15:36:14] nn step 51200, lr: 0.043047.
	loss_policy_0: 0.1332
	accuracy_policy_0: 0.70547
	loss_value_0: 0.15731
	loss_policy_1: 0.03355
	accuracy_policy_1: 0.63455
	loss_value_1: 0.03292
	loss_reward_1: 0.00499
	loss_policy_2: 0.03682
	accuracy_policy_2: 0.60607
	loss_value_2: 0.0342
	loss_reward_2: 0.00492
	loss_policy_3: 0.03966
	accuracy_policy_3: 0.57865
	loss_value_3: 0.03516
	loss_reward_3: 0.00527
	loss_policy_4: 0.04165
	accuracy_policy_4: 0.56389
	loss_value_4: 0.03622
	loss_reward_4: 0.00562
	loss_policy_5: 0.0442
	accuracy_policy_5: 0.54061
	loss_value_5: 0.03748
	loss_reward_5: 0.00645
	loss_policy: 0.32908
	loss_value: 0.33329
	loss_reward: 0.02725
Optimization_Done 51200
[2024-05-07 15:38:19] [command] train weight_iter_51200.pkl 256 257
[2024-05-07 15:38:57] nn step 51300, lr: 0.043047.
	loss_policy_0: 0.19974
	accuracy_policy_0: 0.6473
	loss_value_0: 0.19633
	loss_policy_1: 0.0461
	accuracy_policy_1: 0.59803
	loss_value_1: 0.04098
	loss_reward_1: 0.0067
	loss_policy_2: 0.04931
	accuracy_policy_2: 0.57291
	loss_value_2: 0.04265
	loss_reward_2: 0.00654
	loss_policy_3: 0.05236
	accuracy_policy_3: 0.55312
	loss_value_3: 0.04405
	loss_reward_3: 0.00696
	loss_policy_4: 0.05514
	accuracy_policy_4: 0.5351
	loss_value_4: 0.04554
	loss_reward_4: 0.00738
	loss_policy_5: 0.05732
	accuracy_policy_5: 0.52121
	loss_value_5: 0.04679
	loss_reward_5: 0.00847
	loss_policy: 0.45997
	loss_value: 0.41634
	loss_reward: 0.03604
[2024-05-07 15:39:33] nn step 51400, lr: 0.043047.
	loss_policy_0: 0.15975
	accuracy_policy_0: 0.70062
	loss_value_0: 0.18066
	loss_policy_1: 0.03885
	accuracy_policy_1: 0.63975
	loss_value_1: 0.03765
	loss_reward_1: 0.00644
	loss_policy_2: 0.04285
	accuracy_policy_2: 0.61057
	loss_value_2: 0.03914
	loss_reward_2: 0.00612
	loss_policy_3: 0.04568
	accuracy_policy_3: 0.58467
	loss_value_3: 0.04068
	loss_reward_3: 0.00647
	loss_policy_4: 0.04812
	accuracy_policy_4: 0.57275
	loss_value_4: 0.04227
	loss_reward_4: 0.00691
	loss_policy_5: 0.05108
	accuracy_policy_5: 0.54799
	loss_value_5: 0.04361
	loss_reward_5: 0.00815
	loss_policy: 0.38633
	loss_value: 0.38401
	loss_reward: 0.03409
Optimization_Done 51400
[2024-05-07 15:41:49] [command] train weight_iter_51400.pkl 257 258
[2024-05-07 15:42:26] nn step 51500, lr: 0.043047.
	loss_policy_0: 0.15629
	accuracy_policy_0: 0.69656
	loss_value_0: 0.18658
	loss_policy_1: 0.03861
	accuracy_policy_1: 0.62836
	loss_value_1: 0.03891
	loss_reward_1: 0.00695
	loss_policy_2: 0.04178
	accuracy_policy_2: 0.60535
	loss_value_2: 0.04048
	loss_reward_2: 0.00652
	loss_policy_3: 0.04441
	accuracy_policy_3: 0.58275
	loss_value_3: 0.04183
	loss_reward_3: 0.00728
	loss_policy_4: 0.04749
	accuracy_policy_4: 0.56016
	loss_value_4: 0.04325
	loss_reward_4: 0.00771
	loss_policy_5: 0.04952
	accuracy_policy_5: 0.54229
	loss_value_5: 0.04442
	loss_reward_5: 0.00864
	loss_policy: 0.37809
	loss_value: 0.39547
	loss_reward: 0.03711
[2024-05-07 15:43:02] nn step 51600, lr: 0.043047.
	loss_policy_0: 0.14189
	accuracy_policy_0: 0.73951
	loss_value_0: 0.1936
	loss_policy_1: 0.03666
	accuracy_policy_1: 0.66611
	loss_value_1: 0.04038
	loss_reward_1: 0.00719
	loss_policy_2: 0.04093
	accuracy_policy_2: 0.63088
	loss_value_2: 0.04209
	loss_reward_2: 0.00683
	loss_policy_3: 0.04407
	accuracy_policy_3: 0.60775
	loss_value_3: 0.04359
	loss_reward_3: 0.00766
	loss_policy_4: 0.04668
	accuracy_policy_4: 0.59152
	loss_value_4: 0.04499
	loss_reward_4: 0.00775
	loss_policy_5: 0.04947
	accuracy_policy_5: 0.5682
	loss_value_5: 0.04629
	loss_reward_5: 0.00928
	loss_policy: 0.35969
	loss_value: 0.41094
	loss_reward: 0.03872
Optimization_Done 51600
[2024-05-07 15:45:07] [command] train weight_iter_51600.pkl 258 259
[2024-05-07 15:45:44] nn step 51700, lr: 0.043047.
	loss_policy_0: 0.1492
	accuracy_policy_0: 0.67898
	loss_value_0: 0.17495
	loss_policy_1: 0.0369
	accuracy_policy_1: 0.61037
	loss_value_1: 0.0365
	loss_reward_1: 0.00651
	loss_policy_2: 0.04022
	accuracy_policy_2: 0.57709
	loss_value_2: 0.03776
	loss_reward_2: 0.00632
	loss_policy_3: 0.04256
	accuracy_policy_3: 0.55375
	loss_value_3: 0.03885
	loss_reward_3: 0.00695
	loss_policy_4: 0.04476
	accuracy_policy_4: 0.53113
	loss_value_4: 0.03979
	loss_reward_4: 0.00759
	loss_policy_5: 0.047
	accuracy_policy_5: 0.50846
	loss_value_5: 0.04063
	loss_reward_5: 0.00846
	loss_policy: 0.36064
	loss_value: 0.36848
	loss_reward: 0.03583
[2024-05-07 15:46:20] nn step 51800, lr: 0.043047.
	loss_policy_0: 0.12876
	accuracy_policy_0: 0.73102
	loss_value_0: 0.17804
	loss_policy_1: 0.03426
	accuracy_policy_1: 0.6524
	loss_value_1: 0.03718
	loss_reward_1: 0.00685
	loss_policy_2: 0.0379
	accuracy_policy_2: 0.6174
	loss_value_2: 0.03835
	loss_reward_2: 0.00656
	loss_policy_3: 0.04039
	accuracy_policy_3: 0.59486
	loss_value_3: 0.03945
	loss_reward_3: 0.00706
	loss_policy_4: 0.04368
	accuracy_policy_4: 0.56418
	loss_value_4: 0.04065
	loss_reward_4: 0.00771
	loss_policy_5: 0.04603
	accuracy_policy_5: 0.54291
	loss_value_5: 0.0419
	loss_reward_5: 0.00884
	loss_policy: 0.33101
	loss_value: 0.37557
	loss_reward: 0.03702
Optimization_Done 51800
[2024-05-07 15:48:39] [command] train weight_iter_51800.pkl 259 260
[2024-05-07 15:49:15] nn step 51900, lr: 0.043047.
	loss_policy_0: 0.14428
	accuracy_policy_0: 0.70025
	loss_value_0: 0.17613
	loss_policy_1: 0.03697
	accuracy_policy_1: 0.63008
	loss_value_1: 0.03681
	loss_reward_1: 0.00607
	loss_policy_2: 0.04062
	accuracy_policy_2: 0.59717
	loss_value_2: 0.03824
	loss_reward_2: 0.00622
	loss_policy_3: 0.04337
	accuracy_policy_3: 0.57342
	loss_value_3: 0.03949
	loss_reward_3: 0.0065
	loss_policy_4: 0.046
	accuracy_policy_4: 0.55004
	loss_value_4: 0.0406
	loss_reward_4: 0.00703
	loss_policy_5: 0.04836
	accuracy_policy_5: 0.53416
	loss_value_5: 0.0417
	loss_reward_5: 0.00811
	loss_policy: 0.3596
	loss_value: 0.37297
	loss_reward: 0.03393
[2024-05-07 15:49:51] nn step 52000, lr: 0.043047.
	loss_policy_0: 0.11506
	accuracy_policy_0: 0.7501
	loss_value_0: 0.16985
	loss_policy_1: 0.03207
	accuracy_policy_1: 0.66629
	loss_value_1: 0.03558
	loss_reward_1: 0.00612
	loss_policy_2: 0.03638
	accuracy_policy_2: 0.62906
	loss_value_2: 0.03695
	loss_reward_2: 0.00595
	loss_policy_3: 0.03938
	accuracy_policy_3: 0.60682
	loss_value_3: 0.03846
	loss_reward_3: 0.00646
	loss_policy_4: 0.04188
	accuracy_policy_4: 0.58533
	loss_value_4: 0.0395
	loss_reward_4: 0.007
	loss_policy_5: 0.04444
	accuracy_policy_5: 0.56348
	loss_value_5: 0.04075
	loss_reward_5: 0.00799
	loss_policy: 0.3092
	loss_value: 0.36109
	loss_reward: 0.03353
Optimization_Done 52000
[2024-05-07 15:51:55] [command] train weight_iter_52000.pkl 260 261
[2024-05-07 15:52:32] nn step 52100, lr: 0.043047.
	loss_policy_0: 0.17822
	accuracy_policy_0: 0.66578
	loss_value_0: 0.18897
	loss_policy_1: 0.04227
	accuracy_policy_1: 0.61289
	loss_value_1: 0.03954
	loss_reward_1: 0.00632
	loss_policy_2: 0.04611
	accuracy_policy_2: 0.58381
	loss_value_2: 0.04114
	loss_reward_2: 0.00598
	loss_policy_3: 0.04915
	accuracy_policy_3: 0.56123
	loss_value_3: 0.04264
	loss_reward_3: 0.00656
	loss_policy_4: 0.05184
	accuracy_policy_4: 0.54217
	loss_value_4: 0.04419
	loss_reward_4: 0.007
	loss_policy_5: 0.05445
	accuracy_policy_5: 0.5242
	loss_value_5: 0.04565
	loss_reward_5: 0.00771
	loss_policy: 0.42203
	loss_value: 0.40213
	loss_reward: 0.03356
[2024-05-07 15:53:07] nn step 52200, lr: 0.043047.
	loss_policy_0: 0.13689
	accuracy_policy_0: 0.72459
	loss_value_0: 0.17836
	loss_policy_1: 0.03551
	accuracy_policy_1: 0.65508
	loss_value_1: 0.03737
	loss_reward_1: 0.00577
	loss_policy_2: 0.03975
	accuracy_policy_2: 0.61662
	loss_value_2: 0.03895
	loss_reward_2: 0.00544
	loss_policy_3: 0.04285
	accuracy_policy_3: 0.59439
	loss_value_3: 0.04041
	loss_reward_3: 0.00594
	loss_policy_4: 0.04547
	accuracy_policy_4: 0.57682
	loss_value_4: 0.04169
	loss_reward_4: 0.0065
	loss_policy_5: 0.0482
	accuracy_policy_5: 0.55357
	loss_value_5: 0.04305
	loss_reward_5: 0.00729
	loss_policy: 0.34867
	loss_value: 0.37983
	loss_reward: 0.03094
Optimization_Done 52200
[2024-05-07 15:55:22] [command] train weight_iter_52200.pkl 261 262
[2024-05-07 15:55:59] nn step 52300, lr: 0.043047.
	loss_policy_0: 0.17008
	accuracy_policy_0: 0.66338
	loss_value_0: 0.17361
	loss_policy_1: 0.04035
	accuracy_policy_1: 0.60598
	loss_value_1: 0.03643
	loss_reward_1: 0.00679
	loss_policy_2: 0.04393
	accuracy_policy_2: 0.5751
	loss_value_2: 0.03797
	loss_reward_2: 0.00679
	loss_policy_3: 0.04662
	accuracy_policy_3: 0.55467
	loss_value_3: 0.0393
	loss_reward_3: 0.00717
	loss_policy_4: 0.04929
	accuracy_policy_4: 0.53412
	loss_value_4: 0.04066
	loss_reward_4: 0.00782
	loss_policy_5: 0.05169
	accuracy_policy_5: 0.51371
	loss_value_5: 0.042
	loss_reward_5: 0.00868
	loss_policy: 0.40195
	loss_value: 0.36997
	loss_reward: 0.03726
[2024-05-07 15:56:35] nn step 52400, lr: 0.043047.
	loss_policy_0: 0.14094
	accuracy_policy_0: 0.71801
	loss_value_0: 0.1732
	loss_policy_1: 0.03661
	accuracy_policy_1: 0.64238
	loss_value_1: 0.03642
	loss_reward_1: 0.00682
	loss_policy_2: 0.04069
	accuracy_policy_2: 0.60789
	loss_value_2: 0.038
	loss_reward_2: 0.00648
	loss_policy_3: 0.04381
	accuracy_policy_3: 0.58346
	loss_value_3: 0.03942
	loss_reward_3: 0.00702
	loss_policy_4: 0.04636
	accuracy_policy_4: 0.56359
	loss_value_4: 0.04077
	loss_reward_4: 0.00752
	loss_policy_5: 0.04948
	accuracy_policy_5: 0.53924
	loss_value_5: 0.0421
	loss_reward_5: 0.00849
	loss_policy: 0.35788
	loss_value: 0.3699
	loss_reward: 0.03633
Optimization_Done 52400
[2024-05-07 15:58:49] [command] train weight_iter_52400.pkl 262 263
[2024-05-07 15:59:26] nn step 52500, lr: 0.043047.
	loss_policy_0: 0.12706
	accuracy_policy_0: 0.67551
	loss_value_0: 0.14721
	loss_policy_1: 0.032
	accuracy_policy_1: 0.59977
	loss_value_1: 0.03058
	loss_reward_1: 0.00577
	loss_policy_2: 0.03477
	accuracy_policy_2: 0.57094
	loss_value_2: 0.03177
	loss_reward_2: 0.00562
	loss_policy_3: 0.03663
	accuracy_policy_3: 0.54861
	loss_value_3: 0.0329
	loss_reward_3: 0.00633
	loss_policy_4: 0.03876
	accuracy_policy_4: 0.52908
	loss_value_4: 0.03393
	loss_reward_4: 0.00664
	loss_policy_5: 0.04077
	accuracy_policy_5: 0.51008
	loss_value_5: 0.03498
	loss_reward_5: 0.00744
	loss_policy: 0.30999
	loss_value: 0.31137
	loss_reward: 0.0318
[2024-05-07 16:00:01] nn step 52600, lr: 0.043047.
	loss_policy_0: 0.11104
	accuracy_policy_0: 0.73256
	loss_value_0: 0.15342
	loss_policy_1: 0.03036
	accuracy_policy_1: 0.63738
	loss_value_1: 0.03195
	loss_reward_1: 0.00603
	loss_policy_2: 0.03322
	accuracy_policy_2: 0.61047
	loss_value_2: 0.03329
	loss_reward_2: 0.00578
	loss_policy_3: 0.03581
	accuracy_policy_3: 0.58543
	loss_value_3: 0.03432
	loss_reward_3: 0.00637
	loss_policy_4: 0.03797
	accuracy_policy_4: 0.56506
	loss_value_4: 0.03548
	loss_reward_4: 0.00693
	loss_policy_5: 0.04013
	accuracy_policy_5: 0.54559
	loss_value_5: 0.03666
	loss_reward_5: 0.00779
	loss_policy: 0.28854
	loss_value: 0.32512
	loss_reward: 0.03289
Optimization_Done 52600
[2024-05-07 16:02:18] [command] train weight_iter_52600.pkl 263 264
[2024-05-07 16:02:55] nn step 52700, lr: 0.043047.
	loss_policy_0: 0.13774
	accuracy_policy_0: 0.66332
	loss_value_0: 0.15188
	loss_policy_1: 0.03311
	accuracy_policy_1: 0.59947
	loss_value_1: 0.03165
	loss_reward_1: 0.00504
	loss_policy_2: 0.0358
	accuracy_policy_2: 0.56916
	loss_value_2: 0.03305
	loss_reward_2: 0.00492
	loss_policy_3: 0.03794
	accuracy_policy_3: 0.54891
	loss_value_3: 0.03422
	loss_reward_3: 0.00541
	loss_policy_4: 0.04034
	accuracy_policy_4: 0.52629
	loss_value_4: 0.03527
	loss_reward_4: 0.00575
	loss_policy_5: 0.04225
	accuracy_policy_5: 0.50311
	loss_value_5: 0.03621
	loss_reward_5: 0.00647
	loss_policy: 0.32717
	loss_value: 0.32228
	loss_reward: 0.02759
[2024-05-07 16:03:30] nn step 52800, lr: 0.043047.
	loss_policy_0: 0.1067
	accuracy_policy_0: 0.73818
	loss_value_0: 0.15048
	loss_policy_1: 0.02925
	accuracy_policy_1: 0.64346
	loss_value_1: 0.03133
	loss_reward_1: 0.00506
	loss_policy_2: 0.03221
	accuracy_policy_2: 0.61287
	loss_value_2: 0.03257
	loss_reward_2: 0.00504
	loss_policy_3: 0.0346
	accuracy_policy_3: 0.58975
	loss_value_3: 0.03374
	loss_reward_3: 0.00543
	loss_policy_4: 0.03698
	accuracy_policy_4: 0.5715
	loss_value_4: 0.03485
	loss_reward_4: 0.00576
	loss_policy_5: 0.03922
	accuracy_policy_5: 0.5458
	loss_value_5: 0.03594
	loss_reward_5: 0.00674
	loss_policy: 0.27897
	loss_value: 0.31891
	loss_reward: 0.02803
Optimization_Done 52800
[2024-05-07 16:05:47] [command] train weight_iter_52800.pkl 264 265
[2024-05-07 16:06:24] nn step 52900, lr: 0.043047.
	loss_policy_0: 0.16846
	accuracy_policy_0: 0.645
	loss_value_0: 0.16951
	loss_policy_1: 0.03939
	accuracy_policy_1: 0.59127
	loss_value_1: 0.03518
	loss_reward_1: 0.0056
	loss_policy_2: 0.04242
	accuracy_policy_2: 0.56299
	loss_value_2: 0.03642
	loss_reward_2: 0.0055
	loss_policy_3: 0.04497
	accuracy_policy_3: 0.54137
	loss_value_3: 0.03758
	loss_reward_3: 0.00586
	loss_policy_4: 0.0476
	accuracy_policy_4: 0.51764
	loss_value_4: 0.03875
	loss_reward_4: 0.00613
	loss_policy_5: 0.04979
	accuracy_policy_5: 0.50039
	loss_value_5: 0.03975
	loss_reward_5: 0.00689
	loss_policy: 0.39263
	loss_value: 0.35719
	loss_reward: 0.02998
[2024-05-07 16:06:59] nn step 53000, lr: 0.043047.
	loss_policy_0: 0.13611
	accuracy_policy_0: 0.70973
	loss_value_0: 0.16594
	loss_policy_1: 0.0345
	accuracy_policy_1: 0.63785
	loss_value_1: 0.03463
	loss_reward_1: 0.00555
	loss_policy_2: 0.038
	accuracy_policy_2: 0.60512
	loss_value_2: 0.0359
	loss_reward_2: 0.00562
	loss_policy_3: 0.04073
	accuracy_policy_3: 0.58064
	loss_value_3: 0.03702
	loss_reward_3: 0.00595
	loss_policy_4: 0.04332
	accuracy_policy_4: 0.56137
	loss_value_4: 0.03816
	loss_reward_4: 0.00634
	loss_policy_5: 0.04531
	accuracy_policy_5: 0.54607
	loss_value_5: 0.03944
	loss_reward_5: 0.00708
	loss_policy: 0.33797
	loss_value: 0.35109
	loss_reward: 0.03055
Optimization_Done 53000
[2024-05-07 16:09:04] [command] train weight_iter_53000.pkl 265 266
[2024-05-07 16:09:40] nn step 53100, lr: 0.043047.
	loss_policy_0: 0.18795
	accuracy_policy_0: 0.63557
	loss_value_0: 0.18529
	loss_policy_1: 0.04397
	accuracy_policy_1: 0.57975
	loss_value_1: 0.03838
	loss_reward_1: 0.00625
	loss_policy_2: 0.04678
	accuracy_policy_2: 0.55818
	loss_value_2: 0.03991
	loss_reward_2: 0.00614
	loss_policy_3: 0.04947
	accuracy_policy_3: 0.53744
	loss_value_3: 0.0415
	loss_reward_3: 0.00674
	loss_policy_4: 0.05231
	accuracy_policy_4: 0.5157
	loss_value_4: 0.04286
	loss_reward_4: 0.00728
	loss_policy_5: 0.05456
	accuracy_policy_5: 0.499
	loss_value_5: 0.04412
	loss_reward_5: 0.00809
	loss_policy: 0.43503
	loss_value: 0.39205
	loss_reward: 0.0345
[2024-05-07 16:10:16] nn step 53200, lr: 0.043047.
	loss_policy_0: 0.1652
	accuracy_policy_0: 0.68936
	loss_value_0: 0.18855
	loss_policy_1: 0.04121
	accuracy_policy_1: 0.62027
	loss_value_1: 0.03928
	loss_reward_1: 0.0065
	loss_policy_2: 0.04448
	accuracy_policy_2: 0.5942
	loss_value_2: 0.04079
	loss_reward_2: 0.0064
	loss_policy_3: 0.04757
	accuracy_policy_3: 0.57264
	loss_value_3: 0.04196
	loss_reward_3: 0.00684
	loss_policy_4: 0.05037
	accuracy_policy_4: 0.5498
	loss_value_4: 0.04326
	loss_reward_4: 0.00738
	loss_policy_5: 0.05331
	accuracy_policy_5: 0.52721
	loss_value_5: 0.04461
	loss_reward_5: 0.0085
	loss_policy: 0.40214
	loss_value: 0.39844
	loss_reward: 0.03561
Optimization_Done 53200
[2024-05-07 16:12:30] [command] train weight_iter_53200.pkl 266 267
[2024-05-07 16:13:07] nn step 53300, lr: 0.043047.
	loss_policy_0: 0.16944
	accuracy_policy_0: 0.64676
	loss_value_0: 0.17631
	loss_policy_1: 0.04149
	accuracy_policy_1: 0.57855
	loss_value_1: 0.03685
	loss_reward_1: 0.00678
	loss_policy_2: 0.04457
	accuracy_policy_2: 0.55107
	loss_value_2: 0.03822
	loss_reward_2: 0.00662
	loss_policy_3: 0.04731
	accuracy_policy_3: 0.52746
	loss_value_3: 0.03946
	loss_reward_3: 0.00724
	loss_policy_4: 0.04984
	accuracy_policy_4: 0.50316
	loss_value_4: 0.0407
	loss_reward_4: 0.00785
	loss_policy_5: 0.05189
	accuracy_policy_5: 0.48695
	loss_value_5: 0.04164
	loss_reward_5: 0.00889
	loss_policy: 0.40455
	loss_value: 0.37318
	loss_reward: 0.03737
[2024-05-07 16:13:43] nn step 53400, lr: 0.043047.
	loss_policy_0: 0.15079
	accuracy_policy_0: 0.6984
	loss_value_0: 0.18331
	loss_policy_1: 0.03979
	accuracy_policy_1: 0.61223
	loss_value_1: 0.03833
	loss_reward_1: 0.00705
	loss_policy_2: 0.04307
	accuracy_policy_2: 0.5841
	loss_value_2: 0.03979
	loss_reward_2: 0.00663
	loss_policy_3: 0.04676
	accuracy_policy_3: 0.55924
	loss_value_3: 0.04098
	loss_reward_3: 0.00735
	loss_policy_4: 0.04906
	accuracy_policy_4: 0.53939
	loss_value_4: 0.04234
	loss_reward_4: 0.00802
	loss_policy_5: 0.05201
	accuracy_policy_5: 0.51795
	loss_value_5: 0.04363
	loss_reward_5: 0.00935
	loss_policy: 0.38149
	loss_value: 0.38838
	loss_reward: 0.03841
Optimization_Done 53400
[2024-05-07 16:15:49] [command] train weight_iter_53400.pkl 267 268
[2024-05-07 16:16:26] nn step 53500, lr: 0.043047.
	loss_policy_0: 0.15682
	accuracy_policy_0: 0.64727
	loss_value_0: 0.1749
	loss_policy_1: 0.03948
	accuracy_policy_1: 0.57318
	loss_value_1: 0.03644
	loss_reward_1: 0.00621
	loss_policy_2: 0.04293
	accuracy_policy_2: 0.54225
	loss_value_2: 0.0377
	loss_reward_2: 0.00607
	loss_policy_3: 0.0452
	accuracy_policy_3: 0.52092
	loss_value_3: 0.03876
	loss_reward_3: 0.00681
	loss_policy_4: 0.04752
	accuracy_policy_4: 0.49779
	loss_value_4: 0.03972
	loss_reward_4: 0.00724
	loss_policy_5: 0.04935
	accuracy_policy_5: 0.48467
	loss_value_5: 0.04056
	loss_reward_5: 0.00815
	loss_policy: 0.38129
	loss_value: 0.36809
	loss_reward: 0.03449
[2024-05-07 16:17:01] nn step 53600, lr: 0.043047.
	loss_policy_0: 0.12556
	accuracy_policy_0: 0.70627
	loss_value_0: 0.16567
	loss_policy_1: 0.03469
	accuracy_policy_1: 0.61088
	loss_value_1: 0.03462
	loss_reward_1: 0.00623
	loss_policy_2: 0.03799
	accuracy_policy_2: 0.57939
	loss_value_2: 0.03577
	loss_reward_2: 0.00593
	loss_policy_3: 0.04052
	accuracy_policy_3: 0.56014
	loss_value_3: 0.03688
	loss_reward_3: 0.00659
	loss_policy_4: 0.04305
	accuracy_policy_4: 0.53213
	loss_value_4: 0.03795
	loss_reward_4: 0.00695
	loss_policy_5: 0.04504
	accuracy_policy_5: 0.51553
	loss_value_5: 0.0391
	loss_reward_5: 0.00794
	loss_policy: 0.32685
	loss_value: 0.34999
	loss_reward: 0.03365
Optimization_Done 53600
[2024-05-07 16:19:16] [command] train weight_iter_53600.pkl 268 269
[2024-05-07 16:19:53] nn step 53700, lr: 0.043047.
	loss_policy_0: 0.16448
	accuracy_policy_0: 0.6292
	loss_value_0: 0.16343
	loss_policy_1: 0.03863
	accuracy_policy_1: 0.57492
	loss_value_1: 0.03405
	loss_reward_1: 0.00548
	loss_policy_2: 0.04141
	accuracy_policy_2: 0.54459
	loss_value_2: 0.0355
	loss_reward_2: 0.0054
	loss_policy_3: 0.04364
	accuracy_policy_3: 0.53078
	loss_value_3: 0.03656
	loss_reward_3: 0.00596
	loss_policy_4: 0.04608
	accuracy_policy_4: 0.50592
	loss_value_4: 0.03762
	loss_reward_4: 0.00638
	loss_policy_5: 0.04817
	accuracy_policy_5: 0.48824
	loss_value_5: 0.03856
	loss_reward_5: 0.00704
	loss_policy: 0.38242
	loss_value: 0.34573
	loss_reward: 0.03026
[2024-05-07 16:20:29] nn step 53800, lr: 0.043047.
	loss_policy_0: 0.14045
	accuracy_policy_0: 0.69469
	loss_value_0: 0.16952
	loss_policy_1: 0.03627
	accuracy_policy_1: 0.61539
	loss_value_1: 0.0354
	loss_reward_1: 0.00592
	loss_policy_2: 0.0394
	accuracy_policy_2: 0.59102
	loss_value_2: 0.0368
	loss_reward_2: 0.00578
	loss_policy_3: 0.04203
	accuracy_policy_3: 0.56812
	loss_value_3: 0.03816
	loss_reward_3: 0.00623
	loss_policy_4: 0.04457
	accuracy_policy_4: 0.54918
	loss_value_4: 0.03926
	loss_reward_4: 0.00672
	loss_policy_5: 0.04657
	accuracy_policy_5: 0.53262
	loss_value_5: 0.04044
	loss_reward_5: 0.00772
	loss_policy: 0.34928
	loss_value: 0.35958
	loss_reward: 0.03237
Optimization_Done 53800
[2024-05-07 16:22:36] [command] train weight_iter_53800.pkl 269 270
[2024-05-07 16:23:13] nn step 53900, lr: 0.043047.
	loss_policy_0: 0.19616
	accuracy_policy_0: 0.64818
	loss_value_0: 0.19091
	loss_policy_1: 0.04527
	accuracy_policy_1: 0.59926
	loss_value_1: 0.03997
	loss_reward_1: 0.00648
	loss_policy_2: 0.04864
	accuracy_policy_2: 0.57283
	loss_value_2: 0.04151
	loss_reward_2: 0.00623
	loss_policy_3: 0.05169
	accuracy_policy_3: 0.55555
	loss_value_3: 0.04285
	loss_reward_3: 0.00676
	loss_policy_4: 0.05437
	accuracy_policy_4: 0.53588
	loss_value_4: 0.04435
	loss_reward_4: 0.00735
	loss_policy_5: 0.05726
	accuracy_policy_5: 0.51539
	loss_value_5: 0.0457
	loss_reward_5: 0.00826
	loss_policy: 0.45339
	loss_value: 0.40531
	loss_reward: 0.03508
[2024-05-07 16:23:48] nn step 54000, lr: 0.043047.
	loss_policy_0: 0.15598
	accuracy_policy_0: 0.70248
	loss_value_0: 0.17764
	loss_policy_1: 0.03803
	accuracy_policy_1: 0.64418
	loss_value_1: 0.03696
	loss_reward_1: 0.00613
	loss_policy_2: 0.04164
	accuracy_policy_2: 0.61773
	loss_value_2: 0.0385
	loss_reward_2: 0.00598
	loss_policy_3: 0.0452
	accuracy_policy_3: 0.58797
	loss_value_3: 0.04002
	loss_reward_3: 0.00651
	loss_policy_4: 0.04836
	accuracy_policy_4: 0.56738
	loss_value_4: 0.04145
	loss_reward_4: 0.007
	loss_policy_5: 0.05087
	accuracy_policy_5: 0.5458
	loss_value_5: 0.04286
	loss_reward_5: 0.00789
	loss_policy: 0.38008
	loss_value: 0.37744
	loss_reward: 0.03351
Optimization_Done 54000
[2024-05-07 16:26:03] [command] train weight_iter_54000.pkl 270 271
[2024-05-07 16:26:40] nn step 54100, lr: 0.043047.
	loss_policy_0: 0.1766
	accuracy_policy_0: 0.65275
	loss_value_0: 0.16417
	loss_policy_1: 0.04128
	accuracy_policy_1: 0.60252
	loss_value_1: 0.03435
	loss_reward_1: 0.0063
	loss_policy_2: 0.04433
	accuracy_policy_2: 0.57838
	loss_value_2: 0.03578
	loss_reward_2: 0.0061
	loss_policy_3: 0.04705
	accuracy_policy_3: 0.55672
	loss_value_3: 0.03703
	loss_reward_3: 0.00655
	loss_policy_4: 0.04913
	accuracy_policy_4: 0.54158
	loss_value_4: 0.03836
	loss_reward_4: 0.00711
	loss_policy_5: 0.05129
	accuracy_policy_5: 0.52811
	loss_value_5: 0.03978
	loss_reward_5: 0.00799
	loss_policy: 0.40968
	loss_value: 0.34947
	loss_reward: 0.03406
[2024-05-07 16:27:15] nn step 54200, lr: 0.043047.
	loss_policy_0: 0.14332
	accuracy_policy_0: 0.71258
	loss_value_0: 0.15816
	loss_policy_1: 0.03569
	accuracy_policy_1: 0.64965
	loss_value_1: 0.03328
	loss_reward_1: 0.00589
	loss_policy_2: 0.03958
	accuracy_policy_2: 0.61594
	loss_value_2: 0.03488
	loss_reward_2: 0.00577
	loss_policy_3: 0.0427
	accuracy_policy_3: 0.59041
	loss_value_3: 0.03611
	loss_reward_3: 0.00609
	loss_policy_4: 0.0452
	accuracy_policy_4: 0.5741
	loss_value_4: 0.03749
	loss_reward_4: 0.00661
	loss_policy_5: 0.04782
	accuracy_policy_5: 0.55182
	loss_value_5: 0.03863
	loss_reward_5: 0.00752
	loss_policy: 0.35432
	loss_value: 0.33855
	loss_reward: 0.03188
Optimization_Done 54200
[2024-05-07 16:29:22] [command] train weight_iter_54200.pkl 271 272
[2024-05-07 16:29:59] nn step 54300, lr: 0.043047.
	loss_policy_0: 0.1354
	accuracy_policy_0: 0.69771
	loss_value_0: 0.1531
	loss_policy_1: 0.03274
	accuracy_policy_1: 0.63838
	loss_value_1: 0.03197
	loss_reward_1: 0.00529
	loss_policy_2: 0.03532
	accuracy_policy_2: 0.61252
	loss_value_2: 0.03329
	loss_reward_2: 0.00518
	loss_policy_3: 0.03731
	accuracy_policy_3: 0.59715
	loss_value_3: 0.03431
	loss_reward_3: 0.00564
	loss_policy_4: 0.03908
	accuracy_policy_4: 0.58301
	loss_value_4: 0.03526
	loss_reward_4: 0.00605
	loss_policy_5: 0.04076
	accuracy_policy_5: 0.57025
	loss_value_5: 0.03613
	loss_reward_5: 0.00667
	loss_policy: 0.32062
	loss_value: 0.32406
	loss_reward: 0.02882
[2024-05-07 16:30:34] nn step 54400, lr: 0.043047.
	loss_policy_0: 0.10886
	accuracy_policy_0: 0.7558
	loss_value_0: 0.14763
	loss_policy_1: 0.02862
	accuracy_policy_1: 0.68178
	loss_value_1: 0.03069
	loss_reward_1: 0.00538
	loss_policy_2: 0.03181
	accuracy_policy_2: 0.6509
	loss_value_2: 0.03171
	loss_reward_2: 0.00503
	loss_policy_3: 0.03389
	accuracy_policy_3: 0.6325
	loss_value_3: 0.03262
	loss_reward_3: 0.00543
	loss_policy_4: 0.03582
	accuracy_policy_4: 0.61773
	loss_value_4: 0.03357
	loss_reward_4: 0.00586
	loss_policy_5: 0.03795
	accuracy_policy_5: 0.59832
	loss_value_5: 0.03471
	loss_reward_5: 0.00691
	loss_policy: 0.27696
	loss_value: 0.31093
	loss_reward: 0.0286
Optimization_Done 54400
[2024-05-07 16:32:40] [command] train weight_iter_54400.pkl 272 273
[2024-05-07 16:33:17] nn step 54500, lr: 0.043047.
	loss_policy_0: 0.15569
	accuracy_policy_0: 0.66834
	loss_value_0: 0.14666
	loss_policy_1: 0.03457
	accuracy_policy_1: 0.63117
	loss_value_1: 0.03066
	loss_reward_1: 0.00431
	loss_policy_2: 0.03679
	accuracy_policy_2: 0.61055
	loss_value_2: 0.03192
	loss_reward_2: 0.00421
	loss_policy_3: 0.03854
	accuracy_policy_3: 0.59439
	loss_value_3: 0.03308
	loss_reward_3: 0.00461
	loss_policy_4: 0.03973
	accuracy_policy_4: 0.58488
	loss_value_4: 0.03402
	loss_reward_4: 0.00486
	loss_policy_5: 0.04159
	accuracy_policy_5: 0.57
	loss_value_5: 0.03493
	loss_reward_5: 0.00547
	loss_policy: 0.3469
	loss_value: 0.31128
	loss_reward: 0.02346
[2024-05-07 16:33:52] nn step 54600, lr: 0.043047.
	loss_policy_0: 0.12134
	accuracy_policy_0: 0.74574
	loss_value_0: 0.15162
	loss_policy_1: 0.02994
	accuracy_policy_1: 0.68623
	loss_value_1: 0.03179
	loss_reward_1: 0.00447
	loss_policy_2: 0.03296
	accuracy_policy_2: 0.65982
	loss_value_2: 0.03303
	loss_reward_2: 0.00433
	loss_policy_3: 0.03484
	accuracy_policy_3: 0.64531
	loss_value_3: 0.03417
	loss_reward_3: 0.00472
	loss_policy_4: 0.03654
	accuracy_policy_4: 0.6318
	loss_value_4: 0.03523
	loss_reward_4: 0.00492
	loss_policy_5: 0.03863
	accuracy_policy_5: 0.61721
	loss_value_5: 0.03636
	loss_reward_5: 0.00575
	loss_policy: 0.29426
	loss_value: 0.32219
	loss_reward: 0.02419
Optimization_Done 54600
[2024-05-07 16:36:10] [command] train weight_iter_54600.pkl 273 274
[2024-05-07 16:36:46] nn step 54700, lr: 0.043047.
	loss_policy_0: 0.19118
	accuracy_policy_0: 0.64717
	loss_value_0: 0.17295
	loss_policy_1: 0.04197
	accuracy_policy_1: 0.61207
	loss_value_1: 0.03599
	loss_reward_1: 0.00536
	loss_policy_2: 0.04455
	accuracy_policy_2: 0.59342
	loss_value_2: 0.03748
	loss_reward_2: 0.00521
	loss_policy_3: 0.04731
	accuracy_policy_3: 0.57418
	loss_value_3: 0.0388
	loss_reward_3: 0.00539
	loss_policy_4: 0.04918
	accuracy_policy_4: 0.56148
	loss_value_4: 0.04009
	loss_reward_4: 0.00577
	loss_policy_5: 0.05118
	accuracy_policy_5: 0.55096
	loss_value_5: 0.0413
	loss_reward_5: 0.0066
	loss_policy: 0.42537
	loss_value: 0.36661
	loss_reward: 0.02833
[2024-05-07 16:37:22] nn step 54800, lr: 0.043047.
	loss_policy_0: 0.15377
	accuracy_policy_0: 0.70855
	loss_value_0: 0.16969
	loss_policy_1: 0.03649
	accuracy_policy_1: 0.65818
	loss_value_1: 0.03539
	loss_reward_1: 0.00515
	loss_policy_2: 0.03967
	accuracy_policy_2: 0.63064
	loss_value_2: 0.03676
	loss_reward_2: 0.00495
	loss_policy_3: 0.04262
	accuracy_policy_3: 0.61072
	loss_value_3: 0.03809
	loss_reward_3: 0.00526
	loss_policy_4: 0.04474
	accuracy_policy_4: 0.59711
	loss_value_4: 0.03925
	loss_reward_4: 0.00563
	loss_policy_5: 0.04717
	accuracy_policy_5: 0.57803
	loss_value_5: 0.04039
	loss_reward_5: 0.00652
	loss_policy: 0.36445
	loss_value: 0.35956
	loss_reward: 0.02751
Optimization_Done 54800
[2024-05-07 16:39:38] [command] train weight_iter_54800.pkl 274 275
[2024-05-07 16:40:15] nn step 54900, lr: 0.043047.
	loss_policy_0: 0.17355
	accuracy_policy_0: 0.65789
	loss_value_0: 0.17024
	loss_policy_1: 0.04033
	accuracy_policy_1: 0.61256
	loss_value_1: 0.03563
	loss_reward_1: 0.0065
	loss_policy_2: 0.04396
	accuracy_policy_2: 0.58322
	loss_value_2: 0.03715
	loss_reward_2: 0.00609
	loss_policy_3: 0.04683
	accuracy_policy_3: 0.55834
	loss_value_3: 0.03848
	loss_reward_3: 0.00693
	loss_policy_4: 0.04973
	accuracy_policy_4: 0.53947
	loss_value_4: 0.0397
	loss_reward_4: 0.0074
	loss_policy_5: 0.05203
	accuracy_policy_5: 0.52025
	loss_value_5: 0.04098
	loss_reward_5: 0.00827
	loss_policy: 0.40643
	loss_value: 0.36219
	loss_reward: 0.0352
[2024-05-07 16:40:51] nn step 55000, lr: 0.043047.
	loss_policy_0: 0.14734
	accuracy_policy_0: 0.71811
	loss_value_0: 0.17197
	loss_policy_1: 0.03677
	accuracy_policy_1: 0.65279
	loss_value_1: 0.0361
	loss_reward_1: 0.00664
	loss_policy_2: 0.04096
	accuracy_policy_2: 0.62209
	loss_value_2: 0.03776
	loss_reward_2: 0.00626
	loss_policy_3: 0.04418
	accuracy_policy_3: 0.59543
	loss_value_3: 0.03916
	loss_reward_3: 0.00679
	loss_policy_4: 0.04662
	accuracy_policy_4: 0.5766
	loss_value_4: 0.04055
	loss_reward_4: 0.00746
	loss_policy_5: 0.04918
	accuracy_policy_5: 0.55955
	loss_value_5: 0.04189
	loss_reward_5: 0.00855
	loss_policy: 0.36504
	loss_value: 0.36743
	loss_reward: 0.0357
Optimization_Done 55000
[2024-05-07 16:43:08] [command] train weight_iter_55000.pkl 275 276
[2024-05-07 16:43:44] nn step 55100, lr: 0.043047.
	loss_policy_0: 0.13605
	accuracy_policy_0: 0.69975
	loss_value_0: 0.15294
	loss_policy_1: 0.03458
	accuracy_policy_1: 0.62617
	loss_value_1: 0.03195
	loss_reward_1: 0.00667
	loss_policy_2: 0.03802
	accuracy_policy_2: 0.59025
	loss_value_2: 0.03333
	loss_reward_2: 0.00639
	loss_policy_3: 0.04073
	accuracy_policy_3: 0.57295
	loss_value_3: 0.03437
	loss_reward_3: 0.00708
	loss_policy_4: 0.04274
	accuracy_policy_4: 0.55301
	loss_value_4: 0.03519
	loss_reward_4: 0.00766
	loss_policy_5: 0.04477
	accuracy_policy_5: 0.53846
	loss_value_5: 0.03609
	loss_reward_5: 0.00861
	loss_policy: 0.33689
	loss_value: 0.32388
	loss_reward: 0.03641
[2024-05-07 16:44:20] nn step 55200, lr: 0.043047.
	loss_policy_0: 0.1197
	accuracy_policy_0: 0.74879
	loss_value_0: 0.15589
	loss_policy_1: 0.0328
	accuracy_policy_1: 0.65826
	loss_value_1: 0.0328
	loss_reward_1: 0.00675
	loss_policy_2: 0.03651
	accuracy_policy_2: 0.62219
	loss_value_2: 0.03414
	loss_reward_2: 0.00646
	loss_policy_3: 0.0396
	accuracy_policy_3: 0.59805
	loss_value_3: 0.03537
	loss_reward_3: 0.0071
	loss_policy_4: 0.04213
	accuracy_policy_4: 0.57734
	loss_value_4: 0.03641
	loss_reward_4: 0.00776
	loss_policy_5: 0.04399
	accuracy_policy_5: 0.56469
	loss_value_5: 0.03768
	loss_reward_5: 0.00878
	loss_policy: 0.31472
	loss_value: 0.33228
	loss_reward: 0.03685
Optimization_Done 55200
[2024-05-07 16:46:13] [command] train weight_iter_55200.pkl 276 277
[2024-05-07 16:46:50] nn step 55300, lr: 0.043047.
	loss_policy_0: 0.12561
	accuracy_policy_0: 0.70566
	loss_value_0: 0.15611
	loss_policy_1: 0.03187
	accuracy_policy_1: 0.6315
	loss_value_1: 0.03253
	loss_reward_1: 0.00566
	loss_policy_2: 0.03502
	accuracy_policy_2: 0.60318
	loss_value_2: 0.03365
	loss_reward_2: 0.00564
	loss_policy_3: 0.03736
	accuracy_policy_3: 0.57689
	loss_value_3: 0.03458
	loss_reward_3: 0.00622
	loss_policy_4: 0.03951
	accuracy_policy_4: 0.5575
	loss_value_4: 0.03532
	loss_reward_4: 0.0067
	loss_policy_5: 0.0411
	accuracy_policy_5: 0.54332
	loss_value_5: 0.03622
	loss_reward_5: 0.00762
	loss_policy: 0.31045
	loss_value: 0.32841
	loss_reward: 0.03183
[2024-05-07 16:47:25] nn step 55400, lr: 0.043047.
	loss_policy_0: 0.09359
	accuracy_policy_0: 0.76729
	loss_value_0: 0.14292
	loss_policy_1: 0.02701
	accuracy_policy_1: 0.66916
	loss_value_1: 0.02991
	loss_reward_1: 0.00542
	loss_policy_2: 0.02982
	accuracy_policy_2: 0.64115
	loss_value_2: 0.03076
	loss_reward_2: 0.00533
	loss_policy_3: 0.03196
	accuracy_policy_3: 0.62455
	loss_value_3: 0.03173
	loss_reward_3: 0.006
	loss_policy_4: 0.03411
	accuracy_policy_4: 0.60244
	loss_value_4: 0.03264
	loss_reward_4: 0.00644
	loss_policy_5: 0.03598
	accuracy_policy_5: 0.58617
	loss_value_5: 0.03364
	loss_reward_5: 0.00732
	loss_policy: 0.25248
	loss_value: 0.3016
	loss_reward: 0.0305
Optimization_Done 55400
[2024-05-07 16:49:43] [command] train weight_iter_55400.pkl 277 278
[2024-05-07 16:50:19] nn step 55500, lr: 0.043047.
	loss_policy_0: 0.1815
	accuracy_policy_0: 0.65785
	loss_value_0: 0.1761
	loss_policy_1: 0.04215
	accuracy_policy_1: 0.60631
	loss_value_1: 0.03679
	loss_reward_1: 0.00601
	loss_policy_2: 0.04496
	accuracy_policy_2: 0.58232
	loss_value_2: 0.03816
	loss_reward_2: 0.00608
	loss_policy_3: 0.0476
	accuracy_policy_3: 0.5617
	loss_value_3: 0.03957
	loss_reward_3: 0.00661
	loss_policy_4: 0.04964
	accuracy_policy_4: 0.54311
	loss_value_4: 0.04067
	loss_reward_4: 0.00707
	loss_policy_5: 0.05203
	accuracy_policy_5: 0.52973
	loss_value_5: 0.04186
	loss_reward_5: 0.00796
	loss_policy: 0.41787
	loss_value: 0.37315
	loss_reward: 0.03374
[2024-05-07 16:50:54] nn step 55600, lr: 0.043047.
	loss_policy_0: 0.13652
	accuracy_policy_0: 0.7224
	loss_value_0: 0.17009
	loss_policy_1: 0.03443
	accuracy_policy_1: 0.65094
	loss_value_1: 0.03562
	loss_reward_1: 0.00584
	loss_policy_2: 0.03782
	accuracy_policy_2: 0.62832
	loss_value_2: 0.03704
	loss_reward_2: 0.00579
	loss_policy_3: 0.04098
	accuracy_policy_3: 0.60527
	loss_value_3: 0.03821
	loss_reward_3: 0.00622
	loss_policy_4: 0.04335
	accuracy_policy_4: 0.58494
	loss_value_4: 0.03939
	loss_reward_4: 0.0067
	loss_policy_5: 0.04528
	accuracy_policy_5: 0.57539
	loss_value_5: 0.04068
	loss_reward_5: 0.00759
	loss_policy: 0.33838
	loss_value: 0.36103
	loss_reward: 0.03214
Optimization_Done 55600
[2024-05-07 16:53:10] [command] train weight_iter_55600.pkl 278 279
[2024-05-07 16:53:47] nn step 55700, lr: 0.043047.
	loss_policy_0: 0.16862
	accuracy_policy_0: 0.67553
	loss_value_0: 0.17592
	loss_policy_1: 0.03959
	accuracy_policy_1: 0.62309
	loss_value_1: 0.037
	loss_reward_1: 0.00626
	loss_policy_2: 0.04308
	accuracy_policy_2: 0.59684
	loss_value_2: 0.03873
	loss_reward_2: 0.00622
	loss_policy_3: 0.04609
	accuracy_policy_3: 0.57523
	loss_value_3: 0.0402
	loss_reward_3: 0.00675
	loss_policy_4: 0.04869
	accuracy_policy_4: 0.55682
	loss_value_4: 0.04165
	loss_reward_4: 0.00727
	loss_policy_5: 0.05107
	accuracy_policy_5: 0.54291
	loss_value_5: 0.04289
	loss_reward_5: 0.00801
	loss_policy: 0.39713
	loss_value: 0.37639
	loss_reward: 0.03451
[2024-05-07 16:54:22] nn step 55800, lr: 0.043047.
	loss_policy_0: 0.14771
	accuracy_policy_0: 0.7224
	loss_value_0: 0.1839
	loss_policy_1: 0.03698
	accuracy_policy_1: 0.66023
	loss_value_1: 0.03854
	loss_reward_1: 0.00628
	loss_policy_2: 0.04116
	accuracy_policy_2: 0.62785
	loss_value_2: 0.04015
	loss_reward_2: 0.00631
	loss_policy_3: 0.04421
	accuracy_policy_3: 0.60389
	loss_value_3: 0.04152
	loss_reward_3: 0.0068
	loss_policy_4: 0.04737
	accuracy_policy_4: 0.58502
	loss_value_4: 0.04286
	loss_reward_4: 0.00727
	loss_policy_5: 0.05032
	accuracy_policy_5: 0.56574
	loss_value_5: 0.04439
	loss_reward_5: 0.00854
	loss_policy: 0.36775
	loss_value: 0.39136
	loss_reward: 0.0352
Optimization_Done 55800
[2024-05-07 16:56:36] [command] train weight_iter_55800.pkl 279 280
[2024-05-07 16:57:13] nn step 55900, lr: 0.043047.
	loss_policy_0: 0.12135
	accuracy_policy_0: 0.71723
	loss_value_0: 0.14991
	loss_policy_1: 0.03063
	accuracy_policy_1: 0.65297
	loss_value_1: 0.03145
	loss_reward_1: 0.00569
	loss_policy_2: 0.03355
	accuracy_policy_2: 0.62549
	loss_value_2: 0.03254
	loss_reward_2: 0.00549
	loss_policy_3: 0.03582
	accuracy_policy_3: 0.60299
	loss_value_3: 0.03366
	loss_reward_3: 0.00607
	loss_policy_4: 0.03779
	accuracy_policy_4: 0.59016
	loss_value_4: 0.03474
	loss_reward_4: 0.00656
	loss_policy_5: 0.03987
	accuracy_policy_5: 0.5724
	loss_value_5: 0.03586
	loss_reward_5: 0.00743
	loss_policy: 0.29902
	loss_value: 0.31816
	loss_reward: 0.03123
[2024-05-07 16:57:48] nn step 56000, lr: 0.043047.
	loss_policy_0: 0.10578
	accuracy_policy_0: 0.76695
	loss_value_0: 0.15226
	loss_policy_1: 0.02885
	accuracy_policy_1: 0.68178
	loss_value_1: 0.0318
	loss_reward_1: 0.00577
	loss_policy_2: 0.03188
	accuracy_policy_2: 0.65533
	loss_value_2: 0.03291
	loss_reward_2: 0.00563
	loss_policy_3: 0.03423
	accuracy_policy_3: 0.64008
	loss_value_3: 0.03408
	loss_reward_3: 0.00602
	loss_policy_4: 0.03682
	accuracy_policy_4: 0.6176
	loss_value_4: 0.03511
	loss_reward_4: 0.0066
	loss_policy_5: 0.03967
	accuracy_policy_5: 0.59369
	loss_value_5: 0.03627
	loss_reward_5: 0.00764
	loss_policy: 0.27723
	loss_value: 0.32243
	loss_reward: 0.03165
Optimization_Done 56000
[2024-05-07 16:59:53] [command] train weight_iter_56000.pkl 280 281
[2024-05-07 17:00:30] nn step 56100, lr: 0.043047.
	loss_policy_0: 0.11836
	accuracy_policy_0: 0.70164
	loss_value_0: 0.14311
	loss_policy_1: 0.02844
	accuracy_policy_1: 0.64508
	loss_value_1: 0.02979
	loss_reward_1: 0.00465
	loss_policy_2: 0.03079
	accuracy_policy_2: 0.62334
	loss_value_2: 0.0309
	loss_reward_2: 0.00445
	loss_policy_3: 0.03247
	accuracy_policy_3: 0.60371
	loss_value_3: 0.03182
	loss_reward_3: 0.005
	loss_policy_4: 0.0341
	accuracy_policy_4: 0.58475
	loss_value_4: 0.03271
	loss_reward_4: 0.00524
	loss_policy_5: 0.03596
	accuracy_policy_5: 0.55949
	loss_value_5: 0.03363
	loss_reward_5: 0.00617
	loss_policy: 0.28012
	loss_value: 0.30196
	loss_reward: 0.02551
[2024-05-07 17:01:06] nn step 56200, lr: 0.043047.
	loss_policy_0: 0.08921
	accuracy_policy_0: 0.77146
	loss_value_0: 0.14133
	loss_policy_1: 0.02427
	accuracy_policy_1: 0.69447
	loss_value_1: 0.02945
	loss_reward_1: 0.00469
	loss_policy_2: 0.02695
	accuracy_policy_2: 0.6673
	loss_value_2: 0.03053
	loss_reward_2: 0.00449
	loss_policy_3: 0.02904
	accuracy_policy_3: 0.64305
	loss_value_3: 0.03154
	loss_reward_3: 0.00488
	loss_policy_4: 0.03061
	accuracy_policy_4: 0.62859
	loss_value_4: 0.03242
	loss_reward_4: 0.00506
	loss_policy_5: 0.03248
	accuracy_policy_5: 0.61018
	loss_value_5: 0.03343
	loss_reward_5: 0.0061
	loss_policy: 0.23256
	loss_value: 0.2987
	loss_reward: 0.02522
Optimization_Done 56200
[2024-05-07 17:03:24] [command] train weight_iter_56200.pkl 281 282
[2024-05-07 17:04:01] nn step 56300, lr: 0.043047.
	loss_policy_0: 0.16182
	accuracy_policy_0: 0.6382
	loss_value_0: 0.15063
	loss_policy_1: 0.03569
	accuracy_policy_1: 0.60156
	loss_value_1: 0.03136
	loss_reward_1: 0.00491
	loss_policy_2: 0.03795
	accuracy_policy_2: 0.57963
	loss_value_2: 0.03252
	loss_reward_2: 0.00472
	loss_policy_3: 0.03972
	accuracy_policy_3: 0.56178
	loss_value_3: 0.03359
	loss_reward_3: 0.00522
	loss_policy_4: 0.04159
	accuracy_policy_4: 0.54588
	loss_value_4: 0.0345
	loss_reward_4: 0.00532
	loss_policy_5: 0.04334
	accuracy_policy_5: 0.53215
	loss_value_5: 0.03553
	loss_reward_5: 0.00593
	loss_policy: 0.36011
	loss_value: 0.31812
	loss_reward: 0.02609
[2024-05-07 17:04:36] nn step 56400, lr: 0.043047.
	loss_policy_0: 0.12622
	accuracy_policy_0: 0.71244
	loss_value_0: 0.14959
	loss_policy_1: 0.03049
	accuracy_policy_1: 0.65746
	loss_value_1: 0.03124
	loss_reward_1: 0.00487
	loss_policy_2: 0.03352
	accuracy_policy_2: 0.62793
	loss_value_2: 0.03236
	loss_reward_2: 0.00471
	loss_policy_3: 0.03577
	accuracy_policy_3: 0.60926
	loss_value_3: 0.03344
	loss_reward_3: 0.00516
	loss_policy_4: 0.03769
	accuracy_policy_4: 0.58961
	loss_value_4: 0.03452
	loss_reward_4: 0.0055
	loss_policy_5: 0.03951
	accuracy_policy_5: 0.57193
	loss_value_5: 0.03561
	loss_reward_5: 0.00617
	loss_policy: 0.30321
	loss_value: 0.31676
	loss_reward: 0.02641
Optimization_Done 56400
[2024-05-07 17:06:55] [command] train weight_iter_56400.pkl 282 283
[2024-05-07 17:07:32] nn step 56500, lr: 0.043047.
	loss_policy_0: 0.21333
	accuracy_policy_0: 0.63135
	loss_value_0: 0.19394
	loss_policy_1: 0.04793
	accuracy_policy_1: 0.59371
	loss_value_1: 0.04047
	loss_reward_1: 0.00643
	loss_policy_2: 0.05092
	accuracy_policy_2: 0.56924
	loss_value_2: 0.0421
	loss_reward_2: 0.00611
	loss_policy_3: 0.05369
	accuracy_policy_3: 0.55312
	loss_value_3: 0.04366
	loss_reward_3: 0.00667
	loss_policy_4: 0.05568
	accuracy_policy_4: 0.53449
	loss_value_4: 0.04517
	loss_reward_4: 0.00704
	loss_policy_5: 0.0584
	accuracy_policy_5: 0.51898
	loss_value_5: 0.0465
	loss_reward_5: 0.00801
	loss_policy: 0.47995
	loss_value: 0.41184
	loss_reward: 0.03426
[2024-05-07 17:08:08] nn step 56600, lr: 0.043047.
	loss_policy_0: 0.18136
	accuracy_policy_0: 0.69264
	loss_value_0: 0.19088
	loss_policy_1: 0.04287
	accuracy_policy_1: 0.63842
	loss_value_1: 0.03978
	loss_reward_1: 0.00647
	loss_policy_2: 0.04623
	accuracy_policy_2: 0.61295
	loss_value_2: 0.0414
	loss_reward_2: 0.0062
	loss_policy_3: 0.04911
	accuracy_policy_3: 0.59318
	loss_value_3: 0.04298
	loss_reward_3: 0.00678
	loss_policy_4: 0.05204
	accuracy_policy_4: 0.57264
	loss_value_4: 0.04453
	loss_reward_4: 0.00717
	loss_policy_5: 0.05453
	accuracy_policy_5: 0.55332
	loss_value_5: 0.04589
	loss_reward_5: 0.00824
	loss_policy: 0.42614
	loss_value: 0.40546
	loss_reward: 0.03486
Optimization_Done 56600
[2024-05-07 17:10:16] [command] train weight_iter_56600.pkl 283 284
[2024-05-07 17:10:53] nn step 56700, lr: 0.043047.
	loss_policy_0: 0.16478
	accuracy_policy_0: 0.6733
	loss_value_0: 0.16846
	loss_policy_1: 0.03925
	accuracy_policy_1: 0.62117
	loss_value_1: 0.03542
	loss_reward_1: 0.00657
	loss_policy_2: 0.04234
	accuracy_policy_2: 0.5966
	loss_value_2: 0.03684
	loss_reward_2: 0.00648
	loss_policy_3: 0.04455
	accuracy_policy_3: 0.57682
	loss_value_3: 0.03831
	loss_reward_3: 0.00719
	loss_policy_4: 0.04709
	accuracy_policy_4: 0.56295
	loss_value_4: 0.03966
	loss_reward_4: 0.00737
	loss_policy_5: 0.0495
	accuracy_policy_5: 0.5457
	loss_value_5: 0.04087
	loss_reward_5: 0.00844
	loss_policy: 0.3875
	loss_value: 0.35956
	loss_reward: 0.03606
[2024-05-07 17:11:29] nn step 56800, lr: 0.043047.
	loss_policy_0: 0.14463
	accuracy_policy_0: 0.72309
	loss_value_0: 0.17103
	loss_policy_1: 0.03694
	accuracy_policy_1: 0.65148
	loss_value_1: 0.03581
	loss_reward_1: 0.00668
	loss_policy_2: 0.03982
	accuracy_policy_2: 0.63068
	loss_value_2: 0.03737
	loss_reward_2: 0.00643
	loss_policy_3: 0.04245
	accuracy_policy_3: 0.61498
	loss_value_3: 0.03881
	loss_reward_3: 0.00689
	loss_policy_4: 0.04513
	accuracy_policy_4: 0.59562
	loss_value_4: 0.04023
	loss_reward_4: 0.00737
	loss_policy_5: 0.04735
	accuracy_policy_5: 0.57693
	loss_value_5: 0.04163
	loss_reward_5: 0.00874
	loss_policy: 0.35632
	loss_value: 0.36487
	loss_reward: 0.03612
Optimization_Done 56800
[2024-05-07 17:13:34] [command] train weight_iter_56800.pkl 284 285
[2024-05-07 17:14:11] nn step 56900, lr: 0.043047.
	loss_policy_0: 0.1364
	accuracy_policy_0: 0.69457
	loss_value_0: 0.15835
	loss_policy_1: 0.03379
	accuracy_policy_1: 0.62943
	loss_value_1: 0.03303
	loss_reward_1: 0.00601
	loss_policy_2: 0.03668
	accuracy_policy_2: 0.60273
	loss_value_2: 0.03417
	loss_reward_2: 0.00592
	loss_policy_3: 0.03862
	accuracy_policy_3: 0.58334
	loss_value_3: 0.03516
	loss_reward_3: 0.00648
	loss_policy_4: 0.04035
	accuracy_policy_4: 0.5709
	loss_value_4: 0.03601
	loss_reward_4: 0.00687
	loss_policy_5: 0.04207
	accuracy_policy_5: 0.55602
	loss_value_5: 0.0369
	loss_reward_5: 0.00771
	loss_policy: 0.32791
	loss_value: 0.33362
	loss_reward: 0.03299
[2024-05-07 17:14:47] nn step 57000, lr: 0.043047.
	loss_policy_0: 0.11648
	accuracy_policy_0: 0.74607
	loss_value_0: 0.15669
	loss_policy_1: 0.0313
	accuracy_policy_1: 0.66447
	loss_value_1: 0.03274
	loss_reward_1: 0.00613
	loss_policy_2: 0.0344
	accuracy_policy_2: 0.6374
	loss_value_2: 0.03386
	loss_reward_2: 0.006
	loss_policy_3: 0.03593
	accuracy_policy_3: 0.62369
	loss_value_3: 0.03482
	loss_reward_3: 0.00648
	loss_policy_4: 0.03803
	accuracy_policy_4: 0.61012
	loss_value_4: 0.03583
	loss_reward_4: 0.00679
	loss_policy_5: 0.04059
	accuracy_policy_5: 0.58975
	loss_value_5: 0.03694
	loss_reward_5: 0.00817
	loss_policy: 0.29673
	loss_value: 0.33088
	loss_reward: 0.03357
Optimization_Done 57000
[2024-05-07 17:17:04] [command] train weight_iter_57000.pkl 285 286
[2024-05-07 17:17:40] nn step 57100, lr: 0.043047.
	loss_policy_0: 0.17614
	accuracy_policy_0: 0.63846
	loss_value_0: 0.16134
	loss_policy_1: 0.03999
	accuracy_policy_1: 0.59303
	loss_value_1: 0.03373
	loss_reward_1: 0.00491
	loss_policy_2: 0.04292
	accuracy_policy_2: 0.56342
	loss_value_2: 0.03504
	loss_reward_2: 0.00484
	loss_policy_3: 0.04508
	accuracy_policy_3: 0.54658
	loss_value_3: 0.03628
	loss_reward_3: 0.0052
	loss_policy_4: 0.04737
	accuracy_policy_4: 0.52623
	loss_value_4: 0.03742
	loss_reward_4: 0.00557
	loss_policy_5: 0.04912
	accuracy_policy_5: 0.51271
	loss_value_5: 0.0385
	loss_reward_5: 0.00654
	loss_policy: 0.40061
	loss_value: 0.34231
	loss_reward: 0.02706
[2024-05-07 17:18:16] nn step 57200, lr: 0.043047.
	loss_policy_0: 0.13357
	accuracy_policy_0: 0.71898
	loss_value_0: 0.15992
	loss_policy_1: 0.03366
	accuracy_policy_1: 0.65135
	loss_value_1: 0.03318
	loss_reward_1: 0.00492
	loss_policy_2: 0.03679
	accuracy_policy_2: 0.6242
	loss_value_2: 0.03455
	loss_reward_2: 0.00464
	loss_policy_3: 0.03937
	accuracy_policy_3: 0.60395
	loss_value_3: 0.03582
	loss_reward_3: 0.00523
	loss_policy_4: 0.04175
	accuracy_policy_4: 0.58193
	loss_value_4: 0.03702
	loss_reward_4: 0.00548
	loss_policy_5: 0.04405
	accuracy_policy_5: 0.56248
	loss_value_5: 0.03822
	loss_reward_5: 0.00638
	loss_policy: 0.3292
	loss_value: 0.33871
	loss_reward: 0.02665
Optimization_Done 57200
[2024-05-07 17:20:34] [command] train weight_iter_57200.pkl 286 287
[2024-05-07 17:21:11] nn step 57300, lr: 0.043047.
	loss_policy_0: 0.19381
	accuracy_policy_0: 0.63914
	loss_value_0: 0.17631
	loss_policy_1: 0.04415
	accuracy_policy_1: 0.59586
	loss_value_1: 0.03672
	loss_reward_1: 0.0051
	loss_policy_2: 0.04685
	accuracy_policy_2: 0.57354
	loss_value_2: 0.0382
	loss_reward_2: 0.00485
	loss_policy_3: 0.04958
	accuracy_policy_3: 0.55426
	loss_value_3: 0.03959
	loss_reward_3: 0.00513
	loss_policy_4: 0.05198
	accuracy_policy_4: 0.53746
	loss_value_4: 0.04088
	loss_reward_4: 0.00554
	loss_policy_5: 0.05445
	accuracy_policy_5: 0.52096
	loss_value_5: 0.04224
	loss_reward_5: 0.00636
	loss_policy: 0.44082
	loss_value: 0.37395
	loss_reward: 0.02698
[2024-05-07 17:21:46] nn step 57400, lr: 0.043047.
	loss_policy_0: 0.16457
	accuracy_policy_0: 0.69455
	loss_value_0: 0.1758
	loss_policy_1: 0.0394
	accuracy_policy_1: 0.64023
	loss_value_1: 0.03671
	loss_reward_1: 0.00517
	loss_policy_2: 0.04318
	accuracy_policy_2: 0.61111
	loss_value_2: 0.038
	loss_reward_2: 0.00489
	loss_policy_3: 0.04595
	accuracy_policy_3: 0.59264
	loss_value_3: 0.03924
	loss_reward_3: 0.00532
	loss_policy_4: 0.04883
	accuracy_policy_4: 0.574
	loss_value_4: 0.04047
	loss_reward_4: 0.00566
	loss_policy_5: 0.05166
	accuracy_policy_5: 0.55389
	loss_value_5: 0.04179
	loss_reward_5: 0.0065
	loss_policy: 0.39359
	loss_value: 0.37201
	loss_reward: 0.02755
Optimization_Done 57400
[2024-05-07 17:23:50] [command] train weight_iter_57400.pkl 287 288
[2024-05-07 17:24:27] nn step 57500, lr: 0.043047.
	loss_policy_0: 0.17293
	accuracy_policy_0: 0.64354
	loss_value_0: 0.1623
	loss_policy_1: 0.04033
	accuracy_policy_1: 0.59572
	loss_value_1: 0.03403
	loss_reward_1: 0.00614
	loss_policy_2: 0.04356
	accuracy_policy_2: 0.56854
	loss_value_2: 0.03556
	loss_reward_2: 0.00573
	loss_policy_3: 0.0464
	accuracy_policy_3: 0.54768
	loss_value_3: 0.0369
	loss_reward_3: 0.00624
	loss_policy_4: 0.04863
	accuracy_policy_4: 0.52898
	loss_value_4: 0.03824
	loss_reward_4: 0.00674
	loss_policy_5: 0.05083
	accuracy_policy_5: 0.51295
	loss_value_5: 0.03946
	loss_reward_5: 0.00762
	loss_policy: 0.40268
	loss_value: 0.3465
	loss_reward: 0.03247
[2024-05-07 17:25:02] nn step 57600, lr: 0.043047.
	loss_policy_0: 0.1456
	accuracy_policy_0: 0.70639
	loss_value_0: 0.16455
	loss_policy_1: 0.03648
	accuracy_policy_1: 0.64221
	loss_value_1: 0.03456
	loss_reward_1: 0.00606
	loss_policy_2: 0.03995
	accuracy_policy_2: 0.61137
	loss_value_2: 0.03592
	loss_reward_2: 0.00588
	loss_policy_3: 0.04285
	accuracy_policy_3: 0.58865
	loss_value_3: 0.03726
	loss_reward_3: 0.00624
	loss_policy_4: 0.04529
	accuracy_policy_4: 0.57043
	loss_value_4: 0.0385
	loss_reward_4: 0.00681
	loss_policy_5: 0.04816
	accuracy_policy_5: 0.54988
	loss_value_5: 0.03968
	loss_reward_5: 0.0077
	loss_policy: 0.35834
	loss_value: 0.35048
	loss_reward: 0.03269
Optimization_Done 57600
[2024-05-07 17:27:18] [command] train weight_iter_57600.pkl 288 289
[2024-05-07 17:27:55] nn step 57700, lr: 0.043047.
	loss_policy_0: 0.15196
	accuracy_policy_0: 0.65836
	loss_value_0: 0.15588
	loss_policy_1: 0.03631
	accuracy_policy_1: 0.59986
	loss_value_1: 0.03256
	loss_reward_1: 0.00637
	loss_policy_2: 0.03965
	accuracy_policy_2: 0.56842
	loss_value_2: 0.03389
	loss_reward_2: 0.0061
	loss_policy_3: 0.04176
	accuracy_policy_3: 0.55006
	loss_value_3: 0.03515
	loss_reward_3: 0.00654
	loss_policy_4: 0.044
	accuracy_policy_4: 0.5315
	loss_value_4: 0.03625
	loss_reward_4: 0.00709
	loss_policy_5: 0.04632
	accuracy_policy_5: 0.51318
	loss_value_5: 0.03725
	loss_reward_5: 0.00788
	loss_policy: 0.36
	loss_value: 0.33097
	loss_reward: 0.03398
[2024-05-07 17:28:31] nn step 57800, lr: 0.043047.
	loss_policy_0: 0.12083
	accuracy_policy_0: 0.70891
	loss_value_0: 0.14539
	loss_policy_1: 0.03127
	accuracy_policy_1: 0.63568
	loss_value_1: 0.03036
	loss_reward_1: 0.00616
	loss_policy_2: 0.03445
	accuracy_policy_2: 0.60146
	loss_value_2: 0.03157
	loss_reward_2: 0.00572
	loss_policy_3: 0.03688
	accuracy_policy_3: 0.58246
	loss_value_3: 0.03259
	loss_reward_3: 0.00627
	loss_policy_4: 0.03926
	accuracy_policy_4: 0.56381
	loss_value_4: 0.03367
	loss_reward_4: 0.00681
	loss_policy_5: 0.04163
	accuracy_policy_5: 0.54043
	loss_value_5: 0.03486
	loss_reward_5: 0.00792
	loss_policy: 0.30432
	loss_value: 0.30845
	loss_reward: 0.03288
Optimization_Done 57800
[2024-05-07 17:30:39] [command] train weight_iter_57800.pkl 289 290
[2024-05-07 17:31:16] nn step 57900, lr: 0.043047.
	loss_policy_0: 0.1393
	accuracy_policy_0: 0.66311
	loss_value_0: 0.16268
	loss_policy_1: 0.03311
	accuracy_policy_1: 0.60879
	loss_value_1: 0.03368
	loss_reward_1: 0.0053
	loss_policy_2: 0.03603
	accuracy_policy_2: 0.58094
	loss_value_2: 0.03467
	loss_reward_2: 0.00517
	loss_policy_3: 0.03825
	accuracy_policy_3: 0.56041
	loss_value_3: 0.03558
	loss_reward_3: 0.00575
	loss_policy_4: 0.0404
	accuracy_policy_4: 0.53922
	loss_value_4: 0.03654
	loss_reward_4: 0.00612
	loss_policy_5: 0.04206
	accuracy_policy_5: 0.5223
	loss_value_5: 0.03735
	loss_reward_5: 0.00699
	loss_policy: 0.32915
	loss_value: 0.34051
	loss_reward: 0.02933
[2024-05-07 17:31:52] nn step 58000, lr: 0.043047.
	loss_policy_0: 0.10831
	accuracy_policy_0: 0.73414
	loss_value_0: 0.15749
	loss_policy_1: 0.02848
	accuracy_policy_1: 0.65928
	loss_value_1: 0.03276
	loss_reward_1: 0.00534
	loss_policy_2: 0.03139
	accuracy_policy_2: 0.63412
	loss_value_2: 0.03391
	loss_reward_2: 0.00527
	loss_policy_3: 0.03362
	accuracy_policy_3: 0.60822
	loss_value_3: 0.03497
	loss_reward_3: 0.00556
	loss_policy_4: 0.03588
	accuracy_policy_4: 0.58562
	loss_value_4: 0.03595
	loss_reward_4: 0.00597
	loss_policy_5: 0.03822
	accuracy_policy_5: 0.56605
	loss_value_5: 0.03712
	loss_reward_5: 0.00708
	loss_policy: 0.2759
	loss_value: 0.3322
	loss_reward: 0.02922
Optimization_Done 58000
[2024-05-07 17:34:09] [command] train weight_iter_58000.pkl 290 291
[2024-05-07 17:34:46] nn step 58100, lr: 0.043047.
	loss_policy_0: 0.18181
	accuracy_policy_0: 0.63496
	loss_value_0: 0.1736
	loss_policy_1: 0.04186
	accuracy_policy_1: 0.58574
	loss_value_1: 0.03627
	loss_reward_1: 0.00539
	loss_policy_2: 0.04526
	accuracy_policy_2: 0.55484
	loss_value_2: 0.03785
	loss_reward_2: 0.00524
	loss_policy_3: 0.04815
	accuracy_policy_3: 0.53301
	loss_value_3: 0.03925
	loss_reward_3: 0.0057
	loss_policy_4: 0.05058
	accuracy_policy_4: 0.51244
	loss_value_4: 0.04056
	loss_reward_4: 0.00606
	loss_policy_5: 0.05321
	accuracy_policy_5: 0.49285
	loss_value_5: 0.04188
	loss_reward_5: 0.00697
	loss_policy: 0.42087
	loss_value: 0.36942
	loss_reward: 0.02936
[2024-05-07 17:35:21] nn step 58200, lr: 0.043047.
	loss_policy_0: 0.15212
	accuracy_policy_0: 0.69463
	loss_value_0: 0.17402
	loss_policy_1: 0.03696
	accuracy_policy_1: 0.63434
	loss_value_1: 0.03659
	loss_reward_1: 0.00545
	loss_policy_2: 0.04077
	accuracy_policy_2: 0.60785
	loss_value_2: 0.03804
	loss_reward_2: 0.0055
	loss_policy_3: 0.04416
	accuracy_policy_3: 0.58166
	loss_value_3: 0.03957
	loss_reward_3: 0.00584
	loss_policy_4: 0.04714
	accuracy_policy_4: 0.55455
	loss_value_4: 0.04077
	loss_reward_4: 0.0062
	loss_policy_5: 0.0498
	accuracy_policy_5: 0.53467
	loss_value_5: 0.04214
	loss_reward_5: 0.00729
	loss_policy: 0.37095
	loss_value: 0.37112
	loss_reward: 0.03027
Optimization_Done 58200
[2024-05-07 17:37:38] [command] train weight_iter_58200.pkl 291 292
[2024-05-07 17:38:14] nn step 58300, lr: 0.043047.
	loss_policy_0: 0.18344
	accuracy_policy_0: 0.63434
	loss_value_0: 0.16139
	loss_policy_1: 0.04261
	accuracy_policy_1: 0.5832
	loss_value_1: 0.03417
	loss_reward_1: 0.00624
	loss_policy_2: 0.04557
	accuracy_policy_2: 0.55936
	loss_value_2: 0.03582
	loss_reward_2: 0.00608
	loss_policy_3: 0.04896
	accuracy_policy_3: 0.53875
	loss_value_3: 0.03724
	loss_reward_3: 0.0066
	loss_policy_4: 0.05198
	accuracy_policy_4: 0.51557
	loss_value_4: 0.03855
	loss_reward_4: 0.00713
	loss_policy_5: 0.05441
	accuracy_policy_5: 0.49391
	loss_value_5: 0.03988
	loss_reward_5: 0.00819
	loss_policy: 0.42697
	loss_value: 0.34706
	loss_reward: 0.03425
[2024-05-07 17:38:50] nn step 58400, lr: 0.043047.
	loss_policy_0: 0.15792
	accuracy_policy_0: 0.68789
	loss_value_0: 0.16453
	loss_policy_1: 0.03893
	accuracy_policy_1: 0.62787
	loss_value_1: 0.0347
	loss_reward_1: 0.00644
	loss_policy_2: 0.04275
	accuracy_policy_2: 0.59977
	loss_value_2: 0.03631
	loss_reward_2: 0.00616
	loss_policy_3: 0.04669
	accuracy_policy_3: 0.57221
	loss_value_3: 0.03774
	loss_reward_3: 0.00662
	loss_policy_4: 0.05023
	accuracy_policy_4: 0.54574
	loss_value_4: 0.03914
	loss_reward_4: 0.00702
	loss_policy_5: 0.05316
	accuracy_policy_5: 0.52562
	loss_value_5: 0.04059
	loss_reward_5: 0.00845
	loss_policy: 0.38968
	loss_value: 0.35301
	loss_reward: 0.03469
Optimization_Done 58400
[2024-05-07 17:41:05] [command] train weight_iter_58400.pkl 292 293
[2024-05-07 17:41:41] nn step 58500, lr: 0.043047.
	loss_policy_0: 0.13394
	accuracy_policy_0: 0.68143
	loss_value_0: 0.14519
	loss_policy_1: 0.03339
	accuracy_policy_1: 0.61102
	loss_value_1: 0.03044
	loss_reward_1: 0.00592
	loss_policy_2: 0.03635
	accuracy_policy_2: 0.58412
	loss_value_2: 0.03173
	loss_reward_2: 0.00568
	loss_policy_3: 0.03947
	accuracy_policy_3: 0.55441
	loss_value_3: 0.03284
	loss_reward_3: 0.00618
	loss_policy_4: 0.04213
	accuracy_policy_4: 0.52943
	loss_value_4: 0.0339
	loss_reward_4: 0.0066
	loss_policy_5: 0.04451
	accuracy_policy_5: 0.50951
	loss_value_5: 0.03495
	loss_reward_5: 0.00759
	loss_policy: 0.32979
	loss_value: 0.30904
	loss_reward: 0.03197
[2024-05-07 17:42:17] nn step 58600, lr: 0.043047.
	loss_policy_0: 0.11504
	accuracy_policy_0: 0.73471
	loss_value_0: 0.15219
	loss_policy_1: 0.03147
	accuracy_policy_1: 0.64947
	loss_value_1: 0.03184
	loss_reward_1: 0.00628
	loss_policy_2: 0.03522
	accuracy_policy_2: 0.61939
	loss_value_2: 0.03312
	loss_reward_2: 0.00587
	loss_policy_3: 0.0385
	accuracy_policy_3: 0.59059
	loss_value_3: 0.03431
	loss_reward_3: 0.00651
	loss_policy_4: 0.04147
	accuracy_policy_4: 0.56223
	loss_value_4: 0.03557
	loss_reward_4: 0.00689
	loss_policy_5: 0.04427
	accuracy_policy_5: 0.54012
	loss_value_5: 0.03688
	loss_reward_5: 0.00825
	loss_policy: 0.30597
	loss_value: 0.32391
	loss_reward: 0.0338
Optimization_Done 58600
[2024-05-07 17:44:23] [command] train weight_iter_58600.pkl 293 294
[2024-05-07 17:45:00] nn step 58700, lr: 0.043047.
	loss_policy_0: 0.12698
	accuracy_policy_0: 0.69684
	loss_value_0: 0.14859
	loss_policy_1: 0.03045
	accuracy_policy_1: 0.64074
	loss_value_1: 0.03107
	loss_reward_1: 0.00488
	loss_policy_2: 0.03297
	accuracy_policy_2: 0.6174
	loss_value_2: 0.03231
	loss_reward_2: 0.00473
	loss_policy_3: 0.03564
	accuracy_policy_3: 0.58973
	loss_value_3: 0.0334
	loss_reward_3: 0.00509
	loss_policy_4: 0.03755
	accuracy_policy_4: 0.57639
	loss_value_4: 0.03432
	loss_reward_4: 0.00556
	loss_policy_5: 0.03942
	accuracy_policy_5: 0.555
	loss_value_5: 0.03522
	loss_reward_5: 0.00649
	loss_policy: 0.30302
	loss_value: 0.3149
	loss_reward: 0.02676
[2024-05-07 17:45:35] nn step 58800, lr: 0.043047.
	loss_policy_0: 0.09501
	accuracy_policy_0: 0.76145
	loss_value_0: 0.14398
	loss_policy_1: 0.0257
	accuracy_policy_1: 0.685
	loss_value_1: 0.03004
	loss_reward_1: 0.00486
	loss_policy_2: 0.02851
	accuracy_policy_2: 0.65135
	loss_value_2: 0.03112
	loss_reward_2: 0.00457
	loss_policy_3: 0.03089
	accuracy_policy_3: 0.63195
	loss_value_3: 0.03217
	loss_reward_3: 0.00491
	loss_policy_4: 0.03345
	accuracy_policy_4: 0.61195
	loss_value_4: 0.03308
	loss_reward_4: 0.00526
	loss_policy_5: 0.03581
	accuracy_policy_5: 0.58758
	loss_value_5: 0.03403
	loss_reward_5: 0.00623
	loss_policy: 0.24938
	loss_value: 0.30442
	loss_reward: 0.02583
Optimization_Done 58800
[2024-05-07 17:47:54] [command] train weight_iter_58800.pkl 294 295
[2024-05-07 17:48:30] nn step 58900, lr: 0.043047.
	loss_policy_0: 0.15314
	accuracy_policy_0: 0.6668
	loss_value_0: 0.15813
	loss_policy_1: 0.0354
	accuracy_policy_1: 0.61498
	loss_value_1: 0.03325
	loss_reward_1: 0.00521
	loss_policy_2: 0.03782
	accuracy_policy_2: 0.59541
	loss_value_2: 0.03456
	loss_reward_2: 0.00514
	loss_policy_3: 0.03992
	accuracy_policy_3: 0.57586
	loss_value_3: 0.03575
	loss_reward_3: 0.00568
	loss_policy_4: 0.04236
	accuracy_policy_4: 0.55943
	loss_value_4: 0.03679
	loss_reward_4: 0.00602
	loss_policy_5: 0.04436
	accuracy_policy_5: 0.53916
	loss_value_5: 0.03774
	loss_reward_5: 0.00672
	loss_policy: 0.353
	loss_value: 0.33622
	loss_reward: 0.02877
[2024-05-07 17:49:06] nn step 59000, lr: 0.043047.
	loss_policy_0: 0.12169
	accuracy_policy_0: 0.73418
	loss_value_0: 0.157
	loss_policy_1: 0.03076
	accuracy_policy_1: 0.66637
	loss_value_1: 0.03279
	loss_reward_1: 0.00516
	loss_policy_2: 0.03358
	accuracy_policy_2: 0.63947
	loss_value_2: 0.03422
	loss_reward_2: 0.00507
	loss_policy_3: 0.036
	accuracy_policy_3: 0.61701
	loss_value_3: 0.03532
	loss_reward_3: 0.00547
	loss_policy_4: 0.03816
	accuracy_policy_4: 0.60307
	loss_value_4: 0.03634
	loss_reward_4: 0.00595
	loss_policy_5: 0.04028
	accuracy_policy_5: 0.58438
	loss_value_5: 0.03755
	loss_reward_5: 0.00667
	loss_policy: 0.30047
	loss_value: 0.33321
	loss_reward: 0.02832
Optimization_Done 59000
[2024-05-07 17:51:12] [command] train weight_iter_59000.pkl 295 296
[2024-05-07 17:51:49] nn step 59100, lr: 0.043047.
	loss_policy_0: 0.2018
	accuracy_policy_0: 0.63313
	loss_value_0: 0.18031
	loss_policy_1: 0.04567
	accuracy_policy_1: 0.58912
	loss_value_1: 0.03767
	loss_reward_1: 0.00612
	loss_policy_2: 0.04881
	accuracy_policy_2: 0.56688
	loss_value_2: 0.03929
	loss_reward_2: 0.00608
	loss_policy_3: 0.05179
	accuracy_policy_3: 0.545
	loss_value_3: 0.04074
	loss_reward_3: 0.00646
	loss_policy_4: 0.05467
	accuracy_policy_4: 0.52707
	loss_value_4: 0.04218
	loss_reward_4: 0.00714
	loss_policy_5: 0.05702
	accuracy_policy_5: 0.51305
	loss_value_5: 0.04344
	loss_reward_5: 0.00811
	loss_policy: 0.45975
	loss_value: 0.38363
	loss_reward: 0.03391
[2024-05-07 17:52:24] nn step 59200, lr: 0.043047.
	loss_policy_0: 0.167
	accuracy_policy_0: 0.70066
	loss_value_0: 0.18034
	loss_policy_1: 0.04064
	accuracy_policy_1: 0.63557
	loss_value_1: 0.03775
	loss_reward_1: 0.00612
	loss_policy_2: 0.04457
	accuracy_policy_2: 0.60629
	loss_value_2: 0.03942
	loss_reward_2: 0.00596
	loss_policy_3: 0.04814
	accuracy_policy_3: 0.5768
	loss_value_3: 0.04083
	loss_reward_3: 0.00653
	loss_policy_4: 0.05099
	accuracy_policy_4: 0.55713
	loss_value_4: 0.04233
	loss_reward_4: 0.00705
	loss_policy_5: 0.05336
	accuracy_policy_5: 0.54385
	loss_value_5: 0.04371
	loss_reward_5: 0.00807
	loss_policy: 0.4047
	loss_value: 0.38438
	loss_reward: 0.03372
Optimization_Done 59200
[2024-05-07 17:54:40] [command] train weight_iter_59200.pkl 296 297
[2024-05-07 17:55:17] nn step 59300, lr: 0.043047.
	loss_policy_0: 0.17577
	accuracy_policy_0: 0.64037
	loss_value_0: 0.16198
	loss_policy_1: 0.0408
	accuracy_policy_1: 0.58863
	loss_value_1: 0.03392
	loss_reward_1: 0.00593
	loss_policy_2: 0.04377
	accuracy_policy_2: 0.55885
	loss_value_2: 0.03524
	loss_reward_2: 0.0058
	loss_policy_3: 0.0463
	accuracy_policy_3: 0.53766
	loss_value_3: 0.03663
	loss_reward_3: 0.0064
	loss_policy_4: 0.04889
	accuracy_policy_4: 0.51191
	loss_value_4: 0.03792
	loss_reward_4: 0.00692
	loss_policy_5: 0.05122
	accuracy_policy_5: 0.49348
	loss_value_5: 0.03908
	loss_reward_5: 0.00784
	loss_policy: 0.40675
	loss_value: 0.34477
	loss_reward: 0.03289
[2024-05-07 17:55:53] nn step 59400, lr: 0.043047.
	loss_policy_0: 0.15129
	accuracy_policy_0: 0.69715
	loss_value_0: 0.16283
	loss_policy_1: 0.03728
	accuracy_policy_1: 0.62885
	loss_value_1: 0.03413
	loss_reward_1: 0.00588
	loss_policy_2: 0.0407
	accuracy_policy_2: 0.59703
	loss_value_2: 0.03547
	loss_reward_2: 0.00586
	loss_policy_3: 0.04362
	accuracy_policy_3: 0.5701
	loss_value_3: 0.03683
	loss_reward_3: 0.0062
	loss_policy_4: 0.04631
	accuracy_policy_4: 0.55074
	loss_value_4: 0.03814
	loss_reward_4: 0.00669
	loss_policy_5: 0.04915
	accuracy_policy_5: 0.52621
	loss_value_5: 0.0395
	loss_reward_5: 0.00766
	loss_policy: 0.36837
	loss_value: 0.3469
	loss_reward: 0.03229
Optimization_Done 59400
[2024-05-07 17:57:59] [command] train weight_iter_59400.pkl 297 298
[2024-05-07 17:58:36] nn step 59500, lr: 0.043047.
	loss_policy_0: 0.14503
	accuracy_policy_0: 0.64707
	loss_value_0: 0.14866
	loss_policy_1: 0.03442
	accuracy_policy_1: 0.58998
	loss_value_1: 0.03081
	loss_reward_1: 0.00518
	loss_policy_2: 0.03681
	accuracy_policy_2: 0.55918
	loss_value_2: 0.03199
	loss_reward_2: 0.00503
	loss_policy_3: 0.0391
	accuracy_policy_3: 0.53561
	loss_value_3: 0.03301
	loss_reward_3: 0.00557
	loss_policy_4: 0.04095
	accuracy_policy_4: 0.51652
	loss_value_4: 0.03387
	loss_reward_4: 0.00595
	loss_policy_5: 0.04333
	accuracy_policy_5: 0.49031
	loss_value_5: 0.03477
	loss_reward_5: 0.00696
	loss_policy: 0.33964
	loss_value: 0.31311
	loss_reward: 0.02868
[2024-05-07 17:59:11] nn step 59600, lr: 0.043047.
	loss_policy_0: 0.11993
	accuracy_policy_0: 0.71314
	loss_value_0: 0.14939
	loss_policy_1: 0.03076
	accuracy_policy_1: 0.64025
	loss_value_1: 0.03106
	loss_reward_1: 0.00541
	loss_policy_2: 0.03384
	accuracy_policy_2: 0.60951
	loss_value_2: 0.03219
	loss_reward_2: 0.00514
	loss_policy_3: 0.0366
	accuracy_policy_3: 0.58021
	loss_value_3: 0.03326
	loss_reward_3: 0.00575
	loss_policy_4: 0.03909
	accuracy_policy_4: 0.55133
	loss_value_4: 0.03434
	loss_reward_4: 0.00617
	loss_policy_5: 0.04134
	accuracy_policy_5: 0.52693
	loss_value_5: 0.03545
	loss_reward_5: 0.00723
	loss_policy: 0.30156
	loss_value: 0.31569
	loss_reward: 0.0297
Optimization_Done 59600
[2024-05-07 18:01:28] [command] train weight_iter_59600.pkl 298 299
[2024-05-07 18:02:05] nn step 59700, lr: 0.043047.
	loss_policy_0: 0.15065
	accuracy_policy_0: 0.65389
	loss_value_0: 0.16684
	loss_policy_1: 0.03617
	accuracy_policy_1: 0.59119
	loss_value_1: 0.03473
	loss_reward_1: 0.0052
	loss_policy_2: 0.03922
	accuracy_policy_2: 0.56301
	loss_value_2: 0.03605
	loss_reward_2: 0.00517
	loss_policy_3: 0.04197
	accuracy_policy_3: 0.53758
	loss_value_3: 0.03723
	loss_reward_3: 0.00588
	loss_policy_4: 0.04461
	accuracy_policy_4: 0.51805
	loss_value_4: 0.03836
	loss_reward_4: 0.00615
	loss_policy_5: 0.04654
	accuracy_policy_5: 0.4958
	loss_value_5: 0.03954
	loss_reward_5: 0.00714
	loss_policy: 0.35915
	loss_value: 0.35275
	loss_reward: 0.02954
[2024-05-07 18:02:40] nn step 59800, lr: 0.043047.
	loss_policy_0: 0.11866
	accuracy_policy_0: 0.71729
	loss_value_0: 0.16164
	loss_policy_1: 0.03095
	accuracy_policy_1: 0.64291
	loss_value_1: 0.03385
	loss_reward_1: 0.00522
	loss_policy_2: 0.03405
	accuracy_policy_2: 0.6109
	loss_value_2: 0.03508
	loss_reward_2: 0.00514
	loss_policy_3: 0.03694
	accuracy_policy_3: 0.58779
	loss_value_3: 0.03613
	loss_reward_3: 0.00567
	loss_policy_4: 0.03954
	accuracy_policy_4: 0.56137
	loss_value_4: 0.03733
	loss_reward_4: 0.00611
	loss_policy_5: 0.04218
	accuracy_policy_5: 0.54113
	loss_value_5: 0.03848
	loss_reward_5: 0.00701
	loss_policy: 0.30231
	loss_value: 0.34252
	loss_reward: 0.02916
Optimization_Done 59800
[2024-05-07 18:04:57] [command] train weight_iter_59800.pkl 299 300
[2024-05-07 18:05:34] nn step 59900, lr: 0.043047.
	loss_policy_0: 0.19337
	accuracy_policy_0: 0.63404
	loss_value_0: 0.18244
	loss_policy_1: 0.045
	accuracy_policy_1: 0.5841
	loss_value_1: 0.03817
	loss_reward_1: 0.00583
	loss_policy_2: 0.04849
	accuracy_policy_2: 0.55902
	loss_value_2: 0.03981
	loss_reward_2: 0.00577
	loss_policy_3: 0.05199
	accuracy_policy_3: 0.53357
	loss_value_3: 0.04129
	loss_reward_3: 0.00623
	loss_policy_4: 0.05482
	accuracy_policy_4: 0.51678
	loss_value_4: 0.04294
	loss_reward_4: 0.00683
	loss_policy_5: 0.05748
	accuracy_policy_5: 0.49723
	loss_value_5: 0.04421
	loss_reward_5: 0.0078
	loss_policy: 0.45114
	loss_value: 0.38885
	loss_reward: 0.03246
[2024-05-07 18:06:09] nn step 60000, lr: 0.043047.
	loss_policy_0: 0.16111
	accuracy_policy_0: 0.69053
	loss_value_0: 0.17966
	loss_policy_1: 0.03961
	accuracy_policy_1: 0.62717
	loss_value_1: 0.03772
	loss_reward_1: 0.00585
	loss_policy_2: 0.04354
	accuracy_policy_2: 0.59572
	loss_value_2: 0.0393
	loss_reward_2: 0.00557
	loss_policy_3: 0.04723
	accuracy_policy_3: 0.57039
	loss_value_3: 0.04085
	loss_reward_3: 0.00621
	loss_policy_4: 0.05001
	accuracy_policy_4: 0.55338
	loss_value_4: 0.04238
	loss_reward_4: 0.00676
	loss_policy_5: 0.05327
	accuracy_policy_5: 0.52906
	loss_value_5: 0.04387
	loss_reward_5: 0.00771
	loss_policy: 0.39478
	loss_value: 0.38378
	loss_reward: 0.0321
Optimization_Done 60000
