A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 22:14:26] [command] train weight_iter_0.pkl 1 1
[2024-05-07 22:15:12] nn step 100, lr: 0.02.
	loss_policy_0: 0.03591
	accuracy_policy_0: 0.87059
	loss_value_0: 0.28503
	loss_policy_1: 0.01149
	accuracy_policy_1: 0.74559
	loss_value_1: 0.05437
	loss_reward_1: 0.01544
	loss_policy_2: 0.01195
	accuracy_policy_2: 0.75027
	loss_value_2: 0.05451
	loss_reward_2: 0.01538
	loss_policy_3: 0.01232
	accuracy_policy_3: 0.75604
	loss_value_3: 0.0543
	loss_reward_3: 0.01536
	loss_policy_4: 0.01248
	accuracy_policy_4: 0.73594
	loss_value_4: 0.05435
	loss_reward_4: 0.0154
	loss_policy_5: 0.01215
	accuracy_policy_5: 0.76037
	loss_value_5: 0.05425
	loss_reward_5: 0.0154
	loss_policy: 0.0963
	loss_value: 0.55681
	loss_reward: 0.07698
[2024-05-07 22:15:52] nn step 200, lr: 0.02.
	loss_policy_0: 0.00544
	accuracy_policy_0: 0.94816
	loss_value_0: 0.20622
	loss_policy_1: 0.00259
	accuracy_policy_1: 0.90852
	loss_value_1: 0.04123
	loss_reward_1: 0.0
	loss_policy_2: 0.00363
	accuracy_policy_2: 0.8698
	loss_value_2: 0.0412
	loss_reward_2: 0.0
	loss_policy_3: 0.0042
	accuracy_policy_3: 0.8559
	loss_value_3: 0.04116
	loss_reward_3: 0.0
	loss_policy_4: 0.00493
	accuracy_policy_4: 0.83334
	loss_value_4: 0.04112
	loss_reward_4: 0.0
	loss_policy_5: 0.00431
	accuracy_policy_5: 0.8558
	loss_value_5: 0.04108
	loss_reward_5: 0.0
	loss_policy: 0.02511
	loss_value: 0.41201
	loss_reward: 0.0
Optimization_Done 200
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 22:20:07] [command] train weight_iter_200.pkl 1 2
[2024-05-07 22:20:53] nn step 300, lr: 0.04.
	loss_policy_0: 0.03582
	accuracy_policy_0: 0.75506
	loss_value_0: 0.21299
	loss_policy_1: 0.00964
	accuracy_policy_1: 0.71625
	loss_value_1: 0.04275
	loss_reward_1: 0.0
	loss_policy_2: 0.01019
	accuracy_policy_2: 0.71025
	loss_value_2: 0.04271
	loss_reward_2: 0.0
	loss_policy_3: 0.01081
	accuracy_policy_3: 0.68439
	loss_value_3: 0.04267
	loss_reward_3: 0.0
	loss_policy_4: 0.01117
	accuracy_policy_4: 0.67223
	loss_value_4: 0.04265
	loss_reward_4: 0.0
	loss_policy_5: 0.0112
	accuracy_policy_5: 0.65904
	loss_value_5: 0.04261
	loss_reward_5: 0.0
	loss_policy: 0.08881
	loss_value: 0.42639
	loss_reward: 0.0
[2024-05-07 22:21:33] nn step 400, lr: 0.04.
	loss_policy_0: 0.02425
	accuracy_policy_0: 0.81566
	loss_value_0: 0.21625
	loss_policy_1: 0.00705
	accuracy_policy_1: 0.7608
	loss_value_1: 0.04321
	loss_reward_1: 0.0
	loss_policy_2: 0.00779
	accuracy_policy_2: 0.74777
	loss_value_2: 0.04315
	loss_reward_2: 0.0
	loss_policy_3: 0.00872
	accuracy_policy_3: 0.72355
	loss_value_3: 0.04309
	loss_reward_3: 0.0
	loss_policy_4: 0.00917
	accuracy_policy_4: 0.71037
	loss_value_4: 0.04304
	loss_reward_4: 0.0
	loss_policy_5: 0.00951
	accuracy_policy_5: 0.69229
	loss_value_5: 0.04299
	loss_reward_5: 0.0
	loss_policy: 0.0665
	loss_value: 0.43175
	loss_reward: 0.0
Optimization_Done 400
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 22:31:05] [command] train weight_iter_400.pkl 1 3
[2024-05-07 22:31:52] nn step 500, lr: 0.06.
	loss_policy_0: 0.02093
	accuracy_policy_0: 0.81818
	loss_value_0: 0.2095
	loss_policy_1: 0.00564
	accuracy_policy_1: 0.78979
	loss_value_1: 0.04183
	loss_reward_1: 0.0
	loss_policy_2: 0.00618
	accuracy_policy_2: 0.78604
	loss_value_2: 0.04178
	loss_reward_2: 0.0
	loss_policy_3: 0.00648
	accuracy_policy_3: 0.78225
	loss_value_3: 0.04172
	loss_reward_3: 0.0
	loss_policy_4: 0.00695
	accuracy_policy_4: 0.77572
	loss_value_4: 0.04167
	loss_reward_4: 0.0
	loss_policy_5: 0.00713
	accuracy_policy_5: 0.76186
	loss_value_5: 0.04163
	loss_reward_5: 0.0
	loss_policy: 0.05331
	loss_value: 0.41812
	loss_reward: 0.0
[2024-05-07 22:32:33] nn step 600, lr: 0.06.
	loss_policy_0: 0.01682
	accuracy_policy_0: 0.83947
	loss_value_0: 0.19455
	loss_policy_1: 0.00445
	accuracy_policy_1: 0.81797
	loss_value_1: 0.03888
	loss_reward_1: 0.0
	loss_policy_2: 0.00478
	accuracy_policy_2: 0.81062
	loss_value_2: 0.03883
	loss_reward_2: 0.0
	loss_policy_3: 0.00501
	accuracy_policy_3: 0.80488
	loss_value_3: 0.03878
	loss_reward_3: 0.0
	loss_policy_4: 0.00533
	accuracy_policy_4: 0.8027
	loss_value_4: 0.03873
	loss_reward_4: 0.0
	loss_policy_5: 0.00555
	accuracy_policy_5: 0.7909
	loss_value_5: 0.03867
	loss_reward_5: 0.0
	loss_policy: 0.04195
	loss_value: 0.38844
	loss_reward: 0.0
Optimization_Done 600
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 22:42:21] [command] train weight_iter_600.pkl 1 4
[2024-05-07 22:43:11] nn step 700, lr: 0.08.
	loss_policy_0: 0.01896
	accuracy_policy_0: 0.84834
	loss_value_0: 0.21283
	loss_policy_1: 0.00505
	accuracy_policy_1: 0.83104
	loss_value_1: 0.04254
	loss_reward_1: 0.0
	loss_policy_2: 0.00543
	accuracy_policy_2: 0.83033
	loss_value_2: 0.04248
	loss_reward_2: 0.0
	loss_policy_3: 0.00575
	accuracy_policy_3: 0.82531
	loss_value_3: 0.04242
	loss_reward_3: 0.0
	loss_policy_4: 0.00617
	accuracy_policy_4: 0.81797
	loss_value_4: 0.04237
	loss_reward_4: 0.0
	loss_policy_5: 0.00654
	accuracy_policy_5: 0.8058
	loss_value_5: 0.04232
	loss_reward_5: 0.0
	loss_policy: 0.04791
	loss_value: 0.42495
	loss_reward: 0.0
[2024-05-07 22:43:52] nn step 800, lr: 0.08.
	loss_policy_0: 0.01805
	accuracy_policy_0: 0.8501
	loss_value_0: 0.21049
	loss_policy_1: 0.00459
	accuracy_policy_1: 0.83309
	loss_value_1: 0.04206
	loss_reward_1: 0.0
	loss_policy_2: 0.0048
	accuracy_policy_2: 0.83564
	loss_value_2: 0.04201
	loss_reward_2: 0.0
	loss_policy_3: 0.00506
	accuracy_policy_3: 0.8343
	loss_value_3: 0.04194
	loss_reward_3: 0.0
	loss_policy_4: 0.00535
	accuracy_policy_4: 0.82988
	loss_value_4: 0.04186
	loss_reward_4: 0.0
	loss_policy_5: 0.00562
	accuracy_policy_5: 0.81953
	loss_value_5: 0.0418
	loss_reward_5: 0.0
	loss_policy: 0.04348
	loss_value: 0.42016
	loss_reward: 0.0
Optimization_Done 800
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-07 22:56:59] [command] train weight_iter_800.pkl 1 5
[2024-05-07 22:57:52] nn step 900, lr: 0.1.
	loss_policy_0: 0.0175
	accuracy_policy_0: 0.8558
	loss_value_0: 0.21653
	loss_policy_1: 0.00432
	accuracy_policy_1: 0.84641
	loss_value_1: 0.04328
	loss_reward_1: 0.0
	loss_policy_2: 0.00456
	accuracy_policy_2: 0.84381
	loss_value_2: 0.04321
	loss_reward_2: 0.0
	loss_policy_3: 0.00484
	accuracy_policy_3: 0.84445
	loss_value_3: 0.04315
	loss_reward_3: 0.0
	loss_policy_4: 0.00523
	accuracy_policy_4: 0.8376
	loss_value_4: 0.0431
	loss_reward_4: 0.0
	loss_policy_5: 0.00551
	accuracy_policy_5: 0.8209
	loss_value_5: 0.04305
	loss_reward_5: 0.0
	loss_policy: 0.04195
	loss_value: 0.43232
	loss_reward: 0.0
[2024-05-07 22:58:33] nn step 1000, lr: 0.1.
	loss_policy_0: 0.01533
	accuracy_policy_0: 0.86787
	loss_value_0: 0.20522
	loss_policy_1: 0.00372
	accuracy_policy_1: 0.85277
	loss_value_1: 0.04103
	loss_reward_1: 0.0
	loss_policy_2: 0.00386
	accuracy_policy_2: 0.85447
	loss_value_2: 0.04097
	loss_reward_2: 0.0
	loss_policy_3: 0.00403
	accuracy_policy_3: 0.85639
	loss_value_3: 0.04091
	loss_reward_3: 0.0
	loss_policy_4: 0.0043
	accuracy_policy_4: 0.84498
	loss_value_4: 0.04085
	loss_reward_4: 0.0
	loss_policy_5: 0.00453
	accuracy_policy_5: 0.83672
	loss_value_5: 0.04078
	loss_reward_5: 0.0
	loss_policy: 0.03578
	loss_value: 0.40977
	loss_reward: 0.0
Optimization_Done 1000
[2024-05-07 23:01:14] [command] train weight_iter_1000.pkl 2 6
[2024-05-07 23:01:56] nn step 1100, lr: 0.1.
	loss_policy_0: 0.01347
	accuracy_policy_0: 0.88078
	loss_value_0: 0.20043
	loss_policy_1: 0.00347
	accuracy_policy_1: 0.86969
	loss_value_1: 0.04006
	loss_reward_1: 2e-05
	loss_policy_2: 0.00367
	accuracy_policy_2: 0.87365
	loss_value_2: 0.03999
	loss_reward_2: 3e-05
	loss_policy_3: 0.00393
	accuracy_policy_3: 0.87203
	loss_value_3: 0.03992
	loss_reward_3: 3e-05
	loss_policy_4: 0.0043
	accuracy_policy_4: 0.86295
	loss_value_4: 0.03985
	loss_reward_4: 2e-05
	loss_policy_5: 0.0045
	accuracy_policy_5: 0.85842
	loss_value_5: 0.03978
	loss_reward_5: 3e-05
	loss_policy: 0.03334
	loss_value: 0.40003
	loss_reward: 0.00013
[2024-05-07 23:02:37] nn step 1200, lr: 0.1.
	loss_policy_0: 0.01096
	accuracy_policy_0: 0.89543
	loss_value_0: 0.19419
	loss_policy_1: 0.00279
	accuracy_policy_1: 0.88215
	loss_value_1: 0.03881
	loss_reward_1: 3e-05
	loss_policy_2: 0.00298
	accuracy_policy_2: 0.88422
	loss_value_2: 0.03875
	loss_reward_2: 2e-05
	loss_policy_3: 0.00314
	accuracy_policy_3: 0.88635
	loss_value_3: 0.03868
	loss_reward_3: 2e-05
	loss_policy_4: 0.00344
	accuracy_policy_4: 0.87803
	loss_value_4: 0.0386
	loss_reward_4: 1e-05
	loss_policy_5: 0.00367
	accuracy_policy_5: 0.87361
	loss_value_5: 0.03852
	loss_reward_5: 1e-05
	loss_policy: 0.02696
	loss_value: 0.38756
	loss_reward: 0.0001
Optimization_Done 1200
[2024-05-07 23:05:11] [command] train weight_iter_1200.pkl 3 7
[2024-05-07 23:05:53] nn step 1300, lr: 0.1.
	loss_policy_0: 0.00833
	accuracy_policy_0: 0.93615
	loss_value_0: 0.21005
	loss_policy_1: 0.00231
	accuracy_policy_1: 0.93006
	loss_value_1: 0.04196
	loss_reward_1: 0.00026
	loss_policy_2: 0.00256
	accuracy_policy_2: 0.92744
	loss_value_2: 0.04185
	loss_reward_2: 0.00024
	loss_policy_3: 0.00277
	accuracy_policy_3: 0.92561
	loss_value_3: 0.04174
	loss_reward_3: 0.00027
	loss_policy_4: 0.00311
	accuracy_policy_4: 0.92609
	loss_value_4: 0.04164
	loss_reward_4: 0.0003
	loss_policy_5: 0.00329
	accuracy_policy_5: 0.92379
	loss_value_5: 0.04152
	loss_reward_5: 0.00026
	loss_policy: 0.02236
	loss_value: 0.41877
	loss_reward: 0.00134
[2024-05-07 23:06:34] nn step 1400, lr: 0.1.
	loss_policy_0: 0.00593
	accuracy_policy_0: 0.94861
	loss_value_0: 0.20595
	loss_policy_1: 0.00174
	accuracy_policy_1: 0.94023
	loss_value_1: 0.04115
	loss_reward_1: 0.00018
	loss_policy_2: 0.00194
	accuracy_policy_2: 0.94057
	loss_value_2: 0.04106
	loss_reward_2: 0.00018
	loss_policy_3: 0.00212
	accuracy_policy_3: 0.936
	loss_value_3: 0.04097
	loss_reward_3: 0.00019
	loss_policy_4: 0.00237
	accuracy_policy_4: 0.93592
	loss_value_4: 0.04085
	loss_reward_4: 0.0002
	loss_policy_5: 0.00256
	accuracy_policy_5: 0.93291
	loss_value_5: 0.04074
	loss_reward_5: 0.00019
	loss_policy: 0.01665
	loss_value: 0.41072
	loss_reward: 0.00095
Optimization_Done 1400
[2024-05-07 23:09:07] [command] train weight_iter_1400.pkl 4 8
[2024-05-07 23:09:49] nn step 1500, lr: 0.1.
	loss_policy_0: 0.00595
	accuracy_policy_0: 0.95082
	loss_value_0: 0.1941
	loss_policy_1: 0.00202
	accuracy_policy_1: 0.93262
	loss_value_1: 0.03879
	loss_reward_1: 0.00019
	loss_policy_2: 0.00215
	accuracy_policy_2: 0.93172
	loss_value_2: 0.0387
	loss_reward_2: 0.00015
	loss_policy_3: 0.00237
	accuracy_policy_3: 0.92857
	loss_value_3: 0.03859
	loss_reward_3: 0.00018
	loss_policy_4: 0.00263
	accuracy_policy_4: 0.92529
	loss_value_4: 0.03849
	loss_reward_4: 0.00017
	loss_policy_5: 0.00285
	accuracy_policy_5: 0.92236
	loss_value_5: 0.03837
	loss_reward_5: 0.00018
	loss_policy: 0.01797
	loss_value: 0.38703
	loss_reward: 0.00088
[2024-05-07 23:10:30] nn step 1600, lr: 0.1.
	loss_policy_0: 0.00516
	accuracy_policy_0: 0.95205
	loss_value_0: 0.19192
	loss_policy_1: 0.00175
	accuracy_policy_1: 0.93402
	loss_value_1: 0.03836
	loss_reward_1: 0.00017
	loss_policy_2: 0.00199
	accuracy_policy_2: 0.9308
	loss_value_2: 0.03824
	loss_reward_2: 0.00017
	loss_policy_3: 0.00213
	accuracy_policy_3: 0.9293
	loss_value_3: 0.03813
	loss_reward_3: 0.00015
	loss_policy_4: 0.00238
	accuracy_policy_4: 0.92404
	loss_value_4: 0.03804
	loss_reward_4: 0.00015
	loss_policy_5: 0.00256
	accuracy_policy_5: 0.92297
	loss_value_5: 0.03793
	loss_reward_5: 0.00013
	loss_policy: 0.01597
	loss_value: 0.38263
	loss_reward: 0.00077
Optimization_Done 1600
[2024-05-07 23:12:55] [command] train weight_iter_1600.pkl 5 9
[2024-05-07 23:13:38] nn step 1700, lr: 0.1.
	loss_policy_0: 0.00645
	accuracy_policy_0: 0.94664
	loss_value_0: 0.17949
	loss_policy_1: 0.00271
	accuracy_policy_1: 0.92025
	loss_value_1: 0.03585
	loss_reward_1: 0.00047
	loss_policy_2: 0.00311
	accuracy_policy_2: 0.91146
	loss_value_2: 0.03572
	loss_reward_2: 0.00044
	loss_policy_3: 0.00352
	accuracy_policy_3: 0.90385
	loss_value_3: 0.03559
	loss_reward_3: 0.0004
	loss_policy_4: 0.00382
	accuracy_policy_4: 0.89906
	loss_value_4: 0.03544
	loss_reward_4: 0.00043
	loss_policy_5: 0.00417
	accuracy_policy_5: 0.88902
	loss_value_5: 0.03529
	loss_reward_5: 0.00049
	loss_policy: 0.02378
	loss_value: 0.35739
	loss_reward: 0.00223
[2024-05-07 23:14:20] nn step 1800, lr: 0.1.
	loss_policy_0: 0.0052
	accuracy_policy_0: 0.95191
	loss_value_0: 0.1693
	loss_policy_1: 0.00214
	accuracy_policy_1: 0.92959
	loss_value_1: 0.03381
	loss_reward_1: 0.00034
	loss_policy_2: 0.00241
	accuracy_policy_2: 0.92303
	loss_value_2: 0.03367
	loss_reward_2: 0.00031
	loss_policy_3: 0.00257
	accuracy_policy_3: 0.91654
	loss_value_3: 0.03353
	loss_reward_3: 0.0003
	loss_policy_4: 0.00279
	accuracy_policy_4: 0.91602
	loss_value_4: 0.03339
	loss_reward_4: 0.0003
	loss_policy_5: 0.00295
	accuracy_policy_5: 0.91062
	loss_value_5: 0.03324
	loss_reward_5: 0.00034
	loss_policy: 0.01805
	loss_value: 0.33695
	loss_reward: 0.00158
Optimization_Done 1800
[2024-05-07 23:16:57] [command] train weight_iter_1800.pkl 6 10
[2024-05-07 23:17:39] nn step 1900, lr: 0.1.
	loss_policy_0: 0.0054
	accuracy_policy_0: 0.96336
	loss_value_0: 0.16105
	loss_policy_1: 0.00207
	accuracy_policy_1: 0.94457
	loss_value_1: 0.03202
	loss_reward_1: 0.00184
	loss_policy_2: 0.00234
	accuracy_policy_2: 0.94008
	loss_value_2: 0.03162
	loss_reward_2: 0.00183
	loss_policy_3: 0.00258
	accuracy_policy_3: 0.93391
	loss_value_3: 0.03119
	loss_reward_3: 0.00178
	loss_policy_4: 0.00277
	accuracy_policy_4: 0.93252
	loss_value_4: 0.03075
	loss_reward_4: 0.00185
	loss_policy_5: 0.00294
	accuracy_policy_5: 0.92938
	loss_value_5: 0.03031
	loss_reward_5: 0.00183
	loss_policy: 0.01809
	loss_value: 0.31693
	loss_reward: 0.00912
[2024-05-07 23:18:20] nn step 2000, lr: 0.1.
	loss_policy_0: 0.00409
	accuracy_policy_0: 0.96904
	loss_value_0: 0.15273
	loss_policy_1: 0.00163
	accuracy_policy_1: 0.95109
	loss_value_1: 0.03037
	loss_reward_1: 0.00153
	loss_policy_2: 0.00184
	accuracy_policy_2: 0.94566
	loss_value_2: 0.03004
	loss_reward_2: 0.0015
	loss_policy_3: 0.00202
	accuracy_policy_3: 0.94264
	loss_value_3: 0.02968
	loss_reward_3: 0.00145
	loss_policy_4: 0.00216
	accuracy_policy_4: 0.94178
	loss_value_4: 0.02935
	loss_reward_4: 0.00142
	loss_policy_5: 0.00227
	accuracy_policy_5: 0.94092
	loss_value_5: 0.029
	loss_reward_5: 0.00148
	loss_policy: 0.014
	loss_value: 0.30118
	loss_reward: 0.00738
Optimization_Done 2000
[2024-05-07 23:20:57] [command] train weight_iter_2000.pkl 7 11
[2024-05-07 23:21:39] nn step 2100, lr: 0.1.
	loss_policy_0: 0.00591
	accuracy_policy_0: 0.95996
	loss_value_0: 0.13226
	loss_policy_1: 0.00189
	accuracy_policy_1: 0.94787
	loss_value_1: 0.02621
	loss_reward_1: 0.00312
	loss_policy_2: 0.00203
	accuracy_policy_2: 0.946
	loss_value_2: 0.02573
	loss_reward_2: 0.0031
	loss_policy_3: 0.00214
	accuracy_policy_3: 0.945
	loss_value_3: 0.02522
	loss_reward_3: 0.00313
	loss_policy_4: 0.0022
	accuracy_policy_4: 0.94504
	loss_value_4: 0.0247
	loss_reward_4: 0.0031
	loss_policy_5: 0.00231
	accuracy_policy_5: 0.94365
	loss_value_5: 0.02411
	loss_reward_5: 0.00311
	loss_policy: 0.01647
	loss_value: 0.25823
	loss_reward: 0.01556
[2024-05-07 23:22:20] nn step 2200, lr: 0.1.
	loss_policy_0: 0.0053
	accuracy_policy_0: 0.9608
	loss_value_0: 0.13185
	loss_policy_1: 0.00175
	accuracy_policy_1: 0.94953
	loss_value_1: 0.02627
	loss_reward_1: 0.00264
	loss_policy_2: 0.00187
	accuracy_policy_2: 0.94982
	loss_value_2: 0.02593
	loss_reward_2: 0.00257
	loss_policy_3: 0.00196
	accuracy_policy_3: 0.94699
	loss_value_3: 0.02555
	loss_reward_3: 0.00253
	loss_policy_4: 0.00206
	accuracy_policy_4: 0.94588
	loss_value_4: 0.02509
	loss_reward_4: 0.00259
	loss_policy_5: 0.00209
	accuracy_policy_5: 0.94654
	loss_value_5: 0.02454
	loss_reward_5: 0.00263
	loss_policy: 0.01503
	loss_value: 0.25924
	loss_reward: 0.01296
Optimization_Done 2200
[2024-05-07 23:24:57] [command] train weight_iter_2200.pkl 8 12
[2024-05-07 23:25:40] nn step 2300, lr: 0.1.
	loss_policy_0: 0.00449
	accuracy_policy_0: 0.97512
	loss_value_0: 0.11869
	loss_policy_1: 0.0014
	accuracy_policy_1: 0.96811
	loss_value_1: 0.02368
	loss_reward_1: 0.00331
	loss_policy_2: 0.00151
	accuracy_policy_2: 0.96672
	loss_value_2: 0.02339
	loss_reward_2: 0.00321
	loss_policy_3: 0.00157
	accuracy_policy_3: 0.9652
	loss_value_3: 0.02304
	loss_reward_3: 0.00322
	loss_policy_4: 0.00161
	accuracy_policy_4: 0.96494
	loss_value_4: 0.02257
	loss_reward_4: 0.00327
	loss_policy_5: 0.00172
	accuracy_policy_5: 0.96279
	loss_value_5: 0.02203
	loss_reward_5: 0.00333
	loss_policy: 0.0123
	loss_value: 0.23339
	loss_reward: 0.01634
[2024-05-07 23:26:21] nn step 2400, lr: 0.1.
	loss_policy_0: 0.00437
	accuracy_policy_0: 0.97557
	loss_value_0: 0.1202
	loss_policy_1: 0.00143
	accuracy_policy_1: 0.96654
	loss_value_1: 0.02431
	loss_reward_1: 0.00267
	loss_policy_2: 0.00152
	accuracy_policy_2: 0.96541
	loss_value_2: 0.02423
	loss_reward_2: 0.00263
	loss_policy_3: 0.0016
	accuracy_policy_3: 0.96445
	loss_value_3: 0.02412
	loss_reward_3: 0.00263
	loss_policy_4: 0.0016
	accuracy_policy_4: 0.9641
	loss_value_4: 0.02393
	loss_reward_4: 0.00268
	loss_policy_5: 0.00166
	accuracy_policy_5: 0.96254
	loss_value_5: 0.02364
	loss_reward_5: 0.00285
	loss_policy: 0.01218
	loss_value: 0.24044
	loss_reward: 0.01347
Optimization_Done 2400
[2024-05-07 23:28:45] [command] train weight_iter_2400.pkl 9 13
[2024-05-07 23:29:27] nn step 2500, lr: 0.1.
	loss_policy_0: 0.01174
	accuracy_policy_0: 0.95562
	loss_value_0: 0.12293
	loss_policy_1: 0.00335
	accuracy_policy_1: 0.94248
	loss_value_1: 0.02489
	loss_reward_1: 0.00353
	loss_policy_2: 0.0036
	accuracy_policy_2: 0.94016
	loss_value_2: 0.02489
	loss_reward_2: 0.00351
	loss_policy_3: 0.00374
	accuracy_policy_3: 0.93713
	loss_value_3: 0.02493
	loss_reward_3: 0.0035
	loss_policy_4: 0.00377
	accuracy_policy_4: 0.93748
	loss_value_4: 0.02477
	loss_reward_4: 0.0037
	loss_policy_5: 0.00401
	accuracy_policy_5: 0.93508
	loss_value_5: 0.02451
	loss_reward_5: 0.00386
	loss_policy: 0.0302
	loss_value: 0.24693
	loss_reward: 0.0181
[2024-05-07 23:30:08] nn step 2600, lr: 0.1.
	loss_policy_0: 0.00826
	accuracy_policy_0: 0.965
	loss_value_0: 0.12227
	loss_policy_1: 0.00264
	accuracy_policy_1: 0.95311
	loss_value_1: 0.02487
	loss_reward_1: 0.00336
	loss_policy_2: 0.00283
	accuracy_policy_2: 0.95066
	loss_value_2: 0.02498
	loss_reward_2: 0.00333
	loss_policy_3: 0.00293
	accuracy_policy_3: 0.95039
	loss_value_3: 0.02515
	loss_reward_3: 0.0032
	loss_policy_4: 0.00303
	accuracy_policy_4: 0.94725
	loss_value_4: 0.02523
	loss_reward_4: 0.00341
	loss_policy_5: 0.00313
	accuracy_policy_5: 0.94836
	loss_value_5: 0.02518
	loss_reward_5: 0.00356
	loss_policy: 0.02281
	loss_value: 0.24769
	loss_reward: 0.01686
Optimization_Done 2600
[2024-05-07 23:32:45] [command] train weight_iter_2600.pkl 10 14
[2024-05-07 23:33:27] nn step 2700, lr: 0.1.
	loss_policy_0: 0.01125
	accuracy_policy_0: 0.96064
	loss_value_0: 0.1256
	loss_policy_1: 0.00293
	accuracy_policy_1: 0.95508
	loss_value_1: 0.02566
	loss_reward_1: 0.00378
	loss_policy_2: 0.00308
	accuracy_policy_2: 0.95227
	loss_value_2: 0.02589
	loss_reward_2: 0.00346
	loss_policy_3: 0.00315
	accuracy_policy_3: 0.95262
	loss_value_3: 0.02614
	loss_reward_3: 0.00343
	loss_policy_4: 0.00327
	accuracy_policy_4: 0.95164
	loss_value_4: 0.02638
	loss_reward_4: 0.00356
	loss_policy_5: 0.00344
	accuracy_policy_5: 0.95008
	loss_value_5: 0.02645
	loss_reward_5: 0.00383
	loss_policy: 0.02712
	loss_value: 0.25611
	loss_reward: 0.01806
[2024-05-07 23:34:09] nn step 2800, lr: 0.1.
	loss_policy_0: 0.00903
	accuracy_policy_0: 0.96551
	loss_value_0: 0.12059
	loss_policy_1: 0.0026
	accuracy_policy_1: 0.9568
	loss_value_1: 0.02467
	loss_reward_1: 0.0034
	loss_policy_2: 0.00264
	accuracy_policy_2: 0.95619
	loss_value_2: 0.025
	loss_reward_2: 0.00325
	loss_policy_3: 0.00272
	accuracy_policy_3: 0.95514
	loss_value_3: 0.02528
	loss_reward_3: 0.00318
	loss_policy_4: 0.00281
	accuracy_policy_4: 0.95547
	loss_value_4: 0.02555
	loss_reward_4: 0.00336
	loss_policy_5: 0.00293
	accuracy_policy_5: 0.95324
	loss_value_5: 0.02576
	loss_reward_5: 0.00352
	loss_policy: 0.02273
	loss_value: 0.24683
	loss_reward: 0.01671
Optimization_Done 2800
[2024-05-07 23:36:08] [command] train weight_iter_2800.pkl 11 15
[2024-05-07 23:36:50] nn step 2900, lr: 0.1.
	loss_policy_0: 0.01237
	accuracy_policy_0: 0.95877
	loss_value_0: 0.12283
	loss_policy_1: 0.0032
	accuracy_policy_1: 0.95338
	loss_value_1: 0.02516
	loss_reward_1: 0.00366
	loss_policy_2: 0.00337
	accuracy_policy_2: 0.95115
	loss_value_2: 0.02562
	loss_reward_2: 0.00335
	loss_policy_3: 0.0035
	accuracy_policy_3: 0.94986
	loss_value_3: 0.02597
	loss_reward_3: 0.00329
	loss_policy_4: 0.00362
	accuracy_policy_4: 0.95055
	loss_value_4: 0.02634
	loss_reward_4: 0.00343
	loss_policy_5: 0.00391
	accuracy_policy_5: 0.94768
	loss_value_5: 0.02667
	loss_reward_5: 0.00362
	loss_policy: 0.02996
	loss_value: 0.2526
	loss_reward: 0.01736
[2024-05-07 23:37:31] nn step 3000, lr: 0.1.
	loss_policy_0: 0.01053
	accuracy_policy_0: 0.96383
	loss_value_0: 0.12219
	loss_policy_1: 0.0028
	accuracy_policy_1: 0.95568
	loss_value_1: 0.02501
	loss_reward_1: 0.00353
	loss_policy_2: 0.00296
	accuracy_policy_2: 0.9552
	loss_value_2: 0.02544
	loss_reward_2: 0.00323
	loss_policy_3: 0.00306
	accuracy_policy_3: 0.95529
	loss_value_3: 0.02583
	loss_reward_3: 0.00319
	loss_policy_4: 0.0032
	accuracy_policy_4: 0.95545
	loss_value_4: 0.02619
	loss_reward_4: 0.00333
	loss_policy_5: 0.00338
	accuracy_policy_5: 0.95137
	loss_value_5: 0.02658
	loss_reward_5: 0.0035
	loss_policy: 0.02594
	loss_value: 0.25124
	loss_reward: 0.01678
Optimization_Done 3000
[2024-05-07 23:40:10] [command] train weight_iter_3000.pkl 12 16
[2024-05-07 23:40:53] nn step 3100, lr: 0.1.
	loss_policy_0: 0.0317
	accuracy_policy_0: 0.89248
	loss_value_0: 0.12483
	loss_policy_1: 0.00769
	accuracy_policy_1: 0.87432
	loss_value_1: 0.02566
	loss_reward_1: 0.00407
	loss_policy_2: 0.00787
	accuracy_policy_2: 0.87207
	loss_value_2: 0.02627
	loss_reward_2: 0.00391
	loss_policy_3: 0.00824
	accuracy_policy_3: 0.86846
	loss_value_3: 0.0268
	loss_reward_3: 0.00377
	loss_policy_4: 0.00863
	accuracy_policy_4: 0.8609
	loss_value_4: 0.02726
	loss_reward_4: 0.00393
	loss_policy_5: 0.00873
	accuracy_policy_5: 0.86199
	loss_value_5: 0.02773
	loss_reward_5: 0.00423
	loss_policy: 0.07285
	loss_value: 0.25854
	loss_reward: 0.01992
[2024-05-07 23:41:34] nn step 3200, lr: 0.1.
	loss_policy_0: 0.02533
	accuracy_policy_0: 0.91357
	loss_value_0: 0.13038
	loss_policy_1: 0.00638
	accuracy_policy_1: 0.89617
	loss_value_1: 0.02672
	loss_reward_1: 0.00413
	loss_policy_2: 0.00681
	accuracy_policy_2: 0.89312
	loss_value_2: 0.02733
	loss_reward_2: 0.00391
	loss_policy_3: 0.00716
	accuracy_policy_3: 0.88766
	loss_value_3: 0.0279
	loss_reward_3: 0.00394
	loss_policy_4: 0.0075
	accuracy_policy_4: 0.88584
	loss_value_4: 0.02839
	loss_reward_4: 0.00401
	loss_policy_5: 0.00789
	accuracy_policy_5: 0.88307
	loss_value_5: 0.02893
	loss_reward_5: 0.00423
	loss_policy: 0.06109
	loss_value: 0.26965
	loss_reward: 0.02022
Optimization_Done 3200
[2024-05-07 23:43:57] [command] train weight_iter_3200.pkl 13 17
[2024-05-07 23:44:40] nn step 3300, lr: 0.1.
	loss_policy_0: 0.04358
	accuracy_policy_0: 0.85814
	loss_value_0: 0.13459
	loss_policy_1: 0.01053
	accuracy_policy_1: 0.83979
	loss_value_1: 0.02758
	loss_reward_1: 0.0043
	loss_policy_2: 0.01114
	accuracy_policy_2: 0.83465
	loss_value_2: 0.0282
	loss_reward_2: 0.00401
	loss_policy_3: 0.01167
	accuracy_policy_3: 0.82797
	loss_value_3: 0.02875
	loss_reward_3: 0.00403
	loss_policy_4: 0.01221
	accuracy_policy_4: 0.82258
	loss_value_4: 0.02924
	loss_reward_4: 0.00435
	loss_policy_5: 0.01276
	accuracy_policy_5: 0.81787
	loss_value_5: 0.02976
	loss_reward_5: 0.00441
	loss_policy: 0.1019
	loss_value: 0.27812
	loss_reward: 0.0211
[2024-05-07 23:45:22] nn step 3400, lr: 0.1.
	loss_policy_0: 0.03724
	accuracy_policy_0: 0.87436
	loss_value_0: 0.13379
	loss_policy_1: 0.00942
	accuracy_policy_1: 0.85664
	loss_value_1: 0.02735
	loss_reward_1: 0.00429
	loss_policy_2: 0.01008
	accuracy_policy_2: 0.8482
	loss_value_2: 0.02797
	loss_reward_2: 0.00394
	loss_policy_3: 0.01045
	accuracy_policy_3: 0.84561
	loss_value_3: 0.02859
	loss_reward_3: 0.00405
	loss_policy_4: 0.0111
	accuracy_policy_4: 0.84201
	loss_value_4: 0.02915
	loss_reward_4: 0.00427
	loss_policy_5: 0.01154
	accuracy_policy_5: 0.83717
	loss_value_5: 0.02976
	loss_reward_5: 0.00457
	loss_policy: 0.08984
	loss_value: 0.27661
	loss_reward: 0.02111
Optimization_Done 3400
[2024-05-07 23:48:01] [command] train weight_iter_3400.pkl 14 18
[2024-05-07 23:48:44] nn step 3500, lr: 0.1.
	loss_policy_0: 0.04014
	accuracy_policy_0: 0.8785
	loss_value_0: 0.14386
	loss_policy_1: 0.01
	accuracy_policy_1: 0.8602
	loss_value_1: 0.02938
	loss_reward_1: 0.00413
	loss_policy_2: 0.01063
	accuracy_policy_2: 0.85299
	loss_value_2: 0.03004
	loss_reward_2: 0.00399
	loss_policy_3: 0.01118
	accuracy_policy_3: 0.84787
	loss_value_3: 0.0307
	loss_reward_3: 0.00386
	loss_policy_4: 0.01184
	accuracy_policy_4: 0.84338
	loss_value_4: 0.03133
	loss_reward_4: 0.00424
	loss_policy_5: 0.01244
	accuracy_policy_5: 0.8365
	loss_value_5: 0.03189
	loss_reward_5: 0.00459
	loss_policy: 0.09623
	loss_value: 0.2972
	loss_reward: 0.02081
[2024-05-07 23:49:26] nn step 3600, lr: 0.1.
	loss_policy_0: 0.03533
	accuracy_policy_0: 0.88561
	loss_value_0: 0.14
	loss_policy_1: 0.00914
	accuracy_policy_1: 0.86604
	loss_value_1: 0.02858
	loss_reward_1: 0.00404
	loss_policy_2: 0.00971
	accuracy_policy_2: 0.85889
	loss_value_2: 0.02922
	loss_reward_2: 0.00385
	loss_policy_3: 0.01051
	accuracy_policy_3: 0.85166
	loss_value_3: 0.02981
	loss_reward_3: 0.00395
	loss_policy_4: 0.01108
	accuracy_policy_4: 0.84832
	loss_value_4: 0.03036
	loss_reward_4: 0.00424
	loss_policy_5: 0.01163
	accuracy_policy_5: 0.84314
	loss_value_5: 0.03094
	loss_reward_5: 0.00459
	loss_policy: 0.0874
	loss_value: 0.28892
	loss_reward: 0.02068
Optimization_Done 3600
[2024-05-07 23:52:03] [command] train weight_iter_3600.pkl 15 19
[2024-05-07 23:52:45] nn step 3700, lr: 0.1.
	loss_policy_0: 0.04374
	accuracy_policy_0: 0.85639
	loss_value_0: 0.14128
	loss_policy_1: 0.01117
	accuracy_policy_1: 0.83275
	loss_value_1: 0.0289
	loss_reward_1: 0.00427
	loss_policy_2: 0.01165
	accuracy_policy_2: 0.82904
	loss_value_2: 0.02952
	loss_reward_2: 0.00391
	loss_policy_3: 0.01257
	accuracy_policy_3: 0.82055
	loss_value_3: 0.03007
	loss_reward_3: 0.00398
	loss_policy_4: 0.01339
	accuracy_policy_4: 0.81301
	loss_value_4: 0.03064
	loss_reward_4: 0.00422
	loss_policy_5: 0.01424
	accuracy_policy_5: 0.80568
	loss_value_5: 0.03119
	loss_reward_5: 0.00463
	loss_policy: 0.10676
	loss_value: 0.29159
	loss_reward: 0.02102
[2024-05-07 23:53:26] nn step 3800, lr: 0.1.
	loss_policy_0: 0.04014
	accuracy_policy_0: 0.86764
	loss_value_0: 0.14316
	loss_policy_1: 0.01055
	accuracy_policy_1: 0.84021
	loss_value_1: 0.02928
	loss_reward_1: 0.00425
	loss_policy_2: 0.01121
	accuracy_policy_2: 0.83623
	loss_value_2: 0.02995
	loss_reward_2: 0.00409
	loss_policy_3: 0.01228
	accuracy_policy_3: 0.8258
	loss_value_3: 0.03056
	loss_reward_3: 0.0041
	loss_policy_4: 0.01296
	accuracy_policy_4: 0.81924
	loss_value_4: 0.03114
	loss_reward_4: 0.00444
	loss_policy_5: 0.01389
	accuracy_policy_5: 0.81016
	loss_value_5: 0.03175
	loss_reward_5: 0.00475
	loss_policy: 0.10102
	loss_value: 0.29585
	loss_reward: 0.02163
Optimization_Done 3800
[2024-05-07 23:56:05] [command] train weight_iter_3800.pkl 16 20
[2024-05-07 23:56:48] nn step 3900, lr: 0.1.
	loss_policy_0: 0.05122
	accuracy_policy_0: 0.8492
	loss_value_0: 0.15584
	loss_policy_1: 0.01283
	accuracy_policy_1: 0.82309
	loss_value_1: 0.03181
	loss_reward_1: 0.00495
	loss_policy_2: 0.01376
	accuracy_policy_2: 0.81826
	loss_value_2: 0.03259
	loss_reward_2: 0.00462
	loss_policy_3: 0.01443
	accuracy_policy_3: 0.81299
	loss_value_3: 0.03325
	loss_reward_3: 0.00451
	loss_policy_4: 0.01551
	accuracy_policy_4: 0.80566
	loss_value_4: 0.03384
	loss_reward_4: 0.00494
	loss_policy_5: 0.01643
	accuracy_policy_5: 0.79723
	loss_value_5: 0.03448
	loss_reward_5: 0.0055
	loss_policy: 0.12417
	loss_value: 0.32181
	loss_reward: 0.02451
[2024-05-07 23:57:29] nn step 4000, lr: 0.1.
	loss_policy_0: 0.04464
	accuracy_policy_0: 0.85918
	loss_value_0: 0.14697
	loss_policy_1: 0.01125
	accuracy_policy_1: 0.83373
	loss_value_1: 0.03003
	loss_reward_1: 0.00458
	loss_policy_2: 0.0121
	accuracy_policy_2: 0.82607
	loss_value_2: 0.03068
	loss_reward_2: 0.00432
	loss_policy_3: 0.01309
	accuracy_policy_3: 0.81934
	loss_value_3: 0.03129
	loss_reward_3: 0.00437
	loss_policy_4: 0.014
	accuracy_policy_4: 0.80893
	loss_value_4: 0.03193
	loss_reward_4: 0.0047
	loss_policy_5: 0.01502
	accuracy_policy_5: 0.80129
	loss_value_5: 0.0325
	loss_reward_5: 0.00507
	loss_policy: 0.11011
	loss_value: 0.3034
	loss_reward: 0.02304
Optimization_Done 4000
[2024-05-08 00:00:06] [command] train weight_iter_4000.pkl 17 21
[2024-05-08 00:00:48] nn step 4100, lr: 0.1.
	loss_policy_0: 0.06089
	accuracy_policy_0: 0.80117
	loss_value_0: 0.15406
	loss_policy_1: 0.01473
	accuracy_policy_1: 0.77389
	loss_value_1: 0.0315
	loss_reward_1: 0.00495
	loss_policy_2: 0.01576
	accuracy_policy_2: 0.76088
	loss_value_2: 0.0322
	loss_reward_2: 0.0046
	loss_policy_3: 0.01666
	accuracy_policy_3: 0.75035
	loss_value_3: 0.0329
	loss_reward_3: 0.00464
	loss_policy_4: 0.01782
	accuracy_policy_4: 0.74191
	loss_value_4: 0.03344
	loss_reward_4: 0.00496
	loss_policy_5: 0.01873
	accuracy_policy_5: 0.73268
	loss_value_5: 0.03401
	loss_reward_5: 0.0054
	loss_policy: 0.1446
	loss_value: 0.3181
	loss_reward: 0.02455
[2024-05-08 00:01:29] nn step 4200, lr: 0.1.
	loss_policy_0: 0.05294
	accuracy_policy_0: 0.82455
	loss_value_0: 0.15179
	loss_policy_1: 0.01307
	accuracy_policy_1: 0.79768
	loss_value_1: 0.03101
	loss_reward_1: 0.00479
	loss_policy_2: 0.01413
	accuracy_policy_2: 0.78674
	loss_value_2: 0.03169
	loss_reward_2: 0.00461
	loss_policy_3: 0.01503
	accuracy_policy_3: 0.77844
	loss_value_3: 0.03238
	loss_reward_3: 0.0045
	loss_policy_4: 0.01597
	accuracy_policy_4: 0.76748
	loss_value_4: 0.03301
	loss_reward_4: 0.00492
	loss_policy_5: 0.01692
	accuracy_policy_5: 0.75828
	loss_value_5: 0.03363
	loss_reward_5: 0.00523
	loss_policy: 0.12806
	loss_value: 0.31352
	loss_reward: 0.02405
Optimization_Done 4200
[2024-05-08 00:03:54] [command] train weight_iter_4200.pkl 18 22
[2024-05-08 00:04:36] nn step 4300, lr: 0.1.
	loss_policy_0: 0.06213
	accuracy_policy_0: 0.79414
	loss_value_0: 0.15886
	loss_policy_1: 0.01545
	accuracy_policy_1: 0.76117
	loss_value_1: 0.03237
	loss_reward_1: 0.00495
	loss_policy_2: 0.01671
	accuracy_policy_2: 0.75037
	loss_value_2: 0.03319
	loss_reward_2: 0.00454
	loss_policy_3: 0.01818
	accuracy_policy_3: 0.73533
	loss_value_3: 0.0339
	loss_reward_3: 0.00464
	loss_policy_4: 0.0194
	accuracy_policy_4: 0.72512
	loss_value_4: 0.03444
	loss_reward_4: 0.00491
	loss_policy_5: 0.02056
	accuracy_policy_5: 0.71055
	loss_value_5: 0.03503
	loss_reward_5: 0.00547
	loss_policy: 0.15243
	loss_value: 0.32779
	loss_reward: 0.02451
[2024-05-08 00:05:17] nn step 4400, lr: 0.1.
	loss_policy_0: 0.05459
	accuracy_policy_0: 0.81236
	loss_value_0: 0.15647
	loss_policy_1: 0.01405
	accuracy_policy_1: 0.78221
	loss_value_1: 0.03194
	loss_reward_1: 0.00489
	loss_policy_2: 0.01542
	accuracy_policy_2: 0.76641
	loss_value_2: 0.03267
	loss_reward_2: 0.00452
	loss_policy_3: 0.01662
	accuracy_policy_3: 0.75557
	loss_value_3: 0.03329
	loss_reward_3: 0.00468
	loss_policy_4: 0.01773
	accuracy_policy_4: 0.74793
	loss_value_4: 0.03385
	loss_reward_4: 0.00504
	loss_policy_5: 0.01882
	accuracy_policy_5: 0.73662
	loss_value_5: 0.03437
	loss_reward_5: 0.00547
	loss_policy: 0.13722
	loss_value: 0.32259
	loss_reward: 0.0246
Optimization_Done 4400
[2024-05-08 00:07:54] [command] train weight_iter_4400.pkl 19 23
[2024-05-08 00:08:36] nn step 4500, lr: 0.1.
	loss_policy_0: 0.07569
	accuracy_policy_0: 0.77543
	loss_value_0: 0.16584
	loss_policy_1: 0.01826
	accuracy_policy_1: 0.74637
	loss_value_1: 0.03384
	loss_reward_1: 0.00512
	loss_policy_2: 0.0196
	accuracy_policy_2: 0.73375
	loss_value_2: 0.03455
	loss_reward_2: 0.00484
	loss_policy_3: 0.02098
	accuracy_policy_3: 0.7218
	loss_value_3: 0.03521
	loss_reward_3: 0.00489
	loss_policy_4: 0.02226
	accuracy_policy_4: 0.7076
	loss_value_4: 0.03582
	loss_reward_4: 0.0053
	loss_policy_5: 0.02333
	accuracy_policy_5: 0.69758
	loss_value_5: 0.03646
	loss_reward_5: 0.00573
	loss_policy: 0.18012
	loss_value: 0.34171
	loss_reward: 0.02588
[2024-05-08 00:09:17] nn step 4600, lr: 0.1.
	loss_policy_0: 0.06251
	accuracy_policy_0: 0.79777
	loss_value_0: 0.15509
	loss_policy_1: 0.0155
	accuracy_policy_1: 0.76549
	loss_value_1: 0.03171
	loss_reward_1: 0.00497
	loss_policy_2: 0.01679
	accuracy_policy_2: 0.7558
	loss_value_2: 0.03243
	loss_reward_2: 0.00454
	loss_policy_3: 0.01802
	accuracy_policy_3: 0.74596
	loss_value_3: 0.03307
	loss_reward_3: 0.00461
	loss_policy_4: 0.01932
	accuracy_policy_4: 0.73438
	loss_value_4: 0.03361
	loss_reward_4: 0.00509
	loss_policy_5: 0.02042
	accuracy_policy_5: 0.7208
	loss_value_5: 0.03411
	loss_reward_5: 0.00548
	loss_policy: 0.15256
	loss_value: 0.32002
	loss_reward: 0.02469
Optimization_Done 4600
[2024-05-08 00:11:53] [command] train weight_iter_4600.pkl 20 24
[2024-05-08 00:12:36] nn step 4700, lr: 0.1.
	loss_policy_0: 0.08187
	accuracy_policy_0: 0.77242
	loss_value_0: 0.16755
	loss_policy_1: 0.01961
	accuracy_policy_1: 0.74348
	loss_value_1: 0.03406
	loss_reward_1: 0.00517
	loss_policy_2: 0.02142
	accuracy_policy_2: 0.72709
	loss_value_2: 0.03483
	loss_reward_2: 0.00496
	loss_policy_3: 0.02288
	accuracy_policy_3: 0.71129
	loss_value_3: 0.03551
	loss_reward_3: 0.00493
	loss_policy_4: 0.02406
	accuracy_policy_4: 0.69785
	loss_value_4: 0.0361
	loss_reward_4: 0.00531
	loss_policy_5: 0.02547
	accuracy_policy_5: 0.6848
	loss_value_5: 0.0367
	loss_reward_5: 0.00575
	loss_policy: 0.19531
	loss_value: 0.34474
	loss_reward: 0.02612
[2024-05-08 00:13:18] nn step 4800, lr: 0.1.
	loss_policy_0: 0.06719
	accuracy_policy_0: 0.79012
	loss_value_0: 0.15345
	loss_policy_1: 0.01672
	accuracy_policy_1: 0.75811
	loss_value_1: 0.03131
	loss_reward_1: 0.00471
	loss_policy_2: 0.01809
	accuracy_policy_2: 0.74355
	loss_value_2: 0.03205
	loss_reward_2: 0.00461
	loss_policy_3: 0.01928
	accuracy_policy_3: 0.73289
	loss_value_3: 0.03263
	loss_reward_3: 0.00459
	loss_policy_4: 0.02068
	accuracy_policy_4: 0.7167
	loss_value_4: 0.03322
	loss_reward_4: 0.005
	loss_policy_5: 0.02176
	accuracy_policy_5: 0.70309
	loss_value_5: 0.03385
	loss_reward_5: 0.00538
	loss_policy: 0.16372
	loss_value: 0.31651
	loss_reward: 0.02429
Optimization_Done 4800
[2024-05-08 00:15:54] [command] train weight_iter_4800.pkl 21 25
[2024-05-08 00:16:37] nn step 4900, lr: 0.1.
	loss_policy_0: 0.07736
	accuracy_policy_0: 0.76793
	loss_value_0: 0.16378
	loss_policy_1: 0.01931
	accuracy_policy_1: 0.73295
	loss_value_1: 0.03326
	loss_reward_1: 0.00528
	loss_policy_2: 0.02097
	accuracy_policy_2: 0.71537
	loss_value_2: 0.03399
	loss_reward_2: 0.00494
	loss_policy_3: 0.02256
	accuracy_policy_3: 0.70102
	loss_value_3: 0.03475
	loss_reward_3: 0.00496
	loss_policy_4: 0.02414
	accuracy_policy_4: 0.68463
	loss_value_4: 0.03547
	loss_reward_4: 0.00531
	loss_policy_5: 0.0255
	accuracy_policy_5: 0.66699
	loss_value_5: 0.03604
	loss_reward_5: 0.00575
	loss_policy: 0.18985
	loss_value: 0.3373
	loss_reward: 0.02623
[2024-05-08 00:17:19] nn step 5000, lr: 0.1.
	loss_policy_0: 0.07147
	accuracy_policy_0: 0.78369
	loss_value_0: 0.16551
	loss_policy_1: 0.01818
	accuracy_policy_1: 0.74689
	loss_value_1: 0.03366
	loss_reward_1: 0.00533
	loss_policy_2: 0.02008
	accuracy_policy_2: 0.72752
	loss_value_2: 0.03423
	loss_reward_2: 0.00502
	loss_policy_3: 0.02164
	accuracy_policy_3: 0.71258
	loss_value_3: 0.0349
	loss_reward_3: 0.00493
	loss_policy_4: 0.0234
	accuracy_policy_4: 0.69486
	loss_value_4: 0.03562
	loss_reward_4: 0.00548
	loss_policy_5: 0.02476
	accuracy_policy_5: 0.68008
	loss_value_5: 0.03627
	loss_reward_5: 0.00593
	loss_policy: 0.17954
	loss_value: 0.34018
	loss_reward: 0.0267
Optimization_Done 5000
[2024-05-08 00:19:04] [command] train weight_iter_5000.pkl 22 26
[2024-05-08 00:19:46] nn step 5100, lr: 0.1.
	loss_policy_0: 0.07767
	accuracy_policy_0: 0.77135
	loss_value_0: 0.16453
	loss_policy_1: 0.01966
	accuracy_policy_1: 0.73156
	loss_value_1: 0.03344
	loss_reward_1: 0.00526
	loss_policy_2: 0.02173
	accuracy_policy_2: 0.70982
	loss_value_2: 0.03402
	loss_reward_2: 0.00506
	loss_policy_3: 0.02355
	accuracy_policy_3: 0.69205
	loss_value_3: 0.03459
	loss_reward_3: 0.00512
	loss_policy_4: 0.02499
	accuracy_policy_4: 0.68061
	loss_value_4: 0.03519
	loss_reward_4: 0.00549
	loss_policy_5: 0.02662
	accuracy_policy_5: 0.66473
	loss_value_5: 0.03578
	loss_reward_5: 0.00603
	loss_policy: 0.19422
	loss_value: 0.33755
	loss_reward: 0.02696
[2024-05-08 00:20:28] nn step 5200, lr: 0.1.
	loss_policy_0: 0.07008
	accuracy_policy_0: 0.7885
	loss_value_0: 0.1635
	loss_policy_1: 0.01841
	accuracy_policy_1: 0.74387
	loss_value_1: 0.03325
	loss_reward_1: 0.00521
	loss_policy_2: 0.02029
	accuracy_policy_2: 0.72268
	loss_value_2: 0.03388
	loss_reward_2: 0.005
	loss_policy_3: 0.02205
	accuracy_policy_3: 0.71195
	loss_value_3: 0.03441
	loss_reward_3: 0.00514
	loss_policy_4: 0.0235
	accuracy_policy_4: 0.6959
	loss_value_4: 0.03497
	loss_reward_4: 0.00537
	loss_policy_5: 0.02493
	accuracy_policy_5: 0.67883
	loss_value_5: 0.03547
	loss_reward_5: 0.00586
	loss_policy: 0.17927
	loss_value: 0.33549
	loss_reward: 0.02658
Optimization_Done 5200
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-08 00:50:26] [command] train weight_iter_5200.pkl 24 27
[2024-05-08 00:51:20] nn step 5300, lr: 0.1.
	loss_policy_0: 0.0729
	accuracy_policy_0: 0.79094
	loss_value_0: 0.16821
	loss_policy_1: 0.01952
	accuracy_policy_1: 0.73736
	loss_value_1: 0.03413
	loss_reward_1: 0.00524
	loss_policy_2: 0.02209
	accuracy_policy_2: 0.71207
	loss_value_2: 0.03484
	loss_reward_2: 0.00496
	loss_policy_3: 0.02471
	accuracy_policy_3: 0.68438
	loss_value_3: 0.03559
	loss_reward_3: 0.00505
	loss_policy_4: 0.02666
	accuracy_policy_4: 0.66455
	loss_value_4: 0.03624
	loss_reward_4: 0.00519
	loss_policy_5: 0.02855
	accuracy_policy_5: 0.64998
	loss_value_5: 0.03678
	loss_reward_5: 0.00581
	loss_policy: 0.19443
	loss_value: 0.3458
	loss_reward: 0.02624
[2024-05-08 00:52:00] nn step 5400, lr: 0.1.
	loss_policy_0: 0.06324
	accuracy_policy_0: 0.80615
	loss_value_0: 0.16549
	loss_policy_1: 0.01799
	accuracy_policy_1: 0.75533
	loss_value_1: 0.0337
	loss_reward_1: 0.00521
	loss_policy_2: 0.02054
	accuracy_policy_2: 0.73139
	loss_value_2: 0.0344
	loss_reward_2: 0.00503
	loss_policy_3: 0.02294
	accuracy_policy_3: 0.70613
	loss_value_3: 0.03511
	loss_reward_3: 0.00508
	loss_policy_4: 0.0249
	accuracy_policy_4: 0.68744
	loss_value_4: 0.03581
	loss_reward_4: 0.0055
	loss_policy_5: 0.02673
	accuracy_policy_5: 0.6691
	loss_value_5: 0.03648
	loss_reward_5: 0.00598
	loss_policy: 0.17633
	loss_value: 0.341
	loss_reward: 0.0268
Optimization_Done 5400
[2024-05-08 00:54:50] [command] train weight_iter_5400.pkl 25 28
[2024-05-08 00:55:32] nn step 5500, lr: 0.1.
	loss_policy_0: 0.07401
	accuracy_policy_0: 0.7902
	loss_value_0: 0.17804
	loss_policy_1: 0.02038
	accuracy_policy_1: 0.73629
	loss_value_1: 0.03639
	loss_reward_1: 0.00596
	loss_policy_2: 0.0233
	accuracy_policy_2: 0.71078
	loss_value_2: 0.03726
	loss_reward_2: 0.00579
	loss_policy_3: 0.02566
	accuracy_policy_3: 0.69076
	loss_value_3: 0.0381
	loss_reward_3: 0.00578
	loss_policy_4: 0.02774
	accuracy_policy_4: 0.67107
	loss_value_4: 0.03889
	loss_reward_4: 0.00627
	loss_policy_5: 0.02977
	accuracy_policy_5: 0.6523
	loss_value_5: 0.03959
	loss_reward_5: 0.00688
	loss_policy: 0.20088
	loss_value: 0.36827
	loss_reward: 0.03068
[2024-05-08 00:56:13] nn step 5600, lr: 0.1.
	loss_policy_0: 0.06536
	accuracy_policy_0: 0.80309
	loss_value_0: 0.17456
	loss_policy_1: 0.01862
	accuracy_policy_1: 0.74992
	loss_value_1: 0.03557
	loss_reward_1: 0.00586
	loss_policy_2: 0.02132
	accuracy_policy_2: 0.7248
	loss_value_2: 0.03632
	loss_reward_2: 0.00547
	loss_policy_3: 0.02388
	accuracy_policy_3: 0.70061
	loss_value_3: 0.03703
	loss_reward_3: 0.00553
	loss_policy_4: 0.02578
	accuracy_policy_4: 0.68533
	loss_value_4: 0.03782
	loss_reward_4: 0.00615
	loss_policy_5: 0.02784
	accuracy_policy_5: 0.66266
	loss_value_5: 0.03861
	loss_reward_5: 0.00667
	loss_policy: 0.18281
	loss_value: 0.35992
	loss_reward: 0.02969
Optimization_Done 5600
[2024-05-08 00:58:50] [command] train weight_iter_5600.pkl 26 29
[2024-05-08 00:59:33] nn step 5700, lr: 0.1.
	loss_policy_0: 0.07726
	accuracy_policy_0: 0.77309
	loss_value_0: 0.16076
	loss_policy_1: 0.02054
	accuracy_policy_1: 0.72193
	loss_value_1: 0.03273
	loss_reward_1: 0.00509
	loss_policy_2: 0.02302
	accuracy_policy_2: 0.69527
	loss_value_2: 0.03355
	loss_reward_2: 0.00483
	loss_policy_3: 0.02551
	accuracy_policy_3: 0.67406
	loss_value_3: 0.03432
	loss_reward_3: 0.00503
	loss_policy_4: 0.02792
	accuracy_policy_4: 0.65037
	loss_value_4: 0.03514
	loss_reward_4: 0.00544
	loss_policy_5: 0.02999
	accuracy_policy_5: 0.63033
	loss_value_5: 0.03593
	loss_reward_5: 0.00609
	loss_policy: 0.20424
	loss_value: 0.33243
	loss_reward: 0.02648
[2024-05-08 01:00:14] nn step 5800, lr: 0.1.
	loss_policy_0: 0.07082
	accuracy_policy_0: 0.79938
	loss_value_0: 0.17253
	loss_policy_1: 0.02016
	accuracy_policy_1: 0.73908
	loss_value_1: 0.03517
	loss_reward_1: 0.0056
	loss_policy_2: 0.02304
	accuracy_policy_2: 0.71559
	loss_value_2: 0.03603
	loss_reward_2: 0.00532
	loss_policy_3: 0.02568
	accuracy_policy_3: 0.69492
	loss_value_3: 0.03693
	loss_reward_3: 0.00551
	loss_policy_4: 0.02793
	accuracy_policy_4: 0.6734
	loss_value_4: 0.03765
	loss_reward_4: 0.00604
	loss_policy_5: 0.03016
	accuracy_policy_5: 0.6532
	loss_value_5: 0.03856
	loss_reward_5: 0.00669
	loss_policy: 0.19777
	loss_value: 0.35685
	loss_reward: 0.02916
Optimization_Done 5800
[2024-05-08 01:02:49] [command] train weight_iter_5800.pkl 27 30
[2024-05-08 01:03:32] nn step 5900, lr: 0.1.
	loss_policy_0: 0.08497
	accuracy_policy_0: 0.7815
	loss_value_0: 0.17035
	loss_policy_1: 0.02361
	accuracy_policy_1: 0.72227
	loss_value_1: 0.03466
	loss_reward_1: 0.00585
	loss_policy_2: 0.02677
	accuracy_policy_2: 0.69496
	loss_value_2: 0.03562
	loss_reward_2: 0.00543
	loss_policy_3: 0.02987
	accuracy_policy_3: 0.6676
	loss_value_3: 0.03643
	loss_reward_3: 0.00543
	loss_policy_4: 0.03238
	accuracy_policy_4: 0.64689
	loss_value_4: 0.03736
	loss_reward_4: 0.00614
	loss_policy_5: 0.03446
	accuracy_policy_5: 0.62648
	loss_value_5: 0.03833
	loss_reward_5: 0.00678
	loss_policy: 0.23206
	loss_value: 0.35275
	loss_reward: 0.02963
[2024-05-08 01:04:13] nn step 6000, lr: 0.1.
	loss_policy_0: 0.07103
	accuracy_policy_0: 0.79713
	loss_value_0: 0.16129
	loss_policy_1: 0.02069
	accuracy_policy_1: 0.73873
	loss_value_1: 0.03291
	loss_reward_1: 0.00566
	loss_policy_2: 0.02356
	accuracy_policy_2: 0.70951
	loss_value_2: 0.03379
	loss_reward_2: 0.00517
	loss_policy_3: 0.02658
	accuracy_policy_3: 0.68465
	loss_value_3: 0.03461
	loss_reward_3: 0.00526
	loss_policy_4: 0.0289
	accuracy_policy_4: 0.66725
	loss_value_4: 0.03546
	loss_reward_4: 0.00583
	loss_policy_5: 0.03122
	accuracy_policy_5: 0.64469
	loss_value_5: 0.0364
	loss_reward_5: 0.00649
	loss_policy: 0.20199
	loss_value: 0.33446
	loss_reward: 0.02842
Optimization_Done 6000
[2024-05-08 01:06:39] [command] train weight_iter_6000.pkl 28 31
[2024-05-08 01:07:21] nn step 6100, lr: 0.1.
	loss_policy_0: 0.09242
	accuracy_policy_0: 0.77432
	loss_value_0: 0.17303
	loss_policy_1: 0.02507
	accuracy_policy_1: 0.71746
	loss_value_1: 0.03542
	loss_reward_1: 0.00597
	loss_policy_2: 0.02863
	accuracy_policy_2: 0.68875
	loss_value_2: 0.03622
	loss_reward_2: 0.00549
	loss_policy_3: 0.03202
	accuracy_policy_3: 0.66125
	loss_value_3: 0.03715
	loss_reward_3: 0.00574
	loss_policy_4: 0.03449
	accuracy_policy_4: 0.63768
	loss_value_4: 0.0382
	loss_reward_4: 0.00617
	loss_policy_5: 0.03726
	accuracy_policy_5: 0.61496
	loss_value_5: 0.03906
	loss_reward_5: 0.00687
	loss_policy: 0.24989
	loss_value: 0.35908
	loss_reward: 0.03023
[2024-05-08 01:08:02] nn step 6200, lr: 0.1.
	loss_policy_0: 0.08099
	accuracy_policy_0: 0.78604
	loss_value_0: 0.16496
	loss_policy_1: 0.02297
	accuracy_policy_1: 0.72826
	loss_value_1: 0.03376
	loss_reward_1: 0.00576
	loss_policy_2: 0.02643
	accuracy_policy_2: 0.69701
	loss_value_2: 0.03455
	loss_reward_2: 0.00546
	loss_policy_3: 0.02934
	accuracy_policy_3: 0.67473
	loss_value_3: 0.0355
	loss_reward_3: 0.00541
	loss_policy_4: 0.03196
	accuracy_policy_4: 0.6492
	loss_value_4: 0.03637
	loss_reward_4: 0.00609
	loss_policy_5: 0.0345
	accuracy_policy_5: 0.62893
	loss_value_5: 0.03725
	loss_reward_5: 0.00677
	loss_policy: 0.2262
	loss_value: 0.34238
	loss_reward: 0.02949
Optimization_Done 6200
[2024-05-08 01:10:40] [command] train weight_iter_6200.pkl 29 32
[2024-05-08 01:11:23] nn step 6300, lr: 0.1.
	loss_policy_0: 0.09746
	accuracy_policy_0: 0.7724
	loss_value_0: 0.17801
	loss_policy_1: 0.02678
	accuracy_policy_1: 0.71682
	loss_value_1: 0.03642
	loss_reward_1: 0.00596
	loss_policy_2: 0.03078
	accuracy_policy_2: 0.68461
	loss_value_2: 0.03749
	loss_reward_2: 0.00553
	loss_policy_3: 0.03408
	accuracy_policy_3: 0.66
	loss_value_3: 0.0384
	loss_reward_3: 0.00561
	loss_policy_4: 0.03715
	accuracy_policy_4: 0.63447
	loss_value_4: 0.03943
	loss_reward_4: 0.00621
	loss_policy_5: 0.03951
	accuracy_policy_5: 0.62006
	loss_value_5: 0.0403
	loss_reward_5: 0.00706
	loss_policy: 0.26575
	loss_value: 0.37006
	loss_reward: 0.03037
[2024-05-08 01:12:03] nn step 6400, lr: 0.1.
	loss_policy_0: 0.08848
	accuracy_policy_0: 0.7893
	loss_value_0: 0.17569
	loss_policy_1: 0.02502
	accuracy_policy_1: 0.73365
	loss_value_1: 0.03586
	loss_reward_1: 0.00594
	loss_policy_2: 0.02884
	accuracy_policy_2: 0.69783
	loss_value_2: 0.03682
	loss_reward_2: 0.00549
	loss_policy_3: 0.03211
	accuracy_policy_3: 0.67492
	loss_value_3: 0.0377
	loss_reward_3: 0.00576
	loss_policy_4: 0.03527
	accuracy_policy_4: 0.64988
	loss_value_4: 0.03855
	loss_reward_4: 0.00612
	loss_policy_5: 0.03795
	accuracy_policy_5: 0.6315
	loss_value_5: 0.03941
	loss_reward_5: 0.00711
	loss_policy: 0.24768
	loss_value: 0.36403
	loss_reward: 0.03042
Optimization_Done 6400
[2024-05-08 01:14:42] [command] train weight_iter_6400.pkl 30 33
[2024-05-08 01:15:24] nn step 6500, lr: 0.1.
	loss_policy_0: 0.10114
	accuracy_policy_0: 0.75604
	loss_value_0: 0.1625
	loss_policy_1: 0.02706
	accuracy_policy_1: 0.69748
	loss_value_1: 0.0332
	loss_reward_1: 0.00516
	loss_policy_2: 0.03076
	accuracy_policy_2: 0.67031
	loss_value_2: 0.03408
	loss_reward_2: 0.00451
	loss_policy_3: 0.03398
	accuracy_policy_3: 0.64338
	loss_value_3: 0.03495
	loss_reward_3: 0.00467
	loss_policy_4: 0.0367
	accuracy_policy_4: 0.62422
	loss_value_4: 0.03584
	loss_reward_4: 0.00533
	loss_policy_5: 0.03921
	accuracy_policy_5: 0.61387
	loss_value_5: 0.03654
	loss_reward_5: 0.00608
	loss_policy: 0.26885
	loss_value: 0.3371
	loss_reward: 0.02575
[2024-05-08 01:16:05] nn step 6600, lr: 0.1.
	loss_policy_0: 0.09454
	accuracy_policy_0: 0.78088
	loss_value_0: 0.17378
	loss_policy_1: 0.02687
	accuracy_policy_1: 0.72094
	loss_value_1: 0.0355
	loss_reward_1: 0.00567
	loss_policy_2: 0.03059
	accuracy_policy_2: 0.68973
	loss_value_2: 0.03639
	loss_reward_2: 0.00504
	loss_policy_3: 0.03447
	accuracy_policy_3: 0.65787
	loss_value_3: 0.03728
	loss_reward_3: 0.00529
	loss_policy_4: 0.03729
	accuracy_policy_4: 0.63766
	loss_value_4: 0.03821
	loss_reward_4: 0.00583
	loss_policy_5: 0.04003
	accuracy_policy_5: 0.62025
	loss_value_5: 0.03907
	loss_reward_5: 0.00653
	loss_policy: 0.26379
	loss_value: 0.36023
	loss_reward: 0.02836
Optimization_Done 6600
[2024-05-08 01:18:41] [command] train weight_iter_6600.pkl 31 34
[2024-05-08 01:19:24] nn step 6700, lr: 0.1.
	loss_policy_0: 0.11698
	accuracy_policy_0: 0.73096
	loss_value_0: 0.16949
	loss_policy_1: 0.03156
	accuracy_policy_1: 0.6684
	loss_value_1: 0.03477
	loss_reward_1: 0.00523
	loss_policy_2: 0.03557
	accuracy_policy_2: 0.63648
	loss_value_2: 0.03576
	loss_reward_2: 0.00503
	loss_policy_3: 0.03922
	accuracy_policy_3: 0.6083
	loss_value_3: 0.03674
	loss_reward_3: 0.00502
	loss_policy_4: 0.04224
	accuracy_policy_4: 0.5851
	loss_value_4: 0.03763
	loss_reward_4: 0.00562
	loss_policy_5: 0.04478
	accuracy_policy_5: 0.56711
	loss_value_5: 0.0385
	loss_reward_5: 0.00657
	loss_policy: 0.31035
	loss_value: 0.35289
	loss_reward: 0.02747
[2024-05-08 01:20:05] nn step 6800, lr: 0.1.
	loss_policy_0: 0.10322
	accuracy_policy_0: 0.76307
	loss_value_0: 0.17354
	loss_policy_1: 0.02916
	accuracy_policy_1: 0.69492
	loss_value_1: 0.0356
	loss_reward_1: 0.00546
	loss_policy_2: 0.03333
	accuracy_policy_2: 0.66148
	loss_value_2: 0.0366
	loss_reward_2: 0.00509
	loss_policy_3: 0.03686
	accuracy_policy_3: 0.63449
	loss_value_3: 0.03757
	loss_reward_3: 0.00515
	loss_policy_4: 0.04017
	accuracy_policy_4: 0.61139
	loss_value_4: 0.03854
	loss_reward_4: 0.00565
	loss_policy_5: 0.04298
	accuracy_policy_5: 0.58941
	loss_value_5: 0.03954
	loss_reward_5: 0.00651
	loss_policy: 0.28573
	loss_value: 0.36139
	loss_reward: 0.02786
Optimization_Done 6800
[2024-05-08 01:22:29] [command] train weight_iter_6800.pkl 32 35
[2024-05-08 01:23:11] nn step 6900, lr: 0.1.
	loss_policy_0: 0.11606
	accuracy_policy_0: 0.73645
	loss_value_0: 0.17141
	loss_policy_1: 0.03119
	accuracy_policy_1: 0.66576
	loss_value_1: 0.03529
	loss_reward_1: 0.00539
	loss_policy_2: 0.03511
	accuracy_policy_2: 0.63324
	loss_value_2: 0.0364
	loss_reward_2: 0.00501
	loss_policy_3: 0.03876
	accuracy_policy_3: 0.60584
	loss_value_3: 0.03737
	loss_reward_3: 0.00512
	loss_policy_4: 0.04162
	accuracy_policy_4: 0.58266
	loss_value_4: 0.03839
	loss_reward_4: 0.00572
	loss_policy_5: 0.04416
	accuracy_policy_5: 0.564
	loss_value_5: 0.03925
	loss_reward_5: 0.00667
	loss_policy: 0.3069
	loss_value: 0.35812
	loss_reward: 0.02791
[2024-05-08 01:23:52] nn step 7000, lr: 0.1.
	loss_policy_0: 0.10316
	accuracy_policy_0: 0.76121
	loss_value_0: 0.1701
	loss_policy_1: 0.0286
	accuracy_policy_1: 0.69383
	loss_value_1: 0.035
	loss_reward_1: 0.00546
	loss_policy_2: 0.03316
	accuracy_policy_2: 0.65693
	loss_value_2: 0.03596
	loss_reward_2: 0.00509
	loss_policy_3: 0.03684
	accuracy_policy_3: 0.63096
	loss_value_3: 0.03701
	loss_reward_3: 0.00511
	loss_policy_4: 0.03948
	accuracy_policy_4: 0.60811
	loss_value_4: 0.03794
	loss_reward_4: 0.00577
	loss_policy_5: 0.0422
	accuracy_policy_5: 0.59195
	loss_value_5: 0.03883
	loss_reward_5: 0.00664
	loss_policy: 0.28344
	loss_value: 0.35484
	loss_reward: 0.02807
Optimization_Done 7000
[2024-05-08 01:26:29] [command] train weight_iter_7000.pkl 33 36
[2024-05-08 01:27:12] nn step 7100, lr: 0.1.
	loss_policy_0: 0.12552
	accuracy_policy_0: 0.72189
	loss_value_0: 0.17361
	loss_policy_1: 0.03383
	accuracy_policy_1: 0.65363
	loss_value_1: 0.03584
	loss_reward_1: 0.00516
	loss_policy_2: 0.03792
	accuracy_policy_2: 0.62109
	loss_value_2: 0.03697
	loss_reward_2: 0.005
	loss_policy_3: 0.04198
	accuracy_policy_3: 0.58889
	loss_value_3: 0.03803
	loss_reward_3: 0.00511
	loss_policy_4: 0.04512
	accuracy_policy_4: 0.5642
	loss_value_4: 0.03891
	loss_reward_4: 0.00562
	loss_policy_5: 0.04797
	accuracy_policy_5: 0.54461
	loss_value_5: 0.03989
	loss_reward_5: 0.00657
	loss_policy: 0.33233
	loss_value: 0.36326
	loss_reward: 0.02744
[2024-05-08 01:27:53] nn step 7200, lr: 0.1.
	loss_policy_0: 0.11354
	accuracy_policy_0: 0.74664
	loss_value_0: 0.17249
	loss_policy_1: 0.03203
	accuracy_policy_1: 0.67434
	loss_value_1: 0.03556
	loss_reward_1: 0.00541
	loss_policy_2: 0.03674
	accuracy_policy_2: 0.63363
	loss_value_2: 0.03661
	loss_reward_2: 0.00484
	loss_policy_3: 0.04059
	accuracy_policy_3: 0.60369
	loss_value_3: 0.03764
	loss_reward_3: 0.00514
	loss_policy_4: 0.0437
	accuracy_policy_4: 0.57955
	loss_value_4: 0.03871
	loss_reward_4: 0.00553
	loss_policy_5: 0.04664
	accuracy_policy_5: 0.55773
	loss_value_5: 0.03966
	loss_reward_5: 0.00648
	loss_policy: 0.31325
	loss_value: 0.36069
	loss_reward: 0.0274
Optimization_Done 7200
[2024-05-08 01:29:52] [command] train weight_iter_7200.pkl 34 37
[2024-05-08 01:30:34] nn step 7300, lr: 0.1.
	loss_policy_0: 0.13339
	accuracy_policy_0: 0.72055
	loss_value_0: 0.18139
	loss_policy_1: 0.03601
	accuracy_policy_1: 0.64451
	loss_value_1: 0.03749
	loss_reward_1: 0.00542
	loss_policy_2: 0.04015
	accuracy_policy_2: 0.62012
	loss_value_2: 0.03856
	loss_reward_2: 0.00494
	loss_policy_3: 0.04424
	accuracy_policy_3: 0.58951
	loss_value_3: 0.03974
	loss_reward_3: 0.00521
	loss_policy_4: 0.04745
	accuracy_policy_4: 0.56617
	loss_value_4: 0.04084
	loss_reward_4: 0.0058
	loss_policy_5: 0.05049
	accuracy_policy_5: 0.54227
	loss_value_5: 0.04188
	loss_reward_5: 0.00682
	loss_policy: 0.35173
	loss_value: 0.37989
	loss_reward: 0.02819
[2024-05-08 01:31:16] nn step 7400, lr: 0.1.
	loss_policy_0: 0.1152
	accuracy_policy_0: 0.74387
	loss_value_0: 0.17538
	loss_policy_1: 0.03228
	accuracy_policy_1: 0.66949
	loss_value_1: 0.03619
	loss_reward_1: 0.00523
	loss_policy_2: 0.03705
	accuracy_policy_2: 0.62965
	loss_value_2: 0.03731
	loss_reward_2: 0.00487
	loss_policy_3: 0.04062
	accuracy_policy_3: 0.60609
	loss_value_3: 0.03842
	loss_reward_3: 0.00517
	loss_policy_4: 0.04396
	accuracy_policy_4: 0.57885
	loss_value_4: 0.03948
	loss_reward_4: 0.00572
	loss_policy_5: 0.0469
	accuracy_policy_5: 0.55701
	loss_value_5: 0.04041
	loss_reward_5: 0.00675
	loss_policy: 0.31601
	loss_value: 0.36719
	loss_reward: 0.02773
Optimization_Done 7400
[2024-05-08 01:33:53] [command] train weight_iter_7400.pkl 35 38
[2024-05-08 01:34:35] nn step 7500, lr: 0.1.
	loss_policy_0: 0.12782
	accuracy_policy_0: 0.70123
	loss_value_0: 0.17202
	loss_policy_1: 0.03477
	accuracy_policy_1: 0.62281
	loss_value_1: 0.0355
	loss_reward_1: 0.00544
	loss_policy_2: 0.0392
	accuracy_policy_2: 0.59031
	loss_value_2: 0.03671
	loss_reward_2: 0.00522
	loss_policy_3: 0.04313
	accuracy_policy_3: 0.55811
	loss_value_3: 0.03767
	loss_reward_3: 0.00556
	loss_policy_4: 0.04672
	accuracy_policy_4: 0.53211
	loss_value_4: 0.03865
	loss_reward_4: 0.00596
	loss_policy_5: 0.04948
	accuracy_policy_5: 0.5066
	loss_value_5: 0.03967
	loss_reward_5: 0.00682
	loss_policy: 0.34111
	loss_value: 0.3602
	loss_reward: 0.029
[2024-05-08 01:35:16] nn step 7600, lr: 0.1.
	loss_policy_0: 0.11807
	accuracy_policy_0: 0.73121
	loss_value_0: 0.17375
	loss_policy_1: 0.03334
	accuracy_policy_1: 0.65072
	loss_value_1: 0.03593
	loss_reward_1: 0.00563
	loss_policy_2: 0.03801
	accuracy_policy_2: 0.61256
	loss_value_2: 0.03701
	loss_reward_2: 0.0052
	loss_policy_3: 0.04227
	accuracy_policy_3: 0.57779
	loss_value_3: 0.03813
	loss_reward_3: 0.00559
	loss_policy_4: 0.04567
	accuracy_policy_4: 0.54949
	loss_value_4: 0.03913
	loss_reward_4: 0.00624
	loss_policy_5: 0.04915
	accuracy_policy_5: 0.52359
	loss_value_5: 0.04013
	loss_reward_5: 0.00724
	loss_policy: 0.3265
	loss_value: 0.36408
	loss_reward: 0.02989
Optimization_Done 7600
[2024-05-08 01:37:52] [command] train weight_iter_7600.pkl 36 39
[2024-05-08 01:38:34] nn step 7700, lr: 0.1.
	loss_policy_0: 0.14677
	accuracy_policy_0: 0.67889
	loss_value_0: 0.17751
	loss_policy_1: 0.03919
	accuracy_policy_1: 0.59658
	loss_value_1: 0.03674
	loss_reward_1: 0.00609
	loss_policy_2: 0.04352
	accuracy_policy_2: 0.56398
	loss_value_2: 0.03793
	loss_reward_2: 0.00549
	loss_policy_3: 0.04777
	accuracy_policy_3: 0.536
	loss_value_3: 0.03903
	loss_reward_3: 0.0058
	loss_policy_4: 0.05107
	accuracy_policy_4: 0.50861
	loss_value_4: 0.04011
	loss_reward_4: 0.00637
	loss_policy_5: 0.05411
	accuracy_policy_5: 0.48322
	loss_value_5: 0.04125
	loss_reward_5: 0.0075
	loss_policy: 0.38243
	loss_value: 0.37258
	loss_reward: 0.03126
[2024-05-08 01:39:15] nn step 7800, lr: 0.1.
	loss_policy_0: 0.13295
	accuracy_policy_0: 0.70715
	loss_value_0: 0.17496
	loss_policy_1: 0.03714
	accuracy_policy_1: 0.62189
	loss_value_1: 0.03644
	loss_reward_1: 0.00601
	loss_policy_2: 0.04194
	accuracy_policy_2: 0.58266
	loss_value_2: 0.03759
	loss_reward_2: 0.00558
	loss_policy_3: 0.04605
	accuracy_policy_3: 0.55262
	loss_value_3: 0.03873
	loss_reward_3: 0.00573
	loss_policy_4: 0.04964
	accuracy_policy_4: 0.52572
	loss_value_4: 0.03972
	loss_reward_4: 0.00657
	loss_policy_5: 0.05319
	accuracy_policy_5: 0.50078
	loss_value_5: 0.04081
	loss_reward_5: 0.00749
	loss_policy: 0.36091
	loss_value: 0.36826
	loss_reward: 0.03138
Optimization_Done 7800
[2024-05-08 01:41:50] [command] train weight_iter_7800.pkl 37 40
[2024-05-08 01:42:32] nn step 7900, lr: 0.1.
	loss_policy_0: 0.1493
	accuracy_policy_0: 0.6902
	loss_value_0: 0.17335
	loss_policy_1: 0.03994
	accuracy_policy_1: 0.61041
	loss_value_1: 0.03618
	loss_reward_1: 0.00582
	loss_policy_2: 0.04468
	accuracy_policy_2: 0.57928
	loss_value_2: 0.03735
	loss_reward_2: 0.0053
	loss_policy_3: 0.04887
	accuracy_policy_3: 0.54879
	loss_value_3: 0.03855
	loss_reward_3: 0.00548
	loss_policy_4: 0.05235
	accuracy_policy_4: 0.51863
	loss_value_4: 0.03962
	loss_reward_4: 0.00619
	loss_policy_5: 0.05531
	accuracy_policy_5: 0.50262
	loss_value_5: 0.04083
	loss_reward_5: 0.00714
	loss_policy: 0.39045
	loss_value: 0.36588
	loss_reward: 0.02993
[2024-05-08 01:43:12] nn step 8000, lr: 0.1.
	loss_policy_0: 0.13677
	accuracy_policy_0: 0.71336
	loss_value_0: 0.17318
	loss_policy_1: 0.03788
	accuracy_policy_1: 0.62551
	loss_value_1: 0.03605
	loss_reward_1: 0.00586
	loss_policy_2: 0.0432
	accuracy_policy_2: 0.59061
	loss_value_2: 0.03736
	loss_reward_2: 0.00532
	loss_policy_3: 0.04745
	accuracy_policy_3: 0.56311
	loss_value_3: 0.03863
	loss_reward_3: 0.0055
	loss_policy_4: 0.05128
	accuracy_policy_4: 0.53248
	loss_value_4: 0.03983
	loss_reward_4: 0.00621
	loss_policy_5: 0.0546
	accuracy_policy_5: 0.51309
	loss_value_5: 0.0409
	loss_reward_5: 0.00723
	loss_policy: 0.37119
	loss_value: 0.36595
	loss_reward: 0.03012
Optimization_Done 8000
[2024-05-08 01:45:33] [command] train weight_iter_8000.pkl 38 41
[2024-05-08 01:46:15] nn step 8100, lr: 0.1.
	loss_policy_0: 0.13998
	accuracy_policy_0: 0.69811
	loss_value_0: 0.16795
	loss_policy_1: 0.03764
	accuracy_policy_1: 0.61744
	loss_value_1: 0.03483
	loss_reward_1: 0.00592
	loss_policy_2: 0.04216
	accuracy_policy_2: 0.58504
	loss_value_2: 0.036
	loss_reward_2: 0.0056
	loss_policy_3: 0.04649
	accuracy_policy_3: 0.55299
	loss_value_3: 0.03729
	loss_reward_3: 0.00565
	loss_policy_4: 0.04985
	accuracy_policy_4: 0.53029
	loss_value_4: 0.03844
	loss_reward_4: 0.00637
	loss_policy_5: 0.05287
	accuracy_policy_5: 0.50453
	loss_value_5: 0.0395
	loss_reward_5: 0.00731
	loss_policy: 0.36897
	loss_value: 0.35401
	loss_reward: 0.03086
[2024-05-08 01:46:56] nn step 8200, lr: 0.1.
	loss_policy_0: 0.12653
	accuracy_policy_0: 0.71857
	loss_value_0: 0.16157
	loss_policy_1: 0.03522
	accuracy_policy_1: 0.63285
	loss_value_1: 0.03351
	loss_reward_1: 0.00577
	loss_policy_2: 0.04005
	accuracy_policy_2: 0.59426
	loss_value_2: 0.03473
	loss_reward_2: 0.00536
	loss_policy_3: 0.04418
	accuracy_policy_3: 0.5585
	loss_value_3: 0.03585
	loss_reward_3: 0.00563
	loss_policy_4: 0.04763
	accuracy_policy_4: 0.53732
	loss_value_4: 0.037
	loss_reward_4: 0.00624
	loss_policy_5: 0.05092
	accuracy_policy_5: 0.51322
	loss_value_5: 0.03806
	loss_reward_5: 0.00724
	loss_policy: 0.34453
	loss_value: 0.34072
	loss_reward: 0.03023
Optimization_Done 8200
[2024-05-08 01:49:28] [command] train weight_iter_8200.pkl 39 42
[2024-05-08 01:50:11] nn step 8300, lr: 0.1.
	loss_policy_0: 0.13726
	accuracy_policy_0: 0.70143
	loss_value_0: 0.16623
	loss_policy_1: 0.0367
	accuracy_policy_1: 0.62828
	loss_value_1: 0.03464
	loss_reward_1: 0.00584
	loss_policy_2: 0.04123
	accuracy_policy_2: 0.59203
	loss_value_2: 0.036
	loss_reward_2: 0.00551
	loss_policy_3: 0.04489
	accuracy_policy_3: 0.56883
	loss_value_3: 0.03723
	loss_reward_3: 0.00583
	loss_policy_4: 0.04786
	accuracy_policy_4: 0.54414
	loss_value_4: 0.03851
	loss_reward_4: 0.00637
	loss_policy_5: 0.05072
	accuracy_policy_5: 0.52176
	loss_value_5: 0.03962
	loss_reward_5: 0.00722
	loss_policy: 0.35866
	loss_value: 0.35223
	loss_reward: 0.03077
[2024-05-08 01:50:52] nn step 8400, lr: 0.1.
	loss_policy_0: 0.12203
	accuracy_policy_0: 0.72938
	loss_value_0: 0.16291
	loss_policy_1: 0.03401
	accuracy_policy_1: 0.64564
	loss_value_1: 0.03409
	loss_reward_1: 0.00582
	loss_policy_2: 0.0386
	accuracy_policy_2: 0.61488
	loss_value_2: 0.03547
	loss_reward_2: 0.00539
	loss_policy_3: 0.04277
	accuracy_policy_3: 0.57977
	loss_value_3: 0.03673
	loss_reward_3: 0.00578
	loss_policy_4: 0.04598
	accuracy_policy_4: 0.55824
	loss_value_4: 0.03784
	loss_reward_4: 0.00616
	loss_policy_5: 0.04875
	accuracy_policy_5: 0.53725
	loss_value_5: 0.03896
	loss_reward_5: 0.00721
	loss_policy: 0.33214
	loss_value: 0.346
	loss_reward: 0.03037
Optimization_Done 8400
[2024-05-08 01:53:25] [command] train weight_iter_8400.pkl 40 43
[2024-05-08 01:54:07] nn step 8500, lr: 0.1.
	loss_policy_0: 0.14715
	accuracy_policy_0: 0.68916
	loss_value_0: 0.16354
	loss_policy_1: 0.03881
	accuracy_policy_1: 0.61479
	loss_value_1: 0.03434
	loss_reward_1: 0.00594
	loss_policy_2: 0.04367
	accuracy_policy_2: 0.57926
	loss_value_2: 0.03584
	loss_reward_2: 0.00568
	loss_policy_3: 0.04766
	accuracy_policy_3: 0.55314
	loss_value_3: 0.0373
	loss_reward_3: 0.00618
	loss_policy_4: 0.05105
	accuracy_policy_4: 0.52785
	loss_value_4: 0.03858
	loss_reward_4: 0.00667
	loss_policy_5: 0.05401
	accuracy_policy_5: 0.50428
	loss_value_5: 0.03982
	loss_reward_5: 0.00763
	loss_policy: 0.38234
	loss_value: 0.34942
	loss_reward: 0.0321
[2024-05-08 01:54:48] nn step 8600, lr: 0.1.
	loss_policy_0: 0.13435
	accuracy_policy_0: 0.71664
	loss_value_0: 0.1621
	loss_policy_1: 0.03681
	accuracy_policy_1: 0.63637
	loss_value_1: 0.03398
	loss_reward_1: 0.00603
	loss_policy_2: 0.04149
	accuracy_policy_2: 0.60277
	loss_value_2: 0.03545
	loss_reward_2: 0.00567
	loss_policy_3: 0.04542
	accuracy_policy_3: 0.57275
	loss_value_3: 0.03674
	loss_reward_3: 0.00622
	loss_policy_4: 0.04901
	accuracy_policy_4: 0.55123
	loss_value_4: 0.03814
	loss_reward_4: 0.00678
	loss_policy_5: 0.05182
	accuracy_policy_5: 0.53102
	loss_value_5: 0.03954
	loss_reward_5: 0.00764
	loss_policy: 0.3589
	loss_value: 0.34596
	loss_reward: 0.03234
Optimization_Done 8600
[2024-05-08 01:57:11] [command] train weight_iter_8600.pkl 41 44
[2024-05-08 01:57:53] nn step 8700, lr: 0.1.
	loss_policy_0: 0.14887
	accuracy_policy_0: 0.69256
	loss_value_0: 0.16701
	loss_policy_1: 0.03908
	accuracy_policy_1: 0.61566
	loss_value_1: 0.03498
	loss_reward_1: 0.00592
	loss_policy_2: 0.04352
	accuracy_policy_2: 0.5825
	loss_value_2: 0.03629
	loss_reward_2: 0.0058
	loss_policy_3: 0.04748
	accuracy_policy_3: 0.55486
	loss_value_3: 0.03748
	loss_reward_3: 0.00599
	loss_policy_4: 0.05072
	accuracy_policy_4: 0.53324
	loss_value_4: 0.03873
	loss_reward_4: 0.00657
	loss_policy_5: 0.05366
	accuracy_policy_5: 0.51377
	loss_value_5: 0.03992
	loss_reward_5: 0.00756
	loss_policy: 0.38333
	loss_value: 0.35441
	loss_reward: 0.03183
[2024-05-08 01:58:34] nn step 8800, lr: 0.1.
	loss_policy_0: 0.13545
	accuracy_policy_0: 0.71652
	loss_value_0: 0.16715
	loss_policy_1: 0.03773
	accuracy_policy_1: 0.63303
	loss_value_1: 0.0349
	loss_reward_1: 0.00595
	loss_policy_2: 0.04186
	accuracy_policy_2: 0.59938
	loss_value_2: 0.03631
	loss_reward_2: 0.00565
	loss_policy_3: 0.04584
	accuracy_policy_3: 0.57334
	loss_value_3: 0.03757
	loss_reward_3: 0.00617
	loss_policy_4: 0.04903
	accuracy_policy_4: 0.54996
	loss_value_4: 0.03872
	loss_reward_4: 0.00663
	loss_policy_5: 0.05211
	accuracy_policy_5: 0.53088
	loss_value_5: 0.04001
	loss_reward_5: 0.00773
	loss_policy: 0.36203
	loss_value: 0.35466
	loss_reward: 0.03213
Optimization_Done 8800
[2024-05-08 02:01:07] [command] train weight_iter_8800.pkl 42 45
[2024-05-08 02:01:50] nn step 8900, lr: 0.1.
	loss_policy_0: 0.14677
	accuracy_policy_0: 0.70211
	loss_value_0: 0.16828
	loss_policy_1: 0.03855
	accuracy_policy_1: 0.6298
	loss_value_1: 0.03491
	loss_reward_1: 0.00565
	loss_policy_2: 0.04302
	accuracy_policy_2: 0.59408
	loss_value_2: 0.0363
	loss_reward_2: 0.0055
	loss_policy_3: 0.04692
	accuracy_policy_3: 0.56488
	loss_value_3: 0.03767
	loss_reward_3: 0.00569
	loss_policy_4: 0.04988
	accuracy_policy_4: 0.54469
	loss_value_4: 0.03881
	loss_reward_4: 0.00615
	loss_policy_5: 0.05268
	accuracy_policy_5: 0.52266
	loss_value_5: 0.03988
	loss_reward_5: 0.00696
	loss_policy: 0.37782
	loss_value: 0.35586
	loss_reward: 0.02995
[2024-05-08 02:02:31] nn step 9000, lr: 0.1.
	loss_policy_0: 0.13469
	accuracy_policy_0: 0.72389
	loss_value_0: 0.16848
	loss_policy_1: 0.03717
	accuracy_policy_1: 0.64064
	loss_value_1: 0.03504
	loss_reward_1: 0.00569
	loss_policy_2: 0.04144
	accuracy_policy_2: 0.61117
	loss_value_2: 0.03634
	loss_reward_2: 0.00556
	loss_policy_3: 0.04547
	accuracy_policy_3: 0.57777
	loss_value_3: 0.0376
	loss_reward_3: 0.00578
	loss_policy_4: 0.04898
	accuracy_policy_4: 0.54986
	loss_value_4: 0.03896
	loss_reward_4: 0.00632
	loss_policy_5: 0.05193
	accuracy_policy_5: 0.53162
	loss_value_5: 0.04008
	loss_reward_5: 0.00726
	loss_policy: 0.35969
	loss_value: 0.3565
	loss_reward: 0.03061
Optimization_Done 9000
[2024-05-08 02:05:04] [command] train weight_iter_9000.pkl 43 46
[2024-05-08 02:05:47] nn step 9100, lr: 0.1.
	loss_policy_0: 0.14764
	accuracy_policy_0: 0.69705
	loss_value_0: 0.16778
	loss_policy_1: 0.03826
	accuracy_policy_1: 0.62445
	loss_value_1: 0.03504
	loss_reward_1: 0.00545
	loss_policy_2: 0.04264
	accuracy_policy_2: 0.5943
	loss_value_2: 0.0363
	loss_reward_2: 0.00544
	loss_policy_3: 0.04623
	accuracy_policy_3: 0.56676
	loss_value_3: 0.03755
	loss_reward_3: 0.00564
	loss_policy_4: 0.04923
	accuracy_policy_4: 0.5433
	loss_value_4: 0.0389
	loss_reward_4: 0.00617
	loss_policy_5: 0.05209
	accuracy_policy_5: 0.51734
	loss_value_5: 0.04011
	loss_reward_5: 0.00699
	loss_policy: 0.37608
	loss_value: 0.35568
	loss_reward: 0.02968
[2024-05-08 02:06:28] nn step 9200, lr: 0.1.
	loss_policy_0: 0.13678
	accuracy_policy_0: 0.71844
	loss_value_0: 0.17185
	loss_policy_1: 0.03681
	accuracy_policy_1: 0.64016
	loss_value_1: 0.03586
	loss_reward_1: 0.00571
	loss_policy_2: 0.04166
	accuracy_policy_2: 0.60789
	loss_value_2: 0.03721
	loss_reward_2: 0.00522
	loss_policy_3: 0.04566
	accuracy_policy_3: 0.57963
	loss_value_3: 0.03842
	loss_reward_3: 0.0056
	loss_policy_4: 0.04911
	accuracy_policy_4: 0.55486
	loss_value_4: 0.03972
	loss_reward_4: 0.00617
	loss_policy_5: 0.05245
	accuracy_policy_5: 0.53326
	loss_value_5: 0.04091
	loss_reward_5: 0.00731
	loss_policy: 0.36245
	loss_value: 0.36398
	loss_reward: 0.03
Optimization_Done 9200
[2024-05-08 02:09:00] [command] train weight_iter_9200.pkl 44 47
[2024-05-08 02:09:43] nn step 9300, lr: 0.1.
	loss_policy_0: 0.13897
	accuracy_policy_0: 0.70451
	loss_value_0: 0.1781
	loss_policy_1: 0.03656
	accuracy_policy_1: 0.63199
	loss_value_1: 0.037
	loss_reward_1: 0.00578
	loss_policy_2: 0.04128
	accuracy_policy_2: 0.59439
	loss_value_2: 0.03854
	loss_reward_2: 0.00554
	loss_policy_3: 0.04535
	accuracy_policy_3: 0.56121
	loss_value_3: 0.03975
	loss_reward_3: 0.00589
	loss_policy_4: 0.04829
	accuracy_policy_4: 0.53973
	loss_value_4: 0.04097
	loss_reward_4: 0.00652
	loss_policy_5: 0.05152
	accuracy_policy_5: 0.51299
	loss_value_5: 0.0421
	loss_reward_5: 0.00738
	loss_policy: 0.36196
	loss_value: 0.37646
	loss_reward: 0.0311
[2024-05-08 02:10:24] nn step 9400, lr: 0.1.
	loss_policy_0: 0.12728
	accuracy_policy_0: 0.72623
	loss_value_0: 0.17565
	loss_policy_1: 0.03473
	accuracy_policy_1: 0.64922
	loss_value_1: 0.03672
	loss_reward_1: 0.00587
	loss_policy_2: 0.03966
	accuracy_policy_2: 0.61213
	loss_value_2: 0.0381
	loss_reward_2: 0.00558
	loss_policy_3: 0.043
	accuracy_policy_3: 0.58408
	loss_value_3: 0.03943
	loss_reward_3: 0.00604
	loss_policy_4: 0.04672
	accuracy_policy_4: 0.55637
	loss_value_4: 0.04058
	loss_reward_4: 0.00654
	loss_policy_5: 0.05011
	accuracy_policy_5: 0.53354
	loss_value_5: 0.04161
	loss_reward_5: 0.00747
	loss_policy: 0.34149
	loss_value: 0.3721
	loss_reward: 0.03149
Optimization_Done 9400
[2024-05-08 02:12:22] [command] train weight_iter_9400.pkl 45 48
[2024-05-08 02:13:04] nn step 9500, lr: 0.1.
	loss_policy_0: 0.14042
	accuracy_policy_0: 0.7091
	loss_value_0: 0.1796
	loss_policy_1: 0.03754
	accuracy_policy_1: 0.62871
	loss_value_1: 0.0375
	loss_reward_1: 0.00636
	loss_policy_2: 0.04226
	accuracy_policy_2: 0.59137
	loss_value_2: 0.03897
	loss_reward_2: 0.00616
	loss_policy_3: 0.04617
	accuracy_policy_3: 0.56205
	loss_value_3: 0.04026
	loss_reward_3: 0.00661
	loss_policy_4: 0.04951
	accuracy_policy_4: 0.5358
	loss_value_4: 0.04129
	loss_reward_4: 0.00716
	loss_policy_5: 0.05253
	accuracy_policy_5: 0.51131
	loss_value_5: 0.04233
	loss_reward_5: 0.00825
	loss_policy: 0.36843
	loss_value: 0.37996
	loss_reward: 0.03454
[2024-05-08 02:13:46] nn step 9600, lr: 0.1.
	loss_policy_0: 0.12604
	accuracy_policy_0: 0.72949
	loss_value_0: 0.17381
	loss_policy_1: 0.03499
	accuracy_policy_1: 0.64918
	loss_value_1: 0.03626
	loss_reward_1: 0.00613
	loss_policy_2: 0.03965
	accuracy_policy_2: 0.60666
	loss_value_2: 0.03764
	loss_reward_2: 0.006
	loss_policy_3: 0.0435
	accuracy_policy_3: 0.57723
	loss_value_3: 0.03891
	loss_reward_3: 0.00643
	loss_policy_4: 0.0469
	accuracy_policy_4: 0.55168
	loss_value_4: 0.03995
	loss_reward_4: 0.00706
	loss_policy_5: 0.05032
	accuracy_policy_5: 0.52709
	loss_value_5: 0.04101
	loss_reward_5: 0.00814
	loss_policy: 0.3414
	loss_value: 0.36758
	loss_reward: 0.03375
Optimization_Done 9600
[2024-05-08 02:16:21] [command] train weight_iter_9600.pkl 46 49
[2024-05-08 02:17:03] nn step 9700, lr: 0.1.
	loss_policy_0: 0.14311
	accuracy_policy_0: 0.70646
	loss_value_0: 0.17487
	loss_policy_1: 0.03808
	accuracy_policy_1: 0.6298
	loss_value_1: 0.03649
	loss_reward_1: 0.00609
	loss_policy_2: 0.04265
	accuracy_policy_2: 0.59203
	loss_value_2: 0.03782
	loss_reward_2: 0.00587
	loss_policy_3: 0.04669
	accuracy_policy_3: 0.56145
	loss_value_3: 0.03915
	loss_reward_3: 0.00649
	loss_policy_4: 0.0505
	accuracy_policy_4: 0.53357
	loss_value_4: 0.0404
	loss_reward_4: 0.00704
	loss_policy_5: 0.0533
	accuracy_policy_5: 0.51178
	loss_value_5: 0.0416
	loss_reward_5: 0.00819
	loss_policy: 0.37434
	loss_value: 0.37034
	loss_reward: 0.03367
[2024-05-08 02:17:44] nn step 9800, lr: 0.1.
	loss_policy_0: 0.12782
	accuracy_policy_0: 0.73201
	loss_value_0: 0.16867
	loss_policy_1: 0.03555
	accuracy_policy_1: 0.64789
	loss_value_1: 0.03526
	loss_reward_1: 0.0061
	loss_policy_2: 0.04048
	accuracy_policy_2: 0.60846
	loss_value_2: 0.03656
	loss_reward_2: 0.0058
	loss_policy_3: 0.04448
	accuracy_policy_3: 0.57797
	loss_value_3: 0.03777
	loss_reward_3: 0.00618
	loss_policy_4: 0.04811
	accuracy_policy_4: 0.55057
	loss_value_4: 0.03909
	loss_reward_4: 0.0068
	loss_policy_5: 0.05113
	accuracy_policy_5: 0.52494
	loss_value_5: 0.04017
	loss_reward_5: 0.00814
	loss_policy: 0.34756
	loss_value: 0.35752
	loss_reward: 0.03301
Optimization_Done 9800
[2024-05-08 02:20:06] [command] train weight_iter_9800.pkl 47 50
[2024-05-08 02:20:48] nn step 9900, lr: 0.1.
	loss_policy_0: 0.14458
	accuracy_policy_0: 0.70588
	loss_value_0: 0.1688
	loss_policy_1: 0.03796
	accuracy_policy_1: 0.6302
	loss_value_1: 0.03549
	loss_reward_1: 0.00607
	loss_policy_2: 0.04274
	accuracy_policy_2: 0.59221
	loss_value_2: 0.03695
	loss_reward_2: 0.00575
	loss_policy_3: 0.04664
	accuracy_policy_3: 0.5665
	loss_value_3: 0.03831
	loss_reward_3: 0.00629
	loss_policy_4: 0.05026
	accuracy_policy_4: 0.53557
	loss_value_4: 0.03941
	loss_reward_4: 0.00663
	loss_policy_5: 0.05329
	accuracy_policy_5: 0.51328
	loss_value_5: 0.04069
	loss_reward_5: 0.00783
	loss_policy: 0.37546
	loss_value: 0.35966
	loss_reward: 0.03256
[2024-05-08 02:21:29] nn step 10000, lr: 0.1.
	loss_policy_0: 0.12404
	accuracy_policy_0: 0.72963
	loss_value_0: 0.15903
	loss_policy_1: 0.03399
	accuracy_policy_1: 0.65303
	loss_value_1: 0.03321
	loss_reward_1: 0.00571
	loss_policy_2: 0.03912
	accuracy_policy_2: 0.61301
	loss_value_2: 0.03463
	loss_reward_2: 0.00556
	loss_policy_3: 0.04291
	accuracy_policy_3: 0.58178
	loss_value_3: 0.03597
	loss_reward_3: 0.00589
	loss_policy_4: 0.04609
	accuracy_policy_4: 0.55754
	loss_value_4: 0.03709
	loss_reward_4: 0.00663
	loss_policy_5: 0.04932
	accuracy_policy_5: 0.53184
	loss_value_5: 0.03839
	loss_reward_5: 0.00749
	loss_policy: 0.33547
	loss_value: 0.33832
	loss_reward: 0.03127
Optimization_Done 10000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-08 03:09:36] [command] train weight_iter_10000.pkl 49 51
[2024-05-08 03:10:27] nn step 10100, lr: 0.1.
	loss_policy_0: 0.15809
	accuracy_policy_0: 0.69352
	loss_value_0: 0.17441
	loss_policy_1: 0.03977
	accuracy_policy_1: 0.63018
	loss_value_1: 0.0363
	loss_reward_1: 0.00469
	loss_policy_2: 0.04395
	accuracy_policy_2: 0.60219
	loss_value_2: 0.03779
	loss_reward_2: 0.00473
	loss_policy_3: 0.0482
	accuracy_policy_3: 0.56947
	loss_value_3: 0.03918
	loss_reward_3: 0.00504
	loss_policy_4: 0.05164
	accuracy_policy_4: 0.54711
	loss_value_4: 0.04047
	loss_reward_4: 0.00547
	loss_policy_5: 0.05477
	accuracy_policy_5: 0.52465
	loss_value_5: 0.0418
	loss_reward_5: 0.00619
	loss_policy: 0.39642
	loss_value: 0.36995
	loss_reward: 0.02612
[2024-05-08 03:11:07] nn step 10200, lr: 0.1.
	loss_policy_0: 0.14549
	accuracy_policy_0: 0.72311
	loss_value_0: 0.17179
	loss_policy_1: 0.03833
	accuracy_policy_1: 0.65559
	loss_value_1: 0.03599
	loss_reward_1: 0.005
	loss_policy_2: 0.04318
	accuracy_policy_2: 0.61607
	loss_value_2: 0.03746
	loss_reward_2: 0.00493
	loss_policy_3: 0.04731
	accuracy_policy_3: 0.58982
	loss_value_3: 0.03891
	loss_reward_3: 0.00521
	loss_policy_4: 0.05039
	accuracy_policy_4: 0.57555
	loss_value_4: 0.04032
	loss_reward_4: 0.00566
	loss_policy_5: 0.05369
	accuracy_policy_5: 0.55178
	loss_value_5: 0.04171
	loss_reward_5: 0.00656
	loss_policy: 0.37839
	loss_value: 0.36619
	loss_reward: 0.02736
Optimization_Done 10200
[2024-05-08 03:13:40] [command] train weight_iter_10200.pkl 50 52
[2024-05-08 03:14:22] nn step 10300, lr: 0.1.
	loss_policy_0: 0.17277
	accuracy_policy_0: 0.66406
	loss_value_0: 0.1691
	loss_policy_1: 0.04289
	accuracy_policy_1: 0.59938
	loss_value_1: 0.03526
	loss_reward_1: 0.00502
	loss_policy_2: 0.04716
	accuracy_policy_2: 0.57371
	loss_value_2: 0.03677
	loss_reward_2: 0.0047
	loss_policy_3: 0.05117
	accuracy_policy_3: 0.54773
	loss_value_3: 0.03811
	loss_reward_3: 0.00519
	loss_policy_4: 0.05456
	accuracy_policy_4: 0.524
	loss_value_4: 0.0395
	loss_reward_4: 0.00543
	loss_policy_5: 0.05789
	accuracy_policy_5: 0.50398
	loss_value_5: 0.04095
	loss_reward_5: 0.00604
	loss_policy: 0.42645
	loss_value: 0.35968
	loss_reward: 0.02639
[2024-05-08 03:15:03] nn step 10400, lr: 0.1.
	loss_policy_0: 0.14381
	accuracy_policy_0: 0.70342
	loss_value_0: 0.15829
	loss_policy_1: 0.03726
	accuracy_policy_1: 0.63457
	loss_value_1: 0.03315
	loss_reward_1: 0.00477
	loss_policy_2: 0.04149
	accuracy_policy_2: 0.61008
	loss_value_2: 0.03435
	loss_reward_2: 0.0046
	loss_policy_3: 0.04499
	accuracy_policy_3: 0.58121
	loss_value_3: 0.03557
	loss_reward_3: 0.00475
	loss_policy_4: 0.04821
	accuracy_policy_4: 0.5598
	loss_value_4: 0.03695
	loss_reward_4: 0.00517
	loss_policy_5: 0.05198
	accuracy_policy_5: 0.535
	loss_value_5: 0.03811
	loss_reward_5: 0.00589
	loss_policy: 0.36774
	loss_value: 0.33641
	loss_reward: 0.02518
Optimization_Done 10400
[2024-05-08 03:17:37] [command] train weight_iter_10400.pkl 51 53
[2024-05-08 03:18:19] nn step 10500, lr: 0.1.
	loss_policy_0: 0.14492
	accuracy_policy_0: 0.70777
	loss_value_0: 0.16217
	loss_policy_1: 0.03774
	accuracy_policy_1: 0.64111
	loss_value_1: 0.03396
	loss_reward_1: 0.00487
	loss_policy_2: 0.04143
	accuracy_policy_2: 0.61252
	loss_value_2: 0.0352
	loss_reward_2: 0.00453
	loss_policy_3: 0.04541
	accuracy_policy_3: 0.58527
	loss_value_3: 0.03637
	loss_reward_3: 0.00465
	loss_policy_4: 0.04831
	accuracy_policy_4: 0.56307
	loss_value_4: 0.03752
	loss_reward_4: 0.00505
	loss_policy_5: 0.05192
	accuracy_policy_5: 0.54184
	loss_value_5: 0.03873
	loss_reward_5: 0.00586
	loss_policy: 0.36973
	loss_value: 0.34395
	loss_reward: 0.02497
[2024-05-08 03:19:00] nn step 10600, lr: 0.1.
	loss_policy_0: 0.11746
	accuracy_policy_0: 0.75006
	loss_value_0: 0.16235
	loss_policy_1: 0.03321
	accuracy_policy_1: 0.67158
	loss_value_1: 0.03382
	loss_reward_1: 0.00476
	loss_policy_2: 0.03744
	accuracy_policy_2: 0.64273
	loss_value_2: 0.03522
	loss_reward_2: 0.00432
	loss_policy_3: 0.04169
	accuracy_policy_3: 0.61133
	loss_value_3: 0.03654
	loss_reward_3: 0.00461
	loss_policy_4: 0.04516
	accuracy_policy_4: 0.58998
	loss_value_4: 0.0376
	loss_reward_4: 0.00503
	loss_policy_5: 0.04882
	accuracy_policy_5: 0.56727
	loss_value_5: 0.03876
	loss_reward_5: 0.0058
	loss_policy: 0.32378
	loss_value: 0.3443
	loss_reward: 0.02453
Optimization_Done 10600
[2024-05-08 03:21:35] [command] train weight_iter_10600.pkl 52 54
[2024-05-08 03:22:18] nn step 10700, lr: 0.1.
	loss_policy_0: 0.13787
	accuracy_policy_0: 0.70967
	loss_value_0: 0.15005
	loss_policy_1: 0.03746
	accuracy_policy_1: 0.63334
	loss_value_1: 0.03149
	loss_reward_1: 0.00557
	loss_policy_2: 0.04205
	accuracy_policy_2: 0.5932
	loss_value_2: 0.03275
	loss_reward_2: 0.00534
	loss_policy_3: 0.04623
	accuracy_policy_3: 0.56379
	loss_value_3: 0.03406
	loss_reward_3: 0.00552
	loss_policy_4: 0.04912
	accuracy_policy_4: 0.54172
	loss_value_4: 0.03536
	loss_reward_4: 0.00598
	loss_policy_5: 0.05212
	accuracy_policy_5: 0.5217
	loss_value_5: 0.03655
	loss_reward_5: 0.007
	loss_policy: 0.36484
	loss_value: 0.32025
	loss_reward: 0.02941
[2024-05-08 03:22:59] nn step 10800, lr: 0.1.
	loss_policy_0: 0.1199
	accuracy_policy_0: 0.74389
	loss_value_0: 0.15119
	loss_policy_1: 0.03494
	accuracy_policy_1: 0.65053
	loss_value_1: 0.03176
	loss_reward_1: 0.00552
	loss_policy_2: 0.03932
	accuracy_policy_2: 0.61902
	loss_value_2: 0.03317
	loss_reward_2: 0.00528
	loss_policy_3: 0.0433
	accuracy_policy_3: 0.59031
	loss_value_3: 0.03437
	loss_reward_3: 0.00544
	loss_policy_4: 0.04691
	accuracy_policy_4: 0.56752
	loss_value_4: 0.03559
	loss_reward_4: 0.00586
	loss_policy_5: 0.04978
	accuracy_policy_5: 0.54742
	loss_value_5: 0.03675
	loss_reward_5: 0.0068
	loss_policy: 0.33414
	loss_value: 0.32284
	loss_reward: 0.02889
Optimization_Done 10800
[2024-05-08 03:25:20] [command] train weight_iter_10800.pkl 53 55
[2024-05-08 03:26:02] nn step 10900, lr: 0.1.
	loss_policy_0: 0.13117
	accuracy_policy_0: 0.71322
	loss_value_0: 0.15222
	loss_policy_1: 0.03638
	accuracy_policy_1: 0.62557
	loss_value_1: 0.03199
	loss_reward_1: 0.00529
	loss_policy_2: 0.04048
	accuracy_policy_2: 0.59301
	loss_value_2: 0.03312
	loss_reward_2: 0.00485
	loss_policy_3: 0.04404
	accuracy_policy_3: 0.56193
	loss_value_3: 0.03417
	loss_reward_3: 0.00514
	loss_policy_4: 0.04705
	accuracy_policy_4: 0.53992
	loss_value_4: 0.03511
	loss_reward_4: 0.00574
	loss_policy_5: 0.0502
	accuracy_policy_5: 0.51465
	loss_value_5: 0.03605
	loss_reward_5: 0.00656
	loss_policy: 0.34933
	loss_value: 0.32265
	loss_reward: 0.02757
[2024-05-08 03:26:43] nn step 11000, lr: 0.1.
	loss_policy_0: 0.12115
	accuracy_policy_0: 0.74148
	loss_value_0: 0.157
	loss_policy_1: 0.03524
	accuracy_policy_1: 0.64816
	loss_value_1: 0.03308
	loss_reward_1: 0.00544
	loss_policy_2: 0.0397
	accuracy_policy_2: 0.6118
	loss_value_2: 0.0344
	loss_reward_2: 0.0052
	loss_policy_3: 0.04347
	accuracy_policy_3: 0.58568
	loss_value_3: 0.03562
	loss_reward_3: 0.00553
	loss_policy_4: 0.04682
	accuracy_policy_4: 0.56168
	loss_value_4: 0.03661
	loss_reward_4: 0.006
	loss_policy_5: 0.05004
	accuracy_policy_5: 0.53486
	loss_value_5: 0.03771
	loss_reward_5: 0.00693
	loss_policy: 0.33642
	loss_value: 0.33443
	loss_reward: 0.0291
Optimization_Done 11000
[2024-05-08 03:29:16] [command] train weight_iter_11000.pkl 54 56
[2024-05-08 03:29:58] nn step 11100, lr: 0.1.
	loss_policy_0: 0.14456
	accuracy_policy_0: 0.70217
	loss_value_0: 0.163
	loss_policy_1: 0.0387
	accuracy_policy_1: 0.62004
	loss_value_1: 0.03396
	loss_reward_1: 0.00564
	loss_policy_2: 0.04316
	accuracy_policy_2: 0.58375
	loss_value_2: 0.03537
	loss_reward_2: 0.0055
	loss_policy_3: 0.04677
	accuracy_policy_3: 0.55992
	loss_value_3: 0.03665
	loss_reward_3: 0.00582
	loss_policy_4: 0.05003
	accuracy_policy_4: 0.53547
	loss_value_4: 0.03784
	loss_reward_4: 0.00625
	loss_policy_5: 0.05306
	accuracy_policy_5: 0.51641
	loss_value_5: 0.03894
	loss_reward_5: 0.00711
	loss_policy: 0.37627
	loss_value: 0.34575
	loss_reward: 0.03032
[2024-05-08 03:30:39] nn step 11200, lr: 0.1.
	loss_policy_0: 0.1193
	accuracy_policy_0: 0.73984
	loss_value_0: 0.15702
	loss_policy_1: 0.03458
	accuracy_policy_1: 0.65168
	loss_value_1: 0.03297
	loss_reward_1: 0.00558
	loss_policy_2: 0.03961
	accuracy_policy_2: 0.61256
	loss_value_2: 0.03433
	loss_reward_2: 0.00527
	loss_policy_3: 0.0436
	accuracy_policy_3: 0.58467
	loss_value_3: 0.03565
	loss_reward_3: 0.00555
	loss_policy_4: 0.04718
	accuracy_policy_4: 0.55945
	loss_value_4: 0.03692
	loss_reward_4: 0.00617
	loss_policy_5: 0.05021
	accuracy_policy_5: 0.53498
	loss_value_5: 0.03804
	loss_reward_5: 0.00724
	loss_policy: 0.33447
	loss_value: 0.33493
	loss_reward: 0.02981
Optimization_Done 11200
[2024-05-08 03:33:12] [command] train weight_iter_11200.pkl 55 57
[2024-05-08 03:33:53] nn step 11300, lr: 0.1.
	loss_policy_0: 0.13014
	accuracy_policy_0: 0.72727
	loss_value_0: 0.15061
	loss_policy_1: 0.03523
	accuracy_policy_1: 0.64762
	loss_value_1: 0.03169
	loss_reward_1: 0.00514
	loss_policy_2: 0.03977
	accuracy_policy_2: 0.60959
	loss_value_2: 0.03307
	loss_reward_2: 0.00505
	loss_policy_3: 0.04356
	accuracy_policy_3: 0.58047
	loss_value_3: 0.03429
	loss_reward_3: 0.00541
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.55367
	loss_value_4: 0.03547
	loss_reward_4: 0.00588
	loss_policy_5: 0.0492
	accuracy_policy_5: 0.53926
	loss_value_5: 0.03664
	loss_reward_5: 0.00679
	loss_policy: 0.3446
	loss_value: 0.32178
	loss_reward: 0.02827
[2024-05-08 03:34:34] nn step 11400, lr: 0.1.
	loss_policy_0: 0.11317
	accuracy_policy_0: 0.75426
	loss_value_0: 0.14854
	loss_policy_1: 0.0321
	accuracy_policy_1: 0.66844
	loss_value_1: 0.03112
	loss_reward_1: 0.00504
	loss_policy_2: 0.03666
	accuracy_policy_2: 0.63408
	loss_value_2: 0.03257
	loss_reward_2: 0.00499
	loss_policy_3: 0.04076
	accuracy_policy_3: 0.60002
	loss_value_3: 0.03386
	loss_reward_3: 0.00524
	loss_policy_4: 0.04396
	accuracy_policy_4: 0.57678
	loss_value_4: 0.03506
	loss_reward_4: 0.00566
	loss_policy_5: 0.04717
	accuracy_policy_5: 0.5548
	loss_value_5: 0.0362
	loss_reward_5: 0.00683
	loss_policy: 0.31383
	loss_value: 0.31734
	loss_reward: 0.02775
Optimization_Done 11400
[2024-05-08 03:37:05] [command] train weight_iter_11400.pkl 56 58
[2024-05-08 03:37:47] nn step 11500, lr: 0.1.
	loss_policy_0: 0.13277
	accuracy_policy_0: 0.71139
	loss_value_0: 0.15575
	loss_policy_1: 0.03705
	accuracy_policy_1: 0.62793
	loss_value_1: 0.03293
	loss_reward_1: 0.00601
	loss_policy_2: 0.04271
	accuracy_policy_2: 0.59033
	loss_value_2: 0.03457
	loss_reward_2: 0.00577
	loss_policy_3: 0.04721
	accuracy_policy_3: 0.55676
	loss_value_3: 0.03607
	loss_reward_3: 0.00625
	loss_policy_4: 0.05039
	accuracy_policy_4: 0.53816
	loss_value_4: 0.03747
	loss_reward_4: 0.00677
	loss_policy_5: 0.05345
	accuracy_policy_5: 0.51754
	loss_value_5: 0.03872
	loss_reward_5: 0.00819
	loss_policy: 0.36358
	loss_value: 0.3355
	loss_reward: 0.03298
[2024-05-08 03:38:28] nn step 11600, lr: 0.1.
	loss_policy_0: 0.11523
	accuracy_policy_0: 0.7457
	loss_value_0: 0.15524
	loss_policy_1: 0.03418
	accuracy_policy_1: 0.65568
	loss_value_1: 0.03293
	loss_reward_1: 0.00581
	loss_policy_2: 0.03914
	accuracy_policy_2: 0.61654
	loss_value_2: 0.03442
	loss_reward_2: 0.00581
	loss_policy_3: 0.0434
	accuracy_policy_3: 0.58641
	loss_value_3: 0.03582
	loss_reward_3: 0.00604
	loss_policy_4: 0.04705
	accuracy_policy_4: 0.56383
	loss_value_4: 0.03718
	loss_reward_4: 0.00662
	loss_policy_5: 0.05021
	accuracy_policy_5: 0.54545
	loss_value_5: 0.03838
	loss_reward_5: 0.00781
	loss_policy: 0.32921
	loss_value: 0.33397
	loss_reward: 0.03208
Optimization_Done 11600
[2024-05-08 03:40:48] [command] train weight_iter_11600.pkl 57 59
[2024-05-08 03:41:30] nn step 11700, lr: 0.1.
	loss_policy_0: 0.14774
	accuracy_policy_0: 0.7091
	loss_value_0: 0.16166
	loss_policy_1: 0.03987
	accuracy_policy_1: 0.62973
	loss_value_1: 0.03419
	loss_reward_1: 0.00517
	loss_policy_2: 0.04453
	accuracy_policy_2: 0.59471
	loss_value_2: 0.03593
	loss_reward_2: 0.00486
	loss_policy_3: 0.04911
	accuracy_policy_3: 0.55967
	loss_value_3: 0.03736
	loss_reward_3: 0.00525
	loss_policy_4: 0.05225
	accuracy_policy_4: 0.54246
	loss_value_4: 0.03883
	loss_reward_4: 0.00564
	loss_policy_5: 0.05537
	accuracy_policy_5: 0.51996
	loss_value_5: 0.03999
	loss_reward_5: 0.00686
	loss_policy: 0.38888
	loss_value: 0.34796
	loss_reward: 0.02778
[2024-05-08 03:42:11] nn step 11800, lr: 0.1.
	loss_policy_0: 0.11803
	accuracy_policy_0: 0.7433
	loss_value_0: 0.15373
	loss_policy_1: 0.03379
	accuracy_policy_1: 0.65477
	loss_value_1: 0.03261
	loss_reward_1: 0.00493
	loss_policy_2: 0.03883
	accuracy_policy_2: 0.61658
	loss_value_2: 0.0341
	loss_reward_2: 0.00462
	loss_policy_3: 0.04319
	accuracy_policy_3: 0.5852
	loss_value_3: 0.03548
	loss_reward_3: 0.00498
	loss_policy_4: 0.04607
	accuracy_policy_4: 0.5648
	loss_value_4: 0.03679
	loss_reward_4: 0.00541
	loss_policy_5: 0.04839
	accuracy_policy_5: 0.55285
	loss_value_5: 0.03808
	loss_reward_5: 0.0065
	loss_policy: 0.32831
	loss_value: 0.33078
	loss_reward: 0.02644
Optimization_Done 11800
[2024-05-08 03:44:46] [command] train weight_iter_11800.pkl 58 60
[2024-05-08 03:45:28] nn step 11900, lr: 0.1.
	loss_policy_0: 0.15917
	accuracy_policy_0: 0.69301
	loss_value_0: 0.17676
	loss_policy_1: 0.04278
	accuracy_policy_1: 0.61031
	loss_value_1: 0.03701
	loss_reward_1: 0.00602
	loss_policy_2: 0.04737
	accuracy_policy_2: 0.57693
	loss_value_2: 0.03859
	loss_reward_2: 0.00577
	loss_policy_3: 0.05191
	accuracy_policy_3: 0.54756
	loss_value_3: 0.04001
	loss_reward_3: 0.00633
	loss_policy_4: 0.05477
	accuracy_policy_4: 0.52492
	loss_value_4: 0.04152
	loss_reward_4: 0.00671
	loss_policy_5: 0.05774
	accuracy_policy_5: 0.50801
	loss_value_5: 0.04274
	loss_reward_5: 0.00777
	loss_policy: 0.41373
	loss_value: 0.37662
	loss_reward: 0.03261
[2024-05-08 03:46:09] nn step 12000, lr: 0.1.
	loss_policy_0: 0.14069
	accuracy_policy_0: 0.72756
	loss_value_0: 0.17765
	loss_policy_1: 0.03962
	accuracy_policy_1: 0.64002
	loss_value_1: 0.03727
	loss_reward_1: 0.00597
	loss_policy_2: 0.04466
	accuracy_policy_2: 0.60244
	loss_value_2: 0.03888
	loss_reward_2: 0.00581
	loss_policy_3: 0.04924
	accuracy_policy_3: 0.56727
	loss_value_3: 0.04036
	loss_reward_3: 0.00607
	loss_policy_4: 0.05226
	accuracy_policy_4: 0.55398
	loss_value_4: 0.04184
	loss_reward_4: 0.00659
	loss_policy_5: 0.05506
	accuracy_policy_5: 0.53891
	loss_value_5: 0.04321
	loss_reward_5: 0.00783
	loss_policy: 0.38152
	loss_value: 0.3792
	loss_reward: 0.03227
Optimization_Done 12000
[2024-05-08 03:48:26] [command] train weight_iter_12000.pkl 59 61
[2024-05-08 03:49:09] nn step 12100, lr: 0.1.
	loss_policy_0: 0.1538
	accuracy_policy_0: 0.7123
	loss_value_0: 0.17353
	loss_policy_1: 0.03952
	accuracy_policy_1: 0.64244
	loss_value_1: 0.03653
	loss_reward_1: 0.00591
	loss_policy_2: 0.04406
	accuracy_policy_2: 0.6125
	loss_value_2: 0.03811
	loss_reward_2: 0.00596
	loss_policy_3: 0.04854
	accuracy_policy_3: 0.57715
	loss_value_3: 0.03956
	loss_reward_3: 0.00638
	loss_policy_4: 0.05141
	accuracy_policy_4: 0.56162
	loss_value_4: 0.04081
	loss_reward_4: 0.00676
	loss_policy_5: 0.05446
	accuracy_policy_5: 0.54521
	loss_value_5: 0.04214
	loss_reward_5: 0.00769
	loss_policy: 0.39179
	loss_value: 0.37069
	loss_reward: 0.0327
[2024-05-08 03:49:50] nn step 12200, lr: 0.1.
	loss_policy_0: 0.13765
	accuracy_policy_0: 0.74531
	loss_value_0: 0.1755
	loss_policy_1: 0.0379
	accuracy_policy_1: 0.66012
	loss_value_1: 0.03688
	loss_reward_1: 0.00595
	loss_policy_2: 0.04262
	accuracy_policy_2: 0.62674
	loss_value_2: 0.03851
	loss_reward_2: 0.0058
	loss_policy_3: 0.04662
	accuracy_policy_3: 0.59945
	loss_value_3: 0.03996
	loss_reward_3: 0.00614
	loss_policy_4: 0.05006
	accuracy_policy_4: 0.57881
	loss_value_4: 0.04145
	loss_reward_4: 0.0067
	loss_policy_5: 0.05327
	accuracy_policy_5: 0.5559
	loss_value_5: 0.04282
	loss_reward_5: 0.00783
	loss_policy: 0.36813
	loss_value: 0.37511
	loss_reward: 0.03242
Optimization_Done 12200
[2024-05-08 03:52:23] [command] train weight_iter_12200.pkl 60 62
[2024-05-08 03:53:06] nn step 12300, lr: 0.1.
	loss_policy_0: 0.15422
	accuracy_policy_0: 0.7084
	loss_value_0: 0.1897
	loss_policy_1: 0.04173
	accuracy_policy_1: 0.62301
	loss_value_1: 0.03971
	loss_reward_1: 0.00784
	loss_policy_2: 0.04668
	accuracy_policy_2: 0.58844
	loss_value_2: 0.04145
	loss_reward_2: 0.00724
	loss_policy_3: 0.05069
	accuracy_policy_3: 0.56426
	loss_value_3: 0.04289
	loss_reward_3: 0.00789
	loss_policy_4: 0.05415
	accuracy_policy_4: 0.54352
	loss_value_4: 0.04422
	loss_reward_4: 0.00868
	loss_policy_5: 0.05707
	accuracy_policy_5: 0.52607
	loss_value_5: 0.04564
	loss_reward_5: 0.00972
	loss_policy: 0.40453
	loss_value: 0.40361
	loss_reward: 0.04137
[2024-05-08 03:53:47] nn step 12400, lr: 0.1.
	loss_policy_0: 0.13268
	accuracy_policy_0: 0.73814
	loss_value_0: 0.18087
	loss_policy_1: 0.03727
	accuracy_policy_1: 0.64781
	loss_value_1: 0.03799
	loss_reward_1: 0.00709
	loss_policy_2: 0.04219
	accuracy_policy_2: 0.61107
	loss_value_2: 0.03965
	loss_reward_2: 0.00695
	loss_policy_3: 0.04672
	accuracy_policy_3: 0.57832
	loss_value_3: 0.0411
	loss_reward_3: 0.00749
	loss_policy_4: 0.05016
	accuracy_policy_4: 0.55488
	loss_value_4: 0.04244
	loss_reward_4: 0.00827
	loss_policy_5: 0.05304
	accuracy_policy_5: 0.53809
	loss_value_5: 0.04374
	loss_reward_5: 0.00939
	loss_policy: 0.36208
	loss_value: 0.38579
	loss_reward: 0.03919
Optimization_Done 12400
[2024-05-08 03:56:07] [command] train weight_iter_12400.pkl 61 63
[2024-05-08 03:56:49] nn step 12500, lr: 0.1.
	loss_policy_0: 0.15587
	accuracy_policy_0: 0.70078
	loss_value_0: 0.19153
	loss_policy_1: 0.04222
	accuracy_policy_1: 0.61371
	loss_value_1: 0.04025
	loss_reward_1: 0.00666
	loss_policy_2: 0.04691
	accuracy_policy_2: 0.57701
	loss_value_2: 0.04188
	loss_reward_2: 0.00637
	loss_policy_3: 0.05139
	accuracy_policy_3: 0.54377
	loss_value_3: 0.04315
	loss_reward_3: 0.00696
	loss_policy_4: 0.05467
	accuracy_policy_4: 0.52189
	loss_value_4: 0.04427
	loss_reward_4: 0.00748
	loss_policy_5: 0.05795
	accuracy_policy_5: 0.5009
	loss_value_5: 0.0453
	loss_reward_5: 0.0085
	loss_policy: 0.40901
	loss_value: 0.40639
	loss_reward: 0.03597
[2024-05-08 03:57:31] nn step 12600, lr: 0.1.
	loss_policy_0: 0.12972
	accuracy_policy_0: 0.73896
	loss_value_0: 0.17975
	loss_policy_1: 0.03754
	accuracy_policy_1: 0.64301
	loss_value_1: 0.03773
	loss_reward_1: 0.00643
	loss_policy_2: 0.04271
	accuracy_policy_2: 0.59904
	loss_value_2: 0.03936
	loss_reward_2: 0.00611
	loss_policy_3: 0.04649
	accuracy_policy_3: 0.56684
	loss_value_3: 0.04067
	loss_reward_3: 0.00657
	loss_policy_4: 0.0498
	accuracy_policy_4: 0.54555
	loss_value_4: 0.04173
	loss_reward_4: 0.0071
	loss_policy_5: 0.05266
	accuracy_policy_5: 0.52123
	loss_value_5: 0.04281
	loss_reward_5: 0.00838
	loss_policy: 0.35893
	loss_value: 0.38204
	loss_reward: 0.03459
Optimization_Done 12600
[2024-05-08 03:59:51] [command] train weight_iter_12600.pkl 62 64
[2024-05-08 04:00:32] nn step 12700, lr: 0.1.
	loss_policy_0: 0.1493
	accuracy_policy_0: 0.7101
	loss_value_0: 0.18383
	loss_policy_1: 0.04061
	accuracy_policy_1: 0.62025
	loss_value_1: 0.03841
	loss_reward_1: 0.00594
	loss_policy_2: 0.04596
	accuracy_policy_2: 0.57932
	loss_value_2: 0.03998
	loss_reward_2: 0.00576
	loss_policy_3: 0.05086
	accuracy_policy_3: 0.54438
	loss_value_3: 0.04142
	loss_reward_3: 0.00627
	loss_policy_4: 0.05473
	accuracy_policy_4: 0.51412
	loss_value_4: 0.04261
	loss_reward_4: 0.00661
	loss_policy_5: 0.0583
	accuracy_policy_5: 0.48443
	loss_value_5: 0.04373
	loss_reward_5: 0.00798
	loss_policy: 0.39975
	loss_value: 0.38998
	loss_reward: 0.03257
[2024-05-08 04:01:13] nn step 12800, lr: 0.1.
	loss_policy_0: 0.12362
	accuracy_policy_0: 0.74307
	loss_value_0: 0.17838
	loss_policy_1: 0.03673
	accuracy_policy_1: 0.64734
	loss_value_1: 0.03761
	loss_reward_1: 0.00589
	loss_policy_2: 0.042
	accuracy_policy_2: 0.60439
	loss_value_2: 0.03896
	loss_reward_2: 0.00566
	loss_policy_3: 0.04624
	accuracy_policy_3: 0.57273
	loss_value_3: 0.04032
	loss_reward_3: 0.00604
	loss_policy_4: 0.05022
	accuracy_policy_4: 0.53996
	loss_value_4: 0.04158
	loss_reward_4: 0.00674
	loss_policy_5: 0.05392
	accuracy_policy_5: 0.51098
	loss_value_5: 0.04269
	loss_reward_5: 0.00791
	loss_policy: 0.35272
	loss_value: 0.37955
	loss_reward: 0.03224
Optimization_Done 12800
[2024-05-08 04:03:48] [command] train weight_iter_12800.pkl 63 65
[2024-05-08 04:04:30] nn step 12900, lr: 0.1.
	loss_policy_0: 0.16989
	accuracy_policy_0: 0.67463
	loss_value_0: 0.18437
	loss_policy_1: 0.04413
	accuracy_policy_1: 0.59492
	loss_value_1: 0.03879
	loss_reward_1: 0.00528
	loss_policy_2: 0.04788
	accuracy_policy_2: 0.56525
	loss_value_2: 0.0403
	loss_reward_2: 0.00503
	loss_policy_3: 0.05241
	accuracy_policy_3: 0.53322
	loss_value_3: 0.04187
	loss_reward_3: 0.0053
	loss_policy_4: 0.05586
	accuracy_policy_4: 0.51045
	loss_value_4: 0.0431
	loss_reward_4: 0.00587
	loss_policy_5: 0.05943
	accuracy_policy_5: 0.48271
	loss_value_5: 0.04423
	loss_reward_5: 0.00665
	loss_policy: 0.42959
	loss_value: 0.39266
	loss_reward: 0.02813
[2024-05-08 04:05:11] nn step 13000, lr: 0.1.
	loss_policy_0: 0.13663
	accuracy_policy_0: 0.71854
	loss_value_0: 0.16982
	loss_policy_1: 0.03762
	accuracy_policy_1: 0.63021
	loss_value_1: 0.03568
	loss_reward_1: 0.00493
	loss_policy_2: 0.0418
	accuracy_policy_2: 0.59717
	loss_value_2: 0.03713
	loss_reward_2: 0.00469
	loss_policy_3: 0.04607
	accuracy_policy_3: 0.56641
	loss_value_3: 0.03843
	loss_reward_3: 0.005
	loss_policy_4: 0.0493
	accuracy_policy_4: 0.53936
	loss_value_4: 0.03971
	loss_reward_4: 0.00538
	loss_policy_5: 0.05293
	accuracy_policy_5: 0.51018
	loss_value_5: 0.04086
	loss_reward_5: 0.00616
	loss_policy: 0.36434
	loss_value: 0.36163
	loss_reward: 0.02616
Optimization_Done 13000
[2024-05-08 04:07:48] [command] train weight_iter_13000.pkl 64 66
[2024-05-08 04:08:30] nn step 13100, lr: 0.1.
	loss_policy_0: 0.14412
	accuracy_policy_0: 0.70879
	loss_value_0: 0.18084
	loss_policy_1: 0.03895
	accuracy_policy_1: 0.62684
	loss_value_1: 0.03807
	loss_reward_1: 0.00641
	loss_policy_2: 0.04349
	accuracy_policy_2: 0.58928
	loss_value_2: 0.03979
	loss_reward_2: 0.00628
	loss_policy_3: 0.04746
	accuracy_policy_3: 0.56092
	loss_value_3: 0.04136
	loss_reward_3: 0.00676
	loss_policy_4: 0.05177
	accuracy_policy_4: 0.53266
	loss_value_4: 0.04262
	loss_reward_4: 0.00749
	loss_policy_5: 0.05543
	accuracy_policy_5: 0.50391
	loss_value_5: 0.04398
	loss_reward_5: 0.00864
	loss_policy: 0.38122
	loss_value: 0.38667
	loss_reward: 0.03558
[2024-05-08 04:09:11] nn step 13200, lr: 0.1.
	loss_policy_0: 0.12293
	accuracy_policy_0: 0.74162
	loss_value_0: 0.17537
	loss_policy_1: 0.03551
	accuracy_policy_1: 0.64992
	loss_value_1: 0.03709
	loss_reward_1: 0.00604
	loss_policy_2: 0.04011
	accuracy_policy_2: 0.61461
	loss_value_2: 0.03866
	loss_reward_2: 0.00599
	loss_policy_3: 0.04444
	accuracy_policy_3: 0.58818
	loss_value_3: 0.04018
	loss_reward_3: 0.00639
	loss_policy_4: 0.04875
	accuracy_policy_4: 0.55197
	loss_value_4: 0.04152
	loss_reward_4: 0.00687
	loss_policy_5: 0.05219
	accuracy_policy_5: 0.5285
	loss_value_5: 0.04284
	loss_reward_5: 0.00797
	loss_policy: 0.34393
	loss_value: 0.37566
	loss_reward: 0.03325
Optimization_Done 13200
[2024-05-08 04:11:46] [command] train weight_iter_13200.pkl 65 67
[2024-05-08 04:12:29] nn step 13300, lr: 0.1.
	loss_policy_0: 0.12973
	accuracy_policy_0: 0.72057
	loss_value_0: 0.1752
	loss_policy_1: 0.03675
	accuracy_policy_1: 0.62242
	loss_value_1: 0.0369
	loss_reward_1: 0.00588
	loss_policy_2: 0.04119
	accuracy_policy_2: 0.58586
	loss_value_2: 0.03848
	loss_reward_2: 0.00587
	loss_policy_3: 0.04545
	accuracy_policy_3: 0.55812
	loss_value_3: 0.03984
	loss_reward_3: 0.00632
	loss_policy_4: 0.04881
	accuracy_policy_4: 0.52922
	loss_value_4: 0.0411
	loss_reward_4: 0.00675
	loss_policy_5: 0.05195
	accuracy_policy_5: 0.50852
	loss_value_5: 0.04236
	loss_reward_5: 0.00786
	loss_policy: 0.35387
	loss_value: 0.37388
	loss_reward: 0.03268
[2024-05-08 04:13:10] nn step 13400, lr: 0.1.
	loss_policy_0: 0.11575
	accuracy_policy_0: 0.75225
	loss_value_0: 0.17429
	loss_policy_1: 0.03468
	accuracy_policy_1: 0.64494
	loss_value_1: 0.03676
	loss_reward_1: 0.00593
	loss_policy_2: 0.03943
	accuracy_policy_2: 0.61098
	loss_value_2: 0.03838
	loss_reward_2: 0.00578
	loss_policy_3: 0.04324
	accuracy_policy_3: 0.58332
	loss_value_3: 0.03971
	loss_reward_3: 0.00624
	loss_policy_4: 0.04732
	accuracy_policy_4: 0.55273
	loss_value_4: 0.04096
	loss_reward_4: 0.00655
	loss_policy_5: 0.05056
	accuracy_policy_5: 0.52736
	loss_value_5: 0.04234
	loss_reward_5: 0.0079
	loss_policy: 0.33097
	loss_value: 0.37243
	loss_reward: 0.0324
Optimization_Done 13400
[2024-05-08 04:15:49] [command] train weight_iter_13400.pkl 66 68
[2024-05-08 04:16:32] nn step 13500, lr: 0.1.
	loss_policy_0: 0.13702
	accuracy_policy_0: 0.70303
	loss_value_0: 0.16874
	loss_policy_1: 0.03718
	accuracy_policy_1: 0.6123
	loss_value_1: 0.03544
	loss_reward_1: 0.00539
	loss_policy_2: 0.04095
	accuracy_policy_2: 0.58486
	loss_value_2: 0.03696
	loss_reward_2: 0.0051
	loss_policy_3: 0.0443
	accuracy_policy_3: 0.55686
	loss_value_3: 0.0383
	loss_reward_3: 0.0058
	loss_policy_4: 0.04778
	accuracy_policy_4: 0.52982
	loss_value_4: 0.03954
	loss_reward_4: 0.00625
	loss_policy_5: 0.05005
	accuracy_policy_5: 0.50688
	loss_value_5: 0.04079
	loss_reward_5: 0.00708
	loss_policy: 0.35727
	loss_value: 0.35977
	loss_reward: 0.02961
[2024-05-08 04:17:14] nn step 13600, lr: 0.1.
	loss_policy_0: 0.11043
	accuracy_policy_0: 0.74662
	loss_value_0: 0.1651
	loss_policy_1: 0.03317
	accuracy_policy_1: 0.6441
	loss_value_1: 0.03488
	loss_reward_1: 0.00516
	loss_policy_2: 0.03683
	accuracy_policy_2: 0.61414
	loss_value_2: 0.03645
	loss_reward_2: 0.00497
	loss_policy_3: 0.04056
	accuracy_policy_3: 0.59055
	loss_value_3: 0.03775
	loss_reward_3: 0.00546
	loss_policy_4: 0.04404
	accuracy_policy_4: 0.56063
	loss_value_4: 0.039
	loss_reward_4: 0.00601
	loss_policy_5: 0.0465
	accuracy_policy_5: 0.53639
	loss_value_5: 0.04025
	loss_reward_5: 0.00707
	loss_policy: 0.31152
	loss_value: 0.35343
	loss_reward: 0.02867
Optimization_Done 13600
[2024-05-08 04:19:49] [command] train weight_iter_13600.pkl 67 69
[2024-05-08 04:20:32] nn step 13700, lr: 0.1.
	loss_policy_0: 0.15808
	accuracy_policy_0: 0.65943
	loss_value_0: 0.16566
	loss_policy_1: 0.03882
	accuracy_policy_1: 0.591
	loss_value_1: 0.03479
	loss_reward_1: 0.00477
	loss_policy_2: 0.04194
	accuracy_policy_2: 0.56258
	loss_value_2: 0.0362
	loss_reward_2: 0.00468
	loss_policy_3: 0.04506
	accuracy_policy_3: 0.53711
	loss_value_3: 0.03754
	loss_reward_3: 0.00504
	loss_policy_4: 0.04765
	accuracy_policy_4: 0.51607
	loss_value_4: 0.03868
	loss_reward_4: 0.00524
	loss_policy_5: 0.05011
	accuracy_policy_5: 0.49068
	loss_value_5: 0.03988
	loss_reward_5: 0.00564
	loss_policy: 0.38165
	loss_value: 0.35275
	loss_reward: 0.02537
[2024-05-08 04:21:13] nn step 13800, lr: 0.1.
	loss_policy_0: 0.13133
	accuracy_policy_0: 0.71008
	loss_value_0: 0.16132
	loss_policy_1: 0.03509
	accuracy_policy_1: 0.62535
	loss_value_1: 0.03384
	loss_reward_1: 0.00462
	loss_policy_2: 0.03852
	accuracy_policy_2: 0.59678
	loss_value_2: 0.03534
	loss_reward_2: 0.00464
	loss_policy_3: 0.04151
	accuracy_policy_3: 0.56891
	loss_value_3: 0.0366
	loss_reward_3: 0.00495
	loss_policy_4: 0.04468
	accuracy_policy_4: 0.54357
	loss_value_4: 0.03777
	loss_reward_4: 0.00517
	loss_policy_5: 0.04692
	accuracy_policy_5: 0.52072
	loss_value_5: 0.03886
	loss_reward_5: 0.00574
	loss_policy: 0.33806
	loss_value: 0.34373
	loss_reward: 0.02512
Optimization_Done 13800
[2024-05-08 04:23:31] [command] train weight_iter_13800.pkl 68 70
[2024-05-08 04:24:14] nn step 13900, lr: 0.1.
	loss_policy_0: 0.15909
	accuracy_policy_0: 0.65447
	loss_value_0: 0.16567
	loss_policy_1: 0.03942
	accuracy_policy_1: 0.57949
	loss_value_1: 0.03451
	loss_reward_1: 0.00557
	loss_policy_2: 0.04287
	accuracy_policy_2: 0.54984
	loss_value_2: 0.03606
	loss_reward_2: 0.00536
	loss_policy_3: 0.04634
	accuracy_policy_3: 0.52291
	loss_value_3: 0.03736
	loss_reward_3: 0.0059
	loss_policy_4: 0.04903
	accuracy_policy_4: 0.50545
	loss_value_4: 0.03863
	loss_reward_4: 0.00619
	loss_policy_5: 0.05153
	accuracy_policy_5: 0.48104
	loss_value_5: 0.0398
	loss_reward_5: 0.00706
	loss_policy: 0.38829
	loss_value: 0.35204
	loss_reward: 0.03008
[2024-05-08 04:24:55] nn step 14000, lr: 0.1.
	loss_policy_0: 0.15425
	accuracy_policy_0: 0.69279
	loss_value_0: 0.17833
	loss_policy_1: 0.03999
	accuracy_policy_1: 0.61133
	loss_value_1: 0.03727
	loss_reward_1: 0.00587
	loss_policy_2: 0.04405
	accuracy_policy_2: 0.57996
	loss_value_2: 0.03878
	loss_reward_2: 0.00566
	loss_policy_3: 0.04767
	accuracy_policy_3: 0.55004
	loss_value_3: 0.04024
	loss_reward_3: 0.0062
	loss_policy_4: 0.05124
	accuracy_policy_4: 0.52412
	loss_value_4: 0.0415
	loss_reward_4: 0.00644
	loss_policy_5: 0.05396
	accuracy_policy_5: 0.50043
	loss_value_5: 0.04279
	loss_reward_5: 0.00749
	loss_policy: 0.39115
	loss_value: 0.37892
	loss_reward: 0.03167
Optimization_Done 14000
[2024-05-08 04:27:24] [command] train weight_iter_14000.pkl 69 71
[2024-05-08 04:28:06] nn step 14100, lr: 0.1.
	loss_policy_0: 0.15305
	accuracy_policy_0: 0.69766
	loss_value_0: 0.18415
	loss_policy_1: 0.04118
	accuracy_policy_1: 0.60457
	loss_value_1: 0.03874
	loss_reward_1: 0.00729
	loss_policy_2: 0.04581
	accuracy_policy_2: 0.56795
	loss_value_2: 0.04028
	loss_reward_2: 0.0071
	loss_policy_3: 0.04991
	accuracy_policy_3: 0.54219
	loss_value_3: 0.04169
	loss_reward_3: 0.00778
	loss_policy_4: 0.05287
	accuracy_policy_4: 0.51877
	loss_value_4: 0.04298
	loss_reward_4: 0.00828
	loss_policy_5: 0.05616
	accuracy_policy_5: 0.49832
	loss_value_5: 0.04409
	loss_reward_5: 0.00959
	loss_policy: 0.39897
	loss_value: 0.39193
	loss_reward: 0.04004
[2024-05-08 04:28:48] nn step 14200, lr: 0.1.
	loss_policy_0: 0.14233
	accuracy_policy_0: 0.71982
	loss_value_0: 0.18355
	loss_policy_1: 0.03992
	accuracy_policy_1: 0.62328
	loss_value_1: 0.03848
	loss_reward_1: 0.00729
	loss_policy_2: 0.04466
	accuracy_policy_2: 0.58381
	loss_value_2: 0.0399
	loss_reward_2: 0.00708
	loss_policy_3: 0.04866
	accuracy_policy_3: 0.55494
	loss_value_3: 0.04118
	loss_reward_3: 0.00764
	loss_policy_4: 0.05166
	accuracy_policy_4: 0.53566
	loss_value_4: 0.04251
	loss_reward_4: 0.00843
	loss_policy_5: 0.05462
	accuracy_policy_5: 0.51344
	loss_value_5: 0.04385
	loss_reward_5: 0.00941
	loss_policy: 0.38185
	loss_value: 0.38946
	loss_reward: 0.03984
Optimization_Done 14200
[2024-05-08 04:31:10] [command] train weight_iter_14200.pkl 70 72
[2024-05-08 04:31:53] nn step 14300, lr: 0.1.
	loss_policy_0: 0.14633
	accuracy_policy_0: 0.70666
	loss_value_0: 0.17989
	loss_policy_1: 0.03966
	accuracy_policy_1: 0.62006
	loss_value_1: 0.03765
	loss_reward_1: 0.007
	loss_policy_2: 0.04371
	accuracy_policy_2: 0.58461
	loss_value_2: 0.03909
	loss_reward_2: 0.00679
	loss_policy_3: 0.04775
	accuracy_policy_3: 0.55805
	loss_value_3: 0.04041
	loss_reward_3: 0.00755
	loss_policy_4: 0.05055
	accuracy_policy_4: 0.53486
	loss_value_4: 0.04152
	loss_reward_4: 0.00811
	loss_policy_5: 0.05284
	accuracy_policy_5: 0.51799
	loss_value_5: 0.04271
	loss_reward_5: 0.00931
	loss_policy: 0.38084
	loss_value: 0.38127
	loss_reward: 0.03876
[2024-05-08 04:32:35] nn step 14400, lr: 0.1.
	loss_policy_0: 0.12705
	accuracy_policy_0: 0.746
	loss_value_0: 0.176
	loss_policy_1: 0.03723
	accuracy_policy_1: 0.63867
	loss_value_1: 0.03705
	loss_reward_1: 0.00717
	loss_policy_2: 0.04205
	accuracy_policy_2: 0.59584
	loss_value_2: 0.03848
	loss_reward_2: 0.00683
	loss_policy_3: 0.0462
	accuracy_policy_3: 0.56621
	loss_value_3: 0.03963
	loss_reward_3: 0.00747
	loss_policy_4: 0.04895
	accuracy_policy_4: 0.55021
	loss_value_4: 0.04078
	loss_reward_4: 0.00825
	loss_policy_5: 0.0517
	accuracy_policy_5: 0.52926
	loss_value_5: 0.0419
	loss_reward_5: 0.00935
	loss_policy: 0.35318
	loss_value: 0.37384
	loss_reward: 0.03908
Optimization_Done 14400
[2024-05-08 04:35:10] [command] train weight_iter_14400.pkl 71 73
[2024-05-08 04:35:53] nn step 14500, lr: 0.1.
	loss_policy_0: 0.15783
	accuracy_policy_0: 0.70863
	loss_value_0: 0.1753
	loss_policy_1: 0.04189
	accuracy_policy_1: 0.62748
	loss_value_1: 0.03682
	loss_reward_1: 0.00671
	loss_policy_2: 0.04618
	accuracy_policy_2: 0.59252
	loss_value_2: 0.0384
	loss_reward_2: 0.0062
	loss_policy_3: 0.05009
	accuracy_policy_3: 0.5608
	loss_value_3: 0.03978
	loss_reward_3: 0.00687
	loss_policy_4: 0.05348
	accuracy_policy_4: 0.54008
	loss_value_4: 0.04116
	loss_reward_4: 0.00737
	loss_policy_5: 0.05611
	accuracy_policy_5: 0.52385
	loss_value_5: 0.04243
	loss_reward_5: 0.00851
	loss_policy: 0.40557
	loss_value: 0.37389
	loss_reward: 0.03566
[2024-05-08 04:36:34] nn step 14600, lr: 0.1.
	loss_policy_0: 0.12786
	accuracy_policy_0: 0.75111
	loss_value_0: 0.16485
	loss_policy_1: 0.03625
	accuracy_policy_1: 0.65654
	loss_value_1: 0.03475
	loss_reward_1: 0.00629
	loss_policy_2: 0.04104
	accuracy_policy_2: 0.61467
	loss_value_2: 0.03627
	loss_reward_2: 0.00618
	loss_policy_3: 0.04503
	accuracy_policy_3: 0.59098
	loss_value_3: 0.03748
	loss_reward_3: 0.0066
	loss_policy_4: 0.04861
	accuracy_policy_4: 0.56152
	loss_value_4: 0.03866
	loss_reward_4: 0.00726
	loss_policy_5: 0.05093
	accuracy_policy_5: 0.54514
	loss_value_5: 0.03975
	loss_reward_5: 0.00826
	loss_policy: 0.34972
	loss_value: 0.35177
	loss_reward: 0.03459
Optimization_Done 14600
[2024-05-08 04:38:45] [command] train weight_iter_14600.pkl 72 74
[2024-05-08 04:39:27] nn step 14700, lr: 0.1.
	loss_policy_0: 0.16651
	accuracy_policy_0: 0.68436
	loss_value_0: 0.17674
	loss_policy_1: 0.04197
	accuracy_policy_1: 0.61707
	loss_value_1: 0.03697
	loss_reward_1: 0.0056
	loss_policy_2: 0.0456
	accuracy_policy_2: 0.58809
	loss_value_2: 0.03849
	loss_reward_2: 0.00556
	loss_policy_3: 0.04959
	accuracy_policy_3: 0.55846
	loss_value_3: 0.04004
	loss_reward_3: 0.00591
	loss_policy_4: 0.05306
	accuracy_policy_4: 0.53016
	loss_value_4: 0.04138
	loss_reward_4: 0.00651
	loss_policy_5: 0.05588
	accuracy_policy_5: 0.51205
	loss_value_5: 0.04258
	loss_reward_5: 0.00742
	loss_policy: 0.41261
	loss_value: 0.37621
	loss_reward: 0.03099
[2024-05-08 04:40:08] nn step 14800, lr: 0.1.
	loss_policy_0: 0.14002
	accuracy_policy_0: 0.72559
	loss_value_0: 0.16485
	loss_policy_1: 0.03687
	accuracy_policy_1: 0.65004
	loss_value_1: 0.03476
	loss_reward_1: 0.00536
	loss_policy_2: 0.04086
	accuracy_policy_2: 0.6185
	loss_value_2: 0.03623
	loss_reward_2: 0.0053
	loss_policy_3: 0.04425
	accuracy_policy_3: 0.59061
	loss_value_3: 0.03756
	loss_reward_3: 0.00562
	loss_policy_4: 0.04777
	accuracy_policy_4: 0.56451
	loss_value_4: 0.03876
	loss_reward_4: 0.00617
	loss_policy_5: 0.05106
	accuracy_policy_5: 0.54074
	loss_value_5: 0.04008
	loss_reward_5: 0.007
	loss_policy: 0.36083
	loss_value: 0.35224
	loss_reward: 0.02946
Optimization_Done 14800
[2024-05-08 04:42:39] [command] train weight_iter_14800.pkl 73 75
[2024-05-08 04:43:21] nn step 14900, lr: 0.1.
	loss_policy_0: 0.14954
	accuracy_policy_0: 0.70572
	loss_value_0: 0.16884
	loss_policy_1: 0.03874
	accuracy_policy_1: 0.63197
	loss_value_1: 0.03559
	loss_reward_1: 0.00607
	loss_policy_2: 0.04294
	accuracy_policy_2: 0.60041
	loss_value_2: 0.03725
	loss_reward_2: 0.00589
	loss_policy_3: 0.04657
	accuracy_policy_3: 0.5716
	loss_value_3: 0.03871
	loss_reward_3: 0.00652
	loss_policy_4: 0.05014
	accuracy_policy_4: 0.54674
	loss_value_4: 0.04008
	loss_reward_4: 0.00699
	loss_policy_5: 0.05335
	accuracy_policy_5: 0.52246
	loss_value_5: 0.04143
	loss_reward_5: 0.00802
	loss_policy: 0.38128
	loss_value: 0.36191
	loss_reward: 0.03348
[2024-05-08 04:44:03] nn step 15000, lr: 0.1.
	loss_policy_0: 0.14068
	accuracy_policy_0: 0.73494
	loss_value_0: 0.17382
	loss_policy_1: 0.03779
	accuracy_policy_1: 0.65467
	loss_value_1: 0.03638
	loss_reward_1: 0.00643
	loss_policy_2: 0.04224
	accuracy_policy_2: 0.62139
	loss_value_2: 0.0379
	loss_reward_2: 0.0061
	loss_policy_3: 0.04631
	accuracy_policy_3: 0.5923
	loss_value_3: 0.03935
	loss_reward_3: 0.0066
	loss_policy_4: 0.04952
	accuracy_policy_4: 0.57125
	loss_value_4: 0.04101
	loss_reward_4: 0.00708
	loss_policy_5: 0.05358
	accuracy_policy_5: 0.54521
	loss_value_5: 0.04222
	loss_reward_5: 0.00824
	loss_policy: 0.37012
	loss_value: 0.37067
	loss_reward: 0.03446
Optimization_Done 15000
[2024-05-08 04:46:21] [command] train weight_iter_15000.pkl 74 76
[2024-05-08 04:47:03] nn step 15100, lr: 0.1.
	loss_policy_0: 0.12642
	accuracy_policy_0: 0.725
	loss_value_0: 0.15974
	loss_policy_1: 0.0347
	accuracy_policy_1: 0.64213
	loss_value_1: 0.03371
	loss_reward_1: 0.00593
	loss_policy_2: 0.03808
	accuracy_policy_2: 0.61498
	loss_value_2: 0.03527
	loss_reward_2: 0.0056
	loss_policy_3: 0.04153
	accuracy_policy_3: 0.58297
	loss_value_3: 0.03672
	loss_reward_3: 0.00607
	loss_policy_4: 0.0447
	accuracy_policy_4: 0.55934
	loss_value_4: 0.03792
	loss_reward_4: 0.00663
	loss_policy_5: 0.04758
	accuracy_policy_5: 0.53512
	loss_value_5: 0.03918
	loss_reward_5: 0.00758
	loss_policy: 0.33301
	loss_value: 0.34254
	loss_reward: 0.0318
[2024-05-08 04:47:44] nn step 15200, lr: 0.1.
	loss_policy_0: 0.11341
	accuracy_policy_0: 0.75865
	loss_value_0: 0.15736
	loss_policy_1: 0.03302
	accuracy_policy_1: 0.6617
	loss_value_1: 0.03324
	loss_reward_1: 0.00596
	loss_policy_2: 0.03686
	accuracy_policy_2: 0.63123
	loss_value_2: 0.03472
	loss_reward_2: 0.00562
	loss_policy_3: 0.04051
	accuracy_policy_3: 0.59965
	loss_value_3: 0.03606
	loss_reward_3: 0.00607
	loss_policy_4: 0.04373
	accuracy_policy_4: 0.57367
	loss_value_4: 0.0373
	loss_reward_4: 0.00644
	loss_policy_5: 0.04632
	accuracy_policy_5: 0.55361
	loss_value_5: 0.03862
	loss_reward_5: 0.00769
	loss_policy: 0.31385
	loss_value: 0.33729
	loss_reward: 0.03178
Optimization_Done 15200
[2024-05-08 04:50:17] [command] train weight_iter_15200.pkl 75 77
[2024-05-08 04:51:00] nn step 15300, lr: 0.1.
	loss_policy_0: 0.13714
	accuracy_policy_0: 0.71436
	loss_value_0: 0.15182
	loss_policy_1: 0.03645
	accuracy_policy_1: 0.63648
	loss_value_1: 0.03209
	loss_reward_1: 0.00548
	loss_policy_2: 0.04039
	accuracy_policy_2: 0.6048
	loss_value_2: 0.03361
	loss_reward_2: 0.00514
	loss_policy_3: 0.04366
	accuracy_policy_3: 0.57619
	loss_value_3: 0.03495
	loss_reward_3: 0.00558
	loss_policy_4: 0.0464
	accuracy_policy_4: 0.55357
	loss_value_4: 0.03627
	loss_reward_4: 0.00581
	loss_policy_5: 0.0489
	accuracy_policy_5: 0.53488
	loss_value_5: 0.03745
	loss_reward_5: 0.00673
	loss_policy: 0.35296
	loss_value: 0.32619
	loss_reward: 0.02874
[2024-05-08 04:51:41] nn step 15400, lr: 0.1.
	loss_policy_0: 0.11259
	accuracy_policy_0: 0.75787
	loss_value_0: 0.14735
	loss_policy_1: 0.03232
	accuracy_policy_1: 0.66613
	loss_value_1: 0.03135
	loss_reward_1: 0.00537
	loss_policy_2: 0.03613
	accuracy_policy_2: 0.64162
	loss_value_2: 0.0329
	loss_reward_2: 0.00496
	loss_policy_3: 0.03952
	accuracy_policy_3: 0.6118
	loss_value_3: 0.0341
	loss_reward_3: 0.00548
	loss_policy_4: 0.04246
	accuracy_policy_4: 0.58756
	loss_value_4: 0.03532
	loss_reward_4: 0.00593
	loss_policy_5: 0.04525
	accuracy_policy_5: 0.56836
	loss_value_5: 0.03645
	loss_reward_5: 0.00691
	loss_policy: 0.30828
	loss_value: 0.31747
	loss_reward: 0.02864
Optimization_Done 15400
[2024-05-08 04:54:13] [command] train weight_iter_15400.pkl 76 78
[2024-05-08 04:54:56] nn step 15500, lr: 0.1.
	loss_policy_0: 0.13516
	accuracy_policy_0: 0.71049
	loss_value_0: 0.15458
	loss_policy_1: 0.03515
	accuracy_policy_1: 0.63881
	loss_value_1: 0.0325
	loss_reward_1: 0.00552
	loss_policy_2: 0.03847
	accuracy_policy_2: 0.60947
	loss_value_2: 0.03399
	loss_reward_2: 0.00535
	loss_policy_3: 0.04183
	accuracy_policy_3: 0.58062
	loss_value_3: 0.03526
	loss_reward_3: 0.00574
	loss_policy_4: 0.04459
	accuracy_policy_4: 0.55855
	loss_value_4: 0.03654
	loss_reward_4: 0.00624
	loss_policy_5: 0.04696
	accuracy_policy_5: 0.53834
	loss_value_5: 0.03777
	loss_reward_5: 0.00684
	loss_policy: 0.34215
	loss_value: 0.33064
	loss_reward: 0.02969
[2024-05-08 04:55:38] nn step 15600, lr: 0.1.
	loss_policy_0: 0.12254
	accuracy_policy_0: 0.7476
	loss_value_0: 0.15843
	loss_policy_1: 0.03355
	accuracy_policy_1: 0.66656
	loss_value_1: 0.03363
	loss_reward_1: 0.00566
	loss_policy_2: 0.03745
	accuracy_policy_2: 0.63367
	loss_value_2: 0.03521
	loss_reward_2: 0.00551
	loss_policy_3: 0.04116
	accuracy_policy_3: 0.60303
	loss_value_3: 0.03651
	loss_reward_3: 0.0059
	loss_policy_4: 0.04414
	accuracy_policy_4: 0.58264
	loss_value_4: 0.0378
	loss_reward_4: 0.00617
	loss_policy_5: 0.04697
	accuracy_policy_5: 0.56203
	loss_value_5: 0.03904
	loss_reward_5: 0.00709
	loss_policy: 0.32581
	loss_value: 0.34061
	loss_reward: 0.03032
Optimization_Done 15600
[2024-05-08 04:58:10] [command] train weight_iter_15600.pkl 77 79
[2024-05-08 04:58:52] nn step 15700, lr: 0.1.
	loss_policy_0: 0.13731
	accuracy_policy_0: 0.73717
	loss_value_0: 0.17656
	loss_policy_1: 0.03681
	accuracy_policy_1: 0.66209
	loss_value_1: 0.03738
	loss_reward_1: 0.00728
	loss_policy_2: 0.04103
	accuracy_policy_2: 0.63008
	loss_value_2: 0.03908
	loss_reward_2: 0.00706
	loss_policy_3: 0.04531
	accuracy_policy_3: 0.60406
	loss_value_3: 0.04054
	loss_reward_3: 0.00788
	loss_policy_4: 0.04859
	accuracy_policy_4: 0.5826
	loss_value_4: 0.04198
	loss_reward_4: 0.00841
	loss_policy_5: 0.05129
	accuracy_policy_5: 0.56648
	loss_value_5: 0.04336
	loss_reward_5: 0.00982
	loss_policy: 0.36035
	loss_value: 0.37889
	loss_reward: 0.04045
[2024-05-08 04:59:33] nn step 15800, lr: 0.1.
	loss_policy_0: 0.12482
	accuracy_policy_0: 0.75953
	loss_value_0: 0.17439
	loss_policy_1: 0.03466
	accuracy_policy_1: 0.67984
	loss_value_1: 0.03695
	loss_reward_1: 0.00704
	loss_policy_2: 0.03935
	accuracy_policy_2: 0.64434
	loss_value_2: 0.03848
	loss_reward_2: 0.00703
	loss_policy_3: 0.04321
	accuracy_policy_3: 0.62068
	loss_value_3: 0.04009
	loss_reward_3: 0.00752
	loss_policy_4: 0.04681
	accuracy_policy_4: 0.59854
	loss_value_4: 0.04136
	loss_reward_4: 0.0082
	loss_policy_5: 0.04977
	accuracy_policy_5: 0.58018
	loss_value_5: 0.04278
	loss_reward_5: 0.00929
	loss_policy: 0.33861
	loss_value: 0.37405
	loss_reward: 0.03909
Optimization_Done 15800
[2024-05-08 05:02:03] [command] train weight_iter_15800.pkl 78 80
[2024-05-08 05:02:46] nn step 15900, lr: 0.1.
	loss_policy_0: 0.11485
	accuracy_policy_0: 0.74877
	loss_value_0: 0.16143
	loss_policy_1: 0.03342
	accuracy_policy_1: 0.65324
	loss_value_1: 0.03422
	loss_reward_1: 0.0074
	loss_policy_2: 0.03827
	accuracy_policy_2: 0.61373
	loss_value_2: 0.03568
	loss_reward_2: 0.00704
	loss_policy_3: 0.04211
	accuracy_policy_3: 0.58803
	loss_value_3: 0.037
	loss_reward_3: 0.00779
	loss_policy_4: 0.04514
	accuracy_policy_4: 0.56496
	loss_value_4: 0.03844
	loss_reward_4: 0.00818
	loss_policy_5: 0.04731
	accuracy_policy_5: 0.55064
	loss_value_5: 0.03969
	loss_reward_5: 0.00954
	loss_policy: 0.3211
	loss_value: 0.34646
	loss_reward: 0.03995
[2024-05-08 05:03:28] nn step 16000, lr: 0.1.
	loss_policy_0: 0.10967
	accuracy_policy_0: 0.77324
	loss_value_0: 0.1669
	loss_policy_1: 0.03314
	accuracy_policy_1: 0.67379
	loss_value_1: 0.0353
	loss_reward_1: 0.00756
	loss_policy_2: 0.03854
	accuracy_policy_2: 0.62814
	loss_value_2: 0.03677
	loss_reward_2: 0.00724
	loss_policy_3: 0.04252
	accuracy_policy_3: 0.60123
	loss_value_3: 0.03812
	loss_reward_3: 0.00802
	loss_policy_4: 0.04542
	accuracy_policy_4: 0.5823
	loss_value_4: 0.03927
	loss_reward_4: 0.00885
	loss_policy_5: 0.04792
	accuracy_policy_5: 0.56594
	loss_value_5: 0.04067
	loss_reward_5: 0.00978
	loss_policy: 0.3172
	loss_value: 0.35704
	loss_reward: 0.04145
Optimization_Done 16000
[2024-05-08 05:05:35] [command] train weight_iter_16000.pkl 79 81
[2024-05-08 05:06:17] nn step 16100, lr: 0.1.
	loss_policy_0: 0.12634
	accuracy_policy_0: 0.73584
	loss_value_0: 0.16004
	loss_policy_1: 0.03551
	accuracy_policy_1: 0.64121
	loss_value_1: 0.03376
	loss_reward_1: 0.00702
	loss_policy_2: 0.04009
	accuracy_policy_2: 0.60572
	loss_value_2: 0.03511
	loss_reward_2: 0.00664
	loss_policy_3: 0.04384
	accuracy_policy_3: 0.57563
	loss_value_3: 0.03631
	loss_reward_3: 0.00737
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.5565
	loss_value_4: 0.03753
	loss_reward_4: 0.00805
	loss_policy_5: 0.04938
	accuracy_policy_5: 0.535
	loss_value_5: 0.03883
	loss_reward_5: 0.00906
	loss_policy: 0.34185
	loss_value: 0.34159
	loss_reward: 0.03813
[2024-05-08 05:06:58] nn step 16200, lr: 0.1.
	loss_policy_0: 0.10341
	accuracy_policy_0: 0.77262
	loss_value_0: 0.14993
	loss_policy_1: 0.03169
	accuracy_policy_1: 0.66877
	loss_value_1: 0.03172
	loss_reward_1: 0.00686
	loss_policy_2: 0.03626
	accuracy_policy_2: 0.63213
	loss_value_2: 0.03316
	loss_reward_2: 0.00651
	loss_policy_3: 0.04037
	accuracy_policy_3: 0.60145
	loss_value_3: 0.0344
	loss_reward_3: 0.0071
	loss_policy_4: 0.04334
	accuracy_policy_4: 0.57447
	loss_value_4: 0.03562
	loss_reward_4: 0.00755
	loss_policy_5: 0.04569
	accuracy_policy_5: 0.55699
	loss_value_5: 0.03668
	loss_reward_5: 0.00866
	loss_policy: 0.30076
	loss_value: 0.32153
	loss_reward: 0.03668
Optimization_Done 16200
[2024-05-08 05:09:31] [command] train weight_iter_16200.pkl 80 82
[2024-05-08 05:10:13] nn step 16300, lr: 0.1.
	loss_policy_0: 0.1667
	accuracy_policy_0: 0.6876
	loss_value_0: 0.16584
	loss_policy_1: 0.04286
	accuracy_policy_1: 0.60848
	loss_value_1: 0.03487
	loss_reward_1: 0.006
	loss_policy_2: 0.04715
	accuracy_policy_2: 0.57385
	loss_value_2: 0.0363
	loss_reward_2: 0.00565
	loss_policy_3: 0.0507
	accuracy_policy_3: 0.54814
	loss_value_3: 0.03765
	loss_reward_3: 0.00607
	loss_policy_4: 0.05365
	accuracy_policy_4: 0.52904
	loss_value_4: 0.03903
	loss_reward_4: 0.00676
	loss_policy_5: 0.05634
	accuracy_policy_5: 0.50926
	loss_value_5: 0.04025
	loss_reward_5: 0.00741
	loss_policy: 0.4174
	loss_value: 0.35394
	loss_reward: 0.03189
[2024-05-08 05:10:54] nn step 16400, lr: 0.1.
	loss_policy_0: 0.1396
	accuracy_policy_0: 0.73225
	loss_value_0: 0.16042
	loss_policy_1: 0.03854
	accuracy_policy_1: 0.64234
	loss_value_1: 0.03381
	loss_reward_1: 0.00599
	loss_policy_2: 0.04319
	accuracy_policy_2: 0.60816
	loss_value_2: 0.0352
	loss_reward_2: 0.0057
	loss_policy_3: 0.04728
	accuracy_policy_3: 0.57854
	loss_value_3: 0.03649
	loss_reward_3: 0.00612
	loss_policy_4: 0.05028
	accuracy_policy_4: 0.55699
	loss_value_4: 0.03778
	loss_reward_4: 0.00666
	loss_policy_5: 0.05299
	accuracy_policy_5: 0.53393
	loss_value_5: 0.03914
	loss_reward_5: 0.00751
	loss_policy: 0.37188
	loss_value: 0.34284
	loss_reward: 0.03198
Optimization_Done 16400
[2024-05-08 05:13:27] [command] train weight_iter_16400.pkl 81 83
[2024-05-08 05:14:09] nn step 16500, lr: 0.1.
	loss_policy_0: 0.17053
	accuracy_policy_0: 0.68344
	loss_value_0: 0.16992
	loss_policy_1: 0.04195
	accuracy_policy_1: 0.61814
	loss_value_1: 0.03575
	loss_reward_1: 0.00551
	loss_policy_2: 0.04555
	accuracy_policy_2: 0.59266
	loss_value_2: 0.03725
	loss_reward_2: 0.0054
	loss_policy_3: 0.04961
	accuracy_policy_3: 0.56006
	loss_value_3: 0.03862
	loss_reward_3: 0.00584
	loss_policy_4: 0.05262
	accuracy_policy_4: 0.53871
	loss_value_4: 0.03992
	loss_reward_4: 0.00622
	loss_policy_5: 0.0559
	accuracy_policy_5: 0.51373
	loss_value_5: 0.04117
	loss_reward_5: 0.00718
	loss_policy: 0.41616
	loss_value: 0.36264
	loss_reward: 0.03015
[2024-05-08 05:14:49] nn step 16600, lr: 0.1.
	loss_policy_0: 0.15599
	accuracy_policy_0: 0.719
	loss_value_0: 0.17531
	loss_policy_1: 0.04099
	accuracy_policy_1: 0.63836
	loss_value_1: 0.03679
	loss_reward_1: 0.00582
	loss_policy_2: 0.04501
	accuracy_policy_2: 0.60926
	loss_value_2: 0.03817
	loss_reward_2: 0.00547
	loss_policy_3: 0.04838
	accuracy_policy_3: 0.58195
	loss_value_3: 0.03967
	loss_reward_3: 0.00592
	loss_policy_4: 0.05207
	accuracy_policy_4: 0.56037
	loss_value_4: 0.04109
	loss_reward_4: 0.00624
	loss_policy_5: 0.05497
	accuracy_policy_5: 0.53836
	loss_value_5: 0.04245
	loss_reward_5: 0.0073
	loss_policy: 0.3974
	loss_value: 0.37348
	loss_reward: 0.03076
Optimization_Done 16600
[2024-05-08 05:17:20] [command] train weight_iter_16600.pkl 82 84
[2024-05-08 05:18:03] nn step 16700, lr: 0.1.
	loss_policy_0: 0.1395
	accuracy_policy_0: 0.71686
	loss_value_0: 0.16656
	loss_policy_1: 0.03765
	accuracy_policy_1: 0.62875
	loss_value_1: 0.03515
	loss_reward_1: 0.00622
	loss_policy_2: 0.04185
	accuracy_policy_2: 0.59811
	loss_value_2: 0.03662
	loss_reward_2: 0.00581
	loss_policy_3: 0.04544
	accuracy_policy_3: 0.56736
	loss_value_3: 0.03813
	loss_reward_3: 0.00628
	loss_policy_4: 0.04849
	accuracy_policy_4: 0.54514
	loss_value_4: 0.0393
	loss_reward_4: 0.00665
	loss_policy_5: 0.05164
	accuracy_policy_5: 0.5241
	loss_value_5: 0.04062
	loss_reward_5: 0.00763
	loss_policy: 0.36458
	loss_value: 0.35639
	loss_reward: 0.03259
[2024-05-08 05:18:44] nn step 16800, lr: 0.1.
	loss_policy_0: 0.12645
	accuracy_policy_0: 0.74424
	loss_value_0: 0.16364
	loss_policy_1: 0.03515
	accuracy_policy_1: 0.65037
	loss_value_1: 0.03458
	loss_reward_1: 0.00604
	loss_policy_2: 0.0393
	accuracy_policy_2: 0.61631
	loss_value_2: 0.03615
	loss_reward_2: 0.00573
	loss_policy_3: 0.04313
	accuracy_policy_3: 0.59145
	loss_value_3: 0.03741
	loss_reward_3: 0.00598
	loss_policy_4: 0.04654
	accuracy_policy_4: 0.56312
	loss_value_4: 0.03871
	loss_reward_4: 0.0063
	loss_policy_5: 0.04942
	accuracy_policy_5: 0.54281
	loss_value_5: 0.03987
	loss_reward_5: 0.00743
	loss_policy: 0.33999
	loss_value: 0.35036
	loss_reward: 0.03149
Optimization_Done 16800
[2024-05-08 05:21:03] [command] train weight_iter_16800.pkl 83 85
[2024-05-08 05:21:46] nn step 16900, lr: 0.1.
	loss_policy_0: 0.12197
	accuracy_policy_0: 0.72713
	loss_value_0: 0.16194
	loss_policy_1: 0.03269
	accuracy_policy_1: 0.64828
	loss_value_1: 0.03404
	loss_reward_1: 0.00579
	loss_policy_2: 0.03646
	accuracy_policy_2: 0.61281
	loss_value_2: 0.03561
	loss_reward_2: 0.00539
	loss_policy_3: 0.03983
	accuracy_policy_3: 0.5833
	loss_value_3: 0.03702
	loss_reward_3: 0.00594
	loss_policy_4: 0.04314
	accuracy_policy_4: 0.55553
	loss_value_4: 0.03828
	loss_reward_4: 0.00634
	loss_policy_5: 0.04597
	accuracy_policy_5: 0.53676
	loss_value_5: 0.03945
	loss_reward_5: 0.00738
	loss_policy: 0.32005
	loss_value: 0.34634
	loss_reward: 0.03084
[2024-05-08 05:22:26] nn step 17000, lr: 0.1.
	loss_policy_0: 0.10523
	accuracy_policy_0: 0.75996
	loss_value_0: 0.15657
	loss_policy_1: 0.03039
	accuracy_policy_1: 0.66662
	loss_value_1: 0.03311
	loss_reward_1: 0.00576
	loss_policy_2: 0.03394
	accuracy_policy_2: 0.63457
	loss_value_2: 0.0346
	loss_reward_2: 0.00535
	loss_policy_3: 0.03763
	accuracy_policy_3: 0.60141
	loss_value_3: 0.0359
	loss_reward_3: 0.00576
	loss_policy_4: 0.04079
	accuracy_policy_4: 0.57072
	loss_value_4: 0.03717
	loss_reward_4: 0.00624
	loss_policy_5: 0.04405
	accuracy_policy_5: 0.54789
	loss_value_5: 0.03835
	loss_reward_5: 0.00716
	loss_policy: 0.29203
	loss_value: 0.33569
	loss_reward: 0.03027
Optimization_Done 17000
[2024-05-08 05:25:01] [command] train weight_iter_17000.pkl 84 86
[2024-05-08 05:25:44] nn step 17100, lr: 0.1.
	loss_policy_0: 0.13559
	accuracy_policy_0: 0.69533
	loss_value_0: 0.15166
	loss_policy_1: 0.03497
	accuracy_policy_1: 0.62135
	loss_value_1: 0.0319
	loss_reward_1: 0.00619
	loss_policy_2: 0.03848
	accuracy_policy_2: 0.58904
	loss_value_2: 0.03343
	loss_reward_2: 0.00599
	loss_policy_3: 0.04188
	accuracy_policy_3: 0.5575
	loss_value_3: 0.0347
	loss_reward_3: 0.00642
	loss_policy_4: 0.04446
	accuracy_policy_4: 0.53467
	loss_value_4: 0.03599
	loss_reward_4: 0.0069
	loss_policy_5: 0.04759
	accuracy_policy_5: 0.50789
	loss_value_5: 0.03737
	loss_reward_5: 0.00779
	loss_policy: 0.34297
	loss_value: 0.32505
	loss_reward: 0.03329
[2024-05-08 05:26:25] nn step 17200, lr: 0.1.
	loss_policy_0: 0.11619
	accuracy_policy_0: 0.74301
	loss_value_0: 0.15726
	loss_policy_1: 0.03206
	accuracy_policy_1: 0.65756
	loss_value_1: 0.03324
	loss_reward_1: 0.00631
	loss_policy_2: 0.03589
	accuracy_policy_2: 0.62477
	loss_value_2: 0.03481
	loss_reward_2: 0.00583
	loss_policy_3: 0.03971
	accuracy_policy_3: 0.59016
	loss_value_3: 0.03621
	loss_reward_3: 0.00632
	loss_policy_4: 0.0427
	accuracy_policy_4: 0.5677
	loss_value_4: 0.03754
	loss_reward_4: 0.00696
	loss_policy_5: 0.04584
	accuracy_policy_5: 0.53842
	loss_value_5: 0.03891
	loss_reward_5: 0.00802
	loss_policy: 0.31239
	loss_value: 0.33797
	loss_reward: 0.03343
Optimization_Done 17200
[2024-05-08 05:28:49] [command] train weight_iter_17200.pkl 85 87
[2024-05-08 05:29:31] nn step 17300, lr: 0.1.
	loss_policy_0: 0.15168
	accuracy_policy_0: 0.685
	loss_value_0: 0.16358
	loss_policy_1: 0.03725
	accuracy_policy_1: 0.62322
	loss_value_1: 0.03455
	loss_reward_1: 0.00592
	loss_policy_2: 0.04066
	accuracy_policy_2: 0.5927
	loss_value_2: 0.03595
	loss_reward_2: 0.00573
	loss_policy_3: 0.04349
	accuracy_policy_3: 0.56906
	loss_value_3: 0.03746
	loss_reward_3: 0.00625
	loss_policy_4: 0.04634
	accuracy_policy_4: 0.54459
	loss_value_4: 0.03895
	loss_reward_4: 0.00686
	loss_policy_5: 0.04953
	accuracy_policy_5: 0.52309
	loss_value_5: 0.04027
	loss_reward_5: 0.00768
	loss_policy: 0.36895
	loss_value: 0.35076
	loss_reward: 0.03244
[2024-05-08 05:30:13] nn step 17400, lr: 0.1.
	loss_policy_0: 0.13385
	accuracy_policy_0: 0.72428
	loss_value_0: 0.16608
	loss_policy_1: 0.03485
	accuracy_policy_1: 0.65281
	loss_value_1: 0.03496
	loss_reward_1: 0.00606
	loss_policy_2: 0.03838
	accuracy_policy_2: 0.62441
	loss_value_2: 0.03654
	loss_reward_2: 0.00582
	loss_policy_3: 0.04202
	accuracy_policy_3: 0.59477
	loss_value_3: 0.03811
	loss_reward_3: 0.0064
	loss_policy_4: 0.04492
	accuracy_policy_4: 0.57414
	loss_value_4: 0.03963
	loss_reward_4: 0.00685
	loss_policy_5: 0.04777
	accuracy_policy_5: 0.54885
	loss_value_5: 0.04098
	loss_reward_5: 0.00787
	loss_policy: 0.3418
	loss_value: 0.3563
	loss_reward: 0.033
Optimization_Done 17400
[2024-05-08 05:32:31] [command] train weight_iter_17400.pkl 86 88
[2024-05-08 05:33:14] nn step 17500, lr: 0.1.
	loss_policy_0: 0.14754
	accuracy_policy_0: 0.72021
	loss_value_0: 0.18021
	loss_policy_1: 0.03752
	accuracy_policy_1: 0.65541
	loss_value_1: 0.038
	loss_reward_1: 0.00732
	loss_policy_2: 0.04179
	accuracy_policy_2: 0.62752
	loss_value_2: 0.03969
	loss_reward_2: 0.00716
	loss_policy_3: 0.045
	accuracy_policy_3: 0.60488
	loss_value_3: 0.04114
	loss_reward_3: 0.00767
	loss_policy_4: 0.0483
	accuracy_policy_4: 0.58205
	loss_value_4: 0.04263
	loss_reward_4: 0.0082
	loss_policy_5: 0.05124
	accuracy_policy_5: 0.56102
	loss_value_5: 0.04405
	loss_reward_5: 0.00927
	loss_policy: 0.3714
	loss_value: 0.38572
	loss_reward: 0.03961
[2024-05-08 05:33:55] nn step 17600, lr: 0.1.
	loss_policy_0: 0.12988
	accuracy_policy_0: 0.74336
	loss_value_0: 0.17088
	loss_policy_1: 0.03448
	accuracy_policy_1: 0.67314
	loss_value_1: 0.03604
	loss_reward_1: 0.00686
	loss_policy_2: 0.03844
	accuracy_policy_2: 0.64232
	loss_value_2: 0.03771
	loss_reward_2: 0.00647
	loss_policy_3: 0.04232
	accuracy_policy_3: 0.61201
	loss_value_3: 0.03919
	loss_reward_3: 0.00713
	loss_policy_4: 0.04489
	accuracy_policy_4: 0.59219
	loss_value_4: 0.04054
	loss_reward_4: 0.00768
	loss_policy_5: 0.04859
	accuracy_policy_5: 0.5634
	loss_value_5: 0.04186
	loss_reward_5: 0.00883
	loss_policy: 0.33861
	loss_value: 0.36622
	loss_reward: 0.03698
Optimization_Done 17600
[2024-05-08 05:36:15] [command] train weight_iter_17600.pkl 87 89
[2024-05-08 05:36:57] nn step 17700, lr: 0.1.
	loss_policy_0: 0.14537
	accuracy_policy_0: 0.70438
	loss_value_0: 0.17102
	loss_policy_1: 0.03665
	accuracy_policy_1: 0.64086
	loss_value_1: 0.03602
	loss_reward_1: 0.00645
	loss_policy_2: 0.04047
	accuracy_policy_2: 0.60699
	loss_value_2: 0.03754
	loss_reward_2: 0.00618
	loss_policy_3: 0.04374
	accuracy_policy_3: 0.58367
	loss_value_3: 0.03893
	loss_reward_3: 0.00664
	loss_policy_4: 0.0465
	accuracy_policy_4: 0.56268
	loss_value_4: 0.04023
	loss_reward_4: 0.0071
	loss_policy_5: 0.04882
	accuracy_policy_5: 0.54463
	loss_value_5: 0.04147
	loss_reward_5: 0.0083
	loss_policy: 0.36155
	loss_value: 0.36519
	loss_reward: 0.03467
[2024-05-08 05:37:38] nn step 17800, lr: 0.1.
	loss_policy_0: 0.12433
	accuracy_policy_0: 0.74354
	loss_value_0: 0.16578
	loss_policy_1: 0.03409
	accuracy_policy_1: 0.66336
	loss_value_1: 0.03486
	loss_reward_1: 0.00642
	loss_policy_2: 0.03776
	accuracy_policy_2: 0.63391
	loss_value_2: 0.03636
	loss_reward_2: 0.00616
	loss_policy_3: 0.0415
	accuracy_policy_3: 0.60348
	loss_value_3: 0.0377
	loss_reward_3: 0.0065
	loss_policy_4: 0.0442
	accuracy_policy_4: 0.58326
	loss_value_4: 0.03883
	loss_reward_4: 0.00691
	loss_policy_5: 0.04659
	accuracy_policy_5: 0.56807
	loss_value_5: 0.04007
	loss_reward_5: 0.00819
	loss_policy: 0.32847
	loss_value: 0.3536
	loss_reward: 0.03419
Optimization_Done 17800
[2024-05-08 05:40:12] [command] train weight_iter_17800.pkl 88 90
[2024-05-08 05:40:54] nn step 17900, lr: 0.1.
	loss_policy_0: 0.13532
	accuracy_policy_0: 0.71402
	loss_value_0: 0.16615
	loss_policy_1: 0.03527
	accuracy_policy_1: 0.64523
	loss_value_1: 0.03494
	loss_reward_1: 0.00658
	loss_policy_2: 0.03906
	accuracy_policy_2: 0.61088
	loss_value_2: 0.03655
	loss_reward_2: 0.00625
	loss_policy_3: 0.04225
	accuracy_policy_3: 0.58508
	loss_value_3: 0.03774
	loss_reward_3: 0.00688
	loss_policy_4: 0.04553
	accuracy_policy_4: 0.55936
	loss_value_4: 0.03897
	loss_reward_4: 0.00737
	loss_policy_5: 0.0483
	accuracy_policy_5: 0.53586
	loss_value_5: 0.04021
	loss_reward_5: 0.00844
	loss_policy: 0.34572
	loss_value: 0.35456
	loss_reward: 0.03552
[2024-05-08 05:41:35] nn step 18000, lr: 0.1.
	loss_policy_0: 0.1196
	accuracy_policy_0: 0.75854
	loss_value_0: 0.17312
	loss_policy_1: 0.03361
	accuracy_policy_1: 0.67602
	loss_value_1: 0.03656
	loss_reward_1: 0.00694
	loss_policy_2: 0.03779
	accuracy_policy_2: 0.64279
	loss_value_2: 0.03803
	loss_reward_2: 0.00648
	loss_policy_3: 0.04115
	accuracy_policy_3: 0.616
	loss_value_3: 0.03946
	loss_reward_3: 0.00709
	loss_policy_4: 0.04455
	accuracy_policy_4: 0.5909
	loss_value_4: 0.04077
	loss_reward_4: 0.00765
	loss_policy_5: 0.04733
	accuracy_policy_5: 0.56865
	loss_value_5: 0.04202
	loss_reward_5: 0.00882
	loss_policy: 0.32404
	loss_value: 0.36996
	loss_reward: 0.03697
Optimization_Done 18000
[2024-05-08 05:44:09] [command] train weight_iter_18000.pkl 89 91
[2024-05-08 05:44:51] nn step 18100, lr: 0.1.
	loss_policy_0: 0.14793
	accuracy_policy_0: 0.68189
	loss_value_0: 0.16569
	loss_policy_1: 0.03738
	accuracy_policy_1: 0.6135
	loss_value_1: 0.03475
	loss_reward_1: 0.00582
	loss_policy_2: 0.04039
	accuracy_policy_2: 0.59141
	loss_value_2: 0.03612
	loss_reward_2: 0.00573
	loss_policy_3: 0.04388
	accuracy_policy_3: 0.56266
	loss_value_3: 0.0375
	loss_reward_3: 0.00602
	loss_policy_4: 0.04685
	accuracy_policy_4: 0.53502
	loss_value_4: 0.03867
	loss_reward_4: 0.00635
	loss_policy_5: 0.04928
	accuracy_policy_5: 0.51551
	loss_value_5: 0.03983
	loss_reward_5: 0.00711
	loss_policy: 0.36571
	loss_value: 0.35256
	loss_reward: 0.03103
[2024-05-08 05:45:32] nn step 18200, lr: 0.1.
	loss_policy_0: 0.13816
	accuracy_policy_0: 0.72721
	loss_value_0: 0.17782
	loss_policy_1: 0.03654
	accuracy_policy_1: 0.6527
	loss_value_1: 0.03715
	loss_reward_1: 0.00628
	loss_policy_2: 0.04072
	accuracy_policy_2: 0.61895
	loss_value_2: 0.03866
	loss_reward_2: 0.0059
	loss_policy_3: 0.04421
	accuracy_policy_3: 0.59453
	loss_value_3: 0.04021
	loss_reward_3: 0.00644
	loss_policy_4: 0.04761
	accuracy_policy_4: 0.56332
	loss_value_4: 0.04148
	loss_reward_4: 0.0069
	loss_policy_5: 0.05059
	accuracy_policy_5: 0.54158
	loss_value_5: 0.04294
	loss_reward_5: 0.00784
	loss_policy: 0.35782
	loss_value: 0.37826
	loss_reward: 0.03336
Optimization_Done 18200
[2024-05-08 05:48:05] [command] train weight_iter_18200.pkl 90 92
[2024-05-08 05:48:47] nn step 18300, lr: 0.1.
	loss_policy_0: 0.13753
	accuracy_policy_0: 0.70072
	loss_value_0: 0.17242
	loss_policy_1: 0.03576
	accuracy_policy_1: 0.62539
	loss_value_1: 0.03639
	loss_reward_1: 0.00645
	loss_policy_2: 0.03995
	accuracy_policy_2: 0.58898
	loss_value_2: 0.03782
	loss_reward_2: 0.00625
	loss_policy_3: 0.04326
	accuracy_policy_3: 0.56609
	loss_value_3: 0.03928
	loss_reward_3: 0.00685
	loss_policy_4: 0.04611
	accuracy_policy_4: 0.54057
	loss_value_4: 0.0405
	loss_reward_4: 0.00732
	loss_policy_5: 0.04895
	accuracy_policy_5: 0.51834
	loss_value_5: 0.04163
	loss_reward_5: 0.00833
	loss_policy: 0.35156
	loss_value: 0.36804
	loss_reward: 0.03519
[2024-05-08 05:49:28] nn step 18400, lr: 0.1.
	loss_policy_0: 0.12637
	accuracy_policy_0: 0.74156
	loss_value_0: 0.1772
	loss_policy_1: 0.03491
	accuracy_policy_1: 0.65211
	loss_value_1: 0.03741
	loss_reward_1: 0.00669
	loss_policy_2: 0.03903
	accuracy_policy_2: 0.62246
	loss_value_2: 0.03889
	loss_reward_2: 0.00652
	loss_policy_3: 0.04272
	accuracy_policy_3: 0.59328
	loss_value_3: 0.04031
	loss_reward_3: 0.00692
	loss_policy_4: 0.04608
	accuracy_policy_4: 0.56621
	loss_value_4: 0.04165
	loss_reward_4: 0.00761
	loss_policy_5: 0.04871
	accuracy_policy_5: 0.54492
	loss_value_5: 0.04289
	loss_reward_5: 0.00875
	loss_policy: 0.33782
	loss_value: 0.37835
	loss_reward: 0.03649
Optimization_Done 18400
[2024-05-08 05:51:47] [command] train weight_iter_18400.pkl 91 93
[2024-05-08 05:52:29] nn step 18500, lr: 0.1.
	loss_policy_0: 0.12734
	accuracy_policy_0: 0.70422
	loss_value_0: 0.15381
	loss_policy_1: 0.03375
	accuracy_policy_1: 0.625
	loss_value_1: 0.03237
	loss_reward_1: 0.00611
	loss_policy_2: 0.03722
	accuracy_policy_2: 0.58877
	loss_value_2: 0.03377
	loss_reward_2: 0.00551
	loss_policy_3: 0.04052
	accuracy_policy_3: 0.56193
	loss_value_3: 0.03492
	loss_reward_3: 0.00609
	loss_policy_4: 0.04331
	accuracy_policy_4: 0.53746
	loss_value_4: 0.03619
	loss_reward_4: 0.00642
	loss_policy_5: 0.04563
	accuracy_policy_5: 0.5174
	loss_value_5: 0.03734
	loss_reward_5: 0.00733
	loss_policy: 0.32776
	loss_value: 0.32839
	loss_reward: 0.03147
[2024-05-08 05:53:11] nn step 18600, lr: 0.1.
	loss_policy_0: 0.11613
	accuracy_policy_0: 0.74494
	loss_value_0: 0.15671
	loss_policy_1: 0.03265
	accuracy_policy_1: 0.65113
	loss_value_1: 0.0331
	loss_reward_1: 0.00614
	loss_policy_2: 0.03649
	accuracy_policy_2: 0.61721
	loss_value_2: 0.03452
	loss_reward_2: 0.00571
	loss_policy_3: 0.04
	accuracy_policy_3: 0.58334
	loss_value_3: 0.03577
	loss_reward_3: 0.00621
	loss_policy_4: 0.04299
	accuracy_policy_4: 0.55906
	loss_value_4: 0.03699
	loss_reward_4: 0.00672
	loss_policy_5: 0.04572
	accuracy_policy_5: 0.53746
	loss_value_5: 0.03828
	loss_reward_5: 0.00772
	loss_policy: 0.31399
	loss_value: 0.33537
	loss_reward: 0.0325
Optimization_Done 18600
[2024-05-08 05:55:44] [command] train weight_iter_18600.pkl 92 94
[2024-05-08 05:56:27] nn step 18700, lr: 0.1.
	loss_policy_0: 0.12301
	accuracy_policy_0: 0.72229
	loss_value_0: 0.16192
	loss_policy_1: 0.03284
	accuracy_policy_1: 0.64754
	loss_value_1: 0.03398
	loss_reward_1: 0.00582
	loss_policy_2: 0.03635
	accuracy_policy_2: 0.61359
	loss_value_2: 0.03533
	loss_reward_2: 0.0054
	loss_policy_3: 0.03972
	accuracy_policy_3: 0.58637
	loss_value_3: 0.03661
	loss_reward_3: 0.00605
	loss_policy_4: 0.04257
	accuracy_policy_4: 0.56115
	loss_value_4: 0.03783
	loss_reward_4: 0.00653
	loss_policy_5: 0.04508
	accuracy_policy_5: 0.54215
	loss_value_5: 0.03892
	loss_reward_5: 0.00748
	loss_policy: 0.31957
	loss_value: 0.34459
	loss_reward: 0.03126
[2024-05-08 05:57:08] nn step 18800, lr: 0.1.
	loss_policy_0: 0.10396
	accuracy_policy_0: 0.76584
	loss_value_0: 0.15642
	loss_policy_1: 0.02978
	accuracy_policy_1: 0.67484
	loss_value_1: 0.03289
	loss_reward_1: 0.00585
	loss_policy_2: 0.03369
	accuracy_policy_2: 0.64428
	loss_value_2: 0.03424
	loss_reward_2: 0.00535
	loss_policy_3: 0.03671
	accuracy_policy_3: 0.6158
	loss_value_3: 0.03551
	loss_reward_3: 0.00606
	loss_policy_4: 0.03975
	accuracy_policy_4: 0.58975
	loss_value_4: 0.03671
	loss_reward_4: 0.00638
	loss_policy_5: 0.04238
	accuracy_policy_5: 0.56855
	loss_value_5: 0.03797
	loss_reward_5: 0.00759
	loss_policy: 0.28627
	loss_value: 0.33373
	loss_reward: 0.03123
Optimization_Done 18800
[2024-05-08 05:59:46] [command] train weight_iter_18800.pkl 93 95
[2024-05-08 06:00:29] nn step 18900, lr: 0.1.
	loss_policy_0: 0.15638
	accuracy_policy_0: 0.65189
	loss_value_0: 0.1689
	loss_policy_1: 0.03845
	accuracy_policy_1: 0.58561
	loss_value_1: 0.03508
	loss_reward_1: 0.00563
	loss_policy_2: 0.04152
	accuracy_policy_2: 0.5616
	loss_value_2: 0.03633
	loss_reward_2: 0.00549
	loss_policy_3: 0.04436
	accuracy_policy_3: 0.53623
	loss_value_3: 0.03745
	loss_reward_3: 0.00567
	loss_policy_4: 0.04671
	accuracy_policy_4: 0.51508
	loss_value_4: 0.03864
	loss_reward_4: 0.00611
	loss_policy_5: 0.04889
	accuracy_policy_5: 0.49791
	loss_value_5: 0.03979
	loss_reward_5: 0.0069
	loss_policy: 0.37631
	loss_value: 0.35618
	loss_reward: 0.02981
[2024-05-08 06:01:10] nn step 19000, lr: 0.1.
	loss_policy_0: 0.13098
	accuracy_policy_0: 0.71525
	loss_value_0: 0.16585
	loss_policy_1: 0.03471
	accuracy_policy_1: 0.63516
	loss_value_1: 0.03467
	loss_reward_1: 0.0055
	loss_policy_2: 0.03796
	accuracy_policy_2: 0.6041
	loss_value_2: 0.03591
	loss_reward_2: 0.00545
	loss_policy_3: 0.04084
	accuracy_policy_3: 0.58082
	loss_value_3: 0.0372
	loss_reward_3: 0.00583
	loss_policy_4: 0.0435
	accuracy_policy_4: 0.55414
	loss_value_4: 0.03826
	loss_reward_4: 0.00624
	loss_policy_5: 0.04595
	accuracy_policy_5: 0.53725
	loss_value_5: 0.03943
	loss_reward_5: 0.00695
	loss_policy: 0.33394
	loss_value: 0.35131
	loss_reward: 0.02997
Optimization_Done 19000
[2024-05-08 06:03:59] [command] train weight_iter_19000.pkl 94 96
[2024-05-08 06:04:41] nn step 19100, lr: 0.1.
	loss_policy_0: 0.15376
	accuracy_policy_0: 0.68492
	loss_value_0: 0.176
	loss_policy_1: 0.03893
	accuracy_policy_1: 0.61457
	loss_value_1: 0.03702
	loss_reward_1: 0.00634
	loss_policy_2: 0.04259
	accuracy_policy_2: 0.58631
	loss_value_2: 0.03857
	loss_reward_2: 0.00634
	loss_policy_3: 0.04598
	accuracy_policy_3: 0.55729
	loss_value_3: 0.04005
	loss_reward_3: 0.00688
	loss_policy_4: 0.04894
	accuracy_policy_4: 0.53537
	loss_value_4: 0.04125
	loss_reward_4: 0.00724
	loss_policy_5: 0.05172
	accuracy_policy_5: 0.51236
	loss_value_5: 0.04256
	loss_reward_5: 0.00831
	loss_policy: 0.38192
	loss_value: 0.37544
	loss_reward: 0.03512
[2024-05-08 06:05:22] nn step 19200, lr: 0.1.
	loss_policy_0: 0.1305
	accuracy_policy_0: 0.72432
	loss_value_0: 0.16958
	loss_policy_1: 0.03531
	accuracy_policy_1: 0.6442
	loss_value_1: 0.03553
	loss_reward_1: 0.0063
	loss_policy_2: 0.03865
	accuracy_policy_2: 0.61533
	loss_value_2: 0.03684
	loss_reward_2: 0.00629
	loss_policy_3: 0.04241
	accuracy_policy_3: 0.58303
	loss_value_3: 0.03814
	loss_reward_3: 0.00661
	loss_policy_4: 0.04537
	accuracy_policy_4: 0.55758
	loss_value_4: 0.03954
	loss_reward_4: 0.00708
	loss_policy_5: 0.04825
	accuracy_policy_5: 0.5357
	loss_value_5: 0.04076
	loss_reward_5: 0.00807
	loss_policy: 0.34048
	loss_value: 0.36039
	loss_reward: 0.03435
Optimization_Done 19200
[2024-05-08 06:07:57] [command] train weight_iter_19200.pkl 95 97
[2024-05-08 06:08:40] nn step 19300, lr: 0.1.
	loss_policy_0: 0.14225
	accuracy_policy_0: 0.70512
	loss_value_0: 0.16902
	loss_policy_1: 0.03685
	accuracy_policy_1: 0.63225
	loss_value_1: 0.03545
	loss_reward_1: 0.0068
	loss_policy_2: 0.0407
	accuracy_policy_2: 0.60066
	loss_value_2: 0.03689
	loss_reward_2: 0.00655
	loss_policy_3: 0.04409
	accuracy_policy_3: 0.5699
	loss_value_3: 0.03833
	loss_reward_3: 0.00717
	loss_policy_4: 0.04723
	accuracy_policy_4: 0.55004
	loss_value_4: 0.0397
	loss_reward_4: 0.00761
	loss_policy_5: 0.04964
	accuracy_policy_5: 0.5352
	loss_value_5: 0.04088
	loss_reward_5: 0.00858
	loss_policy: 0.36076
	loss_value: 0.36027
	loss_reward: 0.0367
[2024-05-08 06:09:21] nn step 19400, lr: 0.1.
	loss_policy_0: 0.12719
	accuracy_policy_0: 0.7399
	loss_value_0: 0.17129
	loss_policy_1: 0.03548
	accuracy_policy_1: 0.65385
	loss_value_1: 0.03582
	loss_reward_1: 0.00703
	loss_policy_2: 0.03926
	accuracy_policy_2: 0.62381
	loss_value_2: 0.03747
	loss_reward_2: 0.00682
	loss_policy_3: 0.04295
	accuracy_policy_3: 0.59096
	loss_value_3: 0.03895
	loss_reward_3: 0.00711
	loss_policy_4: 0.04597
	accuracy_policy_4: 0.57123
	loss_value_4: 0.04034
	loss_reward_4: 0.00756
	loss_policy_5: 0.04867
	accuracy_policy_5: 0.54615
	loss_value_5: 0.0415
	loss_reward_5: 0.00872
	loss_policy: 0.33952
	loss_value: 0.36535
	loss_reward: 0.03723
Optimization_Done 19400
[2024-05-08 06:11:55] [command] train weight_iter_19400.pkl 96 98
[2024-05-08 06:12:37] nn step 19500, lr: 0.1.
	loss_policy_0: 0.13721
	accuracy_policy_0: 0.71264
	loss_value_0: 0.16736
	loss_policy_1: 0.03606
	accuracy_policy_1: 0.63479
	loss_value_1: 0.03496
	loss_reward_1: 0.00633
	loss_policy_2: 0.03971
	accuracy_policy_2: 0.611
	loss_value_2: 0.03651
	loss_reward_2: 0.00594
	loss_policy_3: 0.04294
	accuracy_policy_3: 0.5823
	loss_value_3: 0.03786
	loss_reward_3: 0.00645
	loss_policy_4: 0.04611
	accuracy_policy_4: 0.55582
	loss_value_4: 0.03895
	loss_reward_4: 0.00702
	loss_policy_5: 0.04871
	accuracy_policy_5: 0.53658
	loss_value_5: 0.0401
	loss_reward_5: 0.00818
	loss_policy: 0.35073
	loss_value: 0.35575
	loss_reward: 0.03392
[2024-05-08 06:13:18] nn step 19600, lr: 0.1.
	loss_policy_0: 0.11736
	accuracy_policy_0: 0.75182
	loss_value_0: 0.16206
	loss_policy_1: 0.03302
	accuracy_policy_1: 0.66316
	loss_value_1: 0.03404
	loss_reward_1: 0.00609
	loss_policy_2: 0.03725
	accuracy_policy_2: 0.63133
	loss_value_2: 0.03545
	loss_reward_2: 0.00583
	loss_policy_3: 0.04081
	accuracy_policy_3: 0.59914
	loss_value_3: 0.03679
	loss_reward_3: 0.00647
	loss_policy_4: 0.04374
	accuracy_policy_4: 0.57855
	loss_value_4: 0.03787
	loss_reward_4: 0.00693
	loss_policy_5: 0.04654
	accuracy_policy_5: 0.55371
	loss_value_5: 0.03908
	loss_reward_5: 0.00808
	loss_policy: 0.31872
	loss_value: 0.34529
	loss_reward: 0.03338
Optimization_Done 19600
[2024-05-08 06:16:03] [command] train weight_iter_19600.pkl 97 99
[2024-05-08 06:16:45] nn step 19700, lr: 0.1.
	loss_policy_0: 0.15266
	accuracy_policy_0: 0.68977
	loss_value_0: 0.16432
	loss_policy_1: 0.0379
	accuracy_policy_1: 0.62568
	loss_value_1: 0.03426
	loss_reward_1: 0.00525
	loss_policy_2: 0.04115
	accuracy_policy_2: 0.59369
	loss_value_2: 0.03542
	loss_reward_2: 0.00497
	loss_policy_3: 0.04456
	accuracy_policy_3: 0.56766
	loss_value_3: 0.03657
	loss_reward_3: 0.00531
	loss_policy_4: 0.04708
	accuracy_policy_4: 0.54656
	loss_value_4: 0.03775
	loss_reward_4: 0.00571
	loss_policy_5: 0.04956
	accuracy_policy_5: 0.52844
	loss_value_5: 0.03882
	loss_reward_5: 0.00653
	loss_policy: 0.37292
	loss_value: 0.34715
	loss_reward: 0.02778
[2024-05-08 06:17:26] nn step 19800, lr: 0.1.
	loss_policy_0: 0.13266
	accuracy_policy_0: 0.73582
	loss_value_0: 0.16845
	loss_policy_1: 0.03516
	accuracy_policy_1: 0.65838
	loss_value_1: 0.03534
	loss_reward_1: 0.00552
	loss_policy_2: 0.03889
	accuracy_policy_2: 0.62695
	loss_value_2: 0.03663
	loss_reward_2: 0.00511
	loss_policy_3: 0.04248
	accuracy_policy_3: 0.60053
	loss_value_3: 0.03784
	loss_reward_3: 0.00535
	loss_policy_4: 0.04532
	accuracy_policy_4: 0.57631
	loss_value_4: 0.03896
	loss_reward_4: 0.00602
	loss_policy_5: 0.04819
	accuracy_policy_5: 0.55537
	loss_value_5: 0.04005
	loss_reward_5: 0.00691
	loss_policy: 0.34269
	loss_value: 0.35727
	loss_reward: 0.02891
Optimization_Done 19800
[2024-05-08 06:20:05] [command] train weight_iter_19800.pkl 98 100
[2024-05-08 06:20:48] nn step 19900, lr: 0.1.
	loss_policy_0: 0.16622
	accuracy_policy_0: 0.67738
	loss_value_0: 0.17196
	loss_policy_1: 0.04083
	accuracy_policy_1: 0.6132
	loss_value_1: 0.03593
	loss_reward_1: 0.0047
	loss_policy_2: 0.04422
	accuracy_policy_2: 0.58445
	loss_value_2: 0.03734
	loss_reward_2: 0.00459
	loss_policy_3: 0.04745
	accuracy_policy_3: 0.55896
	loss_value_3: 0.03855
	loss_reward_3: 0.00508
	loss_policy_4: 0.05036
	accuracy_policy_4: 0.53725
	loss_value_4: 0.03971
	loss_reward_4: 0.0054
	loss_policy_5: 0.05264
	accuracy_policy_5: 0.5192
	loss_value_5: 0.0409
	loss_reward_5: 0.00615
	loss_policy: 0.40172
	loss_value: 0.36438
	loss_reward: 0.0259
[2024-05-08 06:21:29] nn step 20000, lr: 0.1.
	loss_policy_0: 0.14762
	accuracy_policy_0: 0.71359
	loss_value_0: 0.17339
	loss_policy_1: 0.03817
	accuracy_policy_1: 0.64064
	loss_value_1: 0.0362
	loss_reward_1: 0.00481
	loss_policy_2: 0.04151
	accuracy_policy_2: 0.61289
	loss_value_2: 0.0375
	loss_reward_2: 0.00464
	loss_policy_3: 0.04538
	accuracy_policy_3: 0.58301
	loss_value_3: 0.03866
	loss_reward_3: 0.00493
	loss_policy_4: 0.0476
	accuracy_policy_4: 0.56764
	loss_value_4: 0.03989
	loss_reward_4: 0.00524
	loss_policy_5: 0.05075
	accuracy_policy_5: 0.5426
	loss_value_5: 0.04105
	loss_reward_5: 0.00618
	loss_policy: 0.37102
	loss_value: 0.3667
	loss_reward: 0.0258
Optimization_Done 20000
[2024-05-08 06:24:15] [command] train weight_iter_20000.pkl 99 101
[2024-05-08 06:24:57] nn step 20100, lr: 0.1.
	loss_policy_0: 0.14566
	accuracy_policy_0: 0.71514
	loss_value_0: 0.17065
	loss_policy_1: 0.03721
	accuracy_policy_1: 0.64771
	loss_value_1: 0.03592
	loss_reward_1: 0.00627
	loss_policy_2: 0.04137
	accuracy_policy_2: 0.6185
	loss_value_2: 0.03729
	loss_reward_2: 0.00594
	loss_policy_3: 0.04462
	accuracy_policy_3: 0.59186
	loss_value_3: 0.03853
	loss_reward_3: 0.00642
	loss_policy_4: 0.04725
	accuracy_policy_4: 0.57434
	loss_value_4: 0.03967
	loss_reward_4: 0.00694
	loss_policy_5: 0.05036
	accuracy_policy_5: 0.54984
	loss_value_5: 0.04074
	loss_reward_5: 0.008
	loss_policy: 0.36648
	loss_value: 0.3628
	loss_reward: 0.03357
[2024-05-08 06:25:37] nn step 20200, lr: 0.1.
	loss_policy_0: 0.13556
	accuracy_policy_0: 0.74236
	loss_value_0: 0.17147
	loss_policy_1: 0.0362
	accuracy_policy_1: 0.66598
	loss_value_1: 0.03603
	loss_reward_1: 0.00622
	loss_policy_2: 0.03995
	accuracy_policy_2: 0.6373
	loss_value_2: 0.03753
	loss_reward_2: 0.00584
	loss_policy_3: 0.04374
	accuracy_policy_3: 0.60779
	loss_value_3: 0.03888
	loss_reward_3: 0.00634
	loss_policy_4: 0.04653
	accuracy_policy_4: 0.5875
	loss_value_4: 0.03995
	loss_reward_4: 0.00688
	loss_policy_5: 0.0494
	accuracy_policy_5: 0.56596
	loss_value_5: 0.04123
	loss_reward_5: 0.00805
	loss_policy: 0.35138
	loss_value: 0.36509
	loss_reward: 0.03333
Optimization_Done 20200
[2024-05-08 06:28:11] [command] train weight_iter_20200.pkl 100 102
[2024-05-08 06:28:53] nn step 20300, lr: 0.1.
	loss_policy_0: 0.13632
	accuracy_policy_0: 0.72527
	loss_value_0: 0.16552
	loss_policy_1: 0.03535
	accuracy_policy_1: 0.65162
	loss_value_1: 0.03465
	loss_reward_1: 0.00609
	loss_policy_2: 0.03911
	accuracy_policy_2: 0.62012
	loss_value_2: 0.03597
	loss_reward_2: 0.00586
	loss_policy_3: 0.04212
	accuracy_policy_3: 0.59115
	loss_value_3: 0.03721
	loss_reward_3: 0.00635
	loss_policy_4: 0.04472
	accuracy_policy_4: 0.56893
	loss_value_4: 0.03846
	loss_reward_4: 0.00689
	loss_policy_5: 0.04699
	accuracy_policy_5: 0.55055
	loss_value_5: 0.03951
	loss_reward_5: 0.0079
	loss_policy: 0.3446
	loss_value: 0.35132
	loss_reward: 0.0331
[2024-05-08 06:29:35] nn step 20400, lr: 0.1.
	loss_policy_0: 0.11419
	accuracy_policy_0: 0.75664
	loss_value_0: 0.15792
	loss_policy_1: 0.03128
	accuracy_policy_1: 0.67605
	loss_value_1: 0.03312
	loss_reward_1: 0.00596
	loss_policy_2: 0.035
	accuracy_policy_2: 0.64328
	loss_value_2: 0.03429
	loss_reward_2: 0.00562
	loss_policy_3: 0.03823
	accuracy_policy_3: 0.61625
	loss_value_3: 0.03539
	loss_reward_3: 0.00609
	loss_policy_4: 0.0411
	accuracy_policy_4: 0.59578
	loss_value_4: 0.03649
	loss_reward_4: 0.00677
	loss_policy_5: 0.04358
	accuracy_policy_5: 0.57398
	loss_value_5: 0.03768
	loss_reward_5: 0.00761
	loss_policy: 0.30337
	loss_value: 0.33488
	loss_reward: 0.03204
Optimization_Done 20400
[2024-05-08 06:31:50] [command] train weight_iter_20400.pkl 101 103
[2024-05-08 06:32:32] nn step 20500, lr: 0.1.
	loss_policy_0: 0.13123
	accuracy_policy_0: 0.72861
	loss_value_0: 0.16179
	loss_policy_1: 0.03399
	accuracy_policy_1: 0.66113
	loss_value_1: 0.03373
	loss_reward_1: 0.00577
	loss_policy_2: 0.03779
	accuracy_policy_2: 0.62859
	loss_value_2: 0.03497
	loss_reward_2: 0.00553
	loss_policy_3: 0.04063
	accuracy_policy_3: 0.60383
	loss_value_3: 0.03622
	loss_reward_3: 0.00595
	loss_policy_4: 0.04339
	accuracy_policy_4: 0.58098
	loss_value_4: 0.03746
	loss_reward_4: 0.00644
	loss_policy_5: 0.0458
	accuracy_policy_5: 0.56135
	loss_value_5: 0.03862
	loss_reward_5: 0.0073
	loss_policy: 0.33283
	loss_value: 0.34279
	loss_reward: 0.03099
[2024-05-08 06:33:13] nn step 20600, lr: 0.1.
	loss_policy_0: 0.11293
	accuracy_policy_0: 0.7674
	loss_value_0: 0.16385
	loss_policy_1: 0.03088
	accuracy_policy_1: 0.68957
	loss_value_1: 0.03422
	loss_reward_1: 0.00593
	loss_policy_2: 0.03445
	accuracy_policy_2: 0.65889
	loss_value_2: 0.03577
	loss_reward_2: 0.00563
	loss_policy_3: 0.03786
	accuracy_policy_3: 0.63373
	loss_value_3: 0.0371
	loss_reward_3: 0.00616
	loss_policy_4: 0.04072
	accuracy_policy_4: 0.60967
	loss_value_4: 0.03821
	loss_reward_4: 0.00656
	loss_policy_5: 0.04368
	accuracy_policy_5: 0.58574
	loss_value_5: 0.03941
	loss_reward_5: 0.00752
	loss_policy: 0.30052
	loss_value: 0.34857
	loss_reward: 0.0318
Optimization_Done 20600
[2024-05-08 06:35:59] [command] train weight_iter_20600.pkl 102 104
[2024-05-08 06:36:41] nn step 20700, lr: 0.1.
	loss_policy_0: 0.14614
	accuracy_policy_0: 0.68523
	loss_value_0: 0.16886
	loss_policy_1: 0.036
	accuracy_policy_1: 0.62354
	loss_value_1: 0.03527
	loss_reward_1: 0.00498
	loss_policy_2: 0.03873
	accuracy_policy_2: 0.60271
	loss_value_2: 0.0365
	loss_reward_2: 0.00478
	loss_policy_3: 0.04199
	accuracy_policy_3: 0.57445
	loss_value_3: 0.03773
	loss_reward_3: 0.00516
	loss_policy_4: 0.04496
	accuracy_policy_4: 0.55092
	loss_value_4: 0.03897
	loss_reward_4: 0.00554
	loss_policy_5: 0.04712
	accuracy_policy_5: 0.53205
	loss_value_5: 0.04003
	loss_reward_5: 0.00613
	loss_policy: 0.35494
	loss_value: 0.35735
	loss_reward: 0.02659
[2024-05-08 06:37:22] nn step 20800, lr: 0.1.
	loss_policy_0: 0.12093
	accuracy_policy_0: 0.72975
	loss_value_0: 0.16085
	loss_policy_1: 0.03146
	accuracy_policy_1: 0.65543
	loss_value_1: 0.03357
	loss_reward_1: 0.00475
	loss_policy_2: 0.03412
	accuracy_policy_2: 0.63674
	loss_value_2: 0.03475
	loss_reward_2: 0.00471
	loss_policy_3: 0.03696
	accuracy_policy_3: 0.60729
	loss_value_3: 0.03595
	loss_reward_3: 0.00488
	loss_policy_4: 0.03983
	accuracy_policy_4: 0.58354
	loss_value_4: 0.03692
	loss_reward_4: 0.00533
	loss_policy_5: 0.04252
	accuracy_policy_5: 0.56088
	loss_value_5: 0.03792
	loss_reward_5: 0.00595
	loss_policy: 0.30583
	loss_value: 0.33996
	loss_reward: 0.02562
Optimization_Done 20800
[2024-05-08 06:40:05] [command] train weight_iter_20800.pkl 103 105
[2024-05-08 06:40:48] nn step 20900, lr: 0.1.
	loss_policy_0: 0.15143
	accuracy_policy_0: 0.69121
	loss_value_0: 0.17185
	loss_policy_1: 0.0361
	accuracy_policy_1: 0.63844
	loss_value_1: 0.03613
	loss_reward_1: 0.00563
	loss_policy_2: 0.03914
	accuracy_policy_2: 0.61361
	loss_value_2: 0.03761
	loss_reward_2: 0.00532
	loss_policy_3: 0.04243
	accuracy_policy_3: 0.58807
	loss_value_3: 0.0388
	loss_reward_3: 0.0057
	loss_policy_4: 0.0455
	accuracy_policy_4: 0.56465
	loss_value_4: 0.03985
	loss_reward_4: 0.00631
	loss_policy_5: 0.0483
	accuracy_policy_5: 0.54162
	loss_value_5: 0.041
	loss_reward_5: 0.00717
	loss_policy: 0.36289
	loss_value: 0.36525
	loss_reward: 0.03012
[2024-05-08 06:41:29] nn step 21000, lr: 0.1.
	loss_policy_0: 0.13472
	accuracy_policy_0: 0.72475
	loss_value_0: 0.17176
	loss_policy_1: 0.03405
	accuracy_policy_1: 0.65826
	loss_value_1: 0.03602
	loss_reward_1: 0.0056
	loss_policy_2: 0.03754
	accuracy_policy_2: 0.6291
	loss_value_2: 0.03737
	loss_reward_2: 0.00533
	loss_policy_3: 0.0411
	accuracy_policy_3: 0.60154
	loss_value_3: 0.03862
	loss_reward_3: 0.00576
	loss_policy_4: 0.04391
	accuracy_policy_4: 0.58762
	loss_value_4: 0.03976
	loss_reward_4: 0.00597
	loss_policy_5: 0.04692
	accuracy_policy_5: 0.56133
	loss_value_5: 0.04105
	loss_reward_5: 0.00719
	loss_policy: 0.33825
	loss_value: 0.36458
	loss_reward: 0.02986
Optimization_Done 21000
[2024-05-08 06:44:13] [command] train weight_iter_21000.pkl 104 106
[2024-05-08 06:44:55] nn step 21100, lr: 0.1.
	loss_policy_0: 0.15262
	accuracy_policy_0: 0.6999
	loss_value_0: 0.17834
	loss_policy_1: 0.03806
	accuracy_policy_1: 0.6332
	loss_value_1: 0.03724
	loss_reward_1: 0.00627
	loss_policy_2: 0.04187
	accuracy_policy_2: 0.59912
	loss_value_2: 0.03857
	loss_reward_2: 0.00606
	loss_policy_3: 0.04551
	accuracy_policy_3: 0.57346
	loss_value_3: 0.03993
	loss_reward_3: 0.00634
	loss_policy_4: 0.04827
	accuracy_policy_4: 0.55223
	loss_value_4: 0.0412
	loss_reward_4: 0.00681
	loss_policy_5: 0.05096
	accuracy_policy_5: 0.53021
	loss_value_5: 0.04244
	loss_reward_5: 0.00792
	loss_policy: 0.37729
	loss_value: 0.37771
	loss_reward: 0.03341
[2024-05-08 06:45:36] nn step 21200, lr: 0.1.
	loss_policy_0: 0.13684
	accuracy_policy_0: 0.73016
	loss_value_0: 0.18008
	loss_policy_1: 0.03579
	accuracy_policy_1: 0.65783
	loss_value_1: 0.03746
	loss_reward_1: 0.00633
	loss_policy_2: 0.03937
	accuracy_policy_2: 0.63139
	loss_value_2: 0.03889
	loss_reward_2: 0.00598
	loss_policy_3: 0.04351
	accuracy_policy_3: 0.59598
	loss_value_3: 0.04009
	loss_reward_3: 0.00655
	loss_policy_4: 0.0462
	accuracy_policy_4: 0.5775
	loss_value_4: 0.04126
	loss_reward_4: 0.00694
	loss_policy_5: 0.04977
	accuracy_policy_5: 0.55143
	loss_value_5: 0.04253
	loss_reward_5: 0.00835
	loss_policy: 0.35148
	loss_value: 0.38031
	loss_reward: 0.03415
Optimization_Done 21200
[2024-05-08 06:48:03] [command] train weight_iter_21200.pkl 105 107
[2024-05-08 06:48:45] nn step 21300, lr: 0.1.
	loss_policy_0: 0.14788
	accuracy_policy_0: 0.70965
	loss_value_0: 0.17547
	loss_policy_1: 0.03687
	accuracy_policy_1: 0.64664
	loss_value_1: 0.03669
	loss_reward_1: 0.00574
	loss_policy_2: 0.04026
	accuracy_policy_2: 0.61824
	loss_value_2: 0.03805
	loss_reward_2: 0.0054
	loss_policy_3: 0.04373
	accuracy_policy_3: 0.59234
	loss_value_3: 0.03925
	loss_reward_3: 0.00595
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.57098
	loss_value_4: 0.04044
	loss_reward_4: 0.00647
	loss_policy_5: 0.0492
	accuracy_policy_5: 0.54912
	loss_value_5: 0.04166
	loss_reward_5: 0.00741
	loss_policy: 0.36466
	loss_value: 0.37156
	loss_reward: 0.03098
[2024-05-08 06:49:26] nn step 21400, lr: 0.1.
	loss_policy_0: 0.13556
	accuracy_policy_0: 0.74494
	loss_value_0: 0.1803
	loss_policy_1: 0.03584
	accuracy_policy_1: 0.66955
	loss_value_1: 0.03762
	loss_reward_1: 0.00613
	loss_policy_2: 0.03962
	accuracy_policy_2: 0.63822
	loss_value_2: 0.03905
	loss_reward_2: 0.00565
	loss_policy_3: 0.04371
	accuracy_policy_3: 0.61168
	loss_value_3: 0.04038
	loss_reward_3: 0.00614
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.58906
	loss_value_4: 0.04157
	loss_reward_4: 0.00657
	loss_policy_5: 0.04988
	accuracy_policy_5: 0.56893
	loss_value_5: 0.04272
	loss_reward_5: 0.00789
	loss_policy: 0.35131
	loss_value: 0.38164
	loss_reward: 0.03238
Optimization_Done 21400
[2024-05-08 06:52:06] [command] train weight_iter_21400.pkl 106 108
[2024-05-08 06:52:48] nn step 21500, lr: 0.1.
	loss_policy_0: 0.16002
	accuracy_policy_0: 0.68844
	loss_value_0: 0.17179
	loss_policy_1: 0.03805
	accuracy_policy_1: 0.63652
	loss_value_1: 0.03571
	loss_reward_1: 0.00529
	loss_policy_2: 0.04136
	accuracy_policy_2: 0.60953
	loss_value_2: 0.03706
	loss_reward_2: 0.00503
	loss_policy_3: 0.04454
	accuracy_policy_3: 0.58344
	loss_value_3: 0.03838
	loss_reward_3: 0.00534
	loss_policy_4: 0.04729
	accuracy_policy_4: 0.56484
	loss_value_4: 0.03955
	loss_reward_4: 0.00556
	loss_policy_5: 0.04995
	accuracy_policy_5: 0.54883
	loss_value_5: 0.04066
	loss_reward_5: 0.00643
	loss_policy: 0.38122
	loss_value: 0.36315
	loss_reward: 0.02766
[2024-05-08 06:53:29] nn step 21600, lr: 0.1.
	loss_policy_0: 0.13404
	accuracy_policy_0: 0.73732
	loss_value_0: 0.16609
	loss_policy_1: 0.03434
	accuracy_policy_1: 0.6692
	loss_value_1: 0.0347
	loss_reward_1: 0.00522
	loss_policy_2: 0.03797
	accuracy_policy_2: 0.6385
	loss_value_2: 0.03589
	loss_reward_2: 0.00498
	loss_policy_3: 0.04135
	accuracy_policy_3: 0.61299
	loss_value_3: 0.03713
	loss_reward_3: 0.00526
	loss_policy_4: 0.04427
	accuracy_policy_4: 0.58762
	loss_value_4: 0.03834
	loss_reward_4: 0.0057
	loss_policy_5: 0.04714
	accuracy_policy_5: 0.57227
	loss_value_5: 0.03958
	loss_reward_5: 0.00663
	loss_policy: 0.33911
	loss_value: 0.35173
	loss_reward: 0.0278
Optimization_Done 21600
[2024-05-08 06:55:53] [command] train weight_iter_21600.pkl 107 109
[2024-05-08 06:56:35] nn step 21700, lr: 0.1.
	loss_policy_0: 0.14985
	accuracy_policy_0: 0.69955
	loss_value_0: 0.17207
	loss_policy_1: 0.03567
	accuracy_policy_1: 0.6491
	loss_value_1: 0.03577
	loss_reward_1: 0.0049
	loss_policy_2: 0.03802
	accuracy_policy_2: 0.63123
	loss_value_2: 0.03703
	loss_reward_2: 0.00462
	loss_policy_3: 0.04103
	accuracy_policy_3: 0.61252
	loss_value_3: 0.03841
	loss_reward_3: 0.00504
	loss_policy_4: 0.04332
	accuracy_policy_4: 0.59779
	loss_value_4: 0.03947
	loss_reward_4: 0.00544
	loss_policy_5: 0.04553
	accuracy_policy_5: 0.58076
	loss_value_5: 0.04068
	loss_reward_5: 0.00626
	loss_policy: 0.35342
	loss_value: 0.36343
	loss_reward: 0.02626
[2024-05-08 06:57:16] nn step 21800, lr: 0.1.
	loss_policy_0: 0.12753
	accuracy_policy_0: 0.74418
	loss_value_0: 0.16802
	loss_policy_1: 0.03201
	accuracy_policy_1: 0.68713
	loss_value_1: 0.03535
	loss_reward_1: 0.00475
	loss_policy_2: 0.0351
	accuracy_policy_2: 0.65643
	loss_value_2: 0.0367
	loss_reward_2: 0.00468
	loss_policy_3: 0.03812
	accuracy_policy_3: 0.63791
	loss_value_3: 0.03785
	loss_reward_3: 0.00505
	loss_policy_4: 0.04051
	accuracy_policy_4: 0.62291
	loss_value_4: 0.03912
	loss_reward_4: 0.00536
	loss_policy_5: 0.04335
	accuracy_policy_5: 0.59812
	loss_value_5: 0.04028
	loss_reward_5: 0.0062
	loss_policy: 0.31662
	loss_value: 0.35733
	loss_reward: 0.02604
Optimization_Done 21800
[2024-05-08 06:59:53] [command] train weight_iter_21800.pkl 108 110
[2024-05-08 07:00:34] nn step 21900, lr: 0.1.
	loss_policy_0: 0.15553
	accuracy_policy_0: 0.69908
	loss_value_0: 0.1803
	loss_policy_1: 0.03786
	accuracy_policy_1: 0.6415
	loss_value_1: 0.03775
	loss_reward_1: 0.00609
	loss_policy_2: 0.04153
	accuracy_policy_2: 0.61455
	loss_value_2: 0.03932
	loss_reward_2: 0.00584
	loss_policy_3: 0.0448
	accuracy_policy_3: 0.5967
	loss_value_3: 0.04055
	loss_reward_3: 0.00633
	loss_policy_4: 0.04759
	accuracy_policy_4: 0.57924
	loss_value_4: 0.0419
	loss_reward_4: 0.00674
	loss_policy_5: 0.05033
	accuracy_policy_5: 0.5643
	loss_value_5: 0.0431
	loss_reward_5: 0.0079
	loss_policy: 0.37765
	loss_value: 0.38294
	loss_reward: 0.03291
[2024-05-08 07:01:15] nn step 22000, lr: 0.1.
	loss_policy_0: 0.12786
	accuracy_policy_0: 0.73947
	loss_value_0: 0.16958
	loss_policy_1: 0.03358
	accuracy_policy_1: 0.67223
	loss_value_1: 0.03565
	loss_reward_1: 0.00583
	loss_policy_2: 0.03635
	accuracy_policy_2: 0.65117
	loss_value_2: 0.03709
	loss_reward_2: 0.00558
	loss_policy_3: 0.04002
	accuracy_policy_3: 0.6232
	loss_value_3: 0.03827
	loss_reward_3: 0.00587
	loss_policy_4: 0.04242
	accuracy_policy_4: 0.60697
	loss_value_4: 0.03954
	loss_reward_4: 0.00639
	loss_policy_5: 0.04525
	accuracy_policy_5: 0.59041
	loss_value_5: 0.04082
	loss_reward_5: 0.00729
	loss_policy: 0.32547
	loss_value: 0.36096
	loss_reward: 0.03096
Optimization_Done 22000
[2024-05-08 07:03:43] [command] train weight_iter_22000.pkl 109 111
[2024-05-08 07:04:26] nn step 22100, lr: 0.1.
	loss_policy_0: 0.13616
	accuracy_policy_0: 0.72129
	loss_value_0: 0.16962
	loss_policy_1: 0.03374
	accuracy_policy_1: 0.66201
	loss_value_1: 0.03528
	loss_reward_1: 0.00553
	loss_policy_2: 0.03712
	accuracy_policy_2: 0.63236
	loss_value_2: 0.03656
	loss_reward_2: 0.00538
	loss_policy_3: 0.03982
	accuracy_policy_3: 0.61027
	loss_value_3: 0.03771
	loss_reward_3: 0.00576
	loss_policy_4: 0.0424
	accuracy_policy_4: 0.59215
	loss_value_4: 0.03892
	loss_reward_4: 0.00615
	loss_policy_5: 0.04459
	accuracy_policy_5: 0.57984
	loss_value_5: 0.04007
	loss_reward_5: 0.00718
	loss_policy: 0.33383
	loss_value: 0.35815
	loss_reward: 0.03
[2024-05-08 07:05:06] nn step 22200, lr: 0.1.
	loss_policy_0: 0.12208
	accuracy_policy_0: 0.75289
	loss_value_0: 0.17149
	loss_policy_1: 0.03196
	accuracy_policy_1: 0.68514
	loss_value_1: 0.03589
	loss_reward_1: 0.00581
	loss_policy_2: 0.03532
	accuracy_policy_2: 0.65998
	loss_value_2: 0.03736
	loss_reward_2: 0.00565
	loss_policy_3: 0.0385
	accuracy_policy_3: 0.63838
	loss_value_3: 0.03876
	loss_reward_3: 0.00598
	loss_policy_4: 0.04091
	accuracy_policy_4: 0.61957
	loss_value_4: 0.03987
	loss_reward_4: 0.00637
	loss_policy_5: 0.04378
	accuracy_policy_5: 0.6049
	loss_value_5: 0.04112
	loss_reward_5: 0.00744
	loss_policy: 0.31254
	loss_value: 0.36449
	loss_reward: 0.03124
Optimization_Done 22200
[2024-05-08 07:07:46] [command] train weight_iter_22200.pkl 110 112
[2024-05-08 07:08:28] nn step 22300, lr: 0.1.
	loss_policy_0: 0.16252
	accuracy_policy_0: 0.6883
	loss_value_0: 0.15872
	loss_policy_1: 0.03956
	accuracy_policy_1: 0.62914
	loss_value_1: 0.03317
	loss_reward_1: 0.00541
	loss_policy_2: 0.04323
	accuracy_policy_2: 0.60152
	loss_value_2: 0.0345
	loss_reward_2: 0.0054
	loss_policy_3: 0.04675
	accuracy_policy_3: 0.57471
	loss_value_3: 0.0359
	loss_reward_3: 0.00577
	loss_policy_4: 0.04951
	accuracy_policy_4: 0.5577
	loss_value_4: 0.03707
	loss_reward_4: 0.00615
	loss_policy_5: 0.05233
	accuracy_policy_5: 0.53461
	loss_value_5: 0.03828
	loss_reward_5: 0.00705
	loss_policy: 0.3939
	loss_value: 0.33762
	loss_reward: 0.02978
[2024-05-08 07:09:09] nn step 22400, lr: 0.1.
	loss_policy_0: 0.13358
	accuracy_policy_0: 0.7283
	loss_value_0: 0.15148
	loss_policy_1: 0.03488
	accuracy_policy_1: 0.66012
	loss_value_1: 0.03175
	loss_reward_1: 0.00555
	loss_policy_2: 0.03853
	accuracy_policy_2: 0.6343
	loss_value_2: 0.03305
	loss_reward_2: 0.0053
	loss_policy_3: 0.04185
	accuracy_policy_3: 0.60334
	loss_value_3: 0.03415
	loss_reward_3: 0.00561
	loss_policy_4: 0.04495
	accuracy_policy_4: 0.58445
	loss_value_4: 0.03528
	loss_reward_4: 0.0059
	loss_policy_5: 0.04761
	accuracy_policy_5: 0.56738
	loss_value_5: 0.03643
	loss_reward_5: 0.0069
	loss_policy: 0.34139
	loss_value: 0.32214
	loss_reward: 0.02927
Optimization_Done 22400
[2024-05-08 07:11:45] [command] train weight_iter_22400.pkl 111 113
[2024-05-08 07:12:27] nn step 22500, lr: 0.1.
	loss_policy_0: 0.18326
	accuracy_policy_0: 0.65246
	loss_value_0: 0.16391
	loss_policy_1: 0.04293
	accuracy_policy_1: 0.59871
	loss_value_1: 0.03455
	loss_reward_1: 0.0051
	loss_policy_2: 0.04597
	accuracy_policy_2: 0.57314
	loss_value_2: 0.03591
	loss_reward_2: 0.00498
	loss_policy_3: 0.0494
	accuracy_policy_3: 0.55
	loss_value_3: 0.0372
	loss_reward_3: 0.0055
	loss_policy_4: 0.05257
	accuracy_policy_4: 0.52771
	loss_value_4: 0.03851
	loss_reward_4: 0.00601
	loss_policy_5: 0.05551
	accuracy_policy_5: 0.50566
	loss_value_5: 0.0397
	loss_reward_5: 0.00669
	loss_policy: 0.42965
	loss_value: 0.34978
	loss_reward: 0.02828
[2024-05-08 07:13:08] nn step 22600, lr: 0.1.
	loss_policy_0: 0.149
	accuracy_policy_0: 0.69895
	loss_value_0: 0.15323
	loss_policy_1: 0.03712
	accuracy_policy_1: 0.6302
	loss_value_1: 0.03226
	loss_reward_1: 0.00487
	loss_policy_2: 0.04065
	accuracy_policy_2: 0.60857
	loss_value_2: 0.03358
	loss_reward_2: 0.00466
	loss_policy_3: 0.04382
	accuracy_policy_3: 0.58078
	loss_value_3: 0.03478
	loss_reward_3: 0.00508
	loss_policy_4: 0.04655
	accuracy_policy_4: 0.56152
	loss_value_4: 0.03591
	loss_reward_4: 0.0054
	loss_policy_5: 0.04951
	accuracy_policy_5: 0.53971
	loss_value_5: 0.03699
	loss_reward_5: 0.00629
	loss_policy: 0.36666
	loss_value: 0.32675
	loss_reward: 0.02631
Optimization_Done 22600
[2024-05-08 07:15:20] [command] train weight_iter_22600.pkl 112 114
[2024-05-08 07:16:02] nn step 22700, lr: 0.1.
	loss_policy_0: 0.17037
	accuracy_policy_0: 0.66113
	loss_value_0: 0.16412
	loss_policy_1: 0.04117
	accuracy_policy_1: 0.59562
	loss_value_1: 0.03455
	loss_reward_1: 0.00612
	loss_policy_2: 0.04489
	accuracy_policy_2: 0.56072
	loss_value_2: 0.03581
	loss_reward_2: 0.00592
	loss_policy_3: 0.04825
	accuracy_policy_3: 0.53914
	loss_value_3: 0.03701
	loss_reward_3: 0.00635
	loss_policy_4: 0.05155
	accuracy_policy_4: 0.51982
	loss_value_4: 0.0383
	loss_reward_4: 0.00711
	loss_policy_5: 0.05425
	accuracy_policy_5: 0.50453
	loss_value_5: 0.03944
	loss_reward_5: 0.00798
	loss_policy: 0.41049
	loss_value: 0.34923
	loss_reward: 0.03348
[2024-05-08 07:16:43] nn step 22800, lr: 0.1.
	loss_policy_0: 0.1542
	accuracy_policy_0: 0.69916
	loss_value_0: 0.16728
	loss_policy_1: 0.03933
	accuracy_policy_1: 0.62338
	loss_value_1: 0.03519
	loss_reward_1: 0.006
	loss_policy_2: 0.04283
	accuracy_policy_2: 0.59377
	loss_value_2: 0.03654
	loss_reward_2: 0.00568
	loss_policy_3: 0.04683
	accuracy_policy_3: 0.5642
	loss_value_3: 0.03801
	loss_reward_3: 0.00634
	loss_policy_4: 0.05018
	accuracy_policy_4: 0.546
	loss_value_4: 0.03935
	loss_reward_4: 0.00709
	loss_policy_5: 0.05323
	accuracy_policy_5: 0.52346
	loss_value_5: 0.04066
	loss_reward_5: 0.00775
	loss_policy: 0.3866
	loss_value: 0.35703
	loss_reward: 0.03286
Optimization_Done 22800
[2024-05-08 07:19:06] [command] train weight_iter_22800.pkl 113 115
[2024-05-08 07:19:48] nn step 22900, lr: 0.1.
	loss_policy_0: 0.1491
	accuracy_policy_0: 0.68178
	loss_value_0: 0.15816
	loss_policy_1: 0.03649
	accuracy_policy_1: 0.61828
	loss_value_1: 0.0331
	loss_reward_1: 0.00564
	loss_policy_2: 0.03952
	accuracy_policy_2: 0.5908
	loss_value_2: 0.03442
	loss_reward_2: 0.00542
	loss_policy_3: 0.04255
	accuracy_policy_3: 0.56926
	loss_value_3: 0.03557
	loss_reward_3: 0.00611
	loss_policy_4: 0.04484
	accuracy_policy_4: 0.55342
	loss_value_4: 0.03678
	loss_reward_4: 0.00649
	loss_policy_5: 0.04743
	accuracy_policy_5: 0.53094
	loss_value_5: 0.03778
	loss_reward_5: 0.00738
	loss_policy: 0.35993
	loss_value: 0.33582
	loss_reward: 0.03103
[2024-05-08 07:20:29] nn step 23000, lr: 0.1.
	loss_policy_0: 0.12524
	accuracy_policy_0: 0.71891
	loss_value_0: 0.1518
	loss_policy_1: 0.03258
	accuracy_policy_1: 0.64273
	loss_value_1: 0.03172
	loss_reward_1: 0.00557
	loss_policy_2: 0.03558
	accuracy_policy_2: 0.61949
	loss_value_2: 0.03293
	loss_reward_2: 0.0052
	loss_policy_3: 0.03877
	accuracy_policy_3: 0.59066
	loss_value_3: 0.03411
	loss_reward_3: 0.00577
	loss_policy_4: 0.04159
	accuracy_policy_4: 0.57209
	loss_value_4: 0.03521
	loss_reward_4: 0.00625
	loss_policy_5: 0.04389
	accuracy_policy_5: 0.54836
	loss_value_5: 0.03626
	loss_reward_5: 0.00708
	loss_policy: 0.31764
	loss_value: 0.32203
	loss_reward: 0.02987
Optimization_Done 23000
[2024-05-08 07:23:08] [command] train weight_iter_23000.pkl 114 116
[2024-05-08 07:23:50] nn step 23100, lr: 0.1.
	loss_policy_0: 0.15998
	accuracy_policy_0: 0.65564
	loss_value_0: 0.15459
	loss_policy_1: 0.03813
	accuracy_policy_1: 0.59768
	loss_value_1: 0.03239
	loss_reward_1: 0.00513
	loss_policy_2: 0.04091
	accuracy_policy_2: 0.56996
	loss_value_2: 0.03368
	loss_reward_2: 0.00471
	loss_policy_3: 0.04395
	accuracy_policy_3: 0.54559
	loss_value_3: 0.03484
	loss_reward_3: 0.00528
	loss_policy_4: 0.04648
	accuracy_policy_4: 0.52395
	loss_value_4: 0.03591
	loss_reward_4: 0.00576
	loss_policy_5: 0.04899
	accuracy_policy_5: 0.506
	loss_value_5: 0.03704
	loss_reward_5: 0.00664
	loss_policy: 0.37845
	loss_value: 0.32846
	loss_reward: 0.02751
[2024-05-08 07:24:30] nn step 23200, lr: 0.1.
	loss_policy_0: 0.13439
	accuracy_policy_0: 0.7085
	loss_value_0: 0.15208
	loss_policy_1: 0.03479
	accuracy_policy_1: 0.62842
	loss_value_1: 0.03211
	loss_reward_1: 0.0052
	loss_policy_2: 0.03752
	accuracy_policy_2: 0.60334
	loss_value_2: 0.03332
	loss_reward_2: 0.00498
	loss_policy_3: 0.04078
	accuracy_policy_3: 0.57598
	loss_value_3: 0.03455
	loss_reward_3: 0.00534
	loss_policy_4: 0.04322
	accuracy_policy_4: 0.55787
	loss_value_4: 0.0356
	loss_reward_4: 0.00565
	loss_policy_5: 0.0461
	accuracy_policy_5: 0.53406
	loss_value_5: 0.03674
	loss_reward_5: 0.00668
	loss_policy: 0.3368
	loss_value: 0.32439
	loss_reward: 0.02785
Optimization_Done 23200
[2024-05-08 07:27:08] [command] train weight_iter_23200.pkl 115 117
[2024-05-08 07:27:50] nn step 23300, lr: 0.1.
	loss_policy_0: 0.15754
	accuracy_policy_0: 0.6484
	loss_value_0: 0.14743
	loss_policy_1: 0.03723
	accuracy_policy_1: 0.59555
	loss_value_1: 0.03094
	loss_reward_1: 0.00443
	loss_policy_2: 0.04037
	accuracy_policy_2: 0.56604
	loss_value_2: 0.03211
	loss_reward_2: 0.0041
	loss_policy_3: 0.04275
	accuracy_policy_3: 0.54635
	loss_value_3: 0.03313
	loss_reward_3: 0.00448
	loss_policy_4: 0.04492
	accuracy_policy_4: 0.52941
	loss_value_4: 0.03413
	loss_reward_4: 0.00491
	loss_policy_5: 0.04719
	accuracy_policy_5: 0.50918
	loss_value_5: 0.03517
	loss_reward_5: 0.00546
	loss_policy: 0.36999
	loss_value: 0.31291
	loss_reward: 0.02338
[2024-05-08 07:28:31] nn step 23400, lr: 0.1.
	loss_policy_0: 0.14518
	accuracy_policy_0: 0.69469
	loss_value_0: 0.15574
	loss_policy_1: 0.03631
	accuracy_policy_1: 0.63152
	loss_value_1: 0.0327
	loss_reward_1: 0.00458
	loss_policy_2: 0.03909
	accuracy_policy_2: 0.60895
	loss_value_2: 0.03391
	loss_reward_2: 0.00435
	loss_policy_3: 0.04236
	accuracy_policy_3: 0.58219
	loss_value_3: 0.03513
	loss_reward_3: 0.00475
	loss_policy_4: 0.04483
	accuracy_policy_4: 0.56207
	loss_value_4: 0.03625
	loss_reward_4: 0.00513
	loss_policy_5: 0.04764
	accuracy_policy_5: 0.53924
	loss_value_5: 0.03734
	loss_reward_5: 0.00562
	loss_policy: 0.35541
	loss_value: 0.33106
	loss_reward: 0.02444
Optimization_Done 23400
[2024-05-08 07:31:07] [command] train weight_iter_23400.pkl 116 118
[2024-05-08 07:31:49] nn step 23500, lr: 0.1.
	loss_policy_0: 0.16934
	accuracy_policy_0: 0.65012
	loss_value_0: 0.161
	loss_policy_1: 0.04116
	accuracy_policy_1: 0.58803
	loss_value_1: 0.03378
	loss_reward_1: 0.00584
	loss_policy_2: 0.04427
	accuracy_policy_2: 0.5566
	loss_value_2: 0.03518
	loss_reward_2: 0.00556
	loss_policy_3: 0.04776
	accuracy_policy_3: 0.53311
	loss_value_3: 0.03645
	loss_reward_3: 0.00607
	loss_policy_4: 0.05065
	accuracy_policy_4: 0.51045
	loss_value_4: 0.03759
	loss_reward_4: 0.0067
	loss_policy_5: 0.05324
	accuracy_policy_5: 0.49076
	loss_value_5: 0.03862
	loss_reward_5: 0.00767
	loss_policy: 0.40642
	loss_value: 0.34262
	loss_reward: 0.03185
[2024-05-08 07:32:30] nn step 23600, lr: 0.1.
	loss_policy_0: 0.15617
	accuracy_policy_0: 0.69221
	loss_value_0: 0.16395
	loss_policy_1: 0.03942
	accuracy_policy_1: 0.61895
	loss_value_1: 0.0344
	loss_reward_1: 0.00592
	loss_policy_2: 0.0432
	accuracy_policy_2: 0.58471
	loss_value_2: 0.03586
	loss_reward_2: 0.00577
	loss_policy_3: 0.04685
	accuracy_policy_3: 0.55701
	loss_value_3: 0.03729
	loss_reward_3: 0.00611
	loss_policy_4: 0.05015
	accuracy_policy_4: 0.5318
	loss_value_4: 0.03867
	loss_reward_4: 0.00661
	loss_policy_5: 0.0531
	accuracy_policy_5: 0.51129
	loss_value_5: 0.03976
	loss_reward_5: 0.0077
	loss_policy: 0.38889
	loss_value: 0.34992
	loss_reward: 0.0321
Optimization_Done 23600
[2024-05-08 07:35:07] [command] train weight_iter_23600.pkl 117 119
[2024-05-08 07:35:49] nn step 23700, lr: 0.1.
	loss_policy_0: 0.14162
	accuracy_policy_0: 0.68396
	loss_value_0: 0.15282
	loss_policy_1: 0.03572
	accuracy_policy_1: 0.61252
	loss_value_1: 0.03203
	loss_reward_1: 0.00629
	loss_policy_2: 0.03922
	accuracy_policy_2: 0.57738
	loss_value_2: 0.03331
	loss_reward_2: 0.00587
	loss_policy_3: 0.04269
	accuracy_policy_3: 0.54762
	loss_value_3: 0.03443
	loss_reward_3: 0.00646
	loss_policy_4: 0.04509
	accuracy_policy_4: 0.5283
	loss_value_4: 0.03566
	loss_reward_4: 0.00703
	loss_policy_5: 0.04754
	accuracy_policy_5: 0.51309
	loss_value_5: 0.03671
	loss_reward_5: 0.00788
	loss_policy: 0.35189
	loss_value: 0.32495
	loss_reward: 0.03353
[2024-05-08 07:36:30] nn step 23800, lr: 0.1.
	loss_policy_0: 0.13247
	accuracy_policy_0: 0.71289
	loss_value_0: 0.15436
	loss_policy_1: 0.03457
	accuracy_policy_1: 0.63533
	loss_value_1: 0.0324
	loss_reward_1: 0.00635
	loss_policy_2: 0.03865
	accuracy_policy_2: 0.59855
	loss_value_2: 0.03377
	loss_reward_2: 0.00612
	loss_policy_3: 0.04197
	accuracy_policy_3: 0.57021
	loss_value_3: 0.03491
	loss_reward_3: 0.00658
	loss_policy_4: 0.0449
	accuracy_policy_4: 0.54717
	loss_value_4: 0.03617
	loss_reward_4: 0.00701
	loss_policy_5: 0.04761
	accuracy_policy_5: 0.52682
	loss_value_5: 0.03739
	loss_reward_5: 0.00815
	loss_policy: 0.34017
	loss_value: 0.32901
	loss_reward: 0.03421
Optimization_Done 23800
[2024-05-08 07:38:41] [command] train weight_iter_23800.pkl 118 120
[2024-05-08 07:39:23] nn step 23900, lr: 0.1.
	loss_policy_0: 0.13851
	accuracy_policy_0: 0.70521
	loss_value_0: 0.15308
	loss_policy_1: 0.03459
	accuracy_policy_1: 0.63916
	loss_value_1: 0.03203
	loss_reward_1: 0.0058
	loss_policy_2: 0.038
	accuracy_policy_2: 0.6065
	loss_value_2: 0.03341
	loss_reward_2: 0.00552
	loss_policy_3: 0.04115
	accuracy_policy_3: 0.57879
	loss_value_3: 0.03464
	loss_reward_3: 0.00613
	loss_policy_4: 0.0438
	accuracy_policy_4: 0.55688
	loss_value_4: 0.03569
	loss_reward_4: 0.00673
	loss_policy_5: 0.04611
	accuracy_policy_5: 0.53953
	loss_value_5: 0.03687
	loss_reward_5: 0.00769
	loss_policy: 0.34217
	loss_value: 0.32572
	loss_reward: 0.03186
[2024-05-08 07:40:04] nn step 24000, lr: 0.1.
	loss_policy_0: 0.12194
	accuracy_policy_0: 0.74041
	loss_value_0: 0.15549
	loss_policy_1: 0.03246
	accuracy_policy_1: 0.66568
	loss_value_1: 0.03269
	loss_reward_1: 0.0062
	loss_policy_2: 0.03607
	accuracy_policy_2: 0.63596
	loss_value_2: 0.03411
	loss_reward_2: 0.00587
	loss_policy_3: 0.03937
	accuracy_policy_3: 0.59879
	loss_value_3: 0.03545
	loss_reward_3: 0.0063
	loss_policy_4: 0.04257
	accuracy_policy_4: 0.57836
	loss_value_4: 0.03649
	loss_reward_4: 0.00687
	loss_policy_5: 0.04489
	accuracy_policy_5: 0.56021
	loss_value_5: 0.0375
	loss_reward_5: 0.00792
	loss_policy: 0.3173
	loss_value: 0.33174
	loss_reward: 0.03316
Optimization_Done 24000
[2024-05-08 07:42:43] [command] train weight_iter_24000.pkl 119 121
[2024-05-08 07:43:25] nn step 24100, lr: 0.1.
	loss_policy_0: 0.16161
	accuracy_policy_0: 0.67785
	loss_value_0: 0.16718
	loss_policy_1: 0.03849
	accuracy_policy_1: 0.62336
	loss_value_1: 0.03492
	loss_reward_1: 0.00552
	loss_policy_2: 0.04167
	accuracy_policy_2: 0.59525
	loss_value_2: 0.03626
	loss_reward_2: 0.00542
	loss_policy_3: 0.04483
	accuracy_policy_3: 0.57098
	loss_value_3: 0.03757
	loss_reward_3: 0.00588
	loss_policy_4: 0.0474
	accuracy_policy_4: 0.54959
	loss_value_4: 0.03876
	loss_reward_4: 0.00643
	loss_policy_5: 0.04984
	accuracy_policy_5: 0.53012
	loss_value_5: 0.04
	loss_reward_5: 0.0071
	loss_policy: 0.38384
	loss_value: 0.35469
	loss_reward: 0.03034
[2024-05-08 07:44:06] nn step 24200, lr: 0.1.
	loss_policy_0: 0.13034
	accuracy_policy_0: 0.72477
	loss_value_0: 0.15878
	loss_policy_1: 0.0326
	accuracy_policy_1: 0.65881
	loss_value_1: 0.03326
	loss_reward_1: 0.00511
	loss_policy_2: 0.03597
	accuracy_policy_2: 0.62896
	loss_value_2: 0.03463
	loss_reward_2: 0.00503
	loss_policy_3: 0.03902
	accuracy_policy_3: 0.60697
	loss_value_3: 0.0359
	loss_reward_3: 0.00545
	loss_policy_4: 0.04182
	accuracy_policy_4: 0.58123
	loss_value_4: 0.03699
	loss_reward_4: 0.00576
	loss_policy_5: 0.04418
	accuracy_policy_5: 0.5607
	loss_value_5: 0.03803
	loss_reward_5: 0.00663
	loss_policy: 0.32394
	loss_value: 0.3376
	loss_reward: 0.02797
Optimization_Done 24200
[2024-05-08 07:46:44] [command] train weight_iter_24200.pkl 120 122
[2024-05-08 07:47:26] nn step 24300, lr: 0.1.
	loss_policy_0: 0.16815
	accuracy_policy_0: 0.66545
	loss_value_0: 0.1817
	loss_policy_1: 0.03964
	accuracy_policy_1: 0.6102
	loss_value_1: 0.03812
	loss_reward_1: 0.0054
	loss_policy_2: 0.04253
	accuracy_policy_2: 0.59078
	loss_value_2: 0.03956
	loss_reward_2: 0.00537
	loss_policy_3: 0.04611
	accuracy_policy_3: 0.56078
	loss_value_3: 0.04107
	loss_reward_3: 0.00574
	loss_policy_4: 0.04846
	accuracy_policy_4: 0.54584
	loss_value_4: 0.04225
	loss_reward_4: 0.00626
	loss_policy_5: 0.05136
	accuracy_policy_5: 0.52578
	loss_value_5: 0.04336
	loss_reward_5: 0.00713
	loss_policy: 0.39625
	loss_value: 0.38605
	loss_reward: 0.02991
[2024-05-08 07:48:07] nn step 24400, lr: 0.1.
	loss_policy_0: 0.141
	accuracy_policy_0: 0.70928
	loss_value_0: 0.17136
	loss_policy_1: 0.03442
	accuracy_policy_1: 0.64834
	loss_value_1: 0.03585
	loss_reward_1: 0.00518
	loss_policy_2: 0.03753
	accuracy_policy_2: 0.62559
	loss_value_2: 0.03718
	loss_reward_2: 0.00523
	loss_policy_3: 0.04102
	accuracy_policy_3: 0.5948
	loss_value_3: 0.03848
	loss_reward_3: 0.00554
	loss_policy_4: 0.04346
	accuracy_policy_4: 0.57723
	loss_value_4: 0.0397
	loss_reward_4: 0.00596
	loss_policy_5: 0.04604
	accuracy_policy_5: 0.55912
	loss_value_5: 0.04098
	loss_reward_5: 0.00685
	loss_policy: 0.34347
	loss_value: 0.36355
	loss_reward: 0.02877
Optimization_Done 24400
[2024-05-08 07:50:44] [command] train weight_iter_24400.pkl 121 123
[2024-05-08 07:51:31] nn step 24500, lr: 0.1.
	loss_policy_0: 0.15584
	accuracy_policy_0: 0.6823
	loss_value_0: 0.17276
	loss_policy_1: 0.03804
	accuracy_policy_1: 0.61889
	loss_value_1: 0.03633
	loss_reward_1: 0.00582
	loss_policy_2: 0.04162
	accuracy_policy_2: 0.59029
	loss_value_2: 0.03767
	loss_reward_2: 0.00552
	loss_policy_3: 0.04559
	accuracy_policy_3: 0.55684
	loss_value_3: 0.03896
	loss_reward_3: 0.00582
	loss_policy_4: 0.04826
	accuracy_policy_4: 0.5343
	loss_value_4: 0.04015
	loss_reward_4: 0.00655
	loss_policy_5: 0.05063
	accuracy_policy_5: 0.51625
	loss_value_5: 0.04127
	loss_reward_5: 0.00756
	loss_policy: 0.37999
	loss_value: 0.36714
	loss_reward: 0.03127
[2024-05-08 07:52:14] nn step 24600, lr: 0.1.
	loss_policy_0: 0.14471
	accuracy_policy_0: 0.7118
	loss_value_0: 0.17704
	loss_policy_1: 0.03675
	accuracy_policy_1: 0.64486
	loss_value_1: 0.03725
	loss_reward_1: 0.00632
	loss_policy_2: 0.04072
	accuracy_policy_2: 0.61156
	loss_value_2: 0.03877
	loss_reward_2: 0.006
	loss_policy_3: 0.04432
	accuracy_policy_3: 0.58535
	loss_value_3: 0.04012
	loss_reward_3: 0.00618
	loss_policy_4: 0.04715
	accuracy_policy_4: 0.56447
	loss_value_4: 0.04145
	loss_reward_4: 0.00669
	loss_policy_5: 0.05019
	accuracy_policy_5: 0.54059
	loss_value_5: 0.04268
	loss_reward_5: 0.00779
	loss_policy: 0.36384
	loss_value: 0.3773
	loss_reward: 0.03299
Optimization_Done 24600
[2024-05-08 07:54:47] [command] train weight_iter_24600.pkl 122 124
[2024-05-08 07:55:33] nn step 24700, lr: 0.1.
	loss_policy_0: 0.15721
	accuracy_policy_0: 0.68885
	loss_value_0: 0.17565
	loss_policy_1: 0.03838
	accuracy_policy_1: 0.62527
	loss_value_1: 0.03691
	loss_reward_1: 0.00578
	loss_policy_2: 0.04197
	accuracy_policy_2: 0.59582
	loss_value_2: 0.03832
	loss_reward_2: 0.00556
	loss_policy_3: 0.04504
	accuracy_policy_3: 0.56938
	loss_value_3: 0.03968
	loss_reward_3: 0.00617
	loss_policy_4: 0.04787
	accuracy_policy_4: 0.54797
	loss_value_4: 0.04088
	loss_reward_4: 0.00646
	loss_policy_5: 0.05025
	accuracy_policy_5: 0.52895
	loss_value_5: 0.04192
	loss_reward_5: 0.0074
	loss_policy: 0.38072
	loss_value: 0.37337
	loss_reward: 0.03137
[2024-05-08 07:56:16] nn step 24800, lr: 0.1.
	loss_policy_0: 0.1277
	accuracy_policy_0: 0.72936
	loss_value_0: 0.16351
	loss_policy_1: 0.03323
	accuracy_policy_1: 0.65445
	loss_value_1: 0.03431
	loss_reward_1: 0.00553
	loss_policy_2: 0.03665
	accuracy_policy_2: 0.62662
	loss_value_2: 0.03573
	loss_reward_2: 0.0054
	loss_policy_3: 0.03993
	accuracy_policy_3: 0.59863
	loss_value_3: 0.03696
	loss_reward_3: 0.00577
	loss_policy_4: 0.0426
	accuracy_policy_4: 0.57396
	loss_value_4: 0.03815
	loss_reward_4: 0.00612
	loss_policy_5: 0.04509
	accuracy_policy_5: 0.55549
	loss_value_5: 0.03924
	loss_reward_5: 0.00694
	loss_policy: 0.32521
	loss_value: 0.3479
	loss_reward: 0.02976
Optimization_Done 24800
[2024-05-08 07:58:50] [command] train weight_iter_24800.pkl 123 125
[2024-05-08 07:59:35] nn step 24900, lr: 0.1.
	loss_policy_0: 0.14796
	accuracy_policy_0: 0.70043
	loss_value_0: 0.16848
	loss_policy_1: 0.03628
	accuracy_policy_1: 0.64088
	loss_value_1: 0.03547
	loss_reward_1: 0.0056
	loss_policy_2: 0.04017
	accuracy_policy_2: 0.61234
	loss_value_2: 0.03687
	loss_reward_2: 0.00539
	loss_policy_3: 0.04354
	accuracy_policy_3: 0.58127
	loss_value_3: 0.03795
	loss_reward_3: 0.00576
	loss_policy_4: 0.04633
	accuracy_policy_4: 0.56201
	loss_value_4: 0.03903
	loss_reward_4: 0.00623
	loss_policy_5: 0.04897
	accuracy_policy_5: 0.53646
	loss_value_5: 0.04007
	loss_reward_5: 0.00706
	loss_policy: 0.36326
	loss_value: 0.35787
	loss_reward: 0.03003
[2024-05-08 08:00:20] nn step 25000, lr: 0.1.
	loss_policy_0: 0.12949
	accuracy_policy_0: 0.73664
	loss_value_0: 0.16755
	loss_policy_1: 0.03401
	accuracy_policy_1: 0.6652
	loss_value_1: 0.03519
	loss_reward_1: 0.00568
	loss_policy_2: 0.03743
	accuracy_policy_2: 0.6352
	loss_value_2: 0.03654
	loss_reward_2: 0.00513
	loss_policy_3: 0.04101
	accuracy_policy_3: 0.60775
	loss_value_3: 0.03779
	loss_reward_3: 0.00554
	loss_policy_4: 0.04376
	accuracy_policy_4: 0.5825
	loss_value_4: 0.03881
	loss_reward_4: 0.00613
	loss_policy_5: 0.04697
	accuracy_policy_5: 0.55857
	loss_value_5: 0.03987
	loss_reward_5: 0.00719
	loss_policy: 0.33266
	loss_value: 0.35575
	loss_reward: 0.02968
Optimization_Done 25000
[2024-05-08 08:02:47] [command] train weight_iter_25000.pkl 124 126
[2024-05-08 08:03:30] nn step 25100, lr: 0.1.
	loss_policy_0: 0.16275
	accuracy_policy_0: 0.67523
	loss_value_0: 0.17054
	loss_policy_1: 0.03867
	accuracy_policy_1: 0.62293
	loss_value_1: 0.03563
	loss_reward_1: 0.005
	loss_policy_2: 0.04194
	accuracy_policy_2: 0.59658
	loss_value_2: 0.03711
	loss_reward_2: 0.00496
	loss_policy_3: 0.04518
	accuracy_policy_3: 0.57189
	loss_value_3: 0.03849
	loss_reward_3: 0.00543
	loss_policy_4: 0.0476
	accuracy_policy_4: 0.55078
	loss_value_4: 0.03965
	loss_reward_4: 0.00595
	loss_policy_5: 0.05044
	accuracy_policy_5: 0.52816
	loss_value_5: 0.04077
	loss_reward_5: 0.00694
	loss_policy: 0.38658
	loss_value: 0.36217
	loss_reward: 0.02829
[2024-05-08 08:04:11] nn step 25200, lr: 0.1.
	loss_policy_0: 0.14417
	accuracy_policy_0: 0.72031
	loss_value_0: 0.17317
	loss_policy_1: 0.03656
	accuracy_policy_1: 0.65102
	loss_value_1: 0.0362
	loss_reward_1: 0.00515
	loss_policy_2: 0.04047
	accuracy_policy_2: 0.62107
	loss_value_2: 0.03777
	loss_reward_2: 0.00499
	loss_policy_3: 0.04349
	accuracy_policy_3: 0.59602
	loss_value_3: 0.03918
	loss_reward_3: 0.00552
	loss_policy_4: 0.04688
	accuracy_policy_4: 0.57268
	loss_value_4: 0.04026
	loss_reward_4: 0.00618
	loss_policy_5: 0.04953
	accuracy_policy_5: 0.54764
	loss_value_5: 0.04137
	loss_reward_5: 0.0072
	loss_policy: 0.3611
	loss_value: 0.36795
	loss_reward: 0.02904
Optimization_Done 25200
[2024-05-08 08:06:48] [command] train weight_iter_25200.pkl 125 127
[2024-05-08 08:07:31] nn step 25300, lr: 0.1.
	loss_policy_0: 0.16066
	accuracy_policy_0: 0.68404
	loss_value_0: 0.17696
	loss_policy_1: 0.0392
	accuracy_policy_1: 0.62723
	loss_value_1: 0.03714
	loss_reward_1: 0.00613
	loss_policy_2: 0.04319
	accuracy_policy_2: 0.59801
	loss_value_2: 0.03863
	loss_reward_2: 0.00568
	loss_policy_3: 0.04621
	accuracy_policy_3: 0.57623
	loss_value_3: 0.03993
	loss_reward_3: 0.00646
	loss_policy_4: 0.04966
	accuracy_policy_4: 0.54607
	loss_value_4: 0.04124
	loss_reward_4: 0.00709
	loss_policy_5: 0.05277
	accuracy_policy_5: 0.52568
	loss_value_5: 0.04257
	loss_reward_5: 0.00821
	loss_policy: 0.39168
	loss_value: 0.37647
	loss_reward: 0.03357
[2024-05-08 08:08:15] nn step 25400, lr: 0.1.
	loss_policy_0: 0.1429
	accuracy_policy_0: 0.72258
	loss_value_0: 0.17588
	loss_policy_1: 0.03659
	accuracy_policy_1: 0.65193
	loss_value_1: 0.03694
	loss_reward_1: 0.006
	loss_policy_2: 0.04047
	accuracy_policy_2: 0.62096
	loss_value_2: 0.0384
	loss_reward_2: 0.00586
	loss_policy_3: 0.04421
	accuracy_policy_3: 0.59449
	loss_value_3: 0.03964
	loss_reward_3: 0.00624
	loss_policy_4: 0.0477
	accuracy_policy_4: 0.56918
	loss_value_4: 0.04076
	loss_reward_4: 0.00684
	loss_policy_5: 0.05073
	accuracy_policy_5: 0.54336
	loss_value_5: 0.04191
	loss_reward_5: 0.00799
	loss_policy: 0.36259
	loss_value: 0.37355
	loss_reward: 0.03293
Optimization_Done 25400
[2024-05-08 08:10:43] [command] train weight_iter_25400.pkl 126 128
[2024-05-08 08:11:28] nn step 25500, lr: 0.1.
	loss_policy_0: 0.14577
	accuracy_policy_0: 0.69355
	loss_value_0: 0.16307
	loss_policy_1: 0.03581
	accuracy_policy_1: 0.6348
	loss_value_1: 0.03394
	loss_reward_1: 0.00521
	loss_policy_2: 0.039
	accuracy_policy_2: 0.60775
	loss_value_2: 0.03514
	loss_reward_2: 0.0051
	loss_policy_3: 0.04206
	accuracy_policy_3: 0.58168
	loss_value_3: 0.03627
	loss_reward_3: 0.00549
	loss_policy_4: 0.04492
	accuracy_policy_4: 0.55572
	loss_value_4: 0.03739
	loss_reward_4: 0.00564
	loss_policy_5: 0.04726
	accuracy_policy_5: 0.53471
	loss_value_5: 0.03834
	loss_reward_5: 0.00672
	loss_policy: 0.35481
	loss_value: 0.34415
	loss_reward: 0.02816
[2024-05-08 08:12:12] nn step 25600, lr: 0.1.
	loss_policy_0: 0.13251
	accuracy_policy_0: 0.72969
	loss_value_0: 0.16547
	loss_policy_1: 0.03423
	accuracy_policy_1: 0.66354
	loss_value_1: 0.03472
	loss_reward_1: 0.00564
	loss_policy_2: 0.03807
	accuracy_policy_2: 0.63248
	loss_value_2: 0.03605
	loss_reward_2: 0.00538
	loss_policy_3: 0.04128
	accuracy_policy_3: 0.60627
	loss_value_3: 0.03705
	loss_reward_3: 0.0057
	loss_policy_4: 0.04431
	accuracy_policy_4: 0.58059
	loss_value_4: 0.03812
	loss_reward_4: 0.00597
	loss_policy_5: 0.04682
	accuracy_policy_5: 0.56084
	loss_value_5: 0.03927
	loss_reward_5: 0.00735
	loss_policy: 0.3372
	loss_value: 0.35067
	loss_reward: 0.03004
Optimization_Done 25600
[2024-05-08 08:14:57] [command] train weight_iter_25600.pkl 127 129
[2024-05-08 08:15:43] nn step 25700, lr: 0.1.
	loss_policy_0: 0.13465
	accuracy_policy_0: 0.70842
	loss_value_0: 0.16417
	loss_policy_1: 0.03364
	accuracy_policy_1: 0.64822
	loss_value_1: 0.0343
	loss_reward_1: 0.0056
	loss_policy_2: 0.0369
	accuracy_policy_2: 0.62006
	loss_value_2: 0.03563
	loss_reward_2: 0.00524
	loss_policy_3: 0.04015
	accuracy_policy_3: 0.59072
	loss_value_3: 0.03686
	loss_reward_3: 0.00581
	loss_policy_4: 0.04287
	accuracy_policy_4: 0.56707
	loss_value_4: 0.0378
	loss_reward_4: 0.00621
	loss_policy_5: 0.04565
	accuracy_policy_5: 0.54213
	loss_value_5: 0.03867
	loss_reward_5: 0.00718
	loss_policy: 0.33386
	loss_value: 0.34743
	loss_reward: 0.03004
[2024-05-08 08:16:27] nn step 25800, lr: 0.1.
	loss_policy_0: 0.12115
	accuracy_policy_0: 0.74994
	loss_value_0: 0.17082
	loss_policy_1: 0.03233
	accuracy_policy_1: 0.67355
	loss_value_1: 0.03564
	loss_reward_1: 0.00597
	loss_policy_2: 0.03621
	accuracy_policy_2: 0.64373
	loss_value_2: 0.03701
	loss_reward_2: 0.00557
	loss_policy_3: 0.03969
	accuracy_policy_3: 0.61785
	loss_value_3: 0.03814
	loss_reward_3: 0.006
	loss_policy_4: 0.0427
	accuracy_policy_4: 0.59248
	loss_value_4: 0.03938
	loss_reward_4: 0.00642
	loss_policy_5: 0.04573
	accuracy_policy_5: 0.56896
	loss_value_5: 0.04057
	loss_reward_5: 0.00771
	loss_policy: 0.31782
	loss_value: 0.36155
	loss_reward: 0.03166
Optimization_Done 25800
[2024-05-08 08:18:57] [command] train weight_iter_25800.pkl 128 130
[2024-05-08 08:19:42] nn step 25900, lr: 0.1.
	loss_policy_0: 0.14885
	accuracy_policy_0: 0.68266
	loss_value_0: 0.16793
	loss_policy_1: 0.03583
	accuracy_policy_1: 0.62668
	loss_value_1: 0.03514
	loss_reward_1: 0.00539
	loss_policy_2: 0.0395
	accuracy_policy_2: 0.59812
	loss_value_2: 0.03657
	loss_reward_2: 0.00532
	loss_policy_3: 0.04253
	accuracy_policy_3: 0.57311
	loss_value_3: 0.0378
	loss_reward_3: 0.00564
	loss_policy_4: 0.04558
	accuracy_policy_4: 0.5493
	loss_value_4: 0.03892
	loss_reward_4: 0.00594
	loss_policy_5: 0.04825
	accuracy_policy_5: 0.52451
	loss_value_5: 0.04018
	loss_reward_5: 0.00691
	loss_policy: 0.36054
	loss_value: 0.35654
	loss_reward: 0.0292
[2024-05-08 08:20:26] nn step 26000, lr: 0.1.
	loss_policy_0: 0.12533
	accuracy_policy_0: 0.72609
	loss_value_0: 0.16267
	loss_policy_1: 0.03185
	accuracy_policy_1: 0.66078
	loss_value_1: 0.03404
	loss_reward_1: 0.0052
	loss_policy_2: 0.03509
	accuracy_policy_2: 0.63281
	loss_value_2: 0.03532
	loss_reward_2: 0.00508
	loss_policy_3: 0.03845
	accuracy_policy_3: 0.60627
	loss_value_3: 0.03647
	loss_reward_3: 0.00548
	loss_policy_4: 0.04131
	accuracy_policy_4: 0.58504
	loss_value_4: 0.0376
	loss_reward_4: 0.0058
	loss_policy_5: 0.04427
	accuracy_policy_5: 0.55598
	loss_value_5: 0.03883
	loss_reward_5: 0.00679
	loss_policy: 0.3163
	loss_value: 0.34492
	loss_reward: 0.02835
Optimization_Done 26000
[2024-05-08 08:23:10] [command] train weight_iter_26000.pkl 129 131
[2024-05-08 08:23:55] nn step 26100, lr: 0.1.
	loss_policy_0: 0.15894
	accuracy_policy_0: 0.671
	loss_value_0: 0.16894
	loss_policy_1: 0.03858
	accuracy_policy_1: 0.61311
	loss_value_1: 0.03559
	loss_reward_1: 0.00614
	loss_policy_2: 0.04242
	accuracy_policy_2: 0.58018
	loss_value_2: 0.03693
	loss_reward_2: 0.00582
	loss_policy_3: 0.04592
	accuracy_policy_3: 0.5576
	loss_value_3: 0.03821
	loss_reward_3: 0.00637
	loss_policy_4: 0.04942
	accuracy_policy_4: 0.52953
	loss_value_4: 0.03944
	loss_reward_4: 0.00704
	loss_policy_5: 0.05261
	accuracy_policy_5: 0.50311
	loss_value_5: 0.04078
	loss_reward_5: 0.00805
	loss_policy: 0.3879
	loss_value: 0.35989
	loss_reward: 0.03342
[2024-05-08 08:24:37] nn step 26200, lr: 0.1.
	loss_policy_0: 0.13897
	accuracy_policy_0: 0.71514
	loss_value_0: 0.16703
	loss_policy_1: 0.03546
	accuracy_policy_1: 0.64527
	loss_value_1: 0.03518
	loss_reward_1: 0.00615
	loss_policy_2: 0.03979
	accuracy_policy_2: 0.60928
	loss_value_2: 0.03657
	loss_reward_2: 0.00594
	loss_policy_3: 0.04348
	accuracy_policy_3: 0.5802
	loss_value_3: 0.03777
	loss_reward_3: 0.00617
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.55725
	loss_value_4: 0.03905
	loss_reward_4: 0.00678
	loss_policy_5: 0.04978
	accuracy_policy_5: 0.53125
	loss_value_5: 0.04035
	loss_reward_5: 0.00798
	loss_policy: 0.35418
	loss_value: 0.35595
	loss_reward: 0.03302
Optimization_Done 26200
[2024-05-08 08:27:20] [command] train weight_iter_26200.pkl 130 132
[2024-05-08 08:28:05] nn step 26300, lr: 0.1.
	loss_policy_0: 0.14043
	accuracy_policy_0: 0.68785
	loss_value_0: 0.15618
	loss_policy_1: 0.03491
	accuracy_policy_1: 0.62119
	loss_value_1: 0.0325
	loss_reward_1: 0.00538
	loss_policy_2: 0.03808
	accuracy_policy_2: 0.59025
	loss_value_2: 0.03378
	loss_reward_2: 0.00514
	loss_policy_3: 0.04103
	accuracy_policy_3: 0.56229
	loss_value_3: 0.03494
	loss_reward_3: 0.00565
	loss_policy_4: 0.04375
	accuracy_policy_4: 0.5376
	loss_value_4: 0.03607
	loss_reward_4: 0.00614
	loss_policy_5: 0.04635
	accuracy_policy_5: 0.51141
	loss_value_5: 0.0373
	loss_reward_5: 0.00717
	loss_policy: 0.34457
	loss_value: 0.33077
	loss_reward: 0.02948
[2024-05-08 08:28:49] nn step 26400, lr: 0.1.
	loss_policy_0: 0.12151
	accuracy_policy_0: 0.73072
	loss_value_0: 0.1544
	loss_policy_1: 0.03193
	accuracy_policy_1: 0.65096
	loss_value_1: 0.0321
	loss_reward_1: 0.00537
	loss_policy_2: 0.03537
	accuracy_policy_2: 0.62133
	loss_value_2: 0.03335
	loss_reward_2: 0.00518
	loss_policy_3: 0.0387
	accuracy_policy_3: 0.58818
	loss_value_3: 0.03451
	loss_reward_3: 0.00552
	loss_policy_4: 0.04173
	accuracy_policy_4: 0.56287
	loss_value_4: 0.03572
	loss_reward_4: 0.00597
	loss_policy_5: 0.04474
	accuracy_policy_5: 0.53523
	loss_value_5: 0.03683
	loss_reward_5: 0.00711
	loss_policy: 0.31398
	loss_value: 0.32692
	loss_reward: 0.02914
Optimization_Done 26400
[2024-05-08 08:31:20] [command] train weight_iter_26400.pkl 131 133
[2024-05-08 08:32:04] nn step 26500, lr: 0.1.
	loss_policy_0: 0.13034
	accuracy_policy_0: 0.69719
	loss_value_0: 0.15284
	loss_policy_1: 0.03267
	accuracy_policy_1: 0.62824
	loss_value_1: 0.03202
	loss_reward_1: 0.00508
	loss_policy_2: 0.036
	accuracy_policy_2: 0.5966
	loss_value_2: 0.03307
	loss_reward_2: 0.00492
	loss_policy_3: 0.03897
	accuracy_policy_3: 0.56949
	loss_value_3: 0.03421
	loss_reward_3: 0.00537
	loss_policy_4: 0.04161
	accuracy_policy_4: 0.54186
	loss_value_4: 0.03523
	loss_reward_4: 0.00594
	loss_policy_5: 0.04391
	accuracy_policy_5: 0.52074
	loss_value_5: 0.0364
	loss_reward_5: 0.00692
	loss_policy: 0.32351
	loss_value: 0.32377
	loss_reward: 0.02823
[2024-05-08 08:32:47] nn step 26600, lr: 0.1.
	loss_policy_0: 0.11831
	accuracy_policy_0: 0.73064
	loss_value_0: 0.15701
	loss_policy_1: 0.031
	accuracy_policy_1: 0.65545
	loss_value_1: 0.03278
	loss_reward_1: 0.00527
	loss_policy_2: 0.03435
	accuracy_policy_2: 0.62715
	loss_value_2: 0.03412
	loss_reward_2: 0.00508
	loss_policy_3: 0.03709
	accuracy_policy_3: 0.59842
	loss_value_3: 0.0353
	loss_reward_3: 0.00557
	loss_policy_4: 0.0402
	accuracy_policy_4: 0.57594
	loss_value_4: 0.03641
	loss_reward_4: 0.00599
	loss_policy_5: 0.04302
	accuracy_policy_5: 0.5475
	loss_value_5: 0.03762
	loss_reward_5: 0.00705
	loss_policy: 0.30397
	loss_value: 0.33324
	loss_reward: 0.02895
Optimization_Done 26600
[2024-05-08 08:35:33] [command] train weight_iter_26600.pkl 132 134
[2024-05-08 08:36:18] nn step 26700, lr: 0.1.
	loss_policy_0: 0.1517
	accuracy_policy_0: 0.6627
	loss_value_0: 0.16169
	loss_policy_1: 0.03602
	accuracy_policy_1: 0.61096
	loss_value_1: 0.03378
	loss_reward_1: 0.00467
	loss_policy_2: 0.03885
	accuracy_policy_2: 0.58979
	loss_value_2: 0.03505
	loss_reward_2: 0.00452
	loss_policy_3: 0.04154
	accuracy_policy_3: 0.56305
	loss_value_3: 0.03619
	loss_reward_3: 0.00476
	loss_policy_4: 0.04406
	accuracy_policy_4: 0.53982
	loss_value_4: 0.03716
	loss_reward_4: 0.00525
	loss_policy_5: 0.04646
	accuracy_policy_5: 0.51623
	loss_value_5: 0.03831
	loss_reward_5: 0.00608
	loss_policy: 0.35863
	loss_value: 0.34219
	loss_reward: 0.02528
[2024-05-08 08:37:02] nn step 26800, lr: 0.1.
	loss_policy_0: 0.12815
	accuracy_policy_0: 0.71758
	loss_value_0: 0.16312
	loss_policy_1: 0.03214
	accuracy_policy_1: 0.6484
	loss_value_1: 0.03403
	loss_reward_1: 0.00479
	loss_policy_2: 0.03557
	accuracy_policy_2: 0.62381
	loss_value_2: 0.03522
	loss_reward_2: 0.00472
	loss_policy_3: 0.03848
	accuracy_policy_3: 0.5948
	loss_value_3: 0.0363
	loss_reward_3: 0.00503
	loss_policy_4: 0.04107
	accuracy_policy_4: 0.57646
	loss_value_4: 0.03736
	loss_reward_4: 0.00535
	loss_policy_5: 0.0439
	accuracy_policy_5: 0.54984
	loss_value_5: 0.03845
	loss_reward_5: 0.00628
	loss_policy: 0.31931
	loss_value: 0.34448
	loss_reward: 0.02617
Optimization_Done 26800
[2024-05-08 08:39:42] [command] train weight_iter_26800.pkl 133 135
[2024-05-08 08:40:24] nn step 26900, lr: 0.1.
	loss_policy_0: 0.15931
	accuracy_policy_0: 0.67607
	loss_value_0: 0.18353
	loss_policy_1: 0.03888
	accuracy_policy_1: 0.61752
	loss_value_1: 0.03831
	loss_reward_1: 0.00584
	loss_policy_2: 0.04236
	accuracy_policy_2: 0.58979
	loss_value_2: 0.0398
	loss_reward_2: 0.00583
	loss_policy_3: 0.04601
	accuracy_policy_3: 0.55947
	loss_value_3: 0.04123
	loss_reward_3: 0.00648
	loss_policy_4: 0.04893
	accuracy_policy_4: 0.53943
	loss_value_4: 0.04238
	loss_reward_4: 0.007
	loss_policy_5: 0.05201
	accuracy_policy_5: 0.51477
	loss_value_5: 0.0435
	loss_reward_5: 0.00778
	loss_policy: 0.38751
	loss_value: 0.38875
	loss_reward: 0.03293
[2024-05-08 08:41:06] nn step 27000, lr: 0.1.
	loss_policy_0: 0.1361
	accuracy_policy_0: 0.72039
	loss_value_0: 0.17451
	loss_policy_1: 0.03489
	accuracy_policy_1: 0.64691
	loss_value_1: 0.03659
	loss_reward_1: 0.00566
	loss_policy_2: 0.03835
	accuracy_policy_2: 0.62111
	loss_value_2: 0.0378
	loss_reward_2: 0.00558
	loss_policy_3: 0.04197
	accuracy_policy_3: 0.59277
	loss_value_3: 0.03897
	loss_reward_3: 0.00613
	loss_policy_4: 0.04472
	accuracy_policy_4: 0.5733
	loss_value_4: 0.04016
	loss_reward_4: 0.00663
	loss_policy_5: 0.04777
	accuracy_policy_5: 0.54541
	loss_value_5: 0.04158
	loss_reward_5: 0.00772
	loss_policy: 0.3438
	loss_value: 0.36961
	loss_reward: 0.03172
Optimization_Done 27000
[2024-05-08 08:43:42] [command] train weight_iter_27000.pkl 134 136
[2024-05-08 08:44:25] nn step 27100, lr: 0.1.
	loss_policy_0: 0.15461
	accuracy_policy_0: 0.67545
	loss_value_0: 0.17442
	loss_policy_1: 0.03859
	accuracy_policy_1: 0.60742
	loss_value_1: 0.03645
	loss_reward_1: 0.00613
	loss_policy_2: 0.04232
	accuracy_policy_2: 0.57426
	loss_value_2: 0.0378
	loss_reward_2: 0.0059
	loss_policy_3: 0.04545
	accuracy_policy_3: 0.55553
	loss_value_3: 0.03911
	loss_reward_3: 0.00657
	loss_policy_4: 0.04837
	accuracy_policy_4: 0.53154
	loss_value_4: 0.04033
	loss_reward_4: 0.00697
	loss_policy_5: 0.0514
	accuracy_policy_5: 0.50484
	loss_value_5: 0.04141
	loss_reward_5: 0.0079
	loss_policy: 0.38073
	loss_value: 0.36952
	loss_reward: 0.03346
[2024-05-08 08:45:07] nn step 27200, lr: 0.1.
	loss_policy_0: 0.14345
	accuracy_policy_0: 0.71246
	loss_value_0: 0.18121
	loss_policy_1: 0.03742
	accuracy_policy_1: 0.63465
	loss_value_1: 0.03787
	loss_reward_1: 0.0064
	loss_policy_2: 0.04116
	accuracy_policy_2: 0.60582
	loss_value_2: 0.03934
	loss_reward_2: 0.00624
	loss_policy_3: 0.045
	accuracy_policy_3: 0.57711
	loss_value_3: 0.04054
	loss_reward_3: 0.00666
	loss_policy_4: 0.048
	accuracy_policy_4: 0.55437
	loss_value_4: 0.04169
	loss_reward_4: 0.00716
	loss_policy_5: 0.05143
	accuracy_policy_5: 0.52943
	loss_value_5: 0.04284
	loss_reward_5: 0.00834
	loss_policy: 0.36646
	loss_value: 0.38349
	loss_reward: 0.0348
Optimization_Done 27200
[2024-05-08 08:47:33] [command] train weight_iter_27200.pkl 135 137
[2024-05-08 08:48:16] nn step 27300, lr: 0.1.
	loss_policy_0: 0.14707
	accuracy_policy_0: 0.69008
	loss_value_0: 0.17384
	loss_policy_1: 0.03648
	accuracy_policy_1: 0.62168
	loss_value_1: 0.03629
	loss_reward_1: 0.00604
	loss_policy_2: 0.03984
	accuracy_policy_2: 0.59223
	loss_value_2: 0.03773
	loss_reward_2: 0.00578
	loss_policy_3: 0.043
	accuracy_policy_3: 0.57025
	loss_value_3: 0.0389
	loss_reward_3: 0.00637
	loss_policy_4: 0.0453
	accuracy_policy_4: 0.55109
	loss_value_4: 0.04007
	loss_reward_4: 0.00681
	loss_policy_5: 0.04824
	accuracy_policy_5: 0.5252
	loss_value_5: 0.04101
	loss_reward_5: 0.00776
	loss_policy: 0.35994
	loss_value: 0.36784
	loss_reward: 0.03276
[2024-05-08 08:48:57] nn step 27400, lr: 0.1.
	loss_policy_0: 0.13537
	accuracy_policy_0: 0.72988
	loss_value_0: 0.18102
	loss_policy_1: 0.03495
	accuracy_policy_1: 0.65379
	loss_value_1: 0.03786
	loss_reward_1: 0.00625
	loss_policy_2: 0.03873
	accuracy_policy_2: 0.62459
	loss_value_2: 0.03919
	loss_reward_2: 0.0059
	loss_policy_3: 0.0422
	accuracy_policy_3: 0.59688
	loss_value_3: 0.04035
	loss_reward_3: 0.00658
	loss_policy_4: 0.04481
	accuracy_policy_4: 0.57781
	loss_value_4: 0.04155
	loss_reward_4: 0.00725
	loss_policy_5: 0.04837
	accuracy_policy_5: 0.55156
	loss_value_5: 0.0428
	loss_reward_5: 0.00827
	loss_policy: 0.34444
	loss_value: 0.38277
	loss_reward: 0.03425
Optimization_Done 27400
[2024-05-08 08:51:11] [command] train weight_iter_27400.pkl 136 138
[2024-05-08 08:51:53] nn step 27500, lr: 0.1.
	loss_policy_0: 0.16082
	accuracy_policy_0: 0.69012
	loss_value_0: 0.18848
	loss_policy_1: 0.03871
	accuracy_policy_1: 0.63428
	loss_value_1: 0.03916
	loss_reward_1: 0.00583
	loss_policy_2: 0.04226
	accuracy_policy_2: 0.60646
	loss_value_2: 0.04071
	loss_reward_2: 0.00557
	loss_policy_3: 0.04521
	accuracy_policy_3: 0.58465
	loss_value_3: 0.04193
	loss_reward_3: 0.00604
	loss_policy_4: 0.04819
	accuracy_policy_4: 0.56045
	loss_value_4: 0.04332
	loss_reward_4: 0.00646
	loss_policy_5: 0.0514
	accuracy_policy_5: 0.53795
	loss_value_5: 0.04465
	loss_reward_5: 0.00735
	loss_policy: 0.38659
	loss_value: 0.39825
	loss_reward: 0.03124
[2024-05-08 08:52:34] nn step 27600, lr: 0.1.
	loss_policy_0: 0.13735
	accuracy_policy_0: 0.73301
	loss_value_0: 0.18658
	loss_policy_1: 0.03495
	accuracy_policy_1: 0.66688
	loss_value_1: 0.03925
	loss_reward_1: 0.00582
	loss_policy_2: 0.03878
	accuracy_policy_2: 0.6393
	loss_value_2: 0.0407
	loss_reward_2: 0.00548
	loss_policy_3: 0.04178
	accuracy_policy_3: 0.61705
	loss_value_3: 0.04202
	loss_reward_3: 0.00599
	loss_policy_4: 0.04529
	accuracy_policy_4: 0.59062
	loss_value_4: 0.04326
	loss_reward_4: 0.00648
	loss_policy_5: 0.04879
	accuracy_policy_5: 0.564
	loss_value_5: 0.04448
	loss_reward_5: 0.00733
	loss_policy: 0.34694
	loss_value: 0.3963
	loss_reward: 0.0311
Optimization_Done 27600
[2024-05-08 08:55:12] [command] train weight_iter_27600.pkl 137 139
[2024-05-08 08:55:56] nn step 27700, lr: 0.1.
	loss_policy_0: 0.13987
	accuracy_policy_0: 0.69727
	loss_value_0: 0.17038
	loss_policy_1: 0.03441
	accuracy_policy_1: 0.64531
	loss_value_1: 0.03576
	loss_reward_1: 0.00468
	loss_policy_2: 0.03727
	accuracy_policy_2: 0.62086
	loss_value_2: 0.03702
	loss_reward_2: 0.00451
	loss_policy_3: 0.0407
	accuracy_policy_3: 0.59186
	loss_value_3: 0.03823
	loss_reward_3: 0.00497
	loss_policy_4: 0.04356
	accuracy_policy_4: 0.5693
	loss_value_4: 0.03947
	loss_reward_4: 0.00539
	loss_policy_5: 0.04688
	accuracy_policy_5: 0.54051
	loss_value_5: 0.04057
	loss_reward_5: 0.00616
	loss_policy: 0.34269
	loss_value: 0.36142
	loss_reward: 0.0257
[2024-05-08 08:56:39] nn step 27800, lr: 0.1.
	loss_policy_0: 0.13143
	accuracy_policy_0: 0.73053
	loss_value_0: 0.17871
	loss_policy_1: 0.03316
	accuracy_policy_1: 0.67145
	loss_value_1: 0.03737
	loss_reward_1: 0.00496
	loss_policy_2: 0.03615
	accuracy_policy_2: 0.65232
	loss_value_2: 0.03897
	loss_reward_2: 0.00473
	loss_policy_3: 0.03971
	accuracy_policy_3: 0.62406
	loss_value_3: 0.04029
	loss_reward_3: 0.00512
	loss_policy_4: 0.04352
	accuracy_policy_4: 0.59557
	loss_value_4: 0.0415
	loss_reward_4: 0.00553
	loss_policy_5: 0.04643
	accuracy_policy_5: 0.57229
	loss_value_5: 0.04276
	loss_reward_5: 0.00669
	loss_policy: 0.3304
	loss_value: 0.37961
	loss_reward: 0.02704
Optimization_Done 27800
[2024-05-08 08:59:15] [command] train weight_iter_27800.pkl 138 140
[2024-05-08 08:59:59] nn step 27900, lr: 0.1.
	loss_policy_0: 0.15431
	accuracy_policy_0: 0.68125
	loss_value_0: 0.17773
	loss_policy_1: 0.03775
	accuracy_policy_1: 0.62408
	loss_value_1: 0.0372
	loss_reward_1: 0.0054
	loss_policy_2: 0.0414
	accuracy_policy_2: 0.59365
	loss_value_2: 0.03862
	loss_reward_2: 0.00506
	loss_policy_3: 0.04499
	accuracy_policy_3: 0.5675
	loss_value_3: 0.04002
	loss_reward_3: 0.00557
	loss_policy_4: 0.04801
	accuracy_policy_4: 0.54594
	loss_value_4: 0.04134
	loss_reward_4: 0.00608
	loss_policy_5: 0.05108
	accuracy_policy_5: 0.52697
	loss_value_5: 0.04261
	loss_reward_5: 0.00686
	loss_policy: 0.37754
	loss_value: 0.37752
	loss_reward: 0.02898
[2024-05-08 09:00:42] nn step 28000, lr: 0.1.
	loss_policy_0: 0.122
	accuracy_policy_0: 0.72549
	loss_value_0: 0.16254
	loss_policy_1: 0.03143
	accuracy_policy_1: 0.66076
	loss_value_1: 0.03412
	loss_reward_1: 0.00498
	loss_policy_2: 0.03509
	accuracy_policy_2: 0.63238
	loss_value_2: 0.03546
	loss_reward_2: 0.00483
	loss_policy_3: 0.03806
	accuracy_policy_3: 0.60338
	loss_value_3: 0.0367
	loss_reward_3: 0.005
	loss_policy_4: 0.04134
	accuracy_policy_4: 0.57893
	loss_value_4: 0.03776
	loss_reward_4: 0.00562
	loss_policy_5: 0.04439
	accuracy_policy_5: 0.5527
	loss_value_5: 0.03887
	loss_reward_5: 0.00643
	loss_policy: 0.31231
	loss_value: 0.34544
	loss_reward: 0.02686
Optimization_Done 28000
[2024-05-08 09:03:04] [command] train weight_iter_28000.pkl 139 141
[2024-05-08 09:03:48] nn step 28100, lr: 0.1.
	loss_policy_0: 0.14832
	accuracy_policy_0: 0.68248
	loss_value_0: 0.16938
	loss_policy_1: 0.03668
	accuracy_policy_1: 0.62043
	loss_value_1: 0.03544
	loss_reward_1: 0.00526
	loss_policy_2: 0.04001
	accuracy_policy_2: 0.59365
	loss_value_2: 0.03684
	loss_reward_2: 0.00487
	loss_policy_3: 0.04354
	accuracy_policy_3: 0.56492
	loss_value_3: 0.03806
	loss_reward_3: 0.00528
	loss_policy_4: 0.04635
	accuracy_policy_4: 0.54229
	loss_value_4: 0.0392
	loss_reward_4: 0.00574
	loss_policy_5: 0.04942
	accuracy_policy_5: 0.51709
	loss_value_5: 0.04029
	loss_reward_5: 0.00652
	loss_policy: 0.36432
	loss_value: 0.35921
	loss_reward: 0.02768
[2024-05-08 09:04:31] nn step 28200, lr: 0.1.
	loss_policy_0: 0.14104
	accuracy_policy_0: 0.71377
	loss_value_0: 0.17986
	loss_policy_1: 0.03683
	accuracy_policy_1: 0.64406
	loss_value_1: 0.03757
	loss_reward_1: 0.00547
	loss_policy_2: 0.04075
	accuracy_policy_2: 0.61285
	loss_value_2: 0.03905
	loss_reward_2: 0.00522
	loss_policy_3: 0.04455
	accuracy_policy_3: 0.58475
	loss_value_3: 0.04027
	loss_reward_3: 0.00549
	loss_policy_4: 0.04795
	accuracy_policy_4: 0.56063
	loss_value_4: 0.04152
	loss_reward_4: 0.00589
	loss_policy_5: 0.05113
	accuracy_policy_5: 0.53467
	loss_value_5: 0.04279
	loss_reward_5: 0.00689
	loss_policy: 0.36225
	loss_value: 0.38105
	loss_reward: 0.02897
Optimization_Done 28200
[2024-05-08 09:07:13] [command] train weight_iter_28200.pkl 140 142
[2024-05-08 09:07:56] nn step 28300, lr: 0.1.
	loss_policy_0: 0.15029
	accuracy_policy_0: 0.6941
	loss_value_0: 0.17425
	loss_policy_1: 0.03739
	accuracy_policy_1: 0.63066
	loss_value_1: 0.03639
	loss_reward_1: 0.00514
	loss_policy_2: 0.04069
	accuracy_policy_2: 0.604
	loss_value_2: 0.03791
	loss_reward_2: 0.00495
	loss_policy_3: 0.04401
	accuracy_policy_3: 0.57594
	loss_value_3: 0.03915
	loss_reward_3: 0.00533
	loss_policy_4: 0.04686
	accuracy_policy_4: 0.55371
	loss_value_4: 0.04044
	loss_reward_4: 0.00581
	loss_policy_5: 0.04976
	accuracy_policy_5: 0.53068
	loss_value_5: 0.04154
	loss_reward_5: 0.00658
	loss_policy: 0.36901
	loss_value: 0.36968
	loss_reward: 0.02781
[2024-05-08 09:08:38] nn step 28400, lr: 0.1.
	loss_policy_0: 0.12731
	accuracy_policy_0: 0.73271
	loss_value_0: 0.16741
	loss_policy_1: 0.03341
	accuracy_policy_1: 0.66139
	loss_value_1: 0.03503
	loss_reward_1: 0.00524
	loss_policy_2: 0.03709
	accuracy_policy_2: 0.6282
	loss_value_2: 0.0365
	loss_reward_2: 0.00472
	loss_policy_3: 0.04017
	accuracy_policy_3: 0.60539
	loss_value_3: 0.03768
	loss_reward_3: 0.00527
	loss_policy_4: 0.04289
	accuracy_policy_4: 0.58201
	loss_value_4: 0.0388
	loss_reward_4: 0.00551
	loss_policy_5: 0.04586
	accuracy_policy_5: 0.55977
	loss_value_5: 0.03996
	loss_reward_5: 0.00655
	loss_policy: 0.32672
	loss_value: 0.35538
	loss_reward: 0.02729
Optimization_Done 28400
[2024-05-08 09:11:18] [command] train weight_iter_28400.pkl 141 143
[2024-05-08 09:12:01] nn step 28500, lr: 0.1.
	loss_policy_0: 0.14042
	accuracy_policy_0: 0.71846
	loss_value_0: 0.17704
	loss_policy_1: 0.0344
	accuracy_policy_1: 0.66332
	loss_value_1: 0.03695
	loss_reward_1: 0.00519
	loss_policy_2: 0.03763
	accuracy_policy_2: 0.63598
	loss_value_2: 0.03833
	loss_reward_2: 0.00529
	loss_policy_3: 0.04035
	accuracy_policy_3: 0.61146
	loss_value_3: 0.03967
	loss_reward_3: 0.00568
	loss_policy_4: 0.04282
	accuracy_policy_4: 0.59023
	loss_value_4: 0.04094
	loss_reward_4: 0.00612
	loss_policy_5: 0.04583
	accuracy_policy_5: 0.57018
	loss_value_5: 0.04221
	loss_reward_5: 0.00689
	loss_policy: 0.34146
	loss_value: 0.37514
	loss_reward: 0.02917
[2024-05-08 09:12:43] nn step 28600, lr: 0.1.
	loss_policy_0: 0.12185
	accuracy_policy_0: 0.75002
	loss_value_0: 0.17034
	loss_policy_1: 0.03105
	accuracy_policy_1: 0.68857
	loss_value_1: 0.03577
	loss_reward_1: 0.00512
	loss_policy_2: 0.03433
	accuracy_policy_2: 0.66137
	loss_value_2: 0.03718
	loss_reward_2: 0.00515
	loss_policy_3: 0.03731
	accuracy_policy_3: 0.63811
	loss_value_3: 0.03837
	loss_reward_3: 0.00543
	loss_policy_4: 0.04012
	accuracy_policy_4: 0.61508
	loss_value_4: 0.03954
	loss_reward_4: 0.00582
	loss_policy_5: 0.04256
	accuracy_policy_5: 0.59441
	loss_value_5: 0.04067
	loss_reward_5: 0.00668
	loss_policy: 0.30723
	loss_value: 0.36186
	loss_reward: 0.0282
Optimization_Done 28600
[2024-05-08 09:15:21] [command] train weight_iter_28600.pkl 142 144
[2024-05-08 09:16:03] nn step 28700, lr: 0.1.
	loss_policy_0: 0.14715
	accuracy_policy_0: 0.71129
	loss_value_0: 0.18042
	loss_policy_1: 0.03612
	accuracy_policy_1: 0.65574
	loss_value_1: 0.03774
	loss_reward_1: 0.00573
	loss_policy_2: 0.03932
	accuracy_policy_2: 0.6268
	loss_value_2: 0.03916
	loss_reward_2: 0.0056
	loss_policy_3: 0.04235
	accuracy_policy_3: 0.60441
	loss_value_3: 0.0405
	loss_reward_3: 0.00607
	loss_policy_4: 0.04494
	accuracy_policy_4: 0.58424
	loss_value_4: 0.04183
	loss_reward_4: 0.0066
	loss_policy_5: 0.0479
	accuracy_policy_5: 0.56193
	loss_value_5: 0.04311
	loss_reward_5: 0.00763
	loss_policy: 0.35778
	loss_value: 0.38275
	loss_reward: 0.03164
[2024-05-08 09:16:45] nn step 28800, lr: 0.1.
	loss_policy_0: 0.12728
	accuracy_policy_0: 0.75264
	loss_value_0: 0.17657
	loss_policy_1: 0.03225
	accuracy_policy_1: 0.68902
	loss_value_1: 0.03705
	loss_reward_1: 0.00566
	loss_policy_2: 0.03584
	accuracy_policy_2: 0.66104
	loss_value_2: 0.03858
	loss_reward_2: 0.00574
	loss_policy_3: 0.03884
	accuracy_policy_3: 0.63867
	loss_value_3: 0.03992
	loss_reward_3: 0.00609
	loss_policy_4: 0.04185
	accuracy_policy_4: 0.61457
	loss_value_4: 0.04123
	loss_reward_4: 0.00647
	loss_policy_5: 0.0451
	accuracy_policy_5: 0.58924
	loss_value_5: 0.04255
	loss_reward_5: 0.00761
	loss_policy: 0.32115
	loss_value: 0.3759
	loss_reward: 0.03157
Optimization_Done 28800
[2024-05-08 09:19:09] [command] train weight_iter_28800.pkl 143 145
[2024-05-08 09:19:52] nn step 28900, lr: 0.1.
	loss_policy_0: 0.1416
	accuracy_policy_0: 0.72596
	loss_value_0: 0.18184
	loss_policy_1: 0.03533
	accuracy_policy_1: 0.66311
	loss_value_1: 0.03808
	loss_reward_1: 0.0062
	loss_policy_2: 0.03943
	accuracy_policy_2: 0.6343
	loss_value_2: 0.03964
	loss_reward_2: 0.00597
	loss_policy_3: 0.04305
	accuracy_policy_3: 0.60387
	loss_value_3: 0.04094
	loss_reward_3: 0.00646
	loss_policy_4: 0.04632
	accuracy_policy_4: 0.58506
	loss_value_4: 0.04219
	loss_reward_4: 0.00708
	loss_policy_5: 0.04918
	accuracy_policy_5: 0.56176
	loss_value_5: 0.04338
	loss_reward_5: 0.00811
	loss_policy: 0.35492
	loss_value: 0.38606
	loss_reward: 0.03382
[2024-05-08 09:20:33] nn step 29000, lr: 0.1.
	loss_policy_0: 0.12384
	accuracy_policy_0: 0.75303
	loss_value_0: 0.17486
	loss_policy_1: 0.03235
	accuracy_policy_1: 0.68525
	loss_value_1: 0.03668
	loss_reward_1: 0.00611
	loss_policy_2: 0.03666
	accuracy_policy_2: 0.65295
	loss_value_2: 0.0382
	loss_reward_2: 0.00584
	loss_policy_3: 0.03984
	accuracy_policy_3: 0.63014
	loss_value_3: 0.03951
	loss_reward_3: 0.00655
	loss_policy_4: 0.04275
	accuracy_policy_4: 0.60768
	loss_value_4: 0.04081
	loss_reward_4: 0.00683
	loss_policy_5: 0.04558
	accuracy_policy_5: 0.58727
	loss_value_5: 0.0421
	loss_reward_5: 0.00807
	loss_policy: 0.32101
	loss_value: 0.37216
	loss_reward: 0.03339
Optimization_Done 29000
[2024-05-08 09:23:00] [command] train weight_iter_29000.pkl 144 146
[2024-05-08 09:23:43] nn step 29100, lr: 0.1.
	loss_policy_0: 0.14476
	accuracy_policy_0: 0.70373
	loss_value_0: 0.16725
	loss_policy_1: 0.03573
	accuracy_policy_1: 0.64314
	loss_value_1: 0.03516
	loss_reward_1: 0.00548
	loss_policy_2: 0.03964
	accuracy_policy_2: 0.60777
	loss_value_2: 0.03651
	loss_reward_2: 0.00538
	loss_policy_3: 0.04284
	accuracy_policy_3: 0.58117
	loss_value_3: 0.03768
	loss_reward_3: 0.00586
	loss_policy_4: 0.0454
	accuracy_policy_4: 0.55691
	loss_value_4: 0.0388
	loss_reward_4: 0.00623
	loss_policy_5: 0.0482
	accuracy_policy_5: 0.53816
	loss_value_5: 0.03992
	loss_reward_5: 0.00719
	loss_policy: 0.35656
	loss_value: 0.35532
	loss_reward: 0.03014
[2024-05-08 09:24:24] nn step 29200, lr: 0.1.
	loss_policy_0: 0.12818
	accuracy_policy_0: 0.74279
	loss_value_0: 0.16995
	loss_policy_1: 0.03366
	accuracy_policy_1: 0.67148
	loss_value_1: 0.03566
	loss_reward_1: 0.00574
	loss_policy_2: 0.03731
	accuracy_policy_2: 0.63832
	loss_value_2: 0.03708
	loss_reward_2: 0.00529
	loss_policy_3: 0.04076
	accuracy_policy_3: 0.61594
	loss_value_3: 0.03837
	loss_reward_3: 0.00595
	loss_policy_4: 0.04414
	accuracy_policy_4: 0.58613
	loss_value_4: 0.03954
	loss_reward_4: 0.0063
	loss_policy_5: 0.04722
	accuracy_policy_5: 0.56574
	loss_value_5: 0.04065
	loss_reward_5: 0.00761
	loss_policy: 0.33126
	loss_value: 0.36125
	loss_reward: 0.03089
Optimization_Done 29200
[2024-05-08 09:27:03] [command] train weight_iter_29200.pkl 145 147
[2024-05-08 09:27:46] nn step 29300, lr: 0.1.
	loss_policy_0: 0.16579
	accuracy_policy_0: 0.67516
	loss_value_0: 0.1653
	loss_policy_1: 0.0391
	accuracy_policy_1: 0.62805
	loss_value_1: 0.03468
	loss_reward_1: 0.00515
	loss_policy_2: 0.04277
	accuracy_policy_2: 0.59867
	loss_value_2: 0.03615
	loss_reward_2: 0.0047
	loss_policy_3: 0.04604
	accuracy_policy_3: 0.57752
	loss_value_3: 0.0374
	loss_reward_3: 0.0054
	loss_policy_4: 0.04924
	accuracy_policy_4: 0.55029
	loss_value_4: 0.0387
	loss_reward_4: 0.00568
	loss_policy_5: 0.0523
	accuracy_policy_5: 0.52195
	loss_value_5: 0.03999
	loss_reward_5: 0.0066
	loss_policy: 0.39524
	loss_value: 0.35223
	loss_reward: 0.02753
[2024-05-08 09:28:27] nn step 29400, lr: 0.1.
	loss_policy_0: 0.13883
	accuracy_policy_0: 0.72594
	loss_value_0: 0.16472
	loss_policy_1: 0.0353
	accuracy_policy_1: 0.66432
	loss_value_1: 0.03468
	loss_reward_1: 0.00536
	loss_policy_2: 0.03922
	accuracy_policy_2: 0.63436
	loss_value_2: 0.03614
	loss_reward_2: 0.00494
	loss_policy_3: 0.04233
	accuracy_policy_3: 0.6073
	loss_value_3: 0.03736
	loss_reward_3: 0.00552
	loss_policy_4: 0.04612
	accuracy_policy_4: 0.57975
	loss_value_4: 0.0386
	loss_reward_4: 0.00577
	loss_policy_5: 0.04906
	accuracy_policy_5: 0.55705
	loss_value_5: 0.03967
	loss_reward_5: 0.00682
	loss_policy: 0.35086
	loss_value: 0.35118
	loss_reward: 0.02842
Optimization_Done 29400
[2024-05-08 09:31:05] [command] train weight_iter_29400.pkl 146 148
[2024-05-08 09:31:48] nn step 29500, lr: 0.1.
	loss_policy_0: 0.17825
	accuracy_policy_0: 0.67928
	loss_value_0: 0.17092
	loss_policy_1: 0.04179
	accuracy_policy_1: 0.63211
	loss_value_1: 0.03592
	loss_reward_1: 0.0051
	loss_policy_2: 0.0447
	accuracy_policy_2: 0.60766
	loss_value_2: 0.03733
	loss_reward_2: 0.00522
	loss_policy_3: 0.04855
	accuracy_policy_3: 0.5757
	loss_value_3: 0.03871
	loss_reward_3: 0.00549
	loss_policy_4: 0.05142
	accuracy_policy_4: 0.55551
	loss_value_4: 0.04005
	loss_reward_4: 0.00595
	loss_policy_5: 0.05419
	accuracy_policy_5: 0.53426
	loss_value_5: 0.04128
	loss_reward_5: 0.00673
	loss_policy: 0.4189
	loss_value: 0.3642
	loss_reward: 0.0285
[2024-05-08 09:32:30] nn step 29600, lr: 0.1.
	loss_policy_0: 0.15261
	accuracy_policy_0: 0.71975
	loss_value_0: 0.16813
	loss_policy_1: 0.03713
	accuracy_policy_1: 0.66496
	loss_value_1: 0.03535
	loss_reward_1: 0.00508
	loss_policy_2: 0.04085
	accuracy_policy_2: 0.63291
	loss_value_2: 0.03676
	loss_reward_2: 0.0048
	loss_policy_3: 0.04445
	accuracy_policy_3: 0.60883
	loss_value_3: 0.03805
	loss_reward_3: 0.00526
	loss_policy_4: 0.0476
	accuracy_policy_4: 0.58602
	loss_value_4: 0.03946
	loss_reward_4: 0.00558
	loss_policy_5: 0.05086
	accuracy_policy_5: 0.55887
	loss_value_5: 0.04076
	loss_reward_5: 0.00667
	loss_policy: 0.37351
	loss_value: 0.35853
	loss_reward: 0.02739
Optimization_Done 29600
[2024-05-08 09:34:42] [command] train weight_iter_29600.pkl 147 149
[2024-05-08 09:35:24] nn step 29700, lr: 0.1.
	loss_policy_0: 0.15832
	accuracy_policy_0: 0.69221
	loss_value_0: 0.16815
	loss_policy_1: 0.0378
	accuracy_policy_1: 0.63527
	loss_value_1: 0.03527
	loss_reward_1: 0.00551
	loss_policy_2: 0.04153
	accuracy_policy_2: 0.6108
	loss_value_2: 0.03671
	loss_reward_2: 0.00517
	loss_policy_3: 0.04489
	accuracy_policy_3: 0.58336
	loss_value_3: 0.03788
	loss_reward_3: 0.00548
	loss_policy_4: 0.04795
	accuracy_policy_4: 0.56088
	loss_value_4: 0.03915
	loss_reward_4: 0.00619
	loss_policy_5: 0.05087
	accuracy_policy_5: 0.54217
	loss_value_5: 0.04038
	loss_reward_5: 0.00716
	loss_policy: 0.38136
	loss_value: 0.35754
	loss_reward: 0.02951
[2024-05-08 09:36:06] nn step 29800, lr: 0.1.
	loss_policy_0: 0.14673
	accuracy_policy_0: 0.7241
	loss_value_0: 0.17365
	loss_policy_1: 0.03691
	accuracy_policy_1: 0.66457
	loss_value_1: 0.03653
	loss_reward_1: 0.00555
	loss_policy_2: 0.04078
	accuracy_policy_2: 0.63221
	loss_value_2: 0.03805
	loss_reward_2: 0.00532
	loss_policy_3: 0.04435
	accuracy_policy_3: 0.6093
	loss_value_3: 0.03944
	loss_reward_3: 0.00586
	loss_policy_4: 0.04771
	accuracy_policy_4: 0.5842
	loss_value_4: 0.04072
	loss_reward_4: 0.00622
	loss_policy_5: 0.05117
	accuracy_policy_5: 0.56295
	loss_value_5: 0.04209
	loss_reward_5: 0.00739
	loss_policy: 0.36766
	loss_value: 0.37048
	loss_reward: 0.03034
Optimization_Done 29800
[2024-05-08 09:38:31] [command] train weight_iter_29800.pkl 148 150
[2024-05-08 09:39:13] nn step 29900, lr: 0.1.
	loss_policy_0: 0.15405
	accuracy_policy_0: 0.69266
	loss_value_0: 0.17183
	loss_policy_1: 0.03723
	accuracy_policy_1: 0.64314
	loss_value_1: 0.03603
	loss_reward_1: 0.00515
	loss_policy_2: 0.04074
	accuracy_policy_2: 0.61467
	loss_value_2: 0.03741
	loss_reward_2: 0.00502
	loss_policy_3: 0.04434
	accuracy_policy_3: 0.58627
	loss_value_3: 0.0389
	loss_reward_3: 0.00536
	loss_policy_4: 0.04686
	accuracy_policy_4: 0.56889
	loss_value_4: 0.04013
	loss_reward_4: 0.00585
	loss_policy_5: 0.0497
	accuracy_policy_5: 0.54373
	loss_value_5: 0.04134
	loss_reward_5: 0.00681
	loss_policy: 0.37292
	loss_value: 0.36564
	loss_reward: 0.02819
[2024-05-08 09:39:55] nn step 30000, lr: 0.1.
	loss_policy_0: 0.1307
	accuracy_policy_0: 0.73369
	loss_value_0: 0.16536
	loss_policy_1: 0.03319
	accuracy_policy_1: 0.6684
	loss_value_1: 0.03471
	loss_reward_1: 0.00496
	loss_policy_2: 0.03646
	accuracy_policy_2: 0.64039
	loss_value_2: 0.03606
	loss_reward_2: 0.00478
	loss_policy_3: 0.03966
	accuracy_policy_3: 0.61504
	loss_value_3: 0.03715
	loss_reward_3: 0.0052
	loss_policy_4: 0.04257
	accuracy_policy_4: 0.59146
	loss_value_4: 0.03854
	loss_reward_4: 0.00554
	loss_policy_5: 0.04551
	accuracy_policy_5: 0.56963
	loss_value_5: 0.03965
	loss_reward_5: 0.00658
	loss_policy: 0.32809
	loss_value: 0.35147
	loss_reward: 0.02705
Optimization_Done 30000
[2024-05-08 09:42:35] [command] train weight_iter_30000.pkl 149 151
[2024-05-08 09:43:18] nn step 30100, lr: 0.1.
	loss_policy_0: 0.14122
	accuracy_policy_0: 0.70445
	loss_value_0: 0.16396
	loss_policy_1: 0.03441
	accuracy_policy_1: 0.64617
	loss_value_1: 0.03418
	loss_reward_1: 0.00494
	loss_policy_2: 0.03764
	accuracy_policy_2: 0.61965
	loss_value_2: 0.03544
	loss_reward_2: 0.00474
	loss_policy_3: 0.04075
	accuracy_policy_3: 0.59145
	loss_value_3: 0.0367
	loss_reward_3: 0.00532
	loss_policy_4: 0.04343
	accuracy_policy_4: 0.57344
	loss_value_4: 0.03778
	loss_reward_4: 0.00573
	loss_policy_5: 0.04616
	accuracy_policy_5: 0.54641
	loss_value_5: 0.03893
	loss_reward_5: 0.00664
	loss_policy: 0.34361
	loss_value: 0.347
	loss_reward: 0.02736
[2024-05-08 09:43:59] nn step 30200, lr: 0.1.
	loss_policy_0: 0.12645
	accuracy_policy_0: 0.739
	loss_value_0: 0.16916
	loss_policy_1: 0.03259
	accuracy_policy_1: 0.67064
	loss_value_1: 0.03531
	loss_reward_1: 0.00532
	loss_policy_2: 0.03612
	accuracy_policy_2: 0.64307
	loss_value_2: 0.03666
	loss_reward_2: 0.0049
	loss_policy_3: 0.03937
	accuracy_policy_3: 0.61559
	loss_value_3: 0.03798
	loss_reward_3: 0.00545
	loss_policy_4: 0.04259
	accuracy_policy_4: 0.58908
	loss_value_4: 0.0392
	loss_reward_4: 0.00591
	loss_policy_5: 0.04499
	accuracy_policy_5: 0.57156
	loss_value_5: 0.04031
	loss_reward_5: 0.00712
	loss_policy: 0.3221
	loss_value: 0.35862
	loss_reward: 0.0287
Optimization_Done 30200
[2024-05-08 09:46:39] [command] train weight_iter_30200.pkl 150 152
[2024-05-08 09:47:21] nn step 30300, lr: 0.1.
	loss_policy_0: 0.15441
	accuracy_policy_0: 0.68869
	loss_value_0: 0.16582
	loss_policy_1: 0.03658
	accuracy_policy_1: 0.63656
	loss_value_1: 0.03468
	loss_reward_1: 0.00481
	loss_policy_2: 0.03991
	accuracy_policy_2: 0.61061
	loss_value_2: 0.03604
	loss_reward_2: 0.00478
	loss_policy_3: 0.04308
	accuracy_policy_3: 0.58529
	loss_value_3: 0.03737
	loss_reward_3: 0.00523
	loss_policy_4: 0.04592
	accuracy_policy_4: 0.56246
	loss_value_4: 0.03867
	loss_reward_4: 0.00579
	loss_policy_5: 0.04885
	accuracy_policy_5: 0.53951
	loss_value_5: 0.03977
	loss_reward_5: 0.00668
	loss_policy: 0.36875
	loss_value: 0.35235
	loss_reward: 0.02729
[2024-05-08 09:48:03] nn step 30400, lr: 0.1.
	loss_policy_0: 0.13331
	accuracy_policy_0: 0.72121
	loss_value_0: 0.16342
	loss_policy_1: 0.03294
	accuracy_policy_1: 0.66574
	loss_value_1: 0.03436
	loss_reward_1: 0.00477
	loss_policy_2: 0.03634
	accuracy_policy_2: 0.63783
	loss_value_2: 0.03563
	loss_reward_2: 0.00468
	loss_policy_3: 0.04021
	accuracy_policy_3: 0.60775
	loss_value_3: 0.03683
	loss_reward_3: 0.00505
	loss_policy_4: 0.04291
	accuracy_policy_4: 0.58859
	loss_value_4: 0.03806
	loss_reward_4: 0.00562
	loss_policy_5: 0.04618
	accuracy_policy_5: 0.56068
	loss_value_5: 0.03923
	loss_reward_5: 0.0066
	loss_policy: 0.33189
	loss_value: 0.34754
	loss_reward: 0.02672
Optimization_Done 30400
[2024-05-08 09:50:42] [command] train weight_iter_30400.pkl 151 153
[2024-05-08 09:51:25] nn step 30500, lr: 0.1.
	loss_policy_0: 0.15525
	accuracy_policy_0: 0.68613
	loss_value_0: 0.17773
	loss_policy_1: 0.03727
	accuracy_policy_1: 0.63529
	loss_value_1: 0.03731
	loss_reward_1: 0.00587
	loss_policy_2: 0.0411
	accuracy_policy_2: 0.60611
	loss_value_2: 0.03881
	loss_reward_2: 0.00558
	loss_policy_3: 0.04457
	accuracy_policy_3: 0.57836
	loss_value_3: 0.04015
	loss_reward_3: 0.00609
	loss_policy_4: 0.04742
	accuracy_policy_4: 0.55467
	loss_value_4: 0.04151
	loss_reward_4: 0.0066
	loss_policy_5: 0.0503
	accuracy_policy_5: 0.53189
	loss_value_5: 0.04271
	loss_reward_5: 0.00776
	loss_policy: 0.37592
	loss_value: 0.37822
	loss_reward: 0.03189
[2024-05-08 09:52:07] nn step 30600, lr: 0.1.
	loss_policy_0: 0.13232
	accuracy_policy_0: 0.72924
	loss_value_0: 0.17119
	loss_policy_1: 0.03366
	accuracy_policy_1: 0.66113
	loss_value_1: 0.03592
	loss_reward_1: 0.00551
	loss_policy_2: 0.03736
	accuracy_policy_2: 0.63441
	loss_value_2: 0.03729
	loss_reward_2: 0.00531
	loss_policy_3: 0.04109
	accuracy_policy_3: 0.60312
	loss_value_3: 0.03851
	loss_reward_3: 0.00581
	loss_policy_4: 0.04463
	accuracy_policy_4: 0.57576
	loss_value_4: 0.03971
	loss_reward_4: 0.00621
	loss_policy_5: 0.04758
	accuracy_policy_5: 0.55178
	loss_value_5: 0.04089
	loss_reward_5: 0.00738
	loss_policy: 0.33664
	loss_value: 0.36352
	loss_reward: 0.03023
Optimization_Done 30600
[2024-05-08 09:54:32] [command] train weight_iter_30600.pkl 152 154
[2024-05-08 09:55:15] nn step 30700, lr: 0.1.
	loss_policy_0: 0.14143
	accuracy_policy_0: 0.68967
	loss_value_0: 0.1681
	loss_policy_1: 0.0348
	accuracy_policy_1: 0.62867
	loss_value_1: 0.03517
	loss_reward_1: 0.00579
	loss_policy_2: 0.03829
	accuracy_policy_2: 0.59922
	loss_value_2: 0.03642
	loss_reward_2: 0.0056
	loss_policy_3: 0.04161
	accuracy_policy_3: 0.57125
	loss_value_3: 0.0375
	loss_reward_3: 0.00574
	loss_policy_4: 0.04451
	accuracy_policy_4: 0.54277
	loss_value_4: 0.03848
	loss_reward_4: 0.00635
	loss_policy_5: 0.04721
	accuracy_policy_5: 0.52135
	loss_value_5: 0.03959
	loss_reward_5: 0.0072
	loss_policy: 0.34786
	loss_value: 0.35527
	loss_reward: 0.03068
[2024-05-08 09:55:57] nn step 30800, lr: 0.1.
	loss_policy_0: 0.12218
	accuracy_policy_0: 0.72689
	loss_value_0: 0.16235
	loss_policy_1: 0.03113
	accuracy_policy_1: 0.65785
	loss_value_1: 0.03398
	loss_reward_1: 0.00551
	loss_policy_2: 0.03463
	accuracy_policy_2: 0.6274
	loss_value_2: 0.03528
	loss_reward_2: 0.00533
	loss_policy_3: 0.03793
	accuracy_policy_3: 0.59725
	loss_value_3: 0.03639
	loss_reward_3: 0.00569
	loss_policy_4: 0.04089
	accuracy_policy_4: 0.57242
	loss_value_4: 0.0374
	loss_reward_4: 0.00607
	loss_policy_5: 0.04332
	accuracy_policy_5: 0.54789
	loss_value_5: 0.03849
	loss_reward_5: 0.00725
	loss_policy: 0.31008
	loss_value: 0.34389
	loss_reward: 0.02985
Optimization_Done 30800
[2024-05-08 09:58:21] [command] train weight_iter_30800.pkl 153 155
[2024-05-08 09:59:04] nn step 30900, lr: 0.1.
	loss_policy_0: 0.1229
	accuracy_policy_0: 0.72342
	loss_value_0: 0.16837
	loss_policy_1: 0.03137
	accuracy_policy_1: 0.65885
	loss_value_1: 0.03519
	loss_reward_1: 0.00561
	loss_policy_2: 0.03492
	accuracy_policy_2: 0.62229
	loss_value_2: 0.03666
	loss_reward_2: 0.00545
	loss_policy_3: 0.03834
	accuracy_policy_3: 0.5925
	loss_value_3: 0.03783
	loss_reward_3: 0.00596
	loss_policy_4: 0.04105
	accuracy_policy_4: 0.56725
	loss_value_4: 0.03879
	loss_reward_4: 0.00639
	loss_policy_5: 0.04359
	accuracy_policy_5: 0.54469
	loss_value_5: 0.03985
	loss_reward_5: 0.00735
	loss_policy: 0.31217
	loss_value: 0.3567
	loss_reward: 0.03076
[2024-05-08 09:59:46] nn step 31000, lr: 0.1.
	loss_policy_0: 0.1062
	accuracy_policy_0: 0.75574
	loss_value_0: 0.16464
	loss_policy_1: 0.02829
	accuracy_policy_1: 0.68193
	loss_value_1: 0.03446
	loss_reward_1: 0.00554
	loss_policy_2: 0.03219
	accuracy_policy_2: 0.64676
	loss_value_2: 0.03568
	loss_reward_2: 0.00536
	loss_policy_3: 0.03519
	accuracy_policy_3: 0.61896
	loss_value_3: 0.03676
	loss_reward_3: 0.00567
	loss_policy_4: 0.03809
	accuracy_policy_4: 0.59143
	loss_value_4: 0.03779
	loss_reward_4: 0.00621
	loss_policy_5: 0.04063
	accuracy_policy_5: 0.56971
	loss_value_5: 0.03883
	loss_reward_5: 0.00738
	loss_policy: 0.28058
	loss_value: 0.34816
	loss_reward: 0.03016
Optimization_Done 31000
[2024-05-08 10:02:25] [command] train weight_iter_31000.pkl 154 156
[2024-05-08 10:03:07] nn step 31100, lr: 0.1.
	loss_policy_0: 0.16036
	accuracy_policy_0: 0.67441
	loss_value_0: 0.17427
	loss_policy_1: 0.03745
	accuracy_policy_1: 0.62578
	loss_value_1: 0.03644
	loss_reward_1: 0.00557
	loss_policy_2: 0.04079
	accuracy_policy_2: 0.59592
	loss_value_2: 0.03779
	loss_reward_2: 0.00536
	loss_policy_3: 0.04419
	accuracy_policy_3: 0.56773
	loss_value_3: 0.03899
	loss_reward_3: 0.00587
	loss_policy_4: 0.04668
	accuracy_policy_4: 0.54953
	loss_value_4: 0.04011
	loss_reward_4: 0.00623
	loss_policy_5: 0.04899
	accuracy_policy_5: 0.53059
	loss_value_5: 0.04129
	loss_reward_5: 0.00712
	loss_policy: 0.37847
	loss_value: 0.3689
	loss_reward: 0.03014
[2024-05-08 10:03:49] nn step 31200, lr: 0.1.
	loss_policy_0: 0.13438
	accuracy_policy_0: 0.72719
	loss_value_0: 0.17468
	loss_policy_1: 0.03374
	accuracy_policy_1: 0.66377
	loss_value_1: 0.03654
	loss_reward_1: 0.00549
	loss_policy_2: 0.03729
	accuracy_policy_2: 0.63117
	loss_value_2: 0.03779
	loss_reward_2: 0.00547
	loss_policy_3: 0.04058
	accuracy_policy_3: 0.60711
	loss_value_3: 0.03893
	loss_reward_3: 0.00571
	loss_policy_4: 0.04342
	accuracy_policy_4: 0.58455
	loss_value_4: 0.04017
	loss_reward_4: 0.00629
	loss_policy_5: 0.04613
	accuracy_policy_5: 0.56199
	loss_value_5: 0.04144
	loss_reward_5: 0.00724
	loss_policy: 0.33554
	loss_value: 0.36954
	loss_reward: 0.03021
Optimization_Done 31200
[2024-05-08 10:06:28] [command] train weight_iter_31200.pkl 155 157
[2024-05-08 10:07:11] nn step 31300, lr: 0.1.
	loss_policy_0: 0.15339
	accuracy_policy_0: 0.68693
	loss_value_0: 0.18346
	loss_policy_1: 0.03693
	accuracy_policy_1: 0.63346
	loss_value_1: 0.03823
	loss_reward_1: 0.00606
	loss_policy_2: 0.0399
	accuracy_policy_2: 0.61154
	loss_value_2: 0.03959
	loss_reward_2: 0.00601
	loss_policy_3: 0.0431
	accuracy_policy_3: 0.58727
	loss_value_3: 0.04091
	loss_reward_3: 0.00664
	loss_policy_4: 0.04582
	accuracy_policy_4: 0.56711
	loss_value_4: 0.04214
	loss_reward_4: 0.00729
	loss_policy_5: 0.04868
	accuracy_policy_5: 0.54525
	loss_value_5: 0.04324
	loss_reward_5: 0.00819
	loss_policy: 0.36782
	loss_value: 0.38757
	loss_reward: 0.03419
[2024-05-08 10:07:53] nn step 31400, lr: 0.1.
	loss_policy_0: 0.13826
	accuracy_policy_0: 0.71924
	loss_value_0: 0.18283
	loss_policy_1: 0.03454
	accuracy_policy_1: 0.66104
	loss_value_1: 0.03822
	loss_reward_1: 0.00619
	loss_policy_2: 0.03779
	accuracy_policy_2: 0.63471
	loss_value_2: 0.03969
	loss_reward_2: 0.00603
	loss_policy_3: 0.04166
	accuracy_policy_3: 0.60549
	loss_value_3: 0.04094
	loss_reward_3: 0.00651
	loss_policy_4: 0.04467
	accuracy_policy_4: 0.58326
	loss_value_4: 0.04214
	loss_reward_4: 0.00693
	loss_policy_5: 0.04723
	accuracy_policy_5: 0.56338
	loss_value_5: 0.04334
	loss_reward_5: 0.0079
	loss_policy: 0.34416
	loss_value: 0.38716
	loss_reward: 0.03357
Optimization_Done 31400
[2024-05-08 10:10:27] [command] train weight_iter_31400.pkl 156 158
[2024-05-08 10:11:09] nn step 31500, lr: 0.1.
	loss_policy_0: 0.14082
	accuracy_policy_0: 0.67992
	loss_value_0: 0.16651
	loss_policy_1: 0.03424
	accuracy_policy_1: 0.61949
	loss_value_1: 0.03468
	loss_reward_1: 0.00569
	loss_policy_2: 0.03728
	accuracy_policy_2: 0.59279
	loss_value_2: 0.03593
	loss_reward_2: 0.00555
	loss_policy_3: 0.04042
	accuracy_policy_3: 0.56789
	loss_value_3: 0.03696
	loss_reward_3: 0.00597
	loss_policy_4: 0.04315
	accuracy_policy_4: 0.54789
	loss_value_4: 0.03815
	loss_reward_4: 0.00655
	loss_policy_5: 0.04558
	accuracy_policy_5: 0.52947
	loss_value_5: 0.03921
	loss_reward_5: 0.00717
	loss_policy: 0.3415
	loss_value: 0.35143
	loss_reward: 0.03094
[2024-05-08 10:11:51] nn step 31600, lr: 0.1.
	loss_policy_0: 0.13596
	accuracy_policy_0: 0.71379
	loss_value_0: 0.1758
	loss_policy_1: 0.03439
	accuracy_policy_1: 0.6467
	loss_value_1: 0.03687
	loss_reward_1: 0.00595
	loss_policy_2: 0.038
	accuracy_policy_2: 0.61547
	loss_value_2: 0.0382
	loss_reward_2: 0.00596
	loss_policy_3: 0.04104
	accuracy_policy_3: 0.59426
	loss_value_3: 0.03944
	loss_reward_3: 0.00637
	loss_policy_4: 0.04398
	accuracy_policy_4: 0.57072
	loss_value_4: 0.04059
	loss_reward_4: 0.00666
	loss_policy_5: 0.04705
	accuracy_policy_5: 0.54672
	loss_value_5: 0.04173
	loss_reward_5: 0.00777
	loss_policy: 0.34043
	loss_value: 0.37263
	loss_reward: 0.0327
Optimization_Done 31600
[2024-05-08 10:14:18] [command] train weight_iter_31600.pkl 157 159
[2024-05-08 10:15:00] nn step 31700, lr: 0.1.
	loss_policy_0: 0.13603
	accuracy_policy_0: 0.68779
	loss_value_0: 0.16368
	loss_policy_1: 0.03337
	accuracy_policy_1: 0.6232
	loss_value_1: 0.03422
	loss_reward_1: 0.00552
	loss_policy_2: 0.03634
	accuracy_policy_2: 0.60197
	loss_value_2: 0.03546
	loss_reward_2: 0.00525
	loss_policy_3: 0.03935
	accuracy_policy_3: 0.57613
	loss_value_3: 0.03658
	loss_reward_3: 0.00585
	loss_policy_4: 0.04225
	accuracy_policy_4: 0.55422
	loss_value_4: 0.03764
	loss_reward_4: 0.00627
	loss_policy_5: 0.04502
	accuracy_policy_5: 0.53393
	loss_value_5: 0.03874
	loss_reward_5: 0.00729
	loss_policy: 0.33237
	loss_value: 0.34632
	loss_reward: 0.03018
[2024-05-08 10:15:41] nn step 31800, lr: 0.1.
	loss_policy_0: 0.11909
	accuracy_policy_0: 0.73594
	loss_value_0: 0.1658
	loss_policy_1: 0.03137
	accuracy_policy_1: 0.65506
	loss_value_1: 0.03462
	loss_reward_1: 0.0057
	loss_policy_2: 0.03455
	accuracy_policy_2: 0.63326
	loss_value_2: 0.03579
	loss_reward_2: 0.00556
	loss_policy_3: 0.03756
	accuracy_policy_3: 0.61076
	loss_value_3: 0.03705
	loss_reward_3: 0.00597
	loss_policy_4: 0.04044
	accuracy_policy_4: 0.58879
	loss_value_4: 0.03812
	loss_reward_4: 0.00645
	loss_policy_5: 0.04356
	accuracy_policy_5: 0.55947
	loss_value_5: 0.03931
	loss_reward_5: 0.00764
	loss_policy: 0.30656
	loss_value: 0.35068
	loss_reward: 0.03132
Optimization_Done 31800
[2024-05-08 10:18:20] [command] train weight_iter_31800.pkl 158 160
[2024-05-08 10:19:02] nn step 31900, lr: 0.1.
	loss_policy_0: 0.14922
	accuracy_policy_0: 0.68691
	loss_value_0: 0.16864
	loss_policy_1: 0.03561
	accuracy_policy_1: 0.63191
	loss_value_1: 0.03521
	loss_reward_1: 0.00554
	loss_policy_2: 0.03882
	accuracy_policy_2: 0.60613
	loss_value_2: 0.0365
	loss_reward_2: 0.00529
	loss_policy_3: 0.04185
	accuracy_policy_3: 0.58332
	loss_value_3: 0.03765
	loss_reward_3: 0.00581
	loss_policy_4: 0.04445
	accuracy_policy_4: 0.56297
	loss_value_4: 0.03888
	loss_reward_4: 0.00614
	loss_policy_5: 0.04689
	accuracy_policy_5: 0.5467
	loss_value_5: 0.04003
	loss_reward_5: 0.0071
	loss_policy: 0.35684
	loss_value: 0.35692
	loss_reward: 0.02988
[2024-05-08 10:19:43] nn step 32000, lr: 0.1.
	loss_policy_0: 0.12786
	accuracy_policy_0: 0.7317
	loss_value_0: 0.1676
	loss_policy_1: 0.03177
	accuracy_policy_1: 0.6723
	loss_value_1: 0.03501
	loss_reward_1: 0.00551
	loss_policy_2: 0.03546
	accuracy_policy_2: 0.64002
	loss_value_2: 0.03617
	loss_reward_2: 0.00523
	loss_policy_3: 0.03876
	accuracy_policy_3: 0.61676
	loss_value_3: 0.03725
	loss_reward_3: 0.00577
	loss_policy_4: 0.04126
	accuracy_policy_4: 0.59787
	loss_value_4: 0.03832
	loss_reward_4: 0.00616
	loss_policy_5: 0.04411
	accuracy_policy_5: 0.57736
	loss_value_5: 0.03957
	loss_reward_5: 0.0071
	loss_policy: 0.31922
	loss_value: 0.35392
	loss_reward: 0.02977
Optimization_Done 32000
[2024-05-08 10:22:12] [command] train weight_iter_32000.pkl 159 161
[2024-05-08 10:22:54] nn step 32100, lr: 0.1.
	loss_policy_0: 0.1407
	accuracy_policy_0: 0.70879
	loss_value_0: 0.16982
	loss_policy_1: 0.03413
	accuracy_policy_1: 0.65975
	loss_value_1: 0.03568
	loss_reward_1: 0.00613
	loss_policy_2: 0.03772
	accuracy_policy_2: 0.63133
	loss_value_2: 0.0372
	loss_reward_2: 0.00598
	loss_policy_3: 0.04058
	accuracy_policy_3: 0.6125
	loss_value_3: 0.03858
	loss_reward_3: 0.00675
	loss_policy_4: 0.04382
	accuracy_policy_4: 0.58971
	loss_value_4: 0.03997
	loss_reward_4: 0.0073
	loss_policy_5: 0.04685
	accuracy_policy_5: 0.566
	loss_value_5: 0.04134
	loss_reward_5: 0.00827
	loss_policy: 0.34379
	loss_value: 0.3626
	loss_reward: 0.03444
[2024-05-08 10:23:35] nn step 32200, lr: 0.1.
	loss_policy_0: 0.12154
	accuracy_policy_0: 0.74844
	loss_value_0: 0.16236
	loss_policy_1: 0.03126
	accuracy_policy_1: 0.68227
	loss_value_1: 0.03408
	loss_reward_1: 0.00584
	loss_policy_2: 0.03479
	accuracy_policy_2: 0.6525
	loss_value_2: 0.03545
	loss_reward_2: 0.00575
	loss_policy_3: 0.03776
	accuracy_policy_3: 0.63154
	loss_value_3: 0.03675
	loss_reward_3: 0.00625
	loss_policy_4: 0.0408
	accuracy_policy_4: 0.61363
	loss_value_4: 0.03788
	loss_reward_4: 0.00674
	loss_policy_5: 0.04342
	accuracy_policy_5: 0.59336
	loss_value_5: 0.03927
	loss_reward_5: 0.0079
	loss_policy: 0.30957
	loss_value: 0.34579
	loss_reward: 0.03249
Optimization_Done 32200
[2024-05-08 10:25:58] [command] train weight_iter_32200.pkl 160 162
[2024-05-08 10:26:39] nn step 32300, lr: 0.1.
	loss_policy_0: 0.12742
	accuracy_policy_0: 0.72416
	loss_value_0: 0.15374
	loss_policy_1: 0.03177
	accuracy_policy_1: 0.66787
	loss_value_1: 0.03241
	loss_reward_1: 0.00616
	loss_policy_2: 0.03515
	accuracy_policy_2: 0.64162
	loss_value_2: 0.03382
	loss_reward_2: 0.00605
	loss_policy_3: 0.03871
	accuracy_policy_3: 0.6142
	loss_value_3: 0.03517
	loss_reward_3: 0.00669
	loss_policy_4: 0.04129
	accuracy_policy_4: 0.5909
	loss_value_4: 0.03635
	loss_reward_4: 0.00715
	loss_policy_5: 0.04432
	accuracy_policy_5: 0.57066
	loss_value_5: 0.03776
	loss_reward_5: 0.00821
	loss_policy: 0.31866
	loss_value: 0.32926
	loss_reward: 0.03426
[2024-05-08 10:27:20] nn step 32400, lr: 0.1.
	loss_policy_0: 0.1179
	accuracy_policy_0: 0.7568
	loss_value_0: 0.15976
	loss_policy_1: 0.03097
	accuracy_policy_1: 0.69186
	loss_value_1: 0.0335
	loss_reward_1: 0.00639
	loss_policy_2: 0.03476
	accuracy_policy_2: 0.66162
	loss_value_2: 0.03501
	loss_reward_2: 0.00632
	loss_policy_3: 0.03812
	accuracy_policy_3: 0.63543
	loss_value_3: 0.03636
	loss_reward_3: 0.00689
	loss_policy_4: 0.04133
	accuracy_policy_4: 0.6149
	loss_value_4: 0.03761
	loss_reward_4: 0.00729
	loss_policy_5: 0.0443
	accuracy_policy_5: 0.59227
	loss_value_5: 0.03881
	loss_reward_5: 0.00869
	loss_policy: 0.30738
	loss_value: 0.34105
	loss_reward: 0.03559
Optimization_Done 32400
[2024-05-08 10:29:59] [command] train weight_iter_32400.pkl 161 163
[2024-05-08 10:30:41] nn step 32500, lr: 0.1.
	loss_policy_0: 0.12878
	accuracy_policy_0: 0.71475
	loss_value_0: 0.15139
	loss_policy_1: 0.03207
	accuracy_policy_1: 0.65598
	loss_value_1: 0.03189
	loss_reward_1: 0.00573
	loss_policy_2: 0.03499
	accuracy_policy_2: 0.63188
	loss_value_2: 0.03324
	loss_reward_2: 0.00538
	loss_policy_3: 0.03808
	accuracy_policy_3: 0.61158
	loss_value_3: 0.03459
	loss_reward_3: 0.00577
	loss_policy_4: 0.04046
	accuracy_policy_4: 0.59221
	loss_value_4: 0.03592
	loss_reward_4: 0.00628
	loss_policy_5: 0.04353
	accuracy_policy_5: 0.56809
	loss_value_5: 0.03705
	loss_reward_5: 0.00742
	loss_policy: 0.3179
	loss_value: 0.32408
	loss_reward: 0.03058
[2024-05-08 10:31:22] nn step 32600, lr: 0.1.
	loss_policy_0: 0.10965
	accuracy_policy_0: 0.75637
	loss_value_0: 0.14756
	loss_policy_1: 0.02896
	accuracy_policy_1: 0.68574
	loss_value_1: 0.03099
	loss_reward_1: 0.00569
	loss_policy_2: 0.03208
	accuracy_policy_2: 0.66086
	loss_value_2: 0.03239
	loss_reward_2: 0.0054
	loss_policy_3: 0.0353
	accuracy_policy_3: 0.63533
	loss_value_3: 0.03355
	loss_reward_3: 0.00582
	loss_policy_4: 0.03823
	accuracy_policy_4: 0.61354
	loss_value_4: 0.0348
	loss_reward_4: 0.0063
	loss_policy_5: 0.04102
	accuracy_policy_5: 0.59215
	loss_value_5: 0.03604
	loss_reward_5: 0.00738
	loss_policy: 0.28523
	loss_value: 0.31534
	loss_reward: 0.03059
Optimization_Done 32600
[2024-05-08 10:33:46] [command] train weight_iter_32600.pkl 162 164
[2024-05-08 10:34:28] nn step 32700, lr: 0.1.
	loss_policy_0: 0.14072
	accuracy_policy_0: 0.6951
	loss_value_0: 0.16012
	loss_policy_1: 0.03358
	accuracy_policy_1: 0.64717
	loss_value_1: 0.0335
	loss_reward_1: 0.00532
	loss_policy_2: 0.03665
	accuracy_policy_2: 0.62168
	loss_value_2: 0.03476
	loss_reward_2: 0.00507
	loss_policy_3: 0.03921
	accuracy_policy_3: 0.59762
	loss_value_3: 0.03605
	loss_reward_3: 0.00547
	loss_policy_4: 0.0419
	accuracy_policy_4: 0.57559
	loss_value_4: 0.03718
	loss_reward_4: 0.00579
	loss_policy_5: 0.04464
	accuracy_policy_5: 0.55646
	loss_value_5: 0.0383
	loss_reward_5: 0.00676
	loss_policy: 0.3367
	loss_value: 0.3399
	loss_reward: 0.02842
[2024-05-08 10:35:09] nn step 32800, lr: 0.1.
	loss_policy_0: 0.11882
	accuracy_policy_0: 0.73543
	loss_value_0: 0.15485
	loss_policy_1: 0.02981
	accuracy_policy_1: 0.67906
	loss_value_1: 0.03253
	loss_reward_1: 0.00517
	loss_policy_2: 0.03268
	accuracy_policy_2: 0.65506
	loss_value_2: 0.03382
	loss_reward_2: 0.005
	loss_policy_3: 0.0357
	accuracy_policy_3: 0.62832
	loss_value_3: 0.03506
	loss_reward_3: 0.00548
	loss_policy_4: 0.03835
	accuracy_policy_4: 0.60707
	loss_value_4: 0.03626
	loss_reward_4: 0.00573
	loss_policy_5: 0.04107
	accuracy_policy_5: 0.58619
	loss_value_5: 0.03743
	loss_reward_5: 0.00661
	loss_policy: 0.29643
	loss_value: 0.32994
	loss_reward: 0.028
Optimization_Done 32800
[2024-05-08 10:37:48] [command] train weight_iter_32800.pkl 163 165
[2024-05-08 10:38:31] nn step 32900, lr: 0.1.
	loss_policy_0: 0.14974
	accuracy_policy_0: 0.70383
	loss_value_0: 0.17337
	loss_policy_1: 0.03477
	accuracy_policy_1: 0.65787
	loss_value_1: 0.03633
	loss_reward_1: 0.00512
	loss_policy_2: 0.03728
	accuracy_policy_2: 0.63641
	loss_value_2: 0.03802
	loss_reward_2: 0.00483
	loss_policy_3: 0.04003
	accuracy_policy_3: 0.61609
	loss_value_3: 0.03929
	loss_reward_3: 0.00518
	loss_policy_4: 0.04262
	accuracy_policy_4: 0.59943
	loss_value_4: 0.04055
	loss_reward_4: 0.00552
	loss_policy_5: 0.04486
	accuracy_policy_5: 0.58395
	loss_value_5: 0.04185
	loss_reward_5: 0.00618
	loss_policy: 0.34931
	loss_value: 0.36942
	loss_reward: 0.02683
[2024-05-08 10:39:12] nn step 33000, lr: 0.1.
	loss_policy_0: 0.13116
	accuracy_policy_0: 0.74016
	loss_value_0: 0.17345
	loss_policy_1: 0.03156
	accuracy_policy_1: 0.69102
	loss_value_1: 0.03638
	loss_reward_1: 0.005
	loss_policy_2: 0.03424
	accuracy_policy_2: 0.66803
	loss_value_2: 0.03795
	loss_reward_2: 0.00488
	loss_policy_3: 0.03706
	accuracy_policy_3: 0.65119
	loss_value_3: 0.03931
	loss_reward_3: 0.00523
	loss_policy_4: 0.03978
	accuracy_policy_4: 0.62592
	loss_value_4: 0.04056
	loss_reward_4: 0.00562
	loss_policy_5: 0.04223
	accuracy_policy_5: 0.60854
	loss_value_5: 0.04196
	loss_reward_5: 0.00643
	loss_policy: 0.31603
	loss_value: 0.3696
	loss_reward: 0.02717
Optimization_Done 33000
[2024-05-08 10:41:53] [command] train weight_iter_33000.pkl 164 166
[2024-05-08 10:42:35] nn step 33100, lr: 0.1.
	loss_policy_0: 0.14294
	accuracy_policy_0: 0.70949
	loss_value_0: 0.16637
	loss_policy_1: 0.03358
	accuracy_policy_1: 0.66475
	loss_value_1: 0.03495
	loss_reward_1: 0.00526
	loss_policy_2: 0.03583
	accuracy_policy_2: 0.64814
	loss_value_2: 0.03659
	loss_reward_2: 0.00529
	loss_policy_3: 0.03873
	accuracy_policy_3: 0.62697
	loss_value_3: 0.03796
	loss_reward_3: 0.00544
	loss_policy_4: 0.04135
	accuracy_policy_4: 0.60631
	loss_value_4: 0.03912
	loss_reward_4: 0.00603
	loss_policy_5: 0.04354
	accuracy_policy_5: 0.59029
	loss_value_5: 0.04034
	loss_reward_5: 0.00677
	loss_policy: 0.33597
	loss_value: 0.35533
	loss_reward: 0.02879
[2024-05-08 10:43:17] nn step 33200, lr: 0.1.
	loss_policy_0: 0.13691
	accuracy_policy_0: 0.74484
	loss_value_0: 0.18072
	loss_policy_1: 0.03313
	accuracy_policy_1: 0.69602
	loss_value_1: 0.0378
	loss_reward_1: 0.00574
	loss_policy_2: 0.03576
	accuracy_policy_2: 0.67525
	loss_value_2: 0.03944
	loss_reward_2: 0.00554
	loss_policy_3: 0.03862
	accuracy_policy_3: 0.65865
	loss_value_3: 0.04099
	loss_reward_3: 0.00606
	loss_policy_4: 0.04131
	accuracy_policy_4: 0.636
	loss_value_4: 0.04229
	loss_reward_4: 0.00651
	loss_policy_5: 0.04438
	accuracy_policy_5: 0.6167
	loss_value_5: 0.04353
	loss_reward_5: 0.00733
	loss_policy: 0.33011
	loss_value: 0.38476
	loss_reward: 0.03118
Optimization_Done 33200
[2024-05-08 10:45:25] [command] train weight_iter_33200.pkl 165 167
[2024-05-08 10:46:08] nn step 33300, lr: 0.1.
	loss_policy_0: 0.16084
	accuracy_policy_0: 0.69723
	loss_value_0: 0.18788
	loss_policy_1: 0.03769
	accuracy_policy_1: 0.65188
	loss_value_1: 0.039
	loss_reward_1: 0.00605
	loss_policy_2: 0.04031
	accuracy_policy_2: 0.63094
	loss_value_2: 0.0404
	loss_reward_2: 0.00571
	loss_policy_3: 0.04324
	accuracy_policy_3: 0.61215
	loss_value_3: 0.04162
	loss_reward_3: 0.00611
	loss_policy_4: 0.04577
	accuracy_policy_4: 0.59086
	loss_value_4: 0.04269
	loss_reward_4: 0.00662
	loss_policy_5: 0.04799
	accuracy_policy_5: 0.5776
	loss_value_5: 0.04381
	loss_reward_5: 0.00747
	loss_policy: 0.37585
	loss_value: 0.3954
	loss_reward: 0.03196
[2024-05-08 10:46:49] nn step 33400, lr: 0.1.
	loss_policy_0: 0.13637
	accuracy_policy_0: 0.7391
	loss_value_0: 0.18041
	loss_policy_1: 0.03323
	accuracy_policy_1: 0.68627
	loss_value_1: 0.03764
	loss_reward_1: 0.00604
	loss_policy_2: 0.03646
	accuracy_policy_2: 0.66541
	loss_value_2: 0.03899
	loss_reward_2: 0.00565
	loss_policy_3: 0.03916
	accuracy_policy_3: 0.64326
	loss_value_3: 0.04015
	loss_reward_3: 0.00602
	loss_policy_4: 0.04184
	accuracy_policy_4: 0.62604
	loss_value_4: 0.04124
	loss_reward_4: 0.00642
	loss_policy_5: 0.04439
	accuracy_policy_5: 0.60332
	loss_value_5: 0.04227
	loss_reward_5: 0.00744
	loss_policy: 0.33145
	loss_value: 0.3807
	loss_reward: 0.03157
Optimization_Done 33400
[2024-05-08 10:49:28] [command] train weight_iter_33400.pkl 166 168
[2024-05-08 10:50:11] nn step 33500, lr: 0.1.
	loss_policy_0: 0.13853
	accuracy_policy_0: 0.70812
	loss_value_0: 0.17282
	loss_policy_1: 0.03434
	accuracy_policy_1: 0.65666
	loss_value_1: 0.03604
	loss_reward_1: 0.00544
	loss_policy_2: 0.03777
	accuracy_policy_2: 0.62832
	loss_value_2: 0.03735
	loss_reward_2: 0.00519
	loss_policy_3: 0.04061
	accuracy_policy_3: 0.60836
	loss_value_3: 0.03851
	loss_reward_3: 0.00556
	loss_policy_4: 0.04364
	accuracy_policy_4: 0.57891
	loss_value_4: 0.03955
	loss_reward_4: 0.00608
	loss_policy_5: 0.04663
	accuracy_policy_5: 0.55723
	loss_value_5: 0.04061
	loss_reward_5: 0.00711
	loss_policy: 0.34151
	loss_value: 0.36488
	loss_reward: 0.02938
[2024-05-08 10:50:52] nn step 33600, lr: 0.1.
	loss_policy_0: 0.12427
	accuracy_policy_0: 0.75043
	loss_value_0: 0.1811
	loss_policy_1: 0.03233
	accuracy_policy_1: 0.68926
	loss_value_1: 0.03797
	loss_reward_1: 0.00573
	loss_policy_2: 0.03622
	accuracy_policy_2: 0.65896
	loss_value_2: 0.03938
	loss_reward_2: 0.00547
	loss_policy_3: 0.0395
	accuracy_policy_3: 0.6348
	loss_value_3: 0.04062
	loss_reward_3: 0.0058
	loss_policy_4: 0.04265
	accuracy_policy_4: 0.61248
	loss_value_4: 0.04178
	loss_reward_4: 0.0062
	loss_policy_5: 0.04568
	accuracy_policy_5: 0.58855
	loss_value_5: 0.04295
	loss_reward_5: 0.00733
	loss_policy: 0.32066
	loss_value: 0.38381
	loss_reward: 0.03053
Optimization_Done 33600
[2024-05-08 10:53:32] [command] train weight_iter_33600.pkl 167 169
[2024-05-08 10:54:15] nn step 33700, lr: 0.1.
	loss_policy_0: 0.13955
	accuracy_policy_0: 0.71398
	loss_value_0: 0.1737
	loss_policy_1: 0.03439
	accuracy_policy_1: 0.65822
	loss_value_1: 0.0362
	loss_reward_1: 0.00512
	loss_policy_2: 0.03789
	accuracy_policy_2: 0.63074
	loss_value_2: 0.03755
	loss_reward_2: 0.00499
	loss_policy_3: 0.04111
	accuracy_policy_3: 0.6075
	loss_value_3: 0.03877
	loss_reward_3: 0.00536
	loss_policy_4: 0.04401
	accuracy_policy_4: 0.5823
	loss_value_4: 0.03989
	loss_reward_4: 0.00572
	loss_policy_5: 0.04703
	accuracy_policy_5: 0.56115
	loss_value_5: 0.04109
	loss_reward_5: 0.00669
	loss_policy: 0.34398
	loss_value: 0.3672
	loss_reward: 0.02787
[2024-05-08 10:54:55] nn step 33800, lr: 0.1.
	loss_policy_0: 0.12306
	accuracy_policy_0: 0.74979
	loss_value_0: 0.1769
	loss_policy_1: 0.03166
	accuracy_policy_1: 0.69322
	loss_value_1: 0.03694
	loss_reward_1: 0.00529
	loss_policy_2: 0.03533
	accuracy_policy_2: 0.66322
	loss_value_2: 0.03825
	loss_reward_2: 0.00494
	loss_policy_3: 0.03875
	accuracy_policy_3: 0.64111
	loss_value_3: 0.03958
	loss_reward_3: 0.00536
	loss_policy_4: 0.04202
	accuracy_policy_4: 0.61465
	loss_value_4: 0.04072
	loss_reward_4: 0.00575
	loss_policy_5: 0.0454
	accuracy_policy_5: 0.59244
	loss_value_5: 0.04187
	loss_reward_5: 0.00695
	loss_policy: 0.31622
	loss_value: 0.37426
	loss_reward: 0.02829
Optimization_Done 33800
[2024-05-08 10:57:31] [command] train weight_iter_33800.pkl 168 170
[2024-05-08 10:58:14] nn step 33900, lr: 0.1.
	loss_policy_0: 0.13051
	accuracy_policy_0: 0.72572
	loss_value_0: 0.17057
	loss_policy_1: 0.03194
	accuracy_policy_1: 0.67207
	loss_value_1: 0.03578
	loss_reward_1: 0.00523
	loss_policy_2: 0.03519
	accuracy_policy_2: 0.64936
	loss_value_2: 0.03714
	loss_reward_2: 0.00518
	loss_policy_3: 0.03857
	accuracy_policy_3: 0.62289
	loss_value_3: 0.03844
	loss_reward_3: 0.00562
	loss_policy_4: 0.04208
	accuracy_policy_4: 0.59328
	loss_value_4: 0.03966
	loss_reward_4: 0.00607
	loss_policy_5: 0.04514
	accuracy_policy_5: 0.56906
	loss_value_5: 0.04092
	loss_reward_5: 0.00701
	loss_policy: 0.32344
	loss_value: 0.36251
	loss_reward: 0.02911
[2024-05-08 10:58:54] nn step 34000, lr: 0.1.
	loss_policy_0: 0.11848
	accuracy_policy_0: 0.75848
	loss_value_0: 0.17607
	loss_policy_1: 0.03041
	accuracy_policy_1: 0.70107
	loss_value_1: 0.03687
	loss_reward_1: 0.0053
	loss_policy_2: 0.0342
	accuracy_policy_2: 0.6732
	loss_value_2: 0.03842
	loss_reward_2: 0.00526
	loss_policy_3: 0.03774
	accuracy_policy_3: 0.64508
	loss_value_3: 0.03966
	loss_reward_3: 0.00574
	loss_policy_4: 0.04105
	accuracy_policy_4: 0.61852
	loss_value_4: 0.04078
	loss_reward_4: 0.00608
	loss_policy_5: 0.04447
	accuracy_policy_5: 0.59564
	loss_value_5: 0.04222
	loss_reward_5: 0.00725
	loss_policy: 0.30636
	loss_value: 0.37401
	loss_reward: 0.02964
Optimization_Done 34000
[2024-05-08 11:01:28] [command] train weight_iter_34000.pkl 169 171
[2024-05-08 11:02:10] nn step 34100, lr: 0.1.
	loss_policy_0: 0.10222
	accuracy_policy_0: 0.75113
	loss_value_0: 0.15033
	loss_policy_1: 0.02614
	accuracy_policy_1: 0.69145
	loss_value_1: 0.03129
	loss_reward_1: 0.00464
	loss_policy_2: 0.02931
	accuracy_policy_2: 0.66207
	loss_value_2: 0.03254
	loss_reward_2: 0.00439
	loss_policy_3: 0.03189
	accuracy_policy_3: 0.63746
	loss_value_3: 0.03372
	loss_reward_3: 0.00497
	loss_policy_4: 0.03481
	accuracy_policy_4: 0.61316
	loss_value_4: 0.03479
	loss_reward_4: 0.00528
	loss_policy_5: 0.03712
	accuracy_policy_5: 0.59391
	loss_value_5: 0.03584
	loss_reward_5: 0.00612
	loss_policy: 0.2615
	loss_value: 0.31851
	loss_reward: 0.0254
[2024-05-08 11:02:51] nn step 34200, lr: 0.1.
	loss_policy_0: 0.09879
	accuracy_policy_0: 0.77926
	loss_value_0: 0.16319
	loss_policy_1: 0.02672
	accuracy_policy_1: 0.71041
	loss_value_1: 0.03429
	loss_reward_1: 0.0049
	loss_policy_2: 0.03007
	accuracy_policy_2: 0.68104
	loss_value_2: 0.03564
	loss_reward_2: 0.00466
	loss_policy_3: 0.03337
	accuracy_policy_3: 0.65656
	loss_value_3: 0.03666
	loss_reward_3: 0.00506
	loss_policy_4: 0.03601
	accuracy_policy_4: 0.63459
	loss_value_4: 0.03771
	loss_reward_4: 0.00543
	loss_policy_5: 0.03881
	accuracy_policy_5: 0.61187
	loss_value_5: 0.03891
	loss_reward_5: 0.00658
	loss_policy: 0.26378
	loss_value: 0.3464
	loss_reward: 0.02662
Optimization_Done 34200
[2024-05-08 11:05:25] [command] train weight_iter_34200.pkl 170 172
[2024-05-08 11:06:07] nn step 34300, lr: 0.1.
	loss_policy_0: 0.10061
	accuracy_policy_0: 0.74832
	loss_value_0: 0.15366
	loss_policy_1: 0.02575
	accuracy_policy_1: 0.69125
	loss_value_1: 0.03207
	loss_reward_1: 0.00405
	loss_policy_2: 0.02845
	accuracy_policy_2: 0.66373
	loss_value_2: 0.03325
	loss_reward_2: 0.00402
	loss_policy_3: 0.03097
	accuracy_policy_3: 0.6409
	loss_value_3: 0.03425
	loss_reward_3: 0.00433
	loss_policy_4: 0.03344
	accuracy_policy_4: 0.61664
	loss_value_4: 0.03517
	loss_reward_4: 0.00459
	loss_policy_5: 0.03585
	accuracy_policy_5: 0.59828
	loss_value_5: 0.03615
	loss_reward_5: 0.00542
	loss_policy: 0.25507
	loss_value: 0.32455
	loss_reward: 0.02241
[2024-05-08 11:06:48] nn step 34400, lr: 0.1.
	loss_policy_0: 0.08057
	accuracy_policy_0: 0.79674
	loss_value_0: 0.15075
	loss_policy_1: 0.02255
	accuracy_policy_1: 0.72656
	loss_value_1: 0.03167
	loss_reward_1: 0.00401
	loss_policy_2: 0.02537
	accuracy_policy_2: 0.69992
	loss_value_2: 0.03262
	loss_reward_2: 0.00397
	loss_policy_3: 0.02788
	accuracy_policy_3: 0.67881
	loss_value_3: 0.03362
	loss_reward_3: 0.00432
	loss_policy_4: 0.03016
	accuracy_policy_4: 0.65984
	loss_value_4: 0.03455
	loss_reward_4: 0.00466
	loss_policy_5: 0.03265
	accuracy_policy_5: 0.63736
	loss_value_5: 0.03553
	loss_reward_5: 0.00533
	loss_policy: 0.21918
	loss_value: 0.31874
	loss_reward: 0.02229
Optimization_Done 34400
[2024-05-08 11:09:22] [command] train weight_iter_34400.pkl 171 173
[2024-05-08 11:10:05] nn step 34500, lr: 0.1.
	loss_policy_0: 0.11203
	accuracy_policy_0: 0.73223
	loss_value_0: 0.15363
	loss_policy_1: 0.02758
	accuracy_policy_1: 0.6802
	loss_value_1: 0.03179
	loss_reward_1: 0.00365
	loss_policy_2: 0.02976
	accuracy_policy_2: 0.65598
	loss_value_2: 0.03294
	loss_reward_2: 0.00373
	loss_policy_3: 0.03238
	accuracy_policy_3: 0.63301
	loss_value_3: 0.03392
	loss_reward_3: 0.00403
	loss_policy_4: 0.03421
	accuracy_policy_4: 0.61998
	loss_value_4: 0.03469
	loss_reward_4: 0.00429
	loss_policy_5: 0.03587
	accuracy_policy_5: 0.60609
	loss_value_5: 0.03554
	loss_reward_5: 0.00474
	loss_policy: 0.27184
	loss_value: 0.3225
	loss_reward: 0.02044
[2024-05-08 11:10:46] nn step 34600, lr: 0.1.
	loss_policy_0: 0.09412
	accuracy_policy_0: 0.77592
	loss_value_0: 0.14879
	loss_policy_1: 0.02485
	accuracy_policy_1: 0.71242
	loss_value_1: 0.03105
	loss_reward_1: 0.00364
	loss_policy_2: 0.02737
	accuracy_policy_2: 0.68717
	loss_value_2: 0.03214
	loss_reward_2: 0.00352
	loss_policy_3: 0.02968
	accuracy_policy_3: 0.66738
	loss_value_3: 0.033
	loss_reward_3: 0.00376
	loss_policy_4: 0.03161
	accuracy_policy_4: 0.64939
	loss_value_4: 0.03376
	loss_reward_4: 0.00423
	loss_policy_5: 0.03357
	accuracy_policy_5: 0.63404
	loss_value_5: 0.03468
	loss_reward_5: 0.00492
	loss_policy: 0.24119
	loss_value: 0.31343
	loss_reward: 0.02007
Optimization_Done 34600
[2024-05-08 11:13:06] [command] train weight_iter_34600.pkl 172 174
[2024-05-08 11:13:49] nn step 34700, lr: 0.1.
	loss_policy_0: 0.143
	accuracy_policy_0: 0.71648
	loss_value_0: 0.17706
	loss_policy_1: 0.03392
	accuracy_policy_1: 0.66727
	loss_value_1: 0.03719
	loss_reward_1: 0.0049
	loss_policy_2: 0.03652
	accuracy_policy_2: 0.64932
	loss_value_2: 0.03846
	loss_reward_2: 0.00485
	loss_policy_3: 0.03928
	accuracy_policy_3: 0.62805
	loss_value_3: 0.0397
	loss_reward_3: 0.00517
	loss_policy_4: 0.04168
	accuracy_policy_4: 0.6125
	loss_value_4: 0.04088
	loss_reward_4: 0.00577
	loss_policy_5: 0.04413
	accuracy_policy_5: 0.58865
	loss_value_5: 0.04205
	loss_reward_5: 0.00642
	loss_policy: 0.33852
	loss_value: 0.37533
	loss_reward: 0.02711
[2024-05-08 11:14:30] nn step 34800, lr: 0.1.
	loss_policy_0: 0.11213
	accuracy_policy_0: 0.75822
	loss_value_0: 0.15914
	loss_policy_1: 0.02808
	accuracy_policy_1: 0.70471
	loss_value_1: 0.03354
	loss_reward_1: 0.00438
	loss_policy_2: 0.03065
	accuracy_policy_2: 0.68414
	loss_value_2: 0.03468
	loss_reward_2: 0.00443
	loss_policy_3: 0.03278
	accuracy_policy_3: 0.66588
	loss_value_3: 0.03586
	loss_reward_3: 0.00476
	loss_policy_4: 0.03503
	accuracy_policy_4: 0.64865
	loss_value_4: 0.03692
	loss_reward_4: 0.00509
	loss_policy_5: 0.03753
	accuracy_policy_5: 0.63105
	loss_value_5: 0.03787
	loss_reward_5: 0.00574
	loss_policy: 0.27619
	loss_value: 0.33801
	loss_reward: 0.02441
Optimization_Done 34800
[2024-05-08 11:16:40] [command] train weight_iter_34800.pkl 173 175
[2024-05-08 11:17:23] nn step 34900, lr: 0.1.
	loss_policy_0: 0.14098
	accuracy_policy_0: 0.70889
	loss_value_0: 0.17515
	loss_policy_1: 0.03414
	accuracy_policy_1: 0.65982
	loss_value_1: 0.03655
	loss_reward_1: 0.00535
	loss_policy_2: 0.03718
	accuracy_policy_2: 0.63746
	loss_value_2: 0.03781
	loss_reward_2: 0.0052
	loss_policy_3: 0.03996
	accuracy_policy_3: 0.61984
	loss_value_3: 0.03906
	loss_reward_3: 0.00532
	loss_policy_4: 0.04244
	accuracy_policy_4: 0.60117
	loss_value_4: 0.04009
	loss_reward_4: 0.00586
	loss_policy_5: 0.04475
	accuracy_policy_5: 0.58732
	loss_value_5: 0.04112
	loss_reward_5: 0.00668
	loss_policy: 0.33945
	loss_value: 0.36979
	loss_reward: 0.02841
[2024-05-08 11:18:04] nn step 35000, lr: 0.1.
	loss_policy_0: 0.12494
	accuracy_policy_0: 0.74264
	loss_value_0: 0.17366
	loss_policy_1: 0.03155
	accuracy_policy_1: 0.68916
	loss_value_1: 0.03628
	loss_reward_1: 0.00544
	loss_policy_2: 0.03469
	accuracy_policy_2: 0.6634
	loss_value_2: 0.03776
	loss_reward_2: 0.00504
	loss_policy_3: 0.03743
	accuracy_policy_3: 0.63875
	loss_value_3: 0.03913
	loss_reward_3: 0.00542
	loss_policy_4: 0.04
	accuracy_policy_4: 0.62445
	loss_value_4: 0.0403
	loss_reward_4: 0.00578
	loss_policy_5: 0.04266
	accuracy_policy_5: 0.60996
	loss_value_5: 0.04148
	loss_reward_5: 0.00669
	loss_policy: 0.31128
	loss_value: 0.36861
	loss_reward: 0.02837
Optimization_Done 35000
[2024-05-08 11:20:14] [command] train weight_iter_35000.pkl 174 176
[2024-05-08 11:20:57] nn step 35100, lr: 0.1.
	loss_policy_0: 0.1557
	accuracy_policy_0: 0.69043
	loss_value_0: 0.18087
	loss_policy_1: 0.0371
	accuracy_policy_1: 0.63574
	loss_value_1: 0.03774
	loss_reward_1: 0.00532
	loss_policy_2: 0.03995
	accuracy_policy_2: 0.61412
	loss_value_2: 0.03922
	loss_reward_2: 0.00502
	loss_policy_3: 0.04289
	accuracy_policy_3: 0.58691
	loss_value_3: 0.04037
	loss_reward_3: 0.00534
	loss_policy_4: 0.04533
	accuracy_policy_4: 0.5741
	loss_value_4: 0.04139
	loss_reward_4: 0.00575
	loss_policy_5: 0.04741
	accuracy_policy_5: 0.55533
	loss_value_5: 0.0424
	loss_reward_5: 0.00664
	loss_policy: 0.36837
	loss_value: 0.38199
	loss_reward: 0.02807
[2024-05-08 11:21:38] nn step 35200, lr: 0.1.
	loss_policy_0: 0.12819
	accuracy_policy_0: 0.73439
	loss_value_0: 0.1705
	loss_policy_1: 0.03235
	accuracy_policy_1: 0.67242
	loss_value_1: 0.03579
	loss_reward_1: 0.00505
	loss_policy_2: 0.03562
	accuracy_policy_2: 0.64701
	loss_value_2: 0.03703
	loss_reward_2: 0.0048
	loss_policy_3: 0.03829
	accuracy_policy_3: 0.62561
	loss_value_3: 0.03817
	loss_reward_3: 0.00528
	loss_policy_4: 0.04045
	accuracy_policy_4: 0.60641
	loss_value_4: 0.03933
	loss_reward_4: 0.00564
	loss_policy_5: 0.04284
	accuracy_policy_5: 0.58809
	loss_value_5: 0.04039
	loss_reward_5: 0.00656
	loss_policy: 0.31774
	loss_value: 0.36121
	loss_reward: 0.02734
Optimization_Done 35200
[2024-05-08 11:24:14] [command] train weight_iter_35200.pkl 175 177
[2024-05-08 11:24:56] nn step 35300, lr: 0.1.
	loss_policy_0: 0.17037
	accuracy_policy_0: 0.67924
	loss_value_0: 0.17218
	loss_policy_1: 0.04124
	accuracy_policy_1: 0.62088
	loss_value_1: 0.03614
	loss_reward_1: 0.00505
	loss_policy_2: 0.04465
	accuracy_policy_2: 0.59723
	loss_value_2: 0.03758
	loss_reward_2: 0.00481
	loss_policy_3: 0.04787
	accuracy_policy_3: 0.57457
	loss_value_3: 0.03892
	loss_reward_3: 0.0052
	loss_policy_4: 0.05061
	accuracy_policy_4: 0.55346
	loss_value_4: 0.04019
	loss_reward_4: 0.00561
	loss_policy_5: 0.05355
	accuracy_policy_5: 0.53029
	loss_value_5: 0.04127
	loss_reward_5: 0.00645
	loss_policy: 0.40829
	loss_value: 0.36629
	loss_reward: 0.02713
[2024-05-08 11:25:37] nn step 35400, lr: 0.1.
	loss_policy_0: 0.14179
	accuracy_policy_0: 0.72531
	loss_value_0: 0.16925
	loss_policy_1: 0.03609
	accuracy_policy_1: 0.66088
	loss_value_1: 0.0355
	loss_reward_1: 0.00501
	loss_policy_2: 0.03965
	accuracy_policy_2: 0.63613
	loss_value_2: 0.03694
	loss_reward_2: 0.00463
	loss_policy_3: 0.04308
	accuracy_policy_3: 0.61148
	loss_value_3: 0.03816
	loss_reward_3: 0.00505
	loss_policy_4: 0.04634
	accuracy_policy_4: 0.58625
	loss_value_4: 0.03923
	loss_reward_4: 0.00539
	loss_policy_5: 0.04906
	accuracy_policy_5: 0.56184
	loss_value_5: 0.04031
	loss_reward_5: 0.0064
	loss_policy: 0.35601
	loss_value: 0.3594
	loss_reward: 0.02649
Optimization_Done 35400
[2024-05-08 11:28:13] [command] train weight_iter_35400.pkl 176 178
[2024-05-08 11:28:55] nn step 35500, lr: 0.1.
	loss_policy_0: 0.16658
	accuracy_policy_0: 0.6735
	loss_value_0: 0.16438
	loss_policy_1: 0.03894
	accuracy_policy_1: 0.6259
	loss_value_1: 0.03459
	loss_reward_1: 0.00448
	loss_policy_2: 0.04183
	accuracy_policy_2: 0.60168
	loss_value_2: 0.03605
	loss_reward_2: 0.00442
	loss_policy_3: 0.04481
	accuracy_policy_3: 0.57922
	loss_value_3: 0.03721
	loss_reward_3: 0.00462
	loss_policy_4: 0.04754
	accuracy_policy_4: 0.55783
	loss_value_4: 0.03834
	loss_reward_4: 0.00517
	loss_policy_5: 0.05026
	accuracy_policy_5: 0.53887
	loss_value_5: 0.03948
	loss_reward_5: 0.00605
	loss_policy: 0.38996
	loss_value: 0.35005
	loss_reward: 0.02474
[2024-05-08 11:29:36] nn step 35600, lr: 0.1.
	loss_policy_0: 0.15717
	accuracy_policy_0: 0.71064
	loss_value_0: 0.17619
	loss_policy_1: 0.03817
	accuracy_policy_1: 0.6565
	loss_value_1: 0.03706
	loss_reward_1: 0.00498
	loss_policy_2: 0.04102
	accuracy_policy_2: 0.63525
	loss_value_2: 0.03841
	loss_reward_2: 0.00471
	loss_policy_3: 0.04428
	accuracy_policy_3: 0.61197
	loss_value_3: 0.03981
	loss_reward_3: 0.00499
	loss_policy_4: 0.04755
	accuracy_policy_4: 0.58879
	loss_value_4: 0.04095
	loss_reward_4: 0.00539
	loss_policy_5: 0.05107
	accuracy_policy_5: 0.56119
	loss_value_5: 0.04214
	loss_reward_5: 0.00652
	loss_policy: 0.37924
	loss_value: 0.37457
	loss_reward: 0.02659
Optimization_Done 35600
[2024-05-08 11:32:11] [command] train weight_iter_35600.pkl 177 179
[2024-05-08 11:32:53] nn step 35700, lr: 0.1.
	loss_policy_0: 0.16339
	accuracy_policy_0: 0.67506
	loss_value_0: 0.17131
	loss_policy_1: 0.03902
	accuracy_policy_1: 0.62525
	loss_value_1: 0.03591
	loss_reward_1: 0.00549
	loss_policy_2: 0.04222
	accuracy_policy_2: 0.59908
	loss_value_2: 0.03755
	loss_reward_2: 0.0052
	loss_policy_3: 0.04547
	accuracy_policy_3: 0.57602
	loss_value_3: 0.03878
	loss_reward_3: 0.00536
	loss_policy_4: 0.04845
	accuracy_policy_4: 0.55123
	loss_value_4: 0.04012
	loss_reward_4: 0.00607
	loss_policy_5: 0.0518
	accuracy_policy_5: 0.52885
	loss_value_5: 0.04133
	loss_reward_5: 0.00692
	loss_policy: 0.39036
	loss_value: 0.36499
	loss_reward: 0.02903
[2024-05-08 11:33:34] nn step 35800, lr: 0.1.
	loss_policy_0: 0.13998
	accuracy_policy_0: 0.71348
	loss_value_0: 0.16936
	loss_policy_1: 0.03473
	accuracy_policy_1: 0.65734
	loss_value_1: 0.03546
	loss_reward_1: 0.00515
	loss_policy_2: 0.03799
	accuracy_policy_2: 0.62988
	loss_value_2: 0.03698
	loss_reward_2: 0.00487
	loss_policy_3: 0.04112
	accuracy_policy_3: 0.60668
	loss_value_3: 0.03819
	loss_reward_3: 0.00536
	loss_policy_4: 0.04485
	accuracy_policy_4: 0.58117
	loss_value_4: 0.03939
	loss_reward_4: 0.00565
	loss_policy_5: 0.04807
	accuracy_policy_5: 0.55537
	loss_value_5: 0.04051
	loss_reward_5: 0.00687
	loss_policy: 0.34674
	loss_value: 0.35989
	loss_reward: 0.02791
Optimization_Done 35800
[2024-05-08 11:36:10] [command] train weight_iter_35800.pkl 178 180
[2024-05-08 11:36:52] nn step 35900, lr: 0.1.
	loss_policy_0: 0.1332
	accuracy_policy_0: 0.69773
	loss_value_0: 0.1624
	loss_policy_1: 0.0331
	accuracy_policy_1: 0.64061
	loss_value_1: 0.03393
	loss_reward_1: 0.00457
	loss_policy_2: 0.03586
	accuracy_policy_2: 0.61498
	loss_value_2: 0.03512
	loss_reward_2: 0.00441
	loss_policy_3: 0.03887
	accuracy_policy_3: 0.58826
	loss_value_3: 0.03624
	loss_reward_3: 0.00485
	loss_policy_4: 0.04153
	accuracy_policy_4: 0.56643
	loss_value_4: 0.03727
	loss_reward_4: 0.0052
	loss_policy_5: 0.04429
	accuracy_policy_5: 0.54545
	loss_value_5: 0.03842
	loss_reward_5: 0.00595
	loss_policy: 0.32685
	loss_value: 0.3434
	loss_reward: 0.02497
[2024-05-08 11:37:33] nn step 36000, lr: 0.1.
	loss_policy_0: 0.11087
	accuracy_policy_0: 0.73775
	loss_value_0: 0.15429
	loss_policy_1: 0.02839
	accuracy_policy_1: 0.6757
	loss_value_1: 0.03222
	loss_reward_1: 0.00441
	loss_policy_2: 0.03133
	accuracy_policy_2: 0.64482
	loss_value_2: 0.0334
	loss_reward_2: 0.00418
	loss_policy_3: 0.03397
	accuracy_policy_3: 0.62402
	loss_value_3: 0.0345
	loss_reward_3: 0.00455
	loss_policy_4: 0.03685
	accuracy_policy_4: 0.60088
	loss_value_4: 0.03547
	loss_reward_4: 0.00486
	loss_policy_5: 0.03952
	accuracy_policy_5: 0.57777
	loss_value_5: 0.03649
	loss_reward_5: 0.00571
	loss_policy: 0.28092
	loss_value: 0.32636
	loss_reward: 0.02372
Optimization_Done 36000
[2024-05-08 11:40:08] [command] train weight_iter_36000.pkl 179 181
[2024-05-08 11:40:51] nn step 36100, lr: 0.1.
	loss_policy_0: 0.15507
	accuracy_policy_0: 0.6733
	loss_value_0: 0.16561
	loss_policy_1: 0.03803
	accuracy_policy_1: 0.61455
	loss_value_1: 0.03481
	loss_reward_1: 0.00479
	loss_policy_2: 0.04144
	accuracy_policy_2: 0.58574
	loss_value_2: 0.03623
	loss_reward_2: 0.00452
	loss_policy_3: 0.04466
	accuracy_policy_3: 0.5598
	loss_value_3: 0.03755
	loss_reward_3: 0.00495
	loss_policy_4: 0.04741
	accuracy_policy_4: 0.53715
	loss_value_4: 0.03866
	loss_reward_4: 0.00536
	loss_policy_5: 0.05026
	accuracy_policy_5: 0.51598
	loss_value_5: 0.03988
	loss_reward_5: 0.0061
	loss_policy: 0.37687
	loss_value: 0.35274
	loss_reward: 0.02571
[2024-05-08 11:41:33] nn step 36200, lr: 0.1.
	loss_policy_0: 0.11938
	accuracy_policy_0: 0.7225
	loss_value_0: 0.14921
	loss_policy_1: 0.03076
	accuracy_policy_1: 0.65041
	loss_value_1: 0.03131
	loss_reward_1: 0.00439
	loss_policy_2: 0.03393
	accuracy_policy_2: 0.62305
	loss_value_2: 0.03252
	loss_reward_2: 0.00405
	loss_policy_3: 0.03719
	accuracy_policy_3: 0.5984
	loss_value_3: 0.03354
	loss_reward_3: 0.00438
	loss_policy_4: 0.03961
	accuracy_policy_4: 0.57652
	loss_value_4: 0.0345
	loss_reward_4: 0.00472
	loss_policy_5: 0.04245
	accuracy_policy_5: 0.55363
	loss_value_5: 0.03553
	loss_reward_5: 0.00552
	loss_policy: 0.30333
	loss_value: 0.31662
	loss_reward: 0.02306
Optimization_Done 36200
[2024-05-08 11:44:12] [command] train weight_iter_36200.pkl 180 182
[2024-05-08 11:44:55] nn step 36300, lr: 0.1.
	loss_policy_0: 0.15011
	accuracy_policy_0: 0.67312
	loss_value_0: 0.15948
	loss_policy_1: 0.03546
	accuracy_policy_1: 0.6282
	loss_value_1: 0.03329
	loss_reward_1: 0.00446
	loss_policy_2: 0.03815
	accuracy_policy_2: 0.60283
	loss_value_2: 0.03449
	loss_reward_2: 0.00436
	loss_policy_3: 0.0414
	accuracy_policy_3: 0.57598
	loss_value_3: 0.03569
	loss_reward_3: 0.00468
	loss_policy_4: 0.04371
	accuracy_policy_4: 0.55885
	loss_value_4: 0.03675
	loss_reward_4: 0.00495
	loss_policy_5: 0.04585
	accuracy_policy_5: 0.5441
	loss_value_5: 0.03778
	loss_reward_5: 0.00563
	loss_policy: 0.35469
	loss_value: 0.33748
	loss_reward: 0.02408
[2024-05-08 11:45:36] nn step 36400, lr: 0.1.
	loss_policy_0: 0.12744
	accuracy_policy_0: 0.71209
	loss_value_0: 0.15282
	loss_policy_1: 0.03164
	accuracy_policy_1: 0.65318
	loss_value_1: 0.03196
	loss_reward_1: 0.00438
	loss_policy_2: 0.03456
	accuracy_policy_2: 0.62813
	loss_value_2: 0.03329
	loss_reward_2: 0.00406
	loss_policy_3: 0.03752
	accuracy_policy_3: 0.60502
	loss_value_3: 0.03441
	loss_reward_3: 0.00447
	loss_policy_4: 0.04051
	accuracy_policy_4: 0.58264
	loss_value_4: 0.03556
	loss_reward_4: 0.0048
	loss_policy_5: 0.0427
	accuracy_policy_5: 0.56484
	loss_value_5: 0.03662
	loss_reward_5: 0.0056
	loss_policy: 0.31436
	loss_value: 0.32466
	loss_reward: 0.0233
Optimization_Done 36400
[2024-05-08 11:48:12] [command] train weight_iter_36400.pkl 181 183
[2024-05-08 11:48:54] nn step 36500, lr: 0.1.
	loss_policy_0: 0.1435
	accuracy_policy_0: 0.69363
	loss_value_0: 0.16688
	loss_policy_1: 0.0346
	accuracy_policy_1: 0.64014
	loss_value_1: 0.0349
	loss_reward_1: 0.00509
	loss_policy_2: 0.03712
	accuracy_policy_2: 0.62027
	loss_value_2: 0.03632
	loss_reward_2: 0.00486
	loss_policy_3: 0.04058
	accuracy_policy_3: 0.59541
	loss_value_3: 0.03753
	loss_reward_3: 0.00558
	loss_policy_4: 0.04329
	accuracy_policy_4: 0.58131
	loss_value_4: 0.03867
	loss_reward_4: 0.006
	loss_policy_5: 0.04636
	accuracy_policy_5: 0.5559
	loss_value_5: 0.03982
	loss_reward_5: 0.00689
	loss_policy: 0.34545
	loss_value: 0.3541
	loss_reward: 0.02842
[2024-05-08 11:49:36] nn step 36600, lr: 0.1.
	loss_policy_0: 0.12903
	accuracy_policy_0: 0.7277
	loss_value_0: 0.16627
	loss_policy_1: 0.03242
	accuracy_policy_1: 0.66674
	loss_value_1: 0.03486
	loss_reward_1: 0.00503
	loss_policy_2: 0.03571
	accuracy_policy_2: 0.64049
	loss_value_2: 0.03615
	loss_reward_2: 0.00495
	loss_policy_3: 0.0391
	accuracy_policy_3: 0.61766
	loss_value_3: 0.03748
	loss_reward_3: 0.00529
	loss_policy_4: 0.0417
	accuracy_policy_4: 0.59941
	loss_value_4: 0.03867
	loss_reward_4: 0.00591
	loss_policy_5: 0.04476
	accuracy_policy_5: 0.57736
	loss_value_5: 0.03977
	loss_reward_5: 0.00708
	loss_policy: 0.32273
	loss_value: 0.35319
	loss_reward: 0.02826
Optimization_Done 36600
[2024-05-08 11:51:59] [command] train weight_iter_36600.pkl 182 184
[2024-05-08 11:52:42] nn step 36700, lr: 0.1.
	loss_policy_0: 0.13432
	accuracy_policy_0: 0.70795
	loss_value_0: 0.1737
	loss_policy_1: 0.03305
	accuracy_policy_1: 0.65061
	loss_value_1: 0.03628
	loss_reward_1: 0.00595
	loss_policy_2: 0.03627
	accuracy_policy_2: 0.62266
	loss_value_2: 0.03767
	loss_reward_2: 0.00556
	loss_policy_3: 0.03944
	accuracy_policy_3: 0.60197
	loss_value_3: 0.03901
	loss_reward_3: 0.00606
	loss_policy_4: 0.0421
	accuracy_policy_4: 0.58021
	loss_value_4: 0.04008
	loss_reward_4: 0.00681
	loss_policy_5: 0.04502
	accuracy_policy_5: 0.55656
	loss_value_5: 0.04116
	loss_reward_5: 0.00786
	loss_policy: 0.3302
	loss_value: 0.3679
	loss_reward: 0.03223
[2024-05-08 11:53:23] nn step 36800, lr: 0.1.
	loss_policy_0: 0.12762
	accuracy_policy_0: 0.73607
	loss_value_0: 0.18002
	loss_policy_1: 0.03278
	accuracy_policy_1: 0.66795
	loss_value_1: 0.03779
	loss_reward_1: 0.00615
	loss_policy_2: 0.03632
	accuracy_policy_2: 0.64031
	loss_value_2: 0.03917
	loss_reward_2: 0.00585
	loss_policy_3: 0.03949
	accuracy_policy_3: 0.61812
	loss_value_3: 0.04039
	loss_reward_3: 0.00627
	loss_policy_4: 0.04207
	accuracy_policy_4: 0.60232
	loss_value_4: 0.0416
	loss_reward_4: 0.00692
	loss_policy_5: 0.04552
	accuracy_policy_5: 0.57734
	loss_value_5: 0.04282
	loss_reward_5: 0.00841
	loss_policy: 0.3238
	loss_value: 0.38179
	loss_reward: 0.0336
Optimization_Done 36800
[2024-05-08 11:55:48] [command] train weight_iter_36800.pkl 183 185
[2024-05-08 11:56:33] nn step 36900, lr: 0.1.
	loss_policy_0: 0.14436
	accuracy_policy_0: 0.69689
	loss_value_0: 0.17796
	loss_policy_1: 0.03585
	accuracy_policy_1: 0.63561
	loss_value_1: 0.03718
	loss_reward_1: 0.00548
	loss_policy_2: 0.03946
	accuracy_policy_2: 0.60871
	loss_value_2: 0.03849
	loss_reward_2: 0.0053
	loss_policy_3: 0.04182
	accuracy_policy_3: 0.58918
	loss_value_3: 0.03955
	loss_reward_3: 0.00577
	loss_policy_4: 0.04434
	accuracy_policy_4: 0.5692
	loss_value_4: 0.04057
	loss_reward_4: 0.00603
	loss_policy_5: 0.04705
	accuracy_policy_5: 0.54561
	loss_value_5: 0.04148
	loss_reward_5: 0.00714
	loss_policy: 0.35288
	loss_value: 0.37523
	loss_reward: 0.02973
[2024-05-08 11:57:14] nn step 37000, lr: 0.1.
	loss_policy_0: 0.12757
	accuracy_policy_0: 0.74053
	loss_value_0: 0.17712
	loss_policy_1: 0.03267
	accuracy_policy_1: 0.67176
	loss_value_1: 0.03708
	loss_reward_1: 0.00551
	loss_policy_2: 0.03641
	accuracy_policy_2: 0.64439
	loss_value_2: 0.03844
	loss_reward_2: 0.00554
	loss_policy_3: 0.03921
	accuracy_policy_3: 0.62783
	loss_value_3: 0.03969
	loss_reward_3: 0.00591
	loss_policy_4: 0.0418
	accuracy_policy_4: 0.60449
	loss_value_4: 0.04069
	loss_reward_4: 0.00631
	loss_policy_5: 0.04471
	accuracy_policy_5: 0.58
	loss_value_5: 0.04174
	loss_reward_5: 0.00752
	loss_policy: 0.32237
	loss_value: 0.37476
	loss_reward: 0.03079
Optimization_Done 37000
[2024-05-08 11:59:25] [command] train weight_iter_37000.pkl 184 186
[2024-05-08 12:00:07] nn step 37100, lr: 0.1.
	loss_policy_0: 0.148
	accuracy_policy_0: 0.69318
	loss_value_0: 0.16351
	loss_policy_1: 0.03574
	accuracy_policy_1: 0.64201
	loss_value_1: 0.03418
	loss_reward_1: 0.00559
	loss_policy_2: 0.03931
	accuracy_policy_2: 0.61125
	loss_value_2: 0.03546
	loss_reward_2: 0.00522
	loss_policy_3: 0.04215
	accuracy_policy_3: 0.59055
	loss_value_3: 0.0367
	loss_reward_3: 0.00556
	loss_policy_4: 0.04502
	accuracy_policy_4: 0.57057
	loss_value_4: 0.03769
	loss_reward_4: 0.0059
	loss_policy_5: 0.0475
	accuracy_policy_5: 0.55197
	loss_value_5: 0.03886
	loss_reward_5: 0.00697
	loss_policy: 0.35772
	loss_value: 0.34639
	loss_reward: 0.02925
[2024-05-08 12:00:48] nn step 37200, lr: 0.1.
	loss_policy_0: 0.12932
	accuracy_policy_0: 0.7332
	loss_value_0: 0.16466
	loss_policy_1: 0.03266
	accuracy_policy_1: 0.67105
	loss_value_1: 0.03453
	loss_reward_1: 0.00551
	loss_policy_2: 0.03583
	accuracy_policy_2: 0.64549
	loss_value_2: 0.03571
	loss_reward_2: 0.00517
	loss_policy_3: 0.03925
	accuracy_policy_3: 0.62223
	loss_value_3: 0.03679
	loss_reward_3: 0.00559
	loss_policy_4: 0.04214
	accuracy_policy_4: 0.59607
	loss_value_4: 0.0378
	loss_reward_4: 0.00588
	loss_policy_5: 0.04501
	accuracy_policy_5: 0.57447
	loss_value_5: 0.03884
	loss_reward_5: 0.007
	loss_policy: 0.32421
	loss_value: 0.34833
	loss_reward: 0.02915
Optimization_Done 37200
[2024-05-08 12:03:11] [command] train weight_iter_37200.pkl 185 187
[2024-05-08 12:03:54] nn step 37300, lr: 0.1.
	loss_policy_0: 0.1522
	accuracy_policy_0: 0.68098
	loss_value_0: 0.16424
	loss_policy_1: 0.03603
	accuracy_policy_1: 0.62512
	loss_value_1: 0.03412
	loss_reward_1: 0.00462
	loss_policy_2: 0.03871
	accuracy_policy_2: 0.6024
	loss_value_2: 0.03544
	loss_reward_2: 0.00452
	loss_policy_3: 0.04142
	accuracy_policy_3: 0.58178
	loss_value_3: 0.03676
	loss_reward_3: 0.00494
	loss_policy_4: 0.04422
	accuracy_policy_4: 0.56139
	loss_value_4: 0.03782
	loss_reward_4: 0.00533
	loss_policy_5: 0.04662
	accuracy_policy_5: 0.54275
	loss_value_5: 0.03883
	loss_reward_5: 0.00615
	loss_policy: 0.3592
	loss_value: 0.34722
	loss_reward: 0.02556
[2024-05-08 12:04:35] nn step 37400, lr: 0.1.
	loss_policy_0: 0.14383
	accuracy_policy_0: 0.72377
	loss_value_0: 0.17412
	loss_policy_1: 0.03569
	accuracy_policy_1: 0.66191
	loss_value_1: 0.03641
	loss_reward_1: 0.00513
	loss_policy_2: 0.03864
	accuracy_policy_2: 0.63662
	loss_value_2: 0.03776
	loss_reward_2: 0.00486
	loss_policy_3: 0.04156
	accuracy_policy_3: 0.6173
	loss_value_3: 0.039
	loss_reward_3: 0.00535
	loss_policy_4: 0.04455
	accuracy_policy_4: 0.59869
	loss_value_4: 0.04013
	loss_reward_4: 0.00583
	loss_policy_5: 0.04723
	accuracy_policy_5: 0.57443
	loss_value_5: 0.04147
	loss_reward_5: 0.0069
	loss_policy: 0.3515
	loss_value: 0.3689
	loss_reward: 0.02807
Optimization_Done 37400
[2024-05-08 12:07:09] [command] train weight_iter_37400.pkl 186 188
[2024-05-08 12:07:51] nn step 37500, lr: 0.1.
	loss_policy_0: 0.16207
	accuracy_policy_0: 0.66396
	loss_value_0: 0.16452
	loss_policy_1: 0.03784
	accuracy_policy_1: 0.61773
	loss_value_1: 0.03433
	loss_reward_1: 0.0053
	loss_policy_2: 0.04061
	accuracy_policy_2: 0.59201
	loss_value_2: 0.03568
	loss_reward_2: 0.00502
	loss_policy_3: 0.04404
	accuracy_policy_3: 0.56723
	loss_value_3: 0.03715
	loss_reward_3: 0.00544
	loss_policy_4: 0.0469
	accuracy_policy_4: 0.54959
	loss_value_4: 0.0383
	loss_reward_4: 0.00608
	loss_policy_5: 0.04955
	accuracy_policy_5: 0.53037
	loss_value_5: 0.03944
	loss_reward_5: 0.00691
	loss_policy: 0.38101
	loss_value: 0.34941
	loss_reward: 0.02875
[2024-05-08 12:08:32] nn step 37600, lr: 0.1.
	loss_policy_0: 0.14452
	accuracy_policy_0: 0.70855
	loss_value_0: 0.16848
	loss_policy_1: 0.03534
	accuracy_policy_1: 0.64895
	loss_value_1: 0.03542
	loss_reward_1: 0.00542
	loss_policy_2: 0.03858
	accuracy_policy_2: 0.62791
	loss_value_2: 0.03693
	loss_reward_2: 0.00506
	loss_policy_3: 0.04189
	accuracy_policy_3: 0.60215
	loss_value_3: 0.03835
	loss_reward_3: 0.00556
	loss_policy_4: 0.0451
	accuracy_policy_4: 0.57943
	loss_value_4: 0.03963
	loss_reward_4: 0.00607
	loss_policy_5: 0.04786
	accuracy_policy_5: 0.55867
	loss_value_5: 0.04076
	loss_reward_5: 0.00717
	loss_policy: 0.3533
	loss_value: 0.35957
	loss_reward: 0.02928
Optimization_Done 37600
[2024-05-08 12:11:08] [command] train weight_iter_37600.pkl 187 189
[2024-05-08 12:11:50] nn step 37700, lr: 0.1.
	loss_policy_0: 0.14389
	accuracy_policy_0: 0.68027
	loss_value_0: 0.15819
	loss_policy_1: 0.03425
	accuracy_policy_1: 0.62703
	loss_value_1: 0.03314
	loss_reward_1: 0.00459
	loss_policy_2: 0.03711
	accuracy_policy_2: 0.60416
	loss_value_2: 0.03446
	loss_reward_2: 0.00445
	loss_policy_3: 0.03971
	accuracy_policy_3: 0.58154
	loss_value_3: 0.03559
	loss_reward_3: 0.00475
	loss_policy_4: 0.04239
	accuracy_policy_4: 0.56115
	loss_value_4: 0.03662
	loss_reward_4: 0.00509
	loss_policy_5: 0.0449
	accuracy_policy_5: 0.54051
	loss_value_5: 0.03767
	loss_reward_5: 0.00592
	loss_policy: 0.34224
	loss_value: 0.33567
	loss_reward: 0.0248
[2024-05-08 12:12:31] nn step 37800, lr: 0.1.
	loss_policy_0: 0.12536
	accuracy_policy_0: 0.72174
	loss_value_0: 0.15866
	loss_policy_1: 0.03114
	accuracy_policy_1: 0.6601
	loss_value_1: 0.03328
	loss_reward_1: 0.00458
	loss_policy_2: 0.03405
	accuracy_policy_2: 0.63662
	loss_value_2: 0.03453
	loss_reward_2: 0.00444
	loss_policy_3: 0.03709
	accuracy_policy_3: 0.60822
	loss_value_3: 0.03569
	loss_reward_3: 0.00477
	loss_policy_4: 0.03982
	accuracy_policy_4: 0.59127
	loss_value_4: 0.03685
	loss_reward_4: 0.00525
	loss_policy_5: 0.04267
	accuracy_policy_5: 0.56566
	loss_value_5: 0.03796
	loss_reward_5: 0.00602
	loss_policy: 0.31013
	loss_value: 0.33698
	loss_reward: 0.02506
Optimization_Done 37800
[2024-05-08 12:15:11] [command] train weight_iter_37800.pkl 188 190
[2024-05-08 12:15:54] nn step 37900, lr: 0.1.
	loss_policy_0: 0.14803
	accuracy_policy_0: 0.68314
	loss_value_0: 0.15839
	loss_policy_1: 0.03513
	accuracy_policy_1: 0.63057
	loss_value_1: 0.03313
	loss_reward_1: 0.00437
	loss_policy_2: 0.03771
	accuracy_policy_2: 0.61025
	loss_value_2: 0.03436
	loss_reward_2: 0.0043
	loss_policy_3: 0.04012
	accuracy_policy_3: 0.58932
	loss_value_3: 0.03549
	loss_reward_3: 0.00448
	loss_policy_4: 0.04305
	accuracy_policy_4: 0.56791
	loss_value_4: 0.03647
	loss_reward_4: 0.00495
	loss_policy_5: 0.04532
	accuracy_policy_5: 0.54777
	loss_value_5: 0.03759
	loss_reward_5: 0.00573
	loss_policy: 0.34936
	loss_value: 0.33543
	loss_reward: 0.02382
[2024-05-08 12:16:35] nn step 38000, lr: 0.1.
	loss_policy_0: 0.12347
	accuracy_policy_0: 0.73082
	loss_value_0: 0.15936
	loss_policy_1: 0.0311
	accuracy_policy_1: 0.67219
	loss_value_1: 0.03323
	loss_reward_1: 0.00441
	loss_policy_2: 0.0344
	accuracy_policy_2: 0.64133
	loss_value_2: 0.03457
	loss_reward_2: 0.00418
	loss_policy_3: 0.03685
	accuracy_policy_3: 0.62426
	loss_value_3: 0.03582
	loss_reward_3: 0.00453
	loss_policy_4: 0.03967
	accuracy_policy_4: 0.60006
	loss_value_4: 0.03679
	loss_reward_4: 0.00494
	loss_policy_5: 0.04228
	accuracy_policy_5: 0.58002
	loss_value_5: 0.03781
	loss_reward_5: 0.00583
	loss_policy: 0.30776
	loss_value: 0.33758
	loss_reward: 0.0239
Optimization_Done 38000
[2024-05-08 12:19:12] [command] train weight_iter_38000.pkl 189 191
[2024-05-08 12:19:55] nn step 38100, lr: 0.1.
	loss_policy_0: 0.15178
	accuracy_policy_0: 0.65807
	loss_value_0: 0.15273
	loss_policy_1: 0.03502
	accuracy_policy_1: 0.61639
	loss_value_1: 0.03207
	loss_reward_1: 0.00421
	loss_policy_2: 0.03768
	accuracy_policy_2: 0.59664
	loss_value_2: 0.03329
	loss_reward_2: 0.00407
	loss_policy_3: 0.03991
	accuracy_policy_3: 0.57404
	loss_value_3: 0.03448
	loss_reward_3: 0.00445
	loss_policy_4: 0.04201
	accuracy_policy_4: 0.55658
	loss_value_4: 0.03554
	loss_reward_4: 0.00488
	loss_policy_5: 0.04437
	accuracy_policy_5: 0.53764
	loss_value_5: 0.03666
	loss_reward_5: 0.00551
	loss_policy: 0.35077
	loss_value: 0.32478
	loss_reward: 0.02313
[2024-05-08 12:20:36] nn step 38200, lr: 0.1.
	loss_policy_0: 0.13615
	accuracy_policy_0: 0.71279
	loss_value_0: 0.16081
	loss_policy_1: 0.03285
	accuracy_policy_1: 0.65668
	loss_value_1: 0.03378
	loss_reward_1: 0.00439
	loss_policy_2: 0.03543
	accuracy_policy_2: 0.63225
	loss_value_2: 0.03497
	loss_reward_2: 0.00426
	loss_policy_3: 0.03826
	accuracy_policy_3: 0.6117
	loss_value_3: 0.03614
	loss_reward_3: 0.00452
	loss_policy_4: 0.04091
	accuracy_policy_4: 0.59531
	loss_value_4: 0.03732
	loss_reward_4: 0.00493
	loss_policy_5: 0.04351
	accuracy_policy_5: 0.57592
	loss_value_5: 0.03835
	loss_reward_5: 0.00584
	loss_policy: 0.32711
	loss_value: 0.34137
	loss_reward: 0.02395
Optimization_Done 38200
[2024-05-08 12:23:10] [command] train weight_iter_38200.pkl 190 192
[2024-05-08 12:23:53] nn step 38300, lr: 0.1.
	loss_policy_0: 0.16214
	accuracy_policy_0: 0.68172
	loss_value_0: 0.18098
	loss_policy_1: 0.03776
	accuracy_policy_1: 0.63814
	loss_value_1: 0.03769
	loss_reward_1: 0.00543
	loss_policy_2: 0.04035
	accuracy_policy_2: 0.61615
	loss_value_2: 0.03907
	loss_reward_2: 0.00526
	loss_policy_3: 0.04289
	accuracy_policy_3: 0.60002
	loss_value_3: 0.04037
	loss_reward_3: 0.00567
	loss_policy_4: 0.04534
	accuracy_policy_4: 0.58148
	loss_value_4: 0.04161
	loss_reward_4: 0.0062
	loss_policy_5: 0.04755
	accuracy_policy_5: 0.56607
	loss_value_5: 0.04282
	loss_reward_5: 0.00717
	loss_policy: 0.37605
	loss_value: 0.38255
	loss_reward: 0.02973
[2024-05-08 12:24:34] nn step 38400, lr: 0.1.
	loss_policy_0: 0.14821
	accuracy_policy_0: 0.71961
	loss_value_0: 0.18407
	loss_policy_1: 0.0357
	accuracy_policy_1: 0.66678
	loss_value_1: 0.0385
	loss_reward_1: 0.00553
	loss_policy_2: 0.03846
	accuracy_policy_2: 0.64688
	loss_value_2: 0.04007
	loss_reward_2: 0.00553
	loss_policy_3: 0.04174
	accuracy_policy_3: 0.62357
	loss_value_3: 0.04137
	loss_reward_3: 0.00587
	loss_policy_4: 0.04403
	accuracy_policy_4: 0.61104
	loss_value_4: 0.04268
	loss_reward_4: 0.00615
	loss_policy_5: 0.04686
	accuracy_policy_5: 0.58916
	loss_value_5: 0.04393
	loss_reward_5: 0.00736
	loss_policy: 0.355
	loss_value: 0.39063
	loss_reward: 0.03044
Optimization_Done 38400
[2024-05-08 12:26:39] [command] train weight_iter_38400.pkl 191 193
[2024-05-08 12:27:22] nn step 38500, lr: 0.1.
	loss_policy_0: 0.16797
	accuracy_policy_0: 0.66898
	loss_value_0: 0.1801
	loss_policy_1: 0.03841
	accuracy_policy_1: 0.62486
	loss_value_1: 0.03749
	loss_reward_1: 0.00538
	loss_policy_2: 0.0411
	accuracy_policy_2: 0.60455
	loss_value_2: 0.03889
	loss_reward_2: 0.00499
	loss_policy_3: 0.04365
	accuracy_policy_3: 0.5866
	loss_value_3: 0.04011
	loss_reward_3: 0.00533
	loss_policy_4: 0.0462
	accuracy_policy_4: 0.57006
	loss_value_4: 0.04118
	loss_reward_4: 0.00587
	loss_policy_5: 0.0487
	accuracy_policy_5: 0.54916
	loss_value_5: 0.04236
	loss_reward_5: 0.00691
	loss_policy: 0.38603
	loss_value: 0.38013
	loss_reward: 0.02849
[2024-05-08 12:28:03] nn step 38600, lr: 0.1.
	loss_policy_0: 0.15534
	accuracy_policy_0: 0.71166
	loss_value_0: 0.18333
	loss_policy_1: 0.03668
	accuracy_policy_1: 0.66215
	loss_value_1: 0.03826
	loss_reward_1: 0.00564
	loss_policy_2: 0.03959
	accuracy_policy_2: 0.63715
	loss_value_2: 0.03956
	loss_reward_2: 0.00531
	loss_policy_3: 0.04246
	accuracy_policy_3: 0.61816
	loss_value_3: 0.04093
	loss_reward_3: 0.00572
	loss_policy_4: 0.04513
	accuracy_policy_4: 0.59881
	loss_value_4: 0.04212
	loss_reward_4: 0.00635
	loss_policy_5: 0.04773
	accuracy_policy_5: 0.58227
	loss_value_5: 0.0433
	loss_reward_5: 0.0073
	loss_policy: 0.36692
	loss_value: 0.38749
	loss_reward: 0.03031
Optimization_Done 38600
[2024-05-08 12:30:42] [command] train weight_iter_38600.pkl 192 194
[2024-05-08 12:31:25] nn step 38700, lr: 0.1.
	loss_policy_0: 0.14737
	accuracy_policy_0: 0.69904
	loss_value_0: 0.16986
	loss_policy_1: 0.035
	accuracy_policy_1: 0.65043
	loss_value_1: 0.03537
	loss_reward_1: 0.00494
	loss_policy_2: 0.03793
	accuracy_policy_2: 0.63256
	loss_value_2: 0.03665
	loss_reward_2: 0.00473
	loss_policy_3: 0.04068
	accuracy_policy_3: 0.61012
	loss_value_3: 0.03787
	loss_reward_3: 0.00504
	loss_policy_4: 0.04292
	accuracy_policy_4: 0.58984
	loss_value_4: 0.03887
	loss_reward_4: 0.00562
	loss_policy_5: 0.04542
	accuracy_policy_5: 0.57205
	loss_value_5: 0.03995
	loss_reward_5: 0.00655
	loss_policy: 0.34933
	loss_value: 0.35857
	loss_reward: 0.02688
[2024-05-08 12:32:06] nn step 38800, lr: 0.1.
	loss_policy_0: 0.13097
	accuracy_policy_0: 0.73482
	loss_value_0: 0.17079
	loss_policy_1: 0.03241
	accuracy_policy_1: 0.68244
	loss_value_1: 0.0357
	loss_reward_1: 0.005
	loss_policy_2: 0.03524
	accuracy_policy_2: 0.65664
	loss_value_2: 0.0369
	loss_reward_2: 0.00496
	loss_policy_3: 0.03829
	accuracy_policy_3: 0.63516
	loss_value_3: 0.03808
	loss_reward_3: 0.00532
	loss_policy_4: 0.04061
	accuracy_policy_4: 0.61955
	loss_value_4: 0.03924
	loss_reward_4: 0.0057
	loss_policy_5: 0.04337
	accuracy_policy_5: 0.59766
	loss_value_5: 0.04042
	loss_reward_5: 0.00657
	loss_policy: 0.32089
	loss_value: 0.36112
	loss_reward: 0.02755
Optimization_Done 38800
[2024-05-08 12:34:40] [command] train weight_iter_38800.pkl 193 195
[2024-05-08 12:35:23] nn step 38900, lr: 0.1.
	loss_policy_0: 0.16636
	accuracy_policy_0: 0.69941
	loss_value_0: 0.17724
	loss_policy_1: 0.03894
	accuracy_policy_1: 0.6585
	loss_value_1: 0.03697
	loss_reward_1: 0.00482
	loss_policy_2: 0.04206
	accuracy_policy_2: 0.63537
	loss_value_2: 0.0385
	loss_reward_2: 0.00472
	loss_policy_3: 0.04535
	accuracy_policy_3: 0.61258
	loss_value_3: 0.03983
	loss_reward_3: 0.00517
	loss_policy_4: 0.04782
	accuracy_policy_4: 0.59348
	loss_value_4: 0.041
	loss_reward_4: 0.00537
	loss_policy_5: 0.05058
	accuracy_policy_5: 0.57498
	loss_value_5: 0.04228
	loss_reward_5: 0.00618
	loss_policy: 0.39112
	loss_value: 0.37582
	loss_reward: 0.02626
[2024-05-08 12:36:04] nn step 39000, lr: 0.1.
	loss_policy_0: 0.13089
	accuracy_policy_0: 0.73953
	loss_value_0: 0.16395
	loss_policy_1: 0.03258
	accuracy_policy_1: 0.68369
	loss_value_1: 0.03439
	loss_reward_1: 0.00455
	loss_policy_2: 0.03545
	accuracy_policy_2: 0.66262
	loss_value_2: 0.03571
	loss_reward_2: 0.00446
	loss_policy_3: 0.0385
	accuracy_policy_3: 0.64201
	loss_value_3: 0.037
	loss_reward_3: 0.00459
	loss_policy_4: 0.04124
	accuracy_policy_4: 0.61977
	loss_value_4: 0.03816
	loss_reward_4: 0.00501
	loss_policy_5: 0.04396
	accuracy_policy_5: 0.60043
	loss_value_5: 0.03926
	loss_reward_5: 0.00596
	loss_policy: 0.32262
	loss_value: 0.34846
	loss_reward: 0.02458
Optimization_Done 39000
[2024-05-08 12:38:15] [command] train weight_iter_39000.pkl 194 196
[2024-05-08 12:38:57] nn step 39100, lr: 0.1.
	loss_policy_0: 0.14098
	accuracy_policy_0: 0.70459
	loss_value_0: 0.15855
	loss_policy_1: 0.03361
	accuracy_policy_1: 0.66221
	loss_value_1: 0.03346
	loss_reward_1: 0.00436
	loss_policy_2: 0.03689
	accuracy_policy_2: 0.63734
	loss_value_2: 0.0348
	loss_reward_2: 0.00414
	loss_policy_3: 0.03985
	accuracy_policy_3: 0.61531
	loss_value_3: 0.0361
	loss_reward_3: 0.00448
	loss_policy_4: 0.0424
	accuracy_policy_4: 0.59838
	loss_value_4: 0.03721
	loss_reward_4: 0.00497
	loss_policy_5: 0.04533
	accuracy_policy_5: 0.57889
	loss_value_5: 0.03833
	loss_reward_5: 0.0058
	loss_policy: 0.33906
	loss_value: 0.33844
	loss_reward: 0.02375
[2024-05-08 12:39:38] nn step 39200, lr: 0.1.
	loss_policy_0: 0.12755
	accuracy_policy_0: 0.74162
	loss_value_0: 0.16276
	loss_policy_1: 0.0318
	accuracy_policy_1: 0.68945
	loss_value_1: 0.03439
	loss_reward_1: 0.00454
	loss_policy_2: 0.03497
	accuracy_policy_2: 0.66498
	loss_value_2: 0.03575
	loss_reward_2: 0.00441
	loss_policy_3: 0.03788
	accuracy_policy_3: 0.64547
	loss_value_3: 0.03704
	loss_reward_3: 0.00461
	loss_policy_4: 0.0405
	accuracy_policy_4: 0.63004
	loss_value_4: 0.03844
	loss_reward_4: 0.00509
	loss_policy_5: 0.04357
	accuracy_policy_5: 0.60406
	loss_value_5: 0.03958
	loss_reward_5: 0.00606
	loss_policy: 0.31627
	loss_value: 0.34798
	loss_reward: 0.0247
Optimization_Done 39200
[2024-05-08 12:42:12] [command] train weight_iter_39200.pkl 195 197
[2024-05-08 12:42:54] nn step 39300, lr: 0.1.
	loss_policy_0: 0.14871
	accuracy_policy_0: 0.69363
	loss_value_0: 0.15654
	loss_policy_1: 0.03599
	accuracy_policy_1: 0.64354
	loss_value_1: 0.03284
	loss_reward_1: 0.00483
	loss_policy_2: 0.03942
	accuracy_policy_2: 0.61646
	loss_value_2: 0.03419
	loss_reward_2: 0.00438
	loss_policy_3: 0.0422
	accuracy_policy_3: 0.59406
	loss_value_3: 0.03563
	loss_reward_3: 0.00468
	loss_policy_4: 0.04542
	accuracy_policy_4: 0.57084
	loss_value_4: 0.03696
	loss_reward_4: 0.00521
	loss_policy_5: 0.04805
	accuracy_policy_5: 0.5551
	loss_value_5: 0.03811
	loss_reward_5: 0.00594
	loss_policy: 0.35977
	loss_value: 0.33427
	loss_reward: 0.02504
[2024-05-08 12:43:35] nn step 39400, lr: 0.1.
	loss_policy_0: 0.12283
	accuracy_policy_0: 0.74043
	loss_value_0: 0.15211
	loss_policy_1: 0.03166
	accuracy_policy_1: 0.67736
	loss_value_1: 0.03194
	loss_reward_1: 0.0046
	loss_policy_2: 0.0349
	accuracy_policy_2: 0.65191
	loss_value_2: 0.03332
	loss_reward_2: 0.00443
	loss_policy_3: 0.03782
	accuracy_policy_3: 0.63059
	loss_value_3: 0.03446
	loss_reward_3: 0.00452
	loss_policy_4: 0.04082
	accuracy_policy_4: 0.61021
	loss_value_4: 0.03567
	loss_reward_4: 0.00504
	loss_policy_5: 0.04373
	accuracy_policy_5: 0.5866
	loss_value_5: 0.03681
	loss_reward_5: 0.00587
	loss_policy: 0.31176
	loss_value: 0.32431
	loss_reward: 0.02446
Optimization_Done 39400
[2024-05-08 12:45:59] [command] train weight_iter_39400.pkl 196 198
[2024-05-08 12:46:41] nn step 39500, lr: 0.1.
	loss_policy_0: 0.1435
	accuracy_policy_0: 0.71377
	loss_value_0: 0.16803
	loss_policy_1: 0.03447
	accuracy_policy_1: 0.66699
	loss_value_1: 0.03524
	loss_reward_1: 0.00483
	loss_policy_2: 0.03796
	accuracy_policy_2: 0.64102
	loss_value_2: 0.03663
	loss_reward_2: 0.00456
	loss_policy_3: 0.04096
	accuracy_policy_3: 0.62338
	loss_value_3: 0.03798
	loss_reward_3: 0.00498
	loss_policy_4: 0.04394
	accuracy_policy_4: 0.59717
	loss_value_4: 0.03911
	loss_reward_4: 0.00536
	loss_policy_5: 0.04642
	accuracy_policy_5: 0.58146
	loss_value_5: 0.04027
	loss_reward_5: 0.00626
	loss_policy: 0.34725
	loss_value: 0.35726
	loss_reward: 0.02598
[2024-05-08 12:47:22] nn step 39600, lr: 0.1.
	loss_policy_0: 0.12223
	accuracy_policy_0: 0.75057
	loss_value_0: 0.1653
	loss_policy_1: 0.03176
	accuracy_policy_1: 0.69219
	loss_value_1: 0.0346
	loss_reward_1: 0.00491
	loss_policy_2: 0.03461
	accuracy_policy_2: 0.66631
	loss_value_2: 0.03586
	loss_reward_2: 0.00462
	loss_policy_3: 0.03746
	accuracy_policy_3: 0.64816
	loss_value_3: 0.03727
	loss_reward_3: 0.00495
	loss_policy_4: 0.04032
	accuracy_policy_4: 0.62475
	loss_value_4: 0.03839
	loss_reward_4: 0.00542
	loss_policy_5: 0.0436
	accuracy_policy_5: 0.60053
	loss_value_5: 0.03956
	loss_reward_5: 0.00632
	loss_policy: 0.30998
	loss_value: 0.35099
	loss_reward: 0.02622
Optimization_Done 39600
[2024-05-08 12:50:02] [command] train weight_iter_39600.pkl 197 199
[2024-05-08 12:50:50] nn step 39700, lr: 0.1.
	loss_policy_0: 0.1411
	accuracy_policy_0: 0.70082
	loss_value_0: 0.16607
	loss_policy_1: 0.03362
	accuracy_policy_1: 0.65432
	loss_value_1: 0.03461
	loss_reward_1: 0.00494
	loss_policy_2: 0.03672
	accuracy_policy_2: 0.63078
	loss_value_2: 0.03597
	loss_reward_2: 0.00482
	loss_policy_3: 0.03936
	accuracy_policy_3: 0.61111
	loss_value_3: 0.03732
	loss_reward_3: 0.00517
	loss_policy_4: 0.04177
	accuracy_policy_4: 0.5909
	loss_value_4: 0.0384
	loss_reward_4: 0.00563
	loss_policy_5: 0.04447
	accuracy_policy_5: 0.57113
	loss_value_5: 0.03945
	loss_reward_5: 0.00659
	loss_policy: 0.33704
	loss_value: 0.35183
	loss_reward: 0.02714
[2024-05-08 12:51:38] nn step 39800, lr: 0.1.
	loss_policy_0: 0.12277
	accuracy_policy_0: 0.7442
	loss_value_0: 0.1679
	loss_policy_1: 0.0312
	accuracy_policy_1: 0.68473
	loss_value_1: 0.03518
	loss_reward_1: 0.00509
	loss_policy_2: 0.03438
	accuracy_policy_2: 0.6602
	loss_value_2: 0.03657
	loss_reward_2: 0.00499
	loss_policy_3: 0.03688
	accuracy_policy_3: 0.64514
	loss_value_3: 0.0378
	loss_reward_3: 0.00526
	loss_policy_4: 0.03978
	accuracy_policy_4: 0.62131
	loss_value_4: 0.0389
	loss_reward_4: 0.00575
	loss_policy_5: 0.0426
	accuracy_policy_5: 0.60002
	loss_value_5: 0.04013
	loss_reward_5: 0.00673
	loss_policy: 0.30761
	loss_value: 0.35648
	loss_reward: 0.02783
Optimization_Done 39800
[2024-05-08 12:54:19] [command] train weight_iter_39800.pkl 198 200
[2024-05-08 12:55:04] nn step 39900, lr: 0.1.
	loss_policy_0: 0.14535
	accuracy_policy_0: 0.71547
	loss_value_0: 0.17414
	loss_policy_1: 0.03387
	accuracy_policy_1: 0.67826
	loss_value_1: 0.0365
	loss_reward_1: 0.00502
	loss_policy_2: 0.03652
	accuracy_policy_2: 0.65512
	loss_value_2: 0.03798
	loss_reward_2: 0.00496
	loss_policy_3: 0.03894
	accuracy_policy_3: 0.63873
	loss_value_3: 0.03933
	loss_reward_3: 0.00549
	loss_policy_4: 0.04167
	accuracy_policy_4: 0.62109
	loss_value_4: 0.04051
	loss_reward_4: 0.00589
	loss_policy_5: 0.04448
	accuracy_policy_5: 0.6018
	loss_value_5: 0.04173
	loss_reward_5: 0.00709
	loss_policy: 0.34083
	loss_value: 0.3702
	loss_reward: 0.02846
[2024-05-08 12:55:48] nn step 40000, lr: 0.1.
	loss_policy_0: 0.1179
	accuracy_policy_0: 0.75189
	loss_value_0: 0.16766
	loss_policy_1: 0.02952
	accuracy_policy_1: 0.70203
	loss_value_1: 0.03523
	loss_reward_1: 0.00491
	loss_policy_2: 0.03178
	accuracy_policy_2: 0.68273
	loss_value_2: 0.03647
	loss_reward_2: 0.00488
	loss_policy_3: 0.03473
	accuracy_policy_3: 0.66113
	loss_value_3: 0.03773
	loss_reward_3: 0.00507
	loss_policy_4: 0.03712
	accuracy_policy_4: 0.64578
	loss_value_4: 0.03895
	loss_reward_4: 0.00553
	loss_policy_5: 0.03996
	accuracy_policy_5: 0.62516
	loss_value_5: 0.04003
	loss_reward_5: 0.00662
	loss_policy: 0.29102
	loss_value: 0.35606
	loss_reward: 0.02701
Optimization_Done 40000
[2024-05-08 12:58:27] [command] train weight_iter_40000.pkl 199 201
[2024-05-08 12:59:12] nn step 40100, lr: 0.1.
	loss_policy_0: 0.14145
	accuracy_policy_0: 0.70615
	loss_value_0: 0.16888
	loss_policy_1: 0.03341
	accuracy_policy_1: 0.66096
	loss_value_1: 0.03527
	loss_reward_1: 0.00528
	loss_policy_2: 0.03684
	accuracy_policy_2: 0.63631
	loss_value_2: 0.03663
	loss_reward_2: 0.00528
	loss_policy_3: 0.03948
	accuracy_policy_3: 0.61746
	loss_value_3: 0.03799
	loss_reward_3: 0.00559
	loss_policy_4: 0.04197
	accuracy_policy_4: 0.59762
	loss_value_4: 0.03924
	loss_reward_4: 0.00591
	loss_policy_5: 0.04484
	accuracy_policy_5: 0.57764
	loss_value_5: 0.04039
	loss_reward_5: 0.00697
	loss_policy: 0.33799
	loss_value: 0.35841
	loss_reward: 0.02903
[2024-05-08 12:59:56] nn step 40200, lr: 0.1.
	loss_policy_0: 0.12437
	accuracy_policy_0: 0.74232
	loss_value_0: 0.17045
	loss_policy_1: 0.03126
	accuracy_policy_1: 0.68984
	loss_value_1: 0.03575
	loss_reward_1: 0.00538
	loss_policy_2: 0.03424
	accuracy_policy_2: 0.66455
	loss_value_2: 0.03719
	loss_reward_2: 0.00518
	loss_policy_3: 0.03718
	accuracy_policy_3: 0.64604
	loss_value_3: 0.03851
	loss_reward_3: 0.00561
	loss_policy_4: 0.03993
	accuracy_policy_4: 0.62678
	loss_value_4: 0.03985
	loss_reward_4: 0.00612
	loss_policy_5: 0.04315
	accuracy_policy_5: 0.5999
	loss_value_5: 0.04093
	loss_reward_5: 0.00718
	loss_policy: 0.31013
	loss_value: 0.36268
	loss_reward: 0.02947
Optimization_Done 40200
[2024-05-08 13:02:12] [command] train weight_iter_40200.pkl 200 202
[2024-05-08 13:02:57] nn step 40300, lr: 0.1.
	loss_policy_0: 0.13309
	accuracy_policy_0: 0.69133
	loss_value_0: 0.14994
	loss_policy_1: 0.03209
	accuracy_policy_1: 0.64316
	loss_value_1: 0.03137
	loss_reward_1: 0.00423
	loss_policy_2: 0.03446
	accuracy_policy_2: 0.61891
	loss_value_2: 0.03272
	loss_reward_2: 0.00411
	loss_policy_3: 0.03711
	accuracy_policy_3: 0.59416
	loss_value_3: 0.03381
	loss_reward_3: 0.00456
	loss_policy_4: 0.0394
	accuracy_policy_4: 0.57992
	loss_value_4: 0.03487
	loss_reward_4: 0.0049
	loss_policy_5: 0.04183
	accuracy_policy_5: 0.55918
	loss_value_5: 0.03586
	loss_reward_5: 0.00576
	loss_policy: 0.31798
	loss_value: 0.31858
	loss_reward: 0.02356
[2024-05-08 13:03:42] nn step 40400, lr: 0.1.
	loss_policy_0: 0.12655
	accuracy_policy_0: 0.72951
	loss_value_0: 0.16036
	loss_policy_1: 0.03184
	accuracy_policy_1: 0.67373
	loss_value_1: 0.03384
	loss_reward_1: 0.00473
	loss_policy_2: 0.03474
	accuracy_policy_2: 0.6507
	loss_value_2: 0.03515
	loss_reward_2: 0.00453
	loss_policy_3: 0.03763
	accuracy_policy_3: 0.6249
	loss_value_3: 0.03629
	loss_reward_3: 0.00478
	loss_policy_4: 0.04019
	accuracy_policy_4: 0.60945
	loss_value_4: 0.03748
	loss_reward_4: 0.00523
	loss_policy_5: 0.04335
	accuracy_policy_5: 0.58135
	loss_value_5: 0.03866
	loss_reward_5: 0.00631
	loss_policy: 0.31429
	loss_value: 0.34179
	loss_reward: 0.02559
Optimization_Done 40400
[2024-05-08 13:06:24] [command] train weight_iter_40400.pkl 201 203
[2024-05-08 13:07:09] nn step 40500, lr: 0.1.
	loss_policy_0: 0.13404
	accuracy_policy_0: 0.71453
	loss_value_0: 0.16083
	loss_policy_1: 0.03302
	accuracy_policy_1: 0.6593
	loss_value_1: 0.03388
	loss_reward_1: 0.00477
	loss_policy_2: 0.03589
	accuracy_policy_2: 0.63645
	loss_value_2: 0.03525
	loss_reward_2: 0.00466
	loss_policy_3: 0.03878
	accuracy_policy_3: 0.61074
	loss_value_3: 0.03635
	loss_reward_3: 0.00493
	loss_policy_4: 0.04155
	accuracy_policy_4: 0.5918
	loss_value_4: 0.03746
	loss_reward_4: 0.00563
	loss_policy_5: 0.04404
	accuracy_policy_5: 0.56814
	loss_value_5: 0.0385
	loss_reward_5: 0.00639
	loss_policy: 0.32733
	loss_value: 0.34226
	loss_reward: 0.02638
[2024-05-08 13:07:53] nn step 40600, lr: 0.1.
	loss_policy_0: 0.12334
	accuracy_policy_0: 0.74646
	loss_value_0: 0.16661
	loss_policy_1: 0.03192
	accuracy_policy_1: 0.68424
	loss_value_1: 0.03478
	loss_reward_1: 0.00504
	loss_policy_2: 0.03499
	accuracy_policy_2: 0.65564
	loss_value_2: 0.03619
	loss_reward_2: 0.00484
	loss_policy_3: 0.03762
	accuracy_policy_3: 0.63664
	loss_value_3: 0.03747
	loss_reward_3: 0.00517
	loss_policy_4: 0.04056
	accuracy_policy_4: 0.61469
	loss_value_4: 0.03886
	loss_reward_4: 0.00556
	loss_policy_5: 0.04389
	accuracy_policy_5: 0.58678
	loss_value_5: 0.03996
	loss_reward_5: 0.00676
	loss_policy: 0.31233
	loss_value: 0.35386
	loss_reward: 0.02737
Optimization_Done 40600
[2024-05-08 13:10:32] [command] train weight_iter_40600.pkl 202 204
[2024-05-08 13:11:15] nn step 40700, lr: 0.1.
	loss_policy_0: 0.15407
	accuracy_policy_0: 0.69053
	loss_value_0: 0.16504
	loss_policy_1: 0.03593
	accuracy_policy_1: 0.64783
	loss_value_1: 0.03477
	loss_reward_1: 0.00481
	loss_policy_2: 0.0389
	accuracy_policy_2: 0.62363
	loss_value_2: 0.03619
	loss_reward_2: 0.00465
	loss_policy_3: 0.04192
	accuracy_policy_3: 0.59961
	loss_value_3: 0.03765
	loss_reward_3: 0.00507
	loss_policy_4: 0.04433
	accuracy_policy_4: 0.5783
	loss_value_4: 0.03878
	loss_reward_4: 0.00545
	loss_policy_5: 0.04706
	accuracy_policy_5: 0.55609
	loss_value_5: 0.04
	loss_reward_5: 0.00629
	loss_policy: 0.36221
	loss_value: 0.35243
	loss_reward: 0.02627
[2024-05-08 13:11:56] nn step 40800, lr: 0.1.
	loss_policy_0: 0.13262
	accuracy_policy_0: 0.73385
	loss_value_0: 0.16457
	loss_policy_1: 0.03294
	accuracy_policy_1: 0.67445
	loss_value_1: 0.03469
	loss_reward_1: 0.00477
	loss_policy_2: 0.0355
	accuracy_policy_2: 0.65691
	loss_value_2: 0.0361
	loss_reward_2: 0.00468
	loss_policy_3: 0.03863
	accuracy_policy_3: 0.63186
	loss_value_3: 0.03735
	loss_reward_3: 0.00506
	loss_policy_4: 0.04141
	accuracy_policy_4: 0.60645
	loss_value_4: 0.03851
	loss_reward_4: 0.00541
	loss_policy_5: 0.04404
	accuracy_policy_5: 0.59029
	loss_value_5: 0.03981
	loss_reward_5: 0.00647
	loss_policy: 0.32514
	loss_value: 0.35104
	loss_reward: 0.0264
Optimization_Done 40800
[2024-05-08 13:14:33] [command] train weight_iter_40800.pkl 203 205
[2024-05-08 13:15:15] nn step 40900, lr: 0.1.
	loss_policy_0: 0.15585
	accuracy_policy_0: 0.67979
	loss_value_0: 0.17447
	loss_policy_1: 0.03728
	accuracy_policy_1: 0.62666
	loss_value_1: 0.03671
	loss_reward_1: 0.00576
	loss_policy_2: 0.0404
	accuracy_policy_2: 0.60121
	loss_value_2: 0.03808
	loss_reward_2: 0.00563
	loss_policy_3: 0.04337
	accuracy_policy_3: 0.58314
	loss_value_3: 0.03942
	loss_reward_3: 0.00592
	loss_policy_4: 0.04648
	accuracy_policy_4: 0.5601
	loss_value_4: 0.04069
	loss_reward_4: 0.00639
	loss_policy_5: 0.04942
	accuracy_policy_5: 0.5384
	loss_value_5: 0.04196
	loss_reward_5: 0.00764
	loss_policy: 0.3728
	loss_value: 0.37134
	loss_reward: 0.03133
[2024-05-08 13:15:56] nn step 41000, lr: 0.1.
	loss_policy_0: 0.14323
	accuracy_policy_0: 0.71881
	loss_value_0: 0.1799
	loss_policy_1: 0.03528
	accuracy_policy_1: 0.66133
	loss_value_1: 0.03762
	loss_reward_1: 0.00591
	loss_policy_2: 0.03874
	accuracy_policy_2: 0.63582
	loss_value_2: 0.0392
	loss_reward_2: 0.00564
	loss_policy_3: 0.04209
	accuracy_policy_3: 0.61215
	loss_value_3: 0.04056
	loss_reward_3: 0.00615
	loss_policy_4: 0.04529
	accuracy_policy_4: 0.58951
	loss_value_4: 0.04181
	loss_reward_4: 0.00656
	loss_policy_5: 0.04848
	accuracy_policy_5: 0.5657
	loss_value_5: 0.04315
	loss_reward_5: 0.0079
	loss_policy: 0.35311
	loss_value: 0.38224
	loss_reward: 0.03216
Optimization_Done 41000
[2024-05-08 13:18:15] [command] train weight_iter_41000.pkl 204 206
[2024-05-08 13:18:57] nn step 41100, lr: 0.1.
	loss_policy_0: 0.16351
	accuracy_policy_0: 0.66457
	loss_value_0: 0.17087
	loss_policy_1: 0.0392
	accuracy_policy_1: 0.61133
	loss_value_1: 0.03564
	loss_reward_1: 0.00546
	loss_policy_2: 0.04237
	accuracy_policy_2: 0.58963
	loss_value_2: 0.03697
	loss_reward_2: 0.00521
	loss_policy_3: 0.04562
	accuracy_policy_3: 0.56598
	loss_value_3: 0.03828
	loss_reward_3: 0.00568
	loss_policy_4: 0.04832
	accuracy_policy_4: 0.54184
	loss_value_4: 0.03944
	loss_reward_4: 0.00613
	loss_policy_5: 0.05099
	accuracy_policy_5: 0.52494
	loss_value_5: 0.0405
	loss_reward_5: 0.00691
	loss_policy: 0.39
	loss_value: 0.36171
	loss_reward: 0.02938
[2024-05-08 13:19:37] nn step 41200, lr: 0.1.
	loss_policy_0: 0.1342
	accuracy_policy_0: 0.7084
	loss_value_0: 0.1593
	loss_policy_1: 0.03353
	accuracy_policy_1: 0.64697
	loss_value_1: 0.03334
	loss_reward_1: 0.00513
	loss_policy_2: 0.03664
	accuracy_policy_2: 0.62041
	loss_value_2: 0.03475
	loss_reward_2: 0.00494
	loss_policy_3: 0.03939
	accuracy_policy_3: 0.60045
	loss_value_3: 0.03592
	loss_reward_3: 0.00533
	loss_policy_4: 0.0422
	accuracy_policy_4: 0.57557
	loss_value_4: 0.03708
	loss_reward_4: 0.00556
	loss_policy_5: 0.04515
	accuracy_policy_5: 0.5523
	loss_value_5: 0.03814
	loss_reward_5: 0.00672
	loss_policy: 0.33112
	loss_value: 0.33852
	loss_reward: 0.02769
Optimization_Done 41200
[2024-05-08 13:22:03] [command] train weight_iter_41200.pkl 205 207
[2024-05-08 13:22:45] nn step 41300, lr: 0.1.
	loss_policy_0: 0.1383
	accuracy_policy_0: 0.68016
	loss_value_0: 0.16456
	loss_policy_1: 0.03326
	accuracy_policy_1: 0.62225
	loss_value_1: 0.03421
	loss_reward_1: 0.0049
	loss_policy_2: 0.03606
	accuracy_policy_2: 0.59721
	loss_value_2: 0.03536
	loss_reward_2: 0.00461
	loss_policy_3: 0.03877
	accuracy_policy_3: 0.57479
	loss_value_3: 0.03651
	loss_reward_3: 0.00507
	loss_policy_4: 0.04121
	accuracy_policy_4: 0.55502
	loss_value_4: 0.03747
	loss_reward_4: 0.00545
	loss_policy_5: 0.04356
	accuracy_policy_5: 0.53459
	loss_value_5: 0.03847
	loss_reward_5: 0.00653
	loss_policy: 0.33116
	loss_value: 0.34659
	loss_reward: 0.02656
[2024-05-08 13:23:26] nn step 41400, lr: 0.1.
	loss_policy_0: 0.12764
	accuracy_policy_0: 0.72139
	loss_value_0: 0.17125
	loss_policy_1: 0.03215
	accuracy_policy_1: 0.65707
	loss_value_1: 0.03577
	loss_reward_1: 0.00529
	loss_policy_2: 0.03527
	accuracy_policy_2: 0.63244
	loss_value_2: 0.03699
	loss_reward_2: 0.00512
	loss_policy_3: 0.03835
	accuracy_policy_3: 0.6073
	loss_value_3: 0.03814
	loss_reward_3: 0.00555
	loss_policy_4: 0.04128
	accuracy_policy_4: 0.58418
	loss_value_4: 0.03922
	loss_reward_4: 0.00588
	loss_policy_5: 0.04417
	accuracy_policy_5: 0.56076
	loss_value_5: 0.04027
	loss_reward_5: 0.00705
	loss_policy: 0.31886
	loss_value: 0.36163
	loss_reward: 0.02888
Optimization_Done 41400
[2024-05-08 13:25:47] [command] train weight_iter_41400.pkl 206 208
[2024-05-08 13:26:29] nn step 41500, lr: 0.1.
	loss_policy_0: 0.14571
	accuracy_policy_0: 0.68375
	loss_value_0: 0.16141
	loss_policy_1: 0.03461
	accuracy_policy_1: 0.63586
	loss_value_1: 0.03377
	loss_reward_1: 0.00467
	loss_policy_2: 0.03713
	accuracy_policy_2: 0.61352
	loss_value_2: 0.03526
	loss_reward_2: 0.00461
	loss_policy_3: 0.04042
	accuracy_policy_3: 0.58906
	loss_value_3: 0.03659
	loss_reward_3: 0.00499
	loss_policy_4: 0.04273
	accuracy_policy_4: 0.57088
	loss_value_4: 0.03776
	loss_reward_4: 0.00533
	loss_policy_5: 0.04495
	accuracy_policy_5: 0.55168
	loss_value_5: 0.0388
	loss_reward_5: 0.00618
	loss_policy: 0.34555
	loss_value: 0.34359
	loss_reward: 0.02578
[2024-05-08 13:27:09] nn step 41600, lr: 0.1.
	loss_policy_0: 0.13233
	accuracy_policy_0: 0.72586
	loss_value_0: 0.16834
	loss_policy_1: 0.0331
	accuracy_policy_1: 0.67172
	loss_value_1: 0.03537
	loss_reward_1: 0.00502
	loss_policy_2: 0.03586
	accuracy_policy_2: 0.6473
	loss_value_2: 0.03676
	loss_reward_2: 0.00488
	loss_policy_3: 0.03918
	accuracy_policy_3: 0.62463
	loss_value_3: 0.0381
	loss_reward_3: 0.00513
	loss_policy_4: 0.0418
	accuracy_policy_4: 0.6025
	loss_value_4: 0.03927
	loss_reward_4: 0.00559
	loss_policy_5: 0.0443
	accuracy_policy_5: 0.58307
	loss_value_5: 0.04043
	loss_reward_5: 0.00631
	loss_policy: 0.32657
	loss_value: 0.35826
	loss_reward: 0.02694
Optimization_Done 41600
[2024-05-08 13:29:44] [command] train weight_iter_41600.pkl 207 209
[2024-05-08 13:30:26] nn step 41700, lr: 0.1.
	loss_policy_0: 0.14913
	accuracy_policy_0: 0.68764
	loss_value_0: 0.1783
	loss_policy_1: 0.0359
	accuracy_policy_1: 0.63926
	loss_value_1: 0.03744
	loss_reward_1: 0.00544
	loss_policy_2: 0.03889
	accuracy_policy_2: 0.61545
	loss_value_2: 0.03879
	loss_reward_2: 0.00526
	loss_policy_3: 0.04177
	accuracy_policy_3: 0.59646
	loss_value_3: 0.04006
	loss_reward_3: 0.00561
	loss_policy_4: 0.04449
	accuracy_policy_4: 0.57338
	loss_value_4: 0.04133
	loss_reward_4: 0.00614
	loss_policy_5: 0.04746
	accuracy_policy_5: 0.54801
	loss_value_5: 0.04249
	loss_reward_5: 0.00728
	loss_policy: 0.35763
	loss_value: 0.3784
	loss_reward: 0.02973
[2024-05-08 13:31:07] nn step 41800, lr: 0.1.
	loss_policy_0: 0.13215
	accuracy_policy_0: 0.72447
	loss_value_0: 0.17588
	loss_policy_1: 0.03288
	accuracy_policy_1: 0.66594
	loss_value_1: 0.03687
	loss_reward_1: 0.00541
	loss_policy_2: 0.03614
	accuracy_policy_2: 0.64236
	loss_value_2: 0.03819
	loss_reward_2: 0.00524
	loss_policy_3: 0.03927
	accuracy_policy_3: 0.62135
	loss_value_3: 0.03948
	loss_reward_3: 0.00575
	loss_policy_4: 0.0422
	accuracy_policy_4: 0.5967
	loss_value_4: 0.04086
	loss_reward_4: 0.00613
	loss_policy_5: 0.04537
	accuracy_policy_5: 0.5716
	loss_value_5: 0.0421
	loss_reward_5: 0.00742
	loss_policy: 0.32801
	loss_value: 0.37337
	loss_reward: 0.02995
Optimization_Done 41800
[2024-05-08 13:33:38] [command] train weight_iter_41800.pkl 208 210
[2024-05-08 13:34:20] nn step 41900, lr: 0.1.
	loss_policy_0: 0.15687
	accuracy_policy_0: 0.67186
	loss_value_0: 0.17822
	loss_policy_1: 0.03824
	accuracy_policy_1: 0.61076
	loss_value_1: 0.03723
	loss_reward_1: 0.00604
	loss_policy_2: 0.04137
	accuracy_policy_2: 0.58875
	loss_value_2: 0.03873
	loss_reward_2: 0.00564
	loss_policy_3: 0.04452
	accuracy_policy_3: 0.566
	loss_value_3: 0.03983
	loss_reward_3: 0.00609
	loss_policy_4: 0.04765
	accuracy_policy_4: 0.54078
	loss_value_4: 0.04102
	loss_reward_4: 0.00656
	loss_policy_5: 0.0507
	accuracy_policy_5: 0.51824
	loss_value_5: 0.04192
	loss_reward_5: 0.00752
	loss_policy: 0.37934
	loss_value: 0.37694
	loss_reward: 0.03185
[2024-05-08 13:35:00] nn step 42000, lr: 0.1.
	loss_policy_0: 0.12781
	accuracy_policy_0: 0.71447
	loss_value_0: 0.16672
	loss_policy_1: 0.03286
	accuracy_policy_1: 0.65045
	loss_value_1: 0.03499
	loss_reward_1: 0.00555
	loss_policy_2: 0.03607
	accuracy_policy_2: 0.62453
	loss_value_2: 0.0364
	loss_reward_2: 0.00537
	loss_policy_3: 0.03916
	accuracy_policy_3: 0.59883
	loss_value_3: 0.03748
	loss_reward_3: 0.00569
	loss_policy_4: 0.04211
	accuracy_policy_4: 0.57018
	loss_value_4: 0.03863
	loss_reward_4: 0.00615
	loss_policy_5: 0.04506
	accuracy_policy_5: 0.5525
	loss_value_5: 0.03966
	loss_reward_5: 0.00728
	loss_policy: 0.32307
	loss_value: 0.35388
	loss_reward: 0.03004
Optimization_Done 42000
[2024-05-08 13:37:26] [command] train weight_iter_42000.pkl 209 211
[2024-05-08 13:38:08] nn step 42100, lr: 0.1.
	loss_policy_0: 0.13919
	accuracy_policy_0: 0.68281
	loss_value_0: 0.16881
	loss_policy_1: 0.03444
	accuracy_policy_1: 0.62004
	loss_value_1: 0.03513
	loss_reward_1: 0.00505
	loss_policy_2: 0.03727
	accuracy_policy_2: 0.59838
	loss_value_2: 0.0363
	loss_reward_2: 0.0049
	loss_policy_3: 0.04019
	accuracy_policy_3: 0.57676
	loss_value_3: 0.03732
	loss_reward_3: 0.00535
	loss_policy_4: 0.04279
	accuracy_policy_4: 0.5542
	loss_value_4: 0.03829
	loss_reward_4: 0.00575
	loss_policy_5: 0.0454
	accuracy_policy_5: 0.53467
	loss_value_5: 0.03935
	loss_reward_5: 0.00674
	loss_policy: 0.3393
	loss_value: 0.35519
	loss_reward: 0.02779
[2024-05-08 13:38:49] nn step 42200, lr: 0.1.
	loss_policy_0: 0.12378
	accuracy_policy_0: 0.72117
	loss_value_0: 0.17144
	loss_policy_1: 0.03227
	accuracy_policy_1: 0.65371
	loss_value_1: 0.03584
	loss_reward_1: 0.00533
	loss_policy_2: 0.0351
	accuracy_policy_2: 0.62588
	loss_value_2: 0.03702
	loss_reward_2: 0.00505
	loss_policy_3: 0.03844
	accuracy_policy_3: 0.60258
	loss_value_3: 0.03824
	loss_reward_3: 0.0057
	loss_policy_4: 0.04144
	accuracy_policy_4: 0.57898
	loss_value_4: 0.03915
	loss_reward_4: 0.00594
	loss_policy_5: 0.04453
	accuracy_policy_5: 0.55584
	loss_value_5: 0.04029
	loss_reward_5: 0.00705
	loss_policy: 0.31556
	loss_value: 0.36198
	loss_reward: 0.02906
Optimization_Done 42200
[2024-05-08 13:41:24] [command] train weight_iter_42200.pkl 210 212
[2024-05-08 13:42:06] nn step 42300, lr: 0.1.
	loss_policy_0: 0.14636
	accuracy_policy_0: 0.68596
	loss_value_0: 0.16819
	loss_policy_1: 0.03609
	accuracy_policy_1: 0.6275
	loss_value_1: 0.03506
	loss_reward_1: 0.00545
	loss_policy_2: 0.03942
	accuracy_policy_2: 0.60125
	loss_value_2: 0.03628
	loss_reward_2: 0.00501
	loss_policy_3: 0.04261
	accuracy_policy_3: 0.57775
	loss_value_3: 0.03749
	loss_reward_3: 0.00554
	loss_policy_4: 0.04523
	accuracy_policy_4: 0.55594
	loss_value_4: 0.03855
	loss_reward_4: 0.00591
	loss_policy_5: 0.04834
	accuracy_policy_5: 0.53135
	loss_value_5: 0.03966
	loss_reward_5: 0.00696
	loss_policy: 0.35804
	loss_value: 0.35523
	loss_reward: 0.02888
[2024-05-08 13:42:46] nn step 42400, lr: 0.1.
	loss_policy_0: 0.12529
	accuracy_policy_0: 0.72438
	loss_value_0: 0.16336
	loss_policy_1: 0.0325
	accuracy_policy_1: 0.65727
	loss_value_1: 0.03428
	loss_reward_1: 0.0053
	loss_policy_2: 0.03568
	accuracy_policy_2: 0.63012
	loss_value_2: 0.03558
	loss_reward_2: 0.00499
	loss_policy_3: 0.03866
	accuracy_policy_3: 0.60783
	loss_value_3: 0.03673
	loss_reward_3: 0.00548
	loss_policy_4: 0.0418
	accuracy_policy_4: 0.58555
	loss_value_4: 0.03782
	loss_reward_4: 0.00582
	loss_policy_5: 0.04452
	accuracy_policy_5: 0.56355
	loss_value_5: 0.03907
	loss_reward_5: 0.0068
	loss_policy: 0.31845
	loss_value: 0.34684
	loss_reward: 0.02839
Optimization_Done 42400
[2024-05-08 13:45:12] [command] train weight_iter_42400.pkl 211 213
[2024-05-08 13:45:54] nn step 42500, lr: 0.1.
	loss_policy_0: 0.14301
	accuracy_policy_0: 0.6867
	loss_value_0: 0.16666
	loss_policy_1: 0.0345
	accuracy_policy_1: 0.63445
	loss_value_1: 0.03488
	loss_reward_1: 0.00512
	loss_policy_2: 0.03755
	accuracy_policy_2: 0.61018
	loss_value_2: 0.03619
	loss_reward_2: 0.00508
	loss_policy_3: 0.04004
	accuracy_policy_3: 0.59057
	loss_value_3: 0.03737
	loss_reward_3: 0.00525
	loss_policy_4: 0.04308
	accuracy_policy_4: 0.566
	loss_value_4: 0.0386
	loss_reward_4: 0.00571
	loss_policy_5: 0.04591
	accuracy_policy_5: 0.54719
	loss_value_5: 0.03977
	loss_reward_5: 0.00675
	loss_policy: 0.3441
	loss_value: 0.35347
	loss_reward: 0.02792
[2024-05-08 13:46:34] nn step 42600, lr: 0.1.
	loss_policy_0: 0.12281
	accuracy_policy_0: 0.73373
	loss_value_0: 0.16571
	loss_policy_1: 0.03131
	accuracy_policy_1: 0.66861
	loss_value_1: 0.03476
	loss_reward_1: 0.00505
	loss_policy_2: 0.03426
	accuracy_policy_2: 0.64301
	loss_value_2: 0.0361
	loss_reward_2: 0.00495
	loss_policy_3: 0.0368
	accuracy_policy_3: 0.62734
	loss_value_3: 0.03723
	loss_reward_3: 0.0053
	loss_policy_4: 0.03969
	accuracy_policy_4: 0.60275
	loss_value_4: 0.03848
	loss_reward_4: 0.00563
	loss_policy_5: 0.04289
	accuracy_policy_5: 0.58066
	loss_value_5: 0.0396
	loss_reward_5: 0.0068
	loss_policy: 0.30775
	loss_value: 0.35188
	loss_reward: 0.02775
Optimization_Done 42600
[2024-05-08 13:49:06] [command] train weight_iter_42600.pkl 212 214
[2024-05-08 13:49:47] nn step 42700, lr: 0.1.
	loss_policy_0: 0.15117
	accuracy_policy_0: 0.67906
	loss_value_0: 0.17168
	loss_policy_1: 0.03642
	accuracy_policy_1: 0.62311
	loss_value_1: 0.03598
	loss_reward_1: 0.00562
	loss_policy_2: 0.03917
	accuracy_policy_2: 0.60264
	loss_value_2: 0.0375
	loss_reward_2: 0.00551
	loss_policy_3: 0.04183
	accuracy_policy_3: 0.58289
	loss_value_3: 0.03878
	loss_reward_3: 0.00605
	loss_policy_4: 0.04447
	accuracy_policy_4: 0.56104
	loss_value_4: 0.04002
	loss_reward_4: 0.0064
	loss_policy_5: 0.04717
	accuracy_policy_5: 0.54238
	loss_value_5: 0.04128
	loss_reward_5: 0.00759
	loss_policy: 0.36023
	loss_value: 0.36524
	loss_reward: 0.03117
[2024-05-08 13:50:28] nn step 42800, lr: 0.1.
	loss_policy_0: 0.13036
	accuracy_policy_0: 0.71639
	loss_value_0: 0.1679
	loss_policy_1: 0.03239
	accuracy_policy_1: 0.6592
	loss_value_1: 0.0352
	loss_reward_1: 0.0056
	loss_policy_2: 0.03524
	accuracy_policy_2: 0.63686
	loss_value_2: 0.03653
	loss_reward_2: 0.00545
	loss_policy_3: 0.03791
	accuracy_policy_3: 0.61687
	loss_value_3: 0.03767
	loss_reward_3: 0.00568
	loss_policy_4: 0.04065
	accuracy_policy_4: 0.59734
	loss_value_4: 0.0389
	loss_reward_4: 0.00627
	loss_policy_5: 0.04366
	accuracy_policy_5: 0.57699
	loss_value_5: 0.03999
	loss_reward_5: 0.00742
	loss_policy: 0.32022
	loss_value: 0.35619
	loss_reward: 0.03043
Optimization_Done 42800
[2024-05-08 13:52:55] [command] train weight_iter_42800.pkl 213 215
[2024-05-08 13:53:37] nn step 42900, lr: 0.1.
	loss_policy_0: 0.14635
	accuracy_policy_0: 0.69096
	loss_value_0: 0.17523
	loss_policy_1: 0.03529
	accuracy_policy_1: 0.641
	loss_value_1: 0.03676
	loss_reward_1: 0.00543
	loss_policy_2: 0.03829
	accuracy_policy_2: 0.61572
	loss_value_2: 0.03801
	loss_reward_2: 0.00532
	loss_policy_3: 0.04072
	accuracy_policy_3: 0.5983
	loss_value_3: 0.03929
	loss_reward_3: 0.00563
	loss_policy_4: 0.04319
	accuracy_policy_4: 0.57859
	loss_value_4: 0.04054
	loss_reward_4: 0.00615
	loss_policy_5: 0.04587
	accuracy_policy_5: 0.55693
	loss_value_5: 0.0417
	loss_reward_5: 0.00733
	loss_policy: 0.34971
	loss_value: 0.37154
	loss_reward: 0.02986
[2024-05-08 13:54:17] nn step 43000, lr: 0.1.
	loss_policy_0: 0.11835
	accuracy_policy_0: 0.73768
	loss_value_0: 0.16291
	loss_policy_1: 0.02998
	accuracy_policy_1: 0.67232
	loss_value_1: 0.03409
	loss_reward_1: 0.00504
	loss_policy_2: 0.03268
	accuracy_policy_2: 0.65043
	loss_value_2: 0.03543
	loss_reward_2: 0.00497
	loss_policy_3: 0.03471
	accuracy_policy_3: 0.63672
	loss_value_3: 0.03657
	loss_reward_3: 0.00529
	loss_policy_4: 0.03746
	accuracy_policy_4: 0.60875
	loss_value_4: 0.03775
	loss_reward_4: 0.00571
	loss_policy_5: 0.04011
	accuracy_policy_5: 0.58705
	loss_value_5: 0.03897
	loss_reward_5: 0.00669
	loss_policy: 0.29329
	loss_value: 0.34572
	loss_reward: 0.0277
Optimization_Done 43000
[2024-05-08 13:56:56] [command] train weight_iter_43000.pkl 214 216
[2024-05-08 13:57:38] nn step 43100, lr: 0.1.
	loss_policy_0: 0.13436
	accuracy_policy_0: 0.70172
	loss_value_0: 0.16007
	loss_policy_1: 0.03259
	accuracy_policy_1: 0.65002
	loss_value_1: 0.03346
	loss_reward_1: 0.00495
	loss_policy_2: 0.03541
	accuracy_policy_2: 0.6292
	loss_value_2: 0.03472
	loss_reward_2: 0.00485
	loss_policy_3: 0.0378
	accuracy_policy_3: 0.60553
	loss_value_3: 0.03578
	loss_reward_3: 0.00537
	loss_policy_4: 0.04028
	accuracy_policy_4: 0.59043
	loss_value_4: 0.03687
	loss_reward_4: 0.00571
	loss_policy_5: 0.04263
	accuracy_policy_5: 0.57096
	loss_value_5: 0.03793
	loss_reward_5: 0.0067
	loss_policy: 0.32305
	loss_value: 0.33883
	loss_reward: 0.02758
[2024-05-08 13:58:19] nn step 43200, lr: 0.1.
	loss_policy_0: 0.11912
	accuracy_policy_0: 0.74121
	loss_value_0: 0.15953
	loss_policy_1: 0.03001
	accuracy_policy_1: 0.68102
	loss_value_1: 0.03341
	loss_reward_1: 0.0051
	loss_policy_2: 0.03282
	accuracy_policy_2: 0.65859
	loss_value_2: 0.0346
	loss_reward_2: 0.0049
	loss_policy_3: 0.03576
	accuracy_policy_3: 0.63434
	loss_value_3: 0.03561
	loss_reward_3: 0.0055
	loss_policy_4: 0.03827
	accuracy_policy_4: 0.61611
	loss_value_4: 0.03678
	loss_reward_4: 0.00565
	loss_policy_5: 0.04057
	accuracy_policy_5: 0.59701
	loss_value_5: 0.03786
	loss_reward_5: 0.00692
	loss_policy: 0.29654
	loss_value: 0.33779
	loss_reward: 0.02806
Optimization_Done 43200
[2024-05-08 14:01:03] [command] train weight_iter_43200.pkl 215 217
[2024-05-08 14:01:45] nn step 43300, lr: 0.1.
	loss_policy_0: 0.1474
	accuracy_policy_0: 0.67904
	loss_value_0: 0.15502
	loss_policy_1: 0.03452
	accuracy_policy_1: 0.63576
	loss_value_1: 0.03249
	loss_reward_1: 0.00454
	loss_policy_2: 0.03717
	accuracy_policy_2: 0.6099
	loss_value_2: 0.03383
	loss_reward_2: 0.00447
	loss_policy_3: 0.03956
	accuracy_policy_3: 0.59389
	loss_value_3: 0.03515
	loss_reward_3: 0.00488
	loss_policy_4: 0.04178
	accuracy_policy_4: 0.57527
	loss_value_4: 0.03623
	loss_reward_4: 0.00531
	loss_policy_5: 0.04398
	accuracy_policy_5: 0.56127
	loss_value_5: 0.03733
	loss_reward_5: 0.0062
	loss_policy: 0.34442
	loss_value: 0.33004
	loss_reward: 0.0254
[2024-05-08 14:02:25] nn step 43400, lr: 0.1.
	loss_policy_0: 0.13606
	accuracy_policy_0: 0.72215
	loss_value_0: 0.16316
	loss_policy_1: 0.03287
	accuracy_policy_1: 0.67352
	loss_value_1: 0.03411
	loss_reward_1: 0.00499
	loss_policy_2: 0.03563
	accuracy_policy_2: 0.6502
	loss_value_2: 0.03553
	loss_reward_2: 0.00481
	loss_policy_3: 0.03836
	accuracy_policy_3: 0.63248
	loss_value_3: 0.0368
	loss_reward_3: 0.00521
	loss_policy_4: 0.04075
	accuracy_policy_4: 0.61627
	loss_value_4: 0.03802
	loss_reward_4: 0.00565
	loss_policy_5: 0.04346
	accuracy_policy_5: 0.59373
	loss_value_5: 0.03933
	loss_reward_5: 0.00659
	loss_policy: 0.32713
	loss_value: 0.34695
	loss_reward: 0.02726
Optimization_Done 43400
[2024-05-08 14:05:02] [command] train weight_iter_43400.pkl 216 218
[2024-05-08 14:05:44] nn step 43500, lr: 0.1.
	loss_policy_0: 0.15669
	accuracy_policy_0: 0.69939
	loss_value_0: 0.17129
	loss_policy_1: 0.03649
	accuracy_policy_1: 0.65324
	loss_value_1: 0.03597
	loss_reward_1: 0.00551
	loss_policy_2: 0.03911
	accuracy_policy_2: 0.63105
	loss_value_2: 0.03746
	loss_reward_2: 0.00538
	loss_policy_3: 0.04219
	accuracy_policy_3: 0.61105
	loss_value_3: 0.03882
	loss_reward_3: 0.00579
	loss_policy_4: 0.04478
	accuracy_policy_4: 0.59309
	loss_value_4: 0.04016
	loss_reward_4: 0.00626
	loss_policy_5: 0.04768
	accuracy_policy_5: 0.57301
	loss_value_5: 0.04135
	loss_reward_5: 0.00739
	loss_policy: 0.36694
	loss_value: 0.36505
	loss_reward: 0.03032
[2024-05-08 14:06:25] nn step 43600, lr: 0.1.
	loss_policy_0: 0.14463
	accuracy_policy_0: 0.73006
	loss_value_0: 0.17695
	loss_policy_1: 0.03509
	accuracy_policy_1: 0.67617
	loss_value_1: 0.03715
	loss_reward_1: 0.00559
	loss_policy_2: 0.03802
	accuracy_policy_2: 0.65605
	loss_value_2: 0.03869
	loss_reward_2: 0.0055
	loss_policy_3: 0.04067
	accuracy_policy_3: 0.63826
	loss_value_3: 0.04018
	loss_reward_3: 0.00602
	loss_policy_4: 0.04382
	accuracy_policy_4: 0.61742
	loss_value_4: 0.04135
	loss_reward_4: 0.00649
	loss_policy_5: 0.0469
	accuracy_policy_5: 0.59396
	loss_value_5: 0.04255
	loss_reward_5: 0.00773
	loss_policy: 0.34912
	loss_value: 0.37687
	loss_reward: 0.03133
Optimization_Done 43600
[2024-05-08 14:08:48] [command] train weight_iter_43600.pkl 217 219
[2024-05-08 14:09:30] nn step 43700, lr: 0.1.
	loss_policy_0: 0.1593
	accuracy_policy_0: 0.65982
	loss_value_0: 0.16024
	loss_policy_1: 0.03694
	accuracy_policy_1: 0.61105
	loss_value_1: 0.03355
	loss_reward_1: 0.00474
	loss_policy_2: 0.03935
	accuracy_policy_2: 0.5917
	loss_value_2: 0.03484
	loss_reward_2: 0.00476
	loss_policy_3: 0.04181
	accuracy_policy_3: 0.57312
	loss_value_3: 0.03607
	loss_reward_3: 0.00514
	loss_policy_4: 0.04384
	accuracy_policy_4: 0.55389
	loss_value_4: 0.03712
	loss_reward_4: 0.00553
	loss_policy_5: 0.04619
	accuracy_policy_5: 0.53645
	loss_value_5: 0.03835
	loss_reward_5: 0.0063
	loss_policy: 0.36743
	loss_value: 0.34016
	loss_reward: 0.02648
[2024-05-08 14:10:11] nn step 43800, lr: 0.1.
	loss_policy_0: 0.14332
	accuracy_policy_0: 0.70643
	loss_value_0: 0.16493
	loss_policy_1: 0.03426
	accuracy_policy_1: 0.65094
	loss_value_1: 0.03445
	loss_reward_1: 0.00505
	loss_policy_2: 0.03695
	accuracy_policy_2: 0.6309
	loss_value_2: 0.03577
	loss_reward_2: 0.00496
	loss_policy_3: 0.03935
	accuracy_policy_3: 0.61373
	loss_value_3: 0.0369
	loss_reward_3: 0.0052
	loss_policy_4: 0.04208
	accuracy_policy_4: 0.59055
	loss_value_4: 0.03817
	loss_reward_4: 0.00581
	loss_policy_5: 0.04462
	accuracy_policy_5: 0.57244
	loss_value_5: 0.03943
	loss_reward_5: 0.00669
	loss_policy: 0.3406
	loss_value: 0.34965
	loss_reward: 0.02771
Optimization_Done 43800
[2024-05-08 14:12:23] [command] train weight_iter_43800.pkl 218 220
[2024-05-08 14:13:05] nn step 43900, lr: 0.1.
	loss_policy_0: 0.14712
	accuracy_policy_0: 0.69035
	loss_value_0: 0.17398
	loss_policy_1: 0.0349
	accuracy_policy_1: 0.64207
	loss_value_1: 0.03632
	loss_reward_1: 0.00471
	loss_policy_2: 0.03746
	accuracy_policy_2: 0.62125
	loss_value_2: 0.03757
	loss_reward_2: 0.0047
	loss_policy_3: 0.03998
	accuracy_policy_3: 0.60383
	loss_value_3: 0.03865
	loss_reward_3: 0.0049
	loss_policy_4: 0.04263
	accuracy_policy_4: 0.57969
	loss_value_4: 0.03973
	loss_reward_4: 0.00536
	loss_policy_5: 0.04476
	accuracy_policy_5: 0.56395
	loss_value_5: 0.04082
	loss_reward_5: 0.00615
	loss_policy: 0.34685
	loss_value: 0.36707
	loss_reward: 0.02583
[2024-05-08 14:13:46] nn step 44000, lr: 0.1.
	loss_policy_0: 0.12579
	accuracy_policy_0: 0.72617
	loss_value_0: 0.16844
	loss_policy_1: 0.03107
	accuracy_policy_1: 0.67012
	loss_value_1: 0.03517
	loss_reward_1: 0.00449
	loss_policy_2: 0.03357
	accuracy_policy_2: 0.65072
	loss_value_2: 0.03646
	loss_reward_2: 0.00461
	loss_policy_3: 0.0362
	accuracy_policy_3: 0.62631
	loss_value_3: 0.03758
	loss_reward_3: 0.00468
	loss_policy_4: 0.03868
	accuracy_policy_4: 0.60498
	loss_value_4: 0.03871
	loss_reward_4: 0.00526
	loss_policy_5: 0.04122
	accuracy_policy_5: 0.58834
	loss_value_5: 0.03974
	loss_reward_5: 0.0061
	loss_policy: 0.30653
	loss_value: 0.35612
	loss_reward: 0.02514
Optimization_Done 44000
[2024-05-08 14:16:26] [command] train weight_iter_44000.pkl 219 221
[2024-05-08 14:17:08] nn step 44100, lr: 0.1.
	loss_policy_0: 0.15622
	accuracy_policy_0: 0.68307
	loss_value_0: 0.1722
	loss_policy_1: 0.03653
	accuracy_policy_1: 0.63547
	loss_value_1: 0.03583
	loss_reward_1: 0.0039
	loss_policy_2: 0.03914
	accuracy_policy_2: 0.61221
	loss_value_2: 0.03703
	loss_reward_2: 0.00387
	loss_policy_3: 0.04153
	accuracy_policy_3: 0.59322
	loss_value_3: 0.03822
	loss_reward_3: 0.00439
	loss_policy_4: 0.04378
	accuracy_policy_4: 0.57793
	loss_value_4: 0.03942
	loss_reward_4: 0.00467
	loss_policy_5: 0.04625
	accuracy_policy_5: 0.55572
	loss_value_5: 0.04056
	loss_reward_5: 0.00531
	loss_policy: 0.36345
	loss_value: 0.36327
	loss_reward: 0.02215
[2024-05-08 14:17:48] nn step 44200, lr: 0.1.
	loss_policy_0: 0.13192
	accuracy_policy_0: 0.73049
	loss_value_0: 0.17551
	loss_policy_1: 0.03227
	accuracy_policy_1: 0.67377
	loss_value_1: 0.03668
	loss_reward_1: 0.00423
	loss_policy_2: 0.03485
	accuracy_policy_2: 0.65492
	loss_value_2: 0.03796
	loss_reward_2: 0.00397
	loss_policy_3: 0.03776
	accuracy_policy_3: 0.63277
	loss_value_3: 0.0391
	loss_reward_3: 0.0043
	loss_policy_4: 0.04057
	accuracy_policy_4: 0.60951
	loss_value_4: 0.0404
	loss_reward_4: 0.00458
	loss_policy_5: 0.04302
	accuracy_policy_5: 0.58852
	loss_value_5: 0.04143
	loss_reward_5: 0.00539
	loss_policy: 0.32039
	loss_value: 0.37109
	loss_reward: 0.02246
Optimization_Done 44200
[2024-05-08 14:20:28] [command] train weight_iter_44200.pkl 220 222
[2024-05-08 14:21:10] nn step 44300, lr: 0.1.
	loss_policy_0: 0.16166
	accuracy_policy_0: 0.6826
	loss_value_0: 0.18311
	loss_policy_1: 0.03757
	accuracy_policy_1: 0.64406
	loss_value_1: 0.0383
	loss_reward_1: 0.00451
	loss_policy_2: 0.04007
	accuracy_policy_2: 0.6258
	loss_value_2: 0.03984
	loss_reward_2: 0.00446
	loss_policy_3: 0.04318
	accuracy_policy_3: 0.59512
	loss_value_3: 0.04122
	loss_reward_3: 0.00475
	loss_policy_4: 0.04614
	accuracy_policy_4: 0.57521
	loss_value_4: 0.04253
	loss_reward_4: 0.00507
	loss_policy_5: 0.04834
	accuracy_policy_5: 0.56041
	loss_value_5: 0.04368
	loss_reward_5: 0.00569
	loss_policy: 0.37696
	loss_value: 0.38868
	loss_reward: 0.02448
[2024-05-08 14:21:50] nn step 44400, lr: 0.1.
	loss_policy_0: 0.13202
	accuracy_policy_0: 0.73094
	loss_value_0: 0.1765
	loss_policy_1: 0.03204
	accuracy_policy_1: 0.67828
	loss_value_1: 0.03694
	loss_reward_1: 0.00427
	loss_policy_2: 0.03476
	accuracy_policy_2: 0.65639
	loss_value_2: 0.03814
	loss_reward_2: 0.00418
	loss_policy_3: 0.03812
	accuracy_policy_3: 0.634
	loss_value_3: 0.03936
	loss_reward_3: 0.0046
	loss_policy_4: 0.04056
	accuracy_policy_4: 0.6132
	loss_value_4: 0.04053
	loss_reward_4: 0.00479
	loss_policy_5: 0.04369
	accuracy_policy_5: 0.58992
	loss_value_5: 0.04179
	loss_reward_5: 0.00554
	loss_policy: 0.3212
	loss_value: 0.37325
	loss_reward: 0.02337
Optimization_Done 44400
[2024-05-08 14:24:11] [command] train weight_iter_44400.pkl 221 223
[2024-05-08 14:24:53] nn step 44500, lr: 0.1.
	loss_policy_0: 0.16471
	accuracy_policy_0: 0.67043
	loss_value_0: 0.18139
	loss_policy_1: 0.03865
	accuracy_policy_1: 0.6202
	loss_value_1: 0.03795
	loss_reward_1: 0.00503
	loss_policy_2: 0.0421
	accuracy_policy_2: 0.5951
	loss_value_2: 0.03951
	loss_reward_2: 0.00497
	loss_policy_3: 0.04511
	accuracy_policy_3: 0.57344
	loss_value_3: 0.04089
	loss_reward_3: 0.00526
	loss_policy_4: 0.04817
	accuracy_policy_4: 0.54797
	loss_value_4: 0.04218
	loss_reward_4: 0.00585
	loss_policy_5: 0.05122
	accuracy_policy_5: 0.52838
	loss_value_5: 0.04338
	loss_reward_5: 0.0066
	loss_policy: 0.38995
	loss_value: 0.3853
	loss_reward: 0.02772
[2024-05-08 14:25:34] nn step 44600, lr: 0.1.
	loss_policy_0: 0.14469
	accuracy_policy_0: 0.71012
	loss_value_0: 0.17884
	loss_policy_1: 0.03577
	accuracy_policy_1: 0.6541
	loss_value_1: 0.03754
	loss_reward_1: 0.00516
	loss_policy_2: 0.03892
	accuracy_policy_2: 0.62523
	loss_value_2: 0.03912
	loss_reward_2: 0.00472
	loss_policy_3: 0.04173
	accuracy_policy_3: 0.60205
	loss_value_3: 0.04028
	loss_reward_3: 0.00518
	loss_policy_4: 0.04501
	accuracy_policy_4: 0.58346
	loss_value_4: 0.0415
	loss_reward_4: 0.00554
	loss_policy_5: 0.04873
	accuracy_policy_5: 0.55445
	loss_value_5: 0.04275
	loss_reward_5: 0.00643
	loss_policy: 0.35485
	loss_value: 0.38003
	loss_reward: 0.02702
Optimization_Done 44600
[2024-05-08 14:27:58] [command] train weight_iter_44600.pkl 222 224
[2024-05-08 14:28:40] nn step 44700, lr: 0.1.
	loss_policy_0: 0.16397
	accuracy_policy_0: 0.66965
	loss_value_0: 0.1854
	loss_policy_1: 0.03924
	accuracy_policy_1: 0.61312
	loss_value_1: 0.03878
	loss_reward_1: 0.00521
	loss_policy_2: 0.04227
	accuracy_policy_2: 0.5898
	loss_value_2: 0.04036
	loss_reward_2: 0.00505
	loss_policy_3: 0.04495
	accuracy_policy_3: 0.57457
	loss_value_3: 0.04175
	loss_reward_3: 0.0054
	loss_policy_4: 0.04826
	accuracy_policy_4: 0.54723
	loss_value_4: 0.04294
	loss_reward_4: 0.00581
	loss_policy_5: 0.05086
	accuracy_policy_5: 0.52689
	loss_value_5: 0.04414
	loss_reward_5: 0.00656
	loss_policy: 0.38956
	loss_value: 0.39338
	loss_reward: 0.02803
[2024-05-08 14:29:21] nn step 44800, lr: 0.1.
	loss_policy_0: 0.14192
	accuracy_policy_0: 0.70635
	loss_value_0: 0.17766
	loss_policy_1: 0.03559
	accuracy_policy_1: 0.64113
	loss_value_1: 0.0372
	loss_reward_1: 0.005
	loss_policy_2: 0.03815
	accuracy_policy_2: 0.62037
	loss_value_2: 0.03844
	loss_reward_2: 0.00485
	loss_policy_3: 0.04113
	accuracy_policy_3: 0.59678
	loss_value_3: 0.03955
	loss_reward_3: 0.00524
	loss_policy_4: 0.04466
	accuracy_policy_4: 0.5725
	loss_value_4: 0.04075
	loss_reward_4: 0.00555
	loss_policy_5: 0.04754
	accuracy_policy_5: 0.54936
	loss_value_5: 0.04192
	loss_reward_5: 0.00648
	loss_policy: 0.34899
	loss_value: 0.37552
	loss_reward: 0.02712
Optimization_Done 44800
[2024-05-08 14:31:55] [command] train weight_iter_44800.pkl 223 225
[2024-05-08 14:32:37] nn step 44900, lr: 0.1.
	loss_policy_0: 0.17216
	accuracy_policy_0: 0.6617
	loss_value_0: 0.17994
	loss_policy_1: 0.04074
	accuracy_policy_1: 0.60984
	loss_value_1: 0.03756
	loss_reward_1: 0.00485
	loss_policy_2: 0.04332
	accuracy_policy_2: 0.58805
	loss_value_2: 0.039
	loss_reward_2: 0.00473
	loss_policy_3: 0.04622
	accuracy_policy_3: 0.56795
	loss_value_3: 0.04018
	loss_reward_3: 0.0051
	loss_policy_4: 0.04905
	accuracy_policy_4: 0.54691
	loss_value_4: 0.04143
	loss_reward_4: 0.00541
	loss_policy_5: 0.05181
	accuracy_policy_5: 0.52234
	loss_value_5: 0.04255
	loss_reward_5: 0.00638
	loss_policy: 0.40331
	loss_value: 0.38067
	loss_reward: 0.02648
[2024-05-08 14:33:17] nn step 45000, lr: 0.1.
	loss_policy_0: 0.14353
	accuracy_policy_0: 0.70318
	loss_value_0: 0.17346
	loss_policy_1: 0.03527
	accuracy_policy_1: 0.64305
	loss_value_1: 0.03641
	loss_reward_1: 0.00474
	loss_policy_2: 0.03807
	accuracy_policy_2: 0.6207
	loss_value_2: 0.03779
	loss_reward_2: 0.00495
	loss_policy_3: 0.04125
	accuracy_policy_3: 0.59562
	loss_value_3: 0.03894
	loss_reward_3: 0.00498
	loss_policy_4: 0.04411
	accuracy_policy_4: 0.57176
	loss_value_4: 0.04004
	loss_reward_4: 0.00527
	loss_policy_5: 0.0471
	accuracy_policy_5: 0.55234
	loss_value_5: 0.04128
	loss_reward_5: 0.00633
	loss_policy: 0.34933
	loss_value: 0.36793
	loss_reward: 0.02627
Optimization_Done 45000
[2024-05-08 14:35:59] [command] train weight_iter_45000.pkl 224 226
[2024-05-08 14:36:41] nn step 45100, lr: 0.1.
	loss_policy_0: 0.16283
	accuracy_policy_0: 0.68004
	loss_value_0: 0.17632
	loss_policy_1: 0.03775
	accuracy_policy_1: 0.64029
	loss_value_1: 0.03697
	loss_reward_1: 0.00458
	loss_policy_2: 0.04015
	accuracy_policy_2: 0.62082
	loss_value_2: 0.03832
	loss_reward_2: 0.00461
	loss_policy_3: 0.04307
	accuracy_policy_3: 0.60037
	loss_value_3: 0.03964
	loss_reward_3: 0.00483
	loss_policy_4: 0.04555
	accuracy_policy_4: 0.58279
	loss_value_4: 0.04074
	loss_reward_4: 0.00533
	loss_policy_5: 0.04844
	accuracy_policy_5: 0.56182
	loss_value_5: 0.04187
	loss_reward_5: 0.00596
	loss_policy: 0.3778
	loss_value: 0.37385
	loss_reward: 0.02532
[2024-05-08 14:37:21] nn step 45200, lr: 0.1.
	loss_policy_0: 0.13988
	accuracy_policy_0: 0.71637
	loss_value_0: 0.16966
	loss_policy_1: 0.03365
	accuracy_policy_1: 0.66516
	loss_value_1: 0.03553
	loss_reward_1: 0.00452
	loss_policy_2: 0.0359
	accuracy_policy_2: 0.64545
	loss_value_2: 0.03691
	loss_reward_2: 0.00449
	loss_policy_3: 0.03906
	accuracy_policy_3: 0.62154
	loss_value_3: 0.03791
	loss_reward_3: 0.00471
	loss_policy_4: 0.04171
	accuracy_policy_4: 0.5991
	loss_value_4: 0.03888
	loss_reward_4: 0.00507
	loss_policy_5: 0.04444
	accuracy_policy_5: 0.57781
	loss_value_5: 0.04004
	loss_reward_5: 0.00579
	loss_policy: 0.33463
	loss_value: 0.35892
	loss_reward: 0.02457
Optimization_Done 45200
[2024-05-08 14:39:54] [command] train weight_iter_45200.pkl 225 227
[2024-05-08 14:40:35] nn step 45300, lr: 0.1.
	loss_policy_0: 0.15667
	accuracy_policy_0: 0.68744
	loss_value_0: 0.17768
	loss_policy_1: 0.03647
	accuracy_policy_1: 0.63855
	loss_value_1: 0.03706
	loss_reward_1: 0.00519
	loss_policy_2: 0.03935
	accuracy_policy_2: 0.61988
	loss_value_2: 0.03848
	loss_reward_2: 0.00521
	loss_policy_3: 0.04164
	accuracy_policy_3: 0.6057
	loss_value_3: 0.03978
	loss_reward_3: 0.0056
	loss_policy_4: 0.04462
	accuracy_policy_4: 0.58158
	loss_value_4: 0.04104
	loss_reward_4: 0.00604
	loss_policy_5: 0.04701
	accuracy_policy_5: 0.56537
	loss_value_5: 0.0422
	loss_reward_5: 0.00682
	loss_policy: 0.36576
	loss_value: 0.37624
	loss_reward: 0.02886
[2024-05-08 14:41:16] nn step 45400, lr: 0.1.
	loss_policy_0: 0.14088
	accuracy_policy_0: 0.71859
	loss_value_0: 0.17489
	loss_policy_1: 0.03357
	accuracy_policy_1: 0.67188
	loss_value_1: 0.03652
	loss_reward_1: 0.00523
	loss_policy_2: 0.03586
	accuracy_policy_2: 0.6567
	loss_value_2: 0.03774
	loss_reward_2: 0.00508
	loss_policy_3: 0.03892
	accuracy_policy_3: 0.6334
	loss_value_3: 0.039
	loss_reward_3: 0.00543
	loss_policy_4: 0.04226
	accuracy_policy_4: 0.60914
	loss_value_4: 0.04021
	loss_reward_4: 0.00596
	loss_policy_5: 0.04511
	accuracy_policy_5: 0.5917
	loss_value_5: 0.0413
	loss_reward_5: 0.00681
	loss_policy: 0.33662
	loss_value: 0.36968
	loss_reward: 0.02851
Optimization_Done 45400
[2024-05-08 14:43:44] [command] train weight_iter_45400.pkl 226 228
[2024-05-08 14:44:25] nn step 45500, lr: 0.1.
	loss_policy_0: 0.15577
	accuracy_policy_0: 0.68789
	loss_value_0: 0.17955
	loss_policy_1: 0.03682
	accuracy_policy_1: 0.64143
	loss_value_1: 0.03759
	loss_reward_1: 0.00527
	loss_policy_2: 0.03989
	accuracy_policy_2: 0.6157
	loss_value_2: 0.03907
	loss_reward_2: 0.00487
	loss_policy_3: 0.04263
	accuracy_policy_3: 0.59676
	loss_value_3: 0.04036
	loss_reward_3: 0.00546
	loss_policy_4: 0.04511
	accuracy_policy_4: 0.58041
	loss_value_4: 0.04149
	loss_reward_4: 0.00602
	loss_policy_5: 0.04829
	accuracy_policy_5: 0.55641
	loss_value_5: 0.04256
	loss_reward_5: 0.00674
	loss_policy: 0.36852
	loss_value: 0.38061
	loss_reward: 0.02836
[2024-05-08 14:45:06] nn step 45600, lr: 0.1.
	loss_policy_0: 0.13359
	accuracy_policy_0: 0.72773
	loss_value_0: 0.17612
	loss_policy_1: 0.0337
	accuracy_policy_1: 0.66781
	loss_value_1: 0.03694
	loss_reward_1: 0.00502
	loss_policy_2: 0.03643
	accuracy_policy_2: 0.64645
	loss_value_2: 0.03823
	loss_reward_2: 0.00499
	loss_policy_3: 0.03927
	accuracy_policy_3: 0.62605
	loss_value_3: 0.03936
	loss_reward_3: 0.00538
	loss_policy_4: 0.04219
	accuracy_policy_4: 0.60697
	loss_value_4: 0.04048
	loss_reward_4: 0.00592
	loss_policy_5: 0.04529
	accuracy_policy_5: 0.58143
	loss_value_5: 0.04173
	loss_reward_5: 0.00681
	loss_policy: 0.33047
	loss_value: 0.37286
	loss_reward: 0.02812
Optimization_Done 45600
[2024-05-08 14:47:42] [command] train weight_iter_45600.pkl 227 229
[2024-05-08 14:48:24] nn step 45700, lr: 0.1.
	loss_policy_0: 0.16138
	accuracy_policy_0: 0.67055
	loss_value_0: 0.17702
	loss_policy_1: 0.03814
	accuracy_policy_1: 0.62324
	loss_value_1: 0.037
	loss_reward_1: 0.00543
	loss_policy_2: 0.04111
	accuracy_policy_2: 0.6008
	loss_value_2: 0.03832
	loss_reward_2: 0.00531
	loss_policy_3: 0.04447
	accuracy_policy_3: 0.57654
	loss_value_3: 0.03962
	loss_reward_3: 0.00574
	loss_policy_4: 0.04679
	accuracy_policy_4: 0.55787
	loss_value_4: 0.04077
	loss_reward_4: 0.00636
	loss_policy_5: 0.05001
	accuracy_policy_5: 0.53059
	loss_value_5: 0.04192
	loss_reward_5: 0.00737
	loss_policy: 0.38189
	loss_value: 0.37464
	loss_reward: 0.0302
[2024-05-08 14:49:04] nn step 45800, lr: 0.1.
	loss_policy_0: 0.13919
	accuracy_policy_0: 0.71707
	loss_value_0: 0.17612
	loss_policy_1: 0.0342
	accuracy_policy_1: 0.65867
	loss_value_1: 0.03669
	loss_reward_1: 0.0054
	loss_policy_2: 0.03749
	accuracy_policy_2: 0.63557
	loss_value_2: 0.03794
	loss_reward_2: 0.00532
	loss_policy_3: 0.0401
	accuracy_policy_3: 0.61529
	loss_value_3: 0.03914
	loss_reward_3: 0.00575
	loss_policy_4: 0.04339
	accuracy_policy_4: 0.59271
	loss_value_4: 0.04027
	loss_reward_4: 0.00616
	loss_policy_5: 0.04628
	accuracy_policy_5: 0.56826
	loss_value_5: 0.04153
	loss_reward_5: 0.0073
	loss_policy: 0.34066
	loss_value: 0.37168
	loss_reward: 0.02994
Optimization_Done 45800
[2024-05-08 14:51:41] [command] train weight_iter_45800.pkl 228 230
[2024-05-08 14:52:22] nn step 45900, lr: 0.1.
	loss_policy_0: 0.16643
	accuracy_policy_0: 0.67881
	loss_value_0: 0.18094
	loss_policy_1: 0.03894
	accuracy_policy_1: 0.63004
	loss_value_1: 0.03772
	loss_reward_1: 0.00549
	loss_policy_2: 0.04217
	accuracy_policy_2: 0.60617
	loss_value_2: 0.03919
	loss_reward_2: 0.00556
	loss_policy_3: 0.0452
	accuracy_policy_3: 0.57826
	loss_value_3: 0.04044
	loss_reward_3: 0.00611
	loss_policy_4: 0.04792
	accuracy_policy_4: 0.55928
	loss_value_4: 0.04159
	loss_reward_4: 0.0065
	loss_policy_5: 0.05092
	accuracy_policy_5: 0.53725
	loss_value_5: 0.04285
	loss_reward_5: 0.00758
	loss_policy: 0.39159
	loss_value: 0.38272
	loss_reward: 0.03123
[2024-05-08 14:53:03] nn step 46000, lr: 0.1.
	loss_policy_0: 0.14671
	accuracy_policy_0: 0.70746
	loss_value_0: 0.17475
	loss_policy_1: 0.03473
	accuracy_policy_1: 0.6599
	loss_value_1: 0.03674
	loss_reward_1: 0.00548
	loss_policy_2: 0.03768
	accuracy_policy_2: 0.63775
	loss_value_2: 0.03803
	loss_reward_2: 0.00534
	loss_policy_3: 0.04093
	accuracy_policy_3: 0.61355
	loss_value_3: 0.03933
	loss_reward_3: 0.00579
	loss_policy_4: 0.04379
	accuracy_policy_4: 0.59025
	loss_value_4: 0.04041
	loss_reward_4: 0.00621
	loss_policy_5: 0.04698
	accuracy_policy_5: 0.5643
	loss_value_5: 0.04168
	loss_reward_5: 0.00733
	loss_policy: 0.35082
	loss_value: 0.37093
	loss_reward: 0.03015
Optimization_Done 46000
[2024-05-08 14:55:15] [command] train weight_iter_46000.pkl 229 231
[2024-05-08 14:55:57] nn step 46100, lr: 0.1.
	loss_policy_0: 0.14768
	accuracy_policy_0: 0.71385
	loss_value_0: 0.17385
	loss_policy_1: 0.03431
	accuracy_policy_1: 0.676
	loss_value_1: 0.0364
	loss_reward_1: 0.00598
	loss_policy_2: 0.03719
	accuracy_policy_2: 0.65668
	loss_value_2: 0.03779
	loss_reward_2: 0.00579
	loss_policy_3: 0.04001
	accuracy_policy_3: 0.63334
	loss_value_3: 0.03912
	loss_reward_3: 0.00633
	loss_policy_4: 0.04347
	accuracy_policy_4: 0.60336
	loss_value_4: 0.04027
	loss_reward_4: 0.00685
	loss_policy_5: 0.04651
	accuracy_policy_5: 0.58285
	loss_value_5: 0.04155
	loss_reward_5: 0.0081
	loss_policy: 0.34918
	loss_value: 0.36898
	loss_reward: 0.03305
[2024-05-08 14:56:38] nn step 46200, lr: 0.1.
	loss_policy_0: 0.13297
	accuracy_policy_0: 0.7468
	loss_value_0: 0.17332
	loss_policy_1: 0.03241
	accuracy_policy_1: 0.69465
	loss_value_1: 0.03641
	loss_reward_1: 0.00611
	loss_policy_2: 0.03534
	accuracy_policy_2: 0.67457
	loss_value_2: 0.03784
	loss_reward_2: 0.00586
	loss_policy_3: 0.03802
	accuracy_policy_3: 0.6574
	loss_value_3: 0.03922
	loss_reward_3: 0.00637
	loss_policy_4: 0.0414
	accuracy_policy_4: 0.62592
	loss_value_4: 0.04048
	loss_reward_4: 0.0068
	loss_policy_5: 0.04493
	accuracy_policy_5: 0.60045
	loss_value_5: 0.04164
	loss_reward_5: 0.00801
	loss_policy: 0.32507
	loss_value: 0.3689
	loss_reward: 0.03314
Optimization_Done 46200
[2024-05-08 14:59:13] [command] train weight_iter_46200.pkl 230 232
[2024-05-08 14:59:55] nn step 46300, lr: 0.1.
	loss_policy_0: 0.14661
	accuracy_policy_0: 0.71488
	loss_value_0: 0.16661
	loss_policy_1: 0.03506
	accuracy_policy_1: 0.66418
	loss_value_1: 0.03482
	loss_reward_1: 0.00566
	loss_policy_2: 0.0378
	accuracy_policy_2: 0.64268
	loss_value_2: 0.03625
	loss_reward_2: 0.00553
	loss_policy_3: 0.04048
	accuracy_policy_3: 0.62494
	loss_value_3: 0.0376
	loss_reward_3: 0.00588
	loss_policy_4: 0.04337
	accuracy_policy_4: 0.60225
	loss_value_4: 0.03882
	loss_reward_4: 0.00624
	loss_policy_5: 0.04606
	accuracy_policy_5: 0.57986
	loss_value_5: 0.04
	loss_reward_5: 0.0074
	loss_policy: 0.34938
	loss_value: 0.3541
	loss_reward: 0.03071
[2024-05-08 15:00:36] nn step 46400, lr: 0.1.
	loss_policy_0: 0.11932
	accuracy_policy_0: 0.75113
	loss_value_0: 0.15287
	loss_policy_1: 0.03008
	accuracy_policy_1: 0.69174
	loss_value_1: 0.03198
	loss_reward_1: 0.00531
	loss_policy_2: 0.03256
	accuracy_policy_2: 0.67326
	loss_value_2: 0.03344
	loss_reward_2: 0.00502
	loss_policy_3: 0.0353
	accuracy_policy_3: 0.65375
	loss_value_3: 0.03468
	loss_reward_3: 0.00527
	loss_policy_4: 0.03787
	accuracy_policy_4: 0.62785
	loss_value_4: 0.03568
	loss_reward_4: 0.00593
	loss_policy_5: 0.04079
	accuracy_policy_5: 0.60889
	loss_value_5: 0.0368
	loss_reward_5: 0.00706
	loss_policy: 0.29591
	loss_value: 0.32544
	loss_reward: 0.02859
Optimization_Done 46400
[2024-05-08 15:03:00] [command] train weight_iter_46400.pkl 231 233
[2024-05-08 15:03:41] nn step 46500, lr: 0.1.
	loss_policy_0: 0.1355
	accuracy_policy_0: 0.7133
	loss_value_0: 0.15724
	loss_policy_1: 0.03236
	accuracy_policy_1: 0.66561
	loss_value_1: 0.03302
	loss_reward_1: 0.00498
	loss_policy_2: 0.03507
	accuracy_policy_2: 0.6432
	loss_value_2: 0.03412
	loss_reward_2: 0.00476
	loss_policy_3: 0.03729
	accuracy_policy_3: 0.61988
	loss_value_3: 0.03507
	loss_reward_3: 0.00517
	loss_policy_4: 0.03999
	accuracy_policy_4: 0.59951
	loss_value_4: 0.03608
	loss_reward_4: 0.00552
	loss_policy_5: 0.04231
	accuracy_policy_5: 0.58168
	loss_value_5: 0.03719
	loss_reward_5: 0.00657
	loss_policy: 0.32252
	loss_value: 0.33271
	loss_reward: 0.027
[2024-05-08 15:04:22] nn step 46600, lr: 0.1.
	loss_policy_0: 0.11811
	accuracy_policy_0: 0.75189
	loss_value_0: 0.15749
	loss_policy_1: 0.02977
	accuracy_policy_1: 0.69713
	loss_value_1: 0.03294
	loss_reward_1: 0.00512
	loss_policy_2: 0.03248
	accuracy_policy_2: 0.67326
	loss_value_2: 0.03407
	loss_reward_2: 0.00484
	loss_policy_3: 0.03495
	accuracy_policy_3: 0.65562
	loss_value_3: 0.03525
	loss_reward_3: 0.00536
	loss_policy_4: 0.03783
	accuracy_policy_4: 0.63213
	loss_value_4: 0.03634
	loss_reward_4: 0.00578
	loss_policy_5: 0.04065
	accuracy_policy_5: 0.60986
	loss_value_5: 0.03757
	loss_reward_5: 0.00672
	loss_policy: 0.29379
	loss_value: 0.33365
	loss_reward: 0.02782
Optimization_Done 46600
[2024-05-08 15:07:01] [command] train weight_iter_46600.pkl 232 234
[2024-05-08 15:07:43] nn step 46700, lr: 0.1.
	loss_policy_0: 0.15071
	accuracy_policy_0: 0.69064
	loss_value_0: 0.16395
	loss_policy_1: 0.03484
	accuracy_policy_1: 0.64818
	loss_value_1: 0.03421
	loss_reward_1: 0.00428
	loss_policy_2: 0.0373
	accuracy_policy_2: 0.62572
	loss_value_2: 0.03545
	loss_reward_2: 0.00419
	loss_policy_3: 0.03982
	accuracy_policy_3: 0.60387
	loss_value_3: 0.03669
	loss_reward_3: 0.0045
	loss_policy_4: 0.04223
	accuracy_policy_4: 0.58033
	loss_value_4: 0.03789
	loss_reward_4: 0.00488
	loss_policy_5: 0.04433
	accuracy_policy_5: 0.56623
	loss_value_5: 0.03907
	loss_reward_5: 0.00561
	loss_policy: 0.34923
	loss_value: 0.34726
	loss_reward: 0.02346
[2024-05-08 15:08:23] nn step 46800, lr: 0.1.
	loss_policy_0: 0.1341
	accuracy_policy_0: 0.72742
	loss_value_0: 0.16524
	loss_policy_1: 0.03228
	accuracy_policy_1: 0.67707
	loss_value_1: 0.03459
	loss_reward_1: 0.00444
	loss_policy_2: 0.03484
	accuracy_policy_2: 0.6568
	loss_value_2: 0.03583
	loss_reward_2: 0.00429
	loss_policy_3: 0.0374
	accuracy_policy_3: 0.63285
	loss_value_3: 0.03706
	loss_reward_3: 0.00457
	loss_policy_4: 0.03972
	accuracy_policy_4: 0.61539
	loss_value_4: 0.03819
	loss_reward_4: 0.00503
	loss_policy_5: 0.04257
	accuracy_policy_5: 0.5976
	loss_value_5: 0.03937
	loss_reward_5: 0.00591
	loss_policy: 0.32092
	loss_value: 0.35027
	loss_reward: 0.02424
Optimization_Done 46800
[2024-05-08 15:11:01] [command] train weight_iter_46800.pkl 233 235
[2024-05-08 15:11:43] nn step 46900, lr: 0.1.
	loss_policy_0: 0.17157
	accuracy_policy_0: 0.67307
	loss_value_0: 0.17599
	loss_policy_1: 0.03954
	accuracy_policy_1: 0.62879
	loss_value_1: 0.03676
	loss_reward_1: 0.00533
	loss_policy_2: 0.042
	accuracy_policy_2: 0.6157
	loss_value_2: 0.03835
	loss_reward_2: 0.00512
	loss_policy_3: 0.04517
	accuracy_policy_3: 0.5892
	loss_value_3: 0.03963
	loss_reward_3: 0.00561
	loss_policy_4: 0.0479
	accuracy_policy_4: 0.57029
	loss_value_4: 0.04094
	loss_reward_4: 0.00625
	loss_policy_5: 0.05048
	accuracy_policy_5: 0.55139
	loss_value_5: 0.0421
	loss_reward_5: 0.00706
	loss_policy: 0.39664
	loss_value: 0.37378
	loss_reward: 0.02937
[2024-05-08 15:12:23] nn step 47000, lr: 0.1.
	loss_policy_0: 0.14711
	accuracy_policy_0: 0.72199
	loss_value_0: 0.17585
	loss_policy_1: 0.03531
	accuracy_policy_1: 0.67059
	loss_value_1: 0.03673
	loss_reward_1: 0.00524
	loss_policy_2: 0.03808
	accuracy_policy_2: 0.64932
	loss_value_2: 0.03825
	loss_reward_2: 0.00516
	loss_policy_3: 0.0414
	accuracy_policy_3: 0.62451
	loss_value_3: 0.03977
	loss_reward_3: 0.00556
	loss_policy_4: 0.04407
	accuracy_policy_4: 0.60445
	loss_value_4: 0.04094
	loss_reward_4: 0.00597
	loss_policy_5: 0.0473
	accuracy_policy_5: 0.58355
	loss_value_5: 0.042
	loss_reward_5: 0.00684
	loss_policy: 0.35328
	loss_value: 0.37353
	loss_reward: 0.02877
Optimization_Done 47000
[2024-05-08 15:14:59] [command] train weight_iter_47000.pkl 234 236
[2024-05-08 15:15:41] nn step 47100, lr: 0.1.
	loss_policy_0: 0.16339
	accuracy_policy_0: 0.68371
	loss_value_0: 0.177
	loss_policy_1: 0.03786
	accuracy_policy_1: 0.63734
	loss_value_1: 0.03711
	loss_reward_1: 0.0057
	loss_policy_2: 0.04079
	accuracy_policy_2: 0.6152
	loss_value_2: 0.03848
	loss_reward_2: 0.00545
	loss_policy_3: 0.04312
	accuracy_policy_3: 0.596
	loss_value_3: 0.03972
	loss_reward_3: 0.00567
	loss_policy_4: 0.04598
	accuracy_policy_4: 0.57498
	loss_value_4: 0.04098
	loss_reward_4: 0.00622
	loss_policy_5: 0.04892
	accuracy_policy_5: 0.55186
	loss_value_5: 0.04215
	loss_reward_5: 0.00713
	loss_policy: 0.38006
	loss_value: 0.37544
	loss_reward: 0.03017
[2024-05-08 15:16:22] nn step 47200, lr: 0.1.
	loss_policy_0: 0.14356
	accuracy_policy_0: 0.71627
	loss_value_0: 0.17274
	loss_policy_1: 0.03419
	accuracy_policy_1: 0.66711
	loss_value_1: 0.03609
	loss_reward_1: 0.0054
	loss_policy_2: 0.03713
	accuracy_policy_2: 0.64338
	loss_value_2: 0.03735
	loss_reward_2: 0.00516
	loss_policy_3: 0.04009
	accuracy_policy_3: 0.62197
	loss_value_3: 0.03848
	loss_reward_3: 0.00571
	loss_policy_4: 0.04269
	accuracy_policy_4: 0.60289
	loss_value_4: 0.03966
	loss_reward_4: 0.00584
	loss_policy_5: 0.04572
	accuracy_policy_5: 0.57867
	loss_value_5: 0.04093
	loss_reward_5: 0.00702
	loss_policy: 0.34338
	loss_value: 0.36526
	loss_reward: 0.02913
Optimization_Done 47200
[2024-05-08 15:18:32] [command] train weight_iter_47200.pkl 235 237
[2024-05-08 15:19:14] nn step 47300, lr: 0.1.
	loss_policy_0: 0.1579
	accuracy_policy_0: 0.69107
	loss_value_0: 0.18577
	loss_policy_1: 0.03736
	accuracy_policy_1: 0.64307
	loss_value_1: 0.03867
	loss_reward_1: 0.00543
	loss_policy_2: 0.04054
	accuracy_policy_2: 0.61752
	loss_value_2: 0.0401
	loss_reward_2: 0.00533
	loss_policy_3: 0.04288
	accuracy_policy_3: 0.60061
	loss_value_3: 0.04132
	loss_reward_3: 0.00583
	loss_policy_4: 0.04578
	accuracy_policy_4: 0.57768
	loss_value_4: 0.04233
	loss_reward_4: 0.00626
	loss_policy_5: 0.04857
	accuracy_policy_5: 0.55066
	loss_value_5: 0.04357
	loss_reward_5: 0.00717
	loss_policy: 0.37302
	loss_value: 0.39175
	loss_reward: 0.03002
[2024-05-08 15:19:55] nn step 47400, lr: 0.1.
	loss_policy_0: 0.12963
	accuracy_policy_0: 0.73342
	loss_value_0: 0.17426
	loss_policy_1: 0.03259
	accuracy_policy_1: 0.67279
	loss_value_1: 0.0363
	loss_reward_1: 0.00521
	loss_policy_2: 0.03508
	accuracy_policy_2: 0.65176
	loss_value_2: 0.03747
	loss_reward_2: 0.00498
	loss_policy_3: 0.0382
	accuracy_policy_3: 0.62676
	loss_value_3: 0.03872
	loss_reward_3: 0.00541
	loss_policy_4: 0.04078
	accuracy_policy_4: 0.60656
	loss_value_4: 0.03997
	loss_reward_4: 0.00576
	loss_policy_5: 0.04371
	accuracy_policy_5: 0.58107
	loss_value_5: 0.04115
	loss_reward_5: 0.00667
	loss_policy: 0.31998
	loss_value: 0.36788
	loss_reward: 0.02803
Optimization_Done 47400
[2024-05-08 15:22:34] [command] train weight_iter_47400.pkl 236 238
[2024-05-08 15:23:16] nn step 47500, lr: 0.1.
	loss_policy_0: 0.16702
	accuracy_policy_0: 0.6735
	loss_value_0: 0.17355
	loss_policy_1: 0.03815
	accuracy_policy_1: 0.63209
	loss_value_1: 0.03626
	loss_reward_1: 0.00488
	loss_policy_2: 0.04068
	accuracy_policy_2: 0.61312
	loss_value_2: 0.03761
	loss_reward_2: 0.00478
	loss_policy_3: 0.04287
	accuracy_policy_3: 0.59412
	loss_value_3: 0.03883
	loss_reward_3: 0.00508
	loss_policy_4: 0.04533
	accuracy_policy_4: 0.57764
	loss_value_4: 0.0399
	loss_reward_4: 0.00564
	loss_policy_5: 0.04799
	accuracy_policy_5: 0.55799
	loss_value_5: 0.04101
	loss_reward_5: 0.00643
	loss_policy: 0.38203
	loss_value: 0.36717
	loss_reward: 0.02682
[2024-05-08 15:23:56] nn step 47600, lr: 0.1.
	loss_policy_0: 0.15088
	accuracy_policy_0: 0.71926
	loss_value_0: 0.18073
	loss_policy_1: 0.03582
	accuracy_policy_1: 0.66812
	loss_value_1: 0.03799
	loss_reward_1: 0.00523
	loss_policy_2: 0.03863
	accuracy_policy_2: 0.6493
	loss_value_2: 0.03927
	loss_reward_2: 0.00506
	loss_policy_3: 0.04138
	accuracy_policy_3: 0.63133
	loss_value_3: 0.04051
	loss_reward_3: 0.00547
	loss_policy_4: 0.04416
	accuracy_policy_4: 0.61076
	loss_value_4: 0.04174
	loss_reward_4: 0.00598
	loss_policy_5: 0.04741
	accuracy_policy_5: 0.58848
	loss_value_5: 0.04307
	loss_reward_5: 0.00691
	loss_policy: 0.35828
	loss_value: 0.38331
	loss_reward: 0.02866
Optimization_Done 47600
[2024-05-08 15:26:34] [command] train weight_iter_47600.pkl 237 239
[2024-05-08 15:27:16] nn step 47700, lr: 0.1.
	loss_policy_0: 0.15884
	accuracy_policy_0: 0.69871
	loss_value_0: 0.17793
	loss_policy_1: 0.03648
	accuracy_policy_1: 0.66141
	loss_value_1: 0.03709
	loss_reward_1: 0.00503
	loss_policy_2: 0.03941
	accuracy_policy_2: 0.63846
	loss_value_2: 0.03868
	loss_reward_2: 0.00495
	loss_policy_3: 0.04192
	accuracy_policy_3: 0.62271
	loss_value_3: 0.04012
	loss_reward_3: 0.00519
	loss_policy_4: 0.04427
	accuracy_policy_4: 0.60809
	loss_value_4: 0.04133
	loss_reward_4: 0.00594
	loss_policy_5: 0.04691
	accuracy_policy_5: 0.59066
	loss_value_5: 0.0425
	loss_reward_5: 0.00668
	loss_policy: 0.36783
	loss_value: 0.37766
	loss_reward: 0.02779
[2024-05-08 15:27:56] nn step 47800, lr: 0.1.
	loss_policy_0: 0.13493
	accuracy_policy_0: 0.73254
	loss_value_0: 0.16934
	loss_policy_1: 0.03132
	accuracy_policy_1: 0.69121
	loss_value_1: 0.03527
	loss_reward_1: 0.00477
	loss_policy_2: 0.03406
	accuracy_policy_2: 0.67203
	loss_value_2: 0.03671
	loss_reward_2: 0.00453
	loss_policy_3: 0.03675
	accuracy_policy_3: 0.64918
	loss_value_3: 0.03787
	loss_reward_3: 0.00504
	loss_policy_4: 0.03946
	accuracy_policy_4: 0.63275
	loss_value_4: 0.03905
	loss_reward_4: 0.00536
	loss_policy_5: 0.04227
	accuracy_policy_5: 0.61447
	loss_value_5: 0.04024
	loss_reward_5: 0.00643
	loss_policy: 0.31879
	loss_value: 0.35849
	loss_reward: 0.02613
Optimization_Done 47800
[2024-05-08 15:30:43] [command] train weight_iter_47800.pkl 238 240
[2024-05-08 15:31:25] nn step 47900, lr: 0.1.
	loss_policy_0: 0.15211
	accuracy_policy_0: 0.69158
	loss_value_0: 0.16721
	loss_policy_1: 0.0346
	accuracy_policy_1: 0.65619
	loss_value_1: 0.03497
	loss_reward_1: 0.00494
	loss_policy_2: 0.03691
	accuracy_policy_2: 0.63951
	loss_value_2: 0.03645
	loss_reward_2: 0.00498
	loss_policy_3: 0.03936
	accuracy_policy_3: 0.62078
	loss_value_3: 0.03767
	loss_reward_3: 0.00542
	loss_policy_4: 0.04186
	accuracy_policy_4: 0.60354
	loss_value_4: 0.03894
	loss_reward_4: 0.00571
	loss_policy_5: 0.04422
	accuracy_policy_5: 0.58377
	loss_value_5: 0.04017
	loss_reward_5: 0.00663
	loss_policy: 0.34906
	loss_value: 0.35541
	loss_reward: 0.02769
[2024-05-08 15:32:05] nn step 48000, lr: 0.1.
	loss_policy_0: 0.13475
	accuracy_policy_0: 0.73004
	loss_value_0: 0.1682
	loss_policy_1: 0.03165
	accuracy_policy_1: 0.68977
	loss_value_1: 0.03528
	loss_reward_1: 0.00495
	loss_policy_2: 0.03422
	accuracy_policy_2: 0.67084
	loss_value_2: 0.0367
	loss_reward_2: 0.00473
	loss_policy_3: 0.0366
	accuracy_policy_3: 0.65346
	loss_value_3: 0.03809
	loss_reward_3: 0.00529
	loss_policy_4: 0.03924
	accuracy_policy_4: 0.6358
	loss_value_4: 0.0392
	loss_reward_4: 0.0058
	loss_policy_5: 0.04236
	accuracy_policy_5: 0.61053
	loss_value_5: 0.04045
	loss_reward_5: 0.00678
	loss_policy: 0.31881
	loss_value: 0.35791
	loss_reward: 0.02755
Optimization_Done 48000
[2024-05-08 15:34:28] [command] train weight_iter_48000.pkl 239 241
[2024-05-08 15:35:09] nn step 48100, lr: 0.1.
	loss_policy_0: 0.13961
	accuracy_policy_0: 0.71158
	loss_value_0: 0.16596
	loss_policy_1: 0.03247
	accuracy_policy_1: 0.67432
	loss_value_1: 0.03472
	loss_reward_1: 0.00472
	loss_policy_2: 0.03506
	accuracy_policy_2: 0.65027
	loss_value_2: 0.03606
	loss_reward_2: 0.0046
	loss_policy_3: 0.03785
	accuracy_policy_3: 0.62676
	loss_value_3: 0.03732
	loss_reward_3: 0.00507
	loss_policy_4: 0.03985
	accuracy_policy_4: 0.61074
	loss_value_4: 0.03848
	loss_reward_4: 0.0054
	loss_policy_5: 0.04255
	accuracy_policy_5: 0.58973
	loss_value_5: 0.03959
	loss_reward_5: 0.00617
	loss_policy: 0.3274
	loss_value: 0.35213
	loss_reward: 0.02596
[2024-05-08 15:35:50] nn step 48200, lr: 0.1.
	loss_policy_0: 0.11755
	accuracy_policy_0: 0.75023
	loss_value_0: 0.16108
	loss_policy_1: 0.02839
	accuracy_policy_1: 0.70158
	loss_value_1: 0.03378
	loss_reward_1: 0.00475
	loss_policy_2: 0.0306
	accuracy_policy_2: 0.6876
	loss_value_2: 0.03503
	loss_reward_2: 0.00447
	loss_policy_3: 0.03336
	accuracy_policy_3: 0.66201
	loss_value_3: 0.03621
	loss_reward_3: 0.0048
	loss_policy_4: 0.0359
	accuracy_policy_4: 0.64041
	loss_value_4: 0.03724
	loss_reward_4: 0.00534
	loss_policy_5: 0.03887
	accuracy_policy_5: 0.6193
	loss_value_5: 0.03846
	loss_reward_5: 0.00605
	loss_policy: 0.28466
	loss_value: 0.34181
	loss_reward: 0.0254
Optimization_Done 48200
[2024-05-08 15:38:27] [command] train weight_iter_48200.pkl 240 242
[2024-05-08 15:39:09] nn step 48300, lr: 0.1.
	loss_policy_0: 0.14435
	accuracy_policy_0: 0.69188
	loss_value_0: 0.16346
	loss_policy_1: 0.03365
	accuracy_policy_1: 0.64355
	loss_value_1: 0.03429
	loss_reward_1: 0.00462
	loss_policy_2: 0.03668
	accuracy_policy_2: 0.62074
	loss_value_2: 0.03559
	loss_reward_2: 0.00449
	loss_policy_3: 0.03904
	accuracy_policy_3: 0.60277
	loss_value_3: 0.03676
	loss_reward_3: 0.00492
	loss_policy_4: 0.04136
	accuracy_policy_4: 0.58346
	loss_value_4: 0.03778
	loss_reward_4: 0.00534
	loss_policy_5: 0.04404
	accuracy_policy_5: 0.55699
	loss_value_5: 0.03886
	loss_reward_5: 0.00607
	loss_policy: 0.33912
	loss_value: 0.34674
	loss_reward: 0.02545
[2024-05-08 15:39:50] nn step 48400, lr: 0.1.
	loss_policy_0: 0.12191
	accuracy_policy_0: 0.73438
	loss_value_0: 0.16103
	loss_policy_1: 0.02962
	accuracy_policy_1: 0.68281
	loss_value_1: 0.03382
	loss_reward_1: 0.00457
	loss_policy_2: 0.0323
	accuracy_policy_2: 0.66047
	loss_value_2: 0.03515
	loss_reward_2: 0.00443
	loss_policy_3: 0.03491
	accuracy_policy_3: 0.64301
	loss_value_3: 0.03631
	loss_reward_3: 0.00472
	loss_policy_4: 0.03767
	accuracy_policy_4: 0.62111
	loss_value_4: 0.03745
	loss_reward_4: 0.00523
	loss_policy_5: 0.04028
	accuracy_policy_5: 0.59617
	loss_value_5: 0.0385
	loss_reward_5: 0.00614
	loss_policy: 0.29669
	loss_value: 0.34226
	loss_reward: 0.02509
Optimization_Done 48400
[2024-05-08 15:42:01] [command] train weight_iter_48400.pkl 241 243
[2024-05-08 15:42:43] nn step 48500, lr: 0.1.
	loss_policy_0: 0.15442
	accuracy_policy_0: 0.6917
	loss_value_0: 0.17568
	loss_policy_1: 0.03597
	accuracy_policy_1: 0.6498
	loss_value_1: 0.03675
	loss_reward_1: 0.00475
	loss_policy_2: 0.03849
	accuracy_policy_2: 0.6302
	loss_value_2: 0.03822
	loss_reward_2: 0.0046
	loss_policy_3: 0.0415
	accuracy_policy_3: 0.60768
	loss_value_3: 0.0395
	loss_reward_3: 0.00518
	loss_policy_4: 0.04432
	accuracy_policy_4: 0.58994
	loss_value_4: 0.04072
	loss_reward_4: 0.00538
	loss_policy_5: 0.04682
	accuracy_policy_5: 0.57027
	loss_value_5: 0.04195
	loss_reward_5: 0.00621
	loss_policy: 0.36153
	loss_value: 0.37282
	loss_reward: 0.02613
[2024-05-08 15:43:24] nn step 48600, lr: 0.1.
	loss_policy_0: 0.12616
	accuracy_policy_0: 0.73375
	loss_value_0: 0.16525
	loss_policy_1: 0.03059
	accuracy_policy_1: 0.68672
	loss_value_1: 0.03463
	loss_reward_1: 0.00436
	loss_policy_2: 0.03326
	accuracy_policy_2: 0.66367
	loss_value_2: 0.0359
	loss_reward_2: 0.00429
	loss_policy_3: 0.03598
	accuracy_policy_3: 0.64486
	loss_value_3: 0.03711
	loss_reward_3: 0.00473
	loss_policy_4: 0.0386
	accuracy_policy_4: 0.62328
	loss_value_4: 0.03824
	loss_reward_4: 0.00522
	loss_policy_5: 0.04167
	accuracy_policy_5: 0.5992
	loss_value_5: 0.03942
	loss_reward_5: 0.00599
	loss_policy: 0.30626
	loss_value: 0.35055
	loss_reward: 0.0246
Optimization_Done 48600
[2024-05-08 15:46:00] [command] train weight_iter_48600.pkl 242 244
[2024-05-08 15:46:42] nn step 48700, lr: 0.1.
	loss_policy_0: 0.15017
	accuracy_policy_0: 0.69951
	loss_value_0: 0.17421
	loss_policy_1: 0.03455
	accuracy_policy_1: 0.65869
	loss_value_1: 0.03663
	loss_reward_1: 0.00492
	loss_policy_2: 0.03725
	accuracy_policy_2: 0.63672
	loss_value_2: 0.038
	loss_reward_2: 0.0047
	loss_policy_3: 0.03986
	accuracy_policy_3: 0.62275
	loss_value_3: 0.03924
	loss_reward_3: 0.00508
	loss_policy_4: 0.04265
	accuracy_policy_4: 0.59791
	loss_value_4: 0.04045
	loss_reward_4: 0.00569
	loss_policy_5: 0.04549
	accuracy_policy_5: 0.57568
	loss_value_5: 0.04164
	loss_reward_5: 0.00661
	loss_policy: 0.34997
	loss_value: 0.37017
	loss_reward: 0.02699
[2024-05-08 15:47:23] nn step 48800, lr: 0.1.
	loss_policy_0: 0.1412
	accuracy_policy_0: 0.73188
	loss_value_0: 0.1818
	loss_policy_1: 0.03383
	accuracy_policy_1: 0.67752
	loss_value_1: 0.03813
	loss_reward_1: 0.00508
	loss_policy_2: 0.03626
	accuracy_policy_2: 0.66279
	loss_value_2: 0.0397
	loss_reward_2: 0.005
	loss_policy_3: 0.03928
	accuracy_policy_3: 0.64268
	loss_value_3: 0.04093
	loss_reward_3: 0.00548
	loss_policy_4: 0.04227
	accuracy_policy_4: 0.61834
	loss_value_4: 0.04226
	loss_reward_4: 0.00584
	loss_policy_5: 0.04529
	accuracy_policy_5: 0.59943
	loss_value_5: 0.04342
	loss_reward_5: 0.00681
	loss_policy: 0.33813
	loss_value: 0.38624
	loss_reward: 0.02821
Optimization_Done 48800
[2024-05-08 15:50:00] [command] train weight_iter_48800.pkl 243 245
[2024-05-08 15:50:41] nn step 48900, lr: 0.1.
	loss_policy_0: 0.1539
	accuracy_policy_0: 0.69826
	loss_value_0: 0.17125
	loss_policy_1: 0.03539
	accuracy_policy_1: 0.65521
	loss_value_1: 0.03576
	loss_reward_1: 0.00487
	loss_policy_2: 0.03797
	accuracy_policy_2: 0.63693
	loss_value_2: 0.03715
	loss_reward_2: 0.00486
	loss_policy_3: 0.0407
	accuracy_policy_3: 0.61941
	loss_value_3: 0.03832
	loss_reward_3: 0.0051
	loss_policy_4: 0.04315
	accuracy_policy_4: 0.59961
	loss_value_4: 0.03954
	loss_reward_4: 0.00559
	loss_policy_5: 0.04576
	accuracy_policy_5: 0.58076
	loss_value_5: 0.04063
	loss_reward_5: 0.00655
	loss_policy: 0.35687
	loss_value: 0.36266
	loss_reward: 0.02697
[2024-05-08 15:51:22] nn step 49000, lr: 0.1.
	loss_policy_0: 0.14419
	accuracy_policy_0: 0.72553
	loss_value_0: 0.17832
	loss_policy_1: 0.03431
	accuracy_policy_1: 0.68414
	loss_value_1: 0.03717
	loss_reward_1: 0.00526
	loss_policy_2: 0.0366
	accuracy_policy_2: 0.66316
	loss_value_2: 0.03862
	loss_reward_2: 0.0051
	loss_policy_3: 0.0396
	accuracy_policy_3: 0.64297
	loss_value_3: 0.03999
	loss_reward_3: 0.00533
	loss_policy_4: 0.04227
	accuracy_policy_4: 0.62773
	loss_value_4: 0.04115
	loss_reward_4: 0.00595
	loss_policy_5: 0.04527
	accuracy_policy_5: 0.60381
	loss_value_5: 0.04236
	loss_reward_5: 0.00687
	loss_policy: 0.34223
	loss_value: 0.37762
	loss_reward: 0.0285
Optimization_Done 49000
[2024-05-08 15:53:33] [command] train weight_iter_49000.pkl 244 246
[2024-05-08 15:54:14] nn step 49100, lr: 0.1.
	loss_policy_0: 0.15146
	accuracy_policy_0: 0.70635
	loss_value_0: 0.17875
	loss_policy_1: 0.03535
	accuracy_policy_1: 0.66402
	loss_value_1: 0.03724
	loss_reward_1: 0.00477
	loss_policy_2: 0.03771
	accuracy_policy_2: 0.64635
	loss_value_2: 0.03873
	loss_reward_2: 0.00464
	loss_policy_3: 0.04019
	accuracy_policy_3: 0.62676
	loss_value_3: 0.03994
	loss_reward_3: 0.00484
	loss_policy_4: 0.04278
	accuracy_policy_4: 0.60746
	loss_value_4: 0.041
	loss_reward_4: 0.00527
	loss_policy_5: 0.04535
	accuracy_policy_5: 0.59021
	loss_value_5: 0.04208
	loss_reward_5: 0.00595
	loss_policy: 0.35283
	loss_value: 0.37774
	loss_reward: 0.02546
[2024-05-08 15:54:55] nn step 49200, lr: 0.1.
	loss_policy_0: 0.137
	accuracy_policy_0: 0.74375
	loss_value_0: 0.18272
	loss_policy_1: 0.0333
	accuracy_policy_1: 0.69283
	loss_value_1: 0.03804
	loss_reward_1: 0.00474
	loss_policy_2: 0.03636
	accuracy_policy_2: 0.66736
	loss_value_2: 0.03936
	loss_reward_2: 0.00488
	loss_policy_3: 0.03872
	accuracy_policy_3: 0.65205
	loss_value_3: 0.04055
	loss_reward_3: 0.00513
	loss_policy_4: 0.04137
	accuracy_policy_4: 0.63088
	loss_value_4: 0.04177
	loss_reward_4: 0.00545
	loss_policy_5: 0.04441
	accuracy_policy_5: 0.60654
	loss_value_5: 0.04279
	loss_reward_5: 0.0067
	loss_policy: 0.33115
	loss_value: 0.38523
	loss_reward: 0.0269
Optimization_Done 49200
[2024-05-08 15:57:32] [command] train weight_iter_49200.pkl 245 247
[2024-05-08 15:58:13] nn step 49300, lr: 0.1.
	loss_policy_0: 0.15968
	accuracy_policy_0: 0.68605
	loss_value_0: 0.17298
	loss_policy_1: 0.03673
	accuracy_policy_1: 0.6433
	loss_value_1: 0.03595
	loss_reward_1: 0.00466
	loss_policy_2: 0.03921
	accuracy_policy_2: 0.62793
	loss_value_2: 0.03729
	loss_reward_2: 0.0046
	loss_policy_3: 0.04203
	accuracy_policy_3: 0.60117
	loss_value_3: 0.03858
	loss_reward_3: 0.0049
	loss_policy_4: 0.04434
	accuracy_policy_4: 0.58686
	loss_value_4: 0.03972
	loss_reward_4: 0.00524
	loss_policy_5: 0.04735
	accuracy_policy_5: 0.56232
	loss_value_5: 0.04088
	loss_reward_5: 0.00596
	loss_policy: 0.36934
	loss_value: 0.36539
	loss_reward: 0.02537
[2024-05-08 15:58:54] nn step 49400, lr: 0.1.
	loss_policy_0: 0.14125
	accuracy_policy_0: 0.72523
	loss_value_0: 0.1759
	loss_policy_1: 0.0335
	accuracy_policy_1: 0.68715
	loss_value_1: 0.03683
	loss_reward_1: 0.00482
	loss_policy_2: 0.03661
	accuracy_policy_2: 0.66172
	loss_value_2: 0.03809
	loss_reward_2: 0.00459
	loss_policy_3: 0.03956
	accuracy_policy_3: 0.63732
	loss_value_3: 0.03937
	loss_reward_3: 0.00489
	loss_policy_4: 0.04243
	accuracy_policy_4: 0.61869
	loss_value_4: 0.04054
	loss_reward_4: 0.00528
	loss_policy_5: 0.04544
	accuracy_policy_5: 0.5965
	loss_value_5: 0.04185
	loss_reward_5: 0.00623
	loss_policy: 0.33879
	loss_value: 0.37258
	loss_reward: 0.02581
Optimization_Done 49400
[2024-05-08 16:01:30] [command] train weight_iter_49400.pkl 246 248
[2024-05-08 16:02:12] nn step 49500, lr: 0.1.
	loss_policy_0: 0.15188
	accuracy_policy_0: 0.69939
	loss_value_0: 0.18264
	loss_policy_1: 0.03501
	accuracy_policy_1: 0.65932
	loss_value_1: 0.03811
	loss_reward_1: 0.005
	loss_policy_2: 0.03796
	accuracy_policy_2: 0.64133
	loss_value_2: 0.03945
	loss_reward_2: 0.00501
	loss_policy_3: 0.04081
	accuracy_policy_3: 0.6165
	loss_value_3: 0.04073
	loss_reward_3: 0.00539
	loss_policy_4: 0.04324
	accuracy_policy_4: 0.60369
	loss_value_4: 0.04205
	loss_reward_4: 0.00557
	loss_policy_5: 0.04609
	accuracy_policy_5: 0.58205
	loss_value_5: 0.04322
	loss_reward_5: 0.00669
	loss_policy: 0.35499
	loss_value: 0.38621
	loss_reward: 0.02766
[2024-05-08 16:02:52] nn step 49600, lr: 0.1.
	loss_policy_0: 0.12904
	accuracy_policy_0: 0.74072
	loss_value_0: 0.1766
	loss_policy_1: 0.03095
	accuracy_policy_1: 0.69688
	loss_value_1: 0.03708
	loss_reward_1: 0.00486
	loss_policy_2: 0.03379
	accuracy_policy_2: 0.67709
	loss_value_2: 0.03853
	loss_reward_2: 0.00476
	loss_policy_3: 0.0368
	accuracy_policy_3: 0.65129
	loss_value_3: 0.03975
	loss_reward_3: 0.00514
	loss_policy_4: 0.03901
	accuracy_policy_4: 0.63514
	loss_value_4: 0.0411
	loss_reward_4: 0.00555
	loss_policy_5: 0.04233
	accuracy_policy_5: 0.60908
	loss_value_5: 0.04227
	loss_reward_5: 0.00653
	loss_policy: 0.31193
	loss_value: 0.37533
	loss_reward: 0.02684
Optimization_Done 49600
[2024-05-08 16:05:28] [command] train weight_iter_49600.pkl 247 249
[2024-05-08 16:06:10] nn step 49700, lr: 0.1.
	loss_policy_0: 0.14529
	accuracy_policy_0: 0.7126
	loss_value_0: 0.18068
	loss_policy_1: 0.0351
	accuracy_policy_1: 0.66607
	loss_value_1: 0.03787
	loss_reward_1: 0.00544
	loss_policy_2: 0.03805
	accuracy_policy_2: 0.64148
	loss_value_2: 0.03932
	loss_reward_2: 0.00536
	loss_policy_3: 0.04086
	accuracy_policy_3: 0.62389
	loss_value_3: 0.04056
	loss_reward_3: 0.00581
	loss_policy_4: 0.04381
	accuracy_policy_4: 0.60553
	loss_value_4: 0.0418
	loss_reward_4: 0.00624
	loss_policy_5: 0.04644
	accuracy_policy_5: 0.58518
	loss_value_5: 0.04309
	loss_reward_5: 0.00719
	loss_policy: 0.34955
	loss_value: 0.38332
	loss_reward: 0.03004
[2024-05-08 16:06:51] nn step 49800, lr: 0.1.
	loss_policy_0: 0.12197
	accuracy_policy_0: 0.74809
	loss_value_0: 0.17572
	loss_policy_1: 0.0306
	accuracy_policy_1: 0.69203
	loss_value_1: 0.0368
	loss_reward_1: 0.00508
	loss_policy_2: 0.03335
	accuracy_policy_2: 0.6701
	loss_value_2: 0.03806
	loss_reward_2: 0.00511
	loss_policy_3: 0.03635
	accuracy_policy_3: 0.65209
	loss_value_3: 0.03935
	loss_reward_3: 0.00531
	loss_policy_4: 0.03932
	accuracy_policy_4: 0.62814
	loss_value_4: 0.0405
	loss_reward_4: 0.00589
	loss_policy_5: 0.04261
	accuracy_policy_5: 0.60686
	loss_value_5: 0.04178
	loss_reward_5: 0.00688
	loss_policy: 0.3042
	loss_value: 0.37221
	loss_reward: 0.02826
Optimization_Done 49800
[2024-05-08 16:09:29] [command] train weight_iter_49800.pkl 248 250
[2024-05-08 16:10:11] nn step 49900, lr: 0.1.
	loss_policy_0: 0.13185
	accuracy_policy_0: 0.7191
	loss_value_0: 0.17416
	loss_policy_1: 0.03256
	accuracy_policy_1: 0.66875
	loss_value_1: 0.0365
	loss_reward_1: 0.00463
	loss_policy_2: 0.03555
	accuracy_policy_2: 0.64584
	loss_value_2: 0.03793
	loss_reward_2: 0.00452
	loss_policy_3: 0.03837
	accuracy_policy_3: 0.62262
	loss_value_3: 0.03913
	loss_reward_3: 0.00498
	loss_policy_4: 0.04093
	accuracy_policy_4: 0.60611
	loss_value_4: 0.04019
	loss_reward_4: 0.00526
	loss_policy_5: 0.04328
	accuracy_policy_5: 0.58906
	loss_value_5: 0.04126
	loss_reward_5: 0.00621
	loss_policy: 0.32254
	loss_value: 0.36916
	loss_reward: 0.0256
[2024-05-08 16:10:51] nn step 50000, lr: 0.1.
	loss_policy_0: 0.10511
	accuracy_policy_0: 0.76424
	loss_value_0: 0.16487
	loss_policy_1: 0.02746
	accuracy_policy_1: 0.70439
	loss_value_1: 0.03437
	loss_reward_1: 0.00458
	loss_policy_2: 0.03009
	accuracy_policy_2: 0.68115
	loss_value_2: 0.03569
	loss_reward_2: 0.00438
	loss_policy_3: 0.03253
	accuracy_policy_3: 0.66359
	loss_value_3: 0.03673
	loss_reward_3: 0.00464
	loss_policy_4: 0.03537
	accuracy_policy_4: 0.63865
	loss_value_4: 0.03767
	loss_reward_4: 0.00496
	loss_policy_5: 0.03824
	accuracy_policy_5: 0.61867
	loss_value_5: 0.03863
	loss_reward_5: 0.00607
	loss_policy: 0.26881
	loss_value: 0.34795
	loss_reward: 0.02463
Optimization_Done 50000
[2024-05-08 16:13:18] [command] train weight_iter_50000.pkl 249 251
[2024-05-08 16:14:00] nn step 50100, lr: 0.1.
	loss_policy_0: 0.16818
	accuracy_policy_0: 0.64924
	loss_value_0: 0.17127
	loss_policy_1: 0.03934
	accuracy_policy_1: 0.6067
	loss_value_1: 0.03563
	loss_reward_1: 0.00408
	loss_policy_2: 0.04178
	accuracy_policy_2: 0.58406
	loss_value_2: 0.03684
	loss_reward_2: 0.00412
	loss_policy_3: 0.04423
	accuracy_policy_3: 0.56568
	loss_value_3: 0.03804
	loss_reward_3: 0.00436
	loss_policy_4: 0.04641
	accuracy_policy_4: 0.55201
	loss_value_4: 0.03911
	loss_reward_4: 0.00477
	loss_policy_5: 0.04864
	accuracy_policy_5: 0.53594
	loss_value_5: 0.04004
	loss_reward_5: 0.00562
	loss_policy: 0.3886
	loss_value: 0.36091
	loss_reward: 0.02294
[2024-05-08 16:14:40] nn step 50200, lr: 0.1.
	loss_policy_0: 0.13982
	accuracy_policy_0: 0.70791
	loss_value_0: 0.16518
	loss_policy_1: 0.03375
	accuracy_policy_1: 0.65318
	loss_value_1: 0.03458
	loss_reward_1: 0.00404
	loss_policy_2: 0.03624
	accuracy_policy_2: 0.63813
	loss_value_2: 0.03573
	loss_reward_2: 0.00418
	loss_policy_3: 0.03893
	accuracy_policy_3: 0.61893
	loss_value_3: 0.03677
	loss_reward_3: 0.0044
	loss_policy_4: 0.04149
	accuracy_policy_4: 0.60176
	loss_value_4: 0.03781
	loss_reward_4: 0.0048
	loss_policy_5: 0.04401
	accuracy_policy_5: 0.58266
	loss_value_5: 0.03878
	loss_reward_5: 0.0055
	loss_policy: 0.33423
	loss_value: 0.34884
	loss_reward: 0.02293
Optimization_Done 50200
[2024-05-08 16:17:18] [command] train weight_iter_50200.pkl 250 252
[2024-05-08 16:18:00] nn step 50300, lr: 0.1.
	loss_policy_0: 0.17836
	accuracy_policy_0: 0.64697
	loss_value_0: 0.17638
	loss_policy_1: 0.04053
	accuracy_policy_1: 0.61039
	loss_value_1: 0.03673
	loss_reward_1: 0.00435
	loss_policy_2: 0.04282
	accuracy_policy_2: 0.59166
	loss_value_2: 0.03798
	loss_reward_2: 0.00433
	loss_policy_3: 0.04532
	accuracy_policy_3: 0.5743
	loss_value_3: 0.03915
	loss_reward_3: 0.00466
	loss_policy_4: 0.04781
	accuracy_policy_4: 0.5574
	loss_value_4: 0.04035
	loss_reward_4: 0.0051
	loss_policy_5: 0.05017
	accuracy_policy_5: 0.54072
	loss_value_5: 0.04154
	loss_reward_5: 0.0059
	loss_policy: 0.405
	loss_value: 0.37213
	loss_reward: 0.02434
[2024-05-08 16:18:40] nn step 50400, lr: 0.1.
	loss_policy_0: 0.1481
	accuracy_policy_0: 0.69701
	loss_value_0: 0.16416
	loss_policy_1: 0.03531
	accuracy_policy_1: 0.64648
	loss_value_1: 0.03425
	loss_reward_1: 0.00411
	loss_policy_2: 0.03788
	accuracy_policy_2: 0.62854
	loss_value_2: 0.03551
	loss_reward_2: 0.00421
	loss_policy_3: 0.04032
	accuracy_policy_3: 0.6099
	loss_value_3: 0.03656
	loss_reward_3: 0.00447
	loss_policy_4: 0.04322
	accuracy_policy_4: 0.58879
	loss_value_4: 0.03752
	loss_reward_4: 0.00487
	loss_policy_5: 0.04574
	accuracy_policy_5: 0.56938
	loss_value_5: 0.03858
	loss_reward_5: 0.0055
	loss_policy: 0.35057
	loss_value: 0.34659
	loss_reward: 0.02317
Optimization_Done 50400
[2024-05-08 16:21:18] [command] train weight_iter_50400.pkl 251 253
[2024-05-08 16:22:00] nn step 50500, lr: 0.1.
	loss_policy_0: 0.17292
	accuracy_policy_0: 0.67266
	loss_value_0: 0.17455
	loss_policy_1: 0.03851
	accuracy_policy_1: 0.63588
	loss_value_1: 0.03622
	loss_reward_1: 0.00506
	loss_policy_2: 0.04105
	accuracy_policy_2: 0.61877
	loss_value_2: 0.0376
	loss_reward_2: 0.00484
	loss_policy_3: 0.04342
	accuracy_policy_3: 0.60922
	loss_value_3: 0.03892
	loss_reward_3: 0.00544
	loss_policy_4: 0.04593
	accuracy_policy_4: 0.5917
	loss_value_4: 0.04008
	loss_reward_4: 0.00576
	loss_policy_5: 0.0481
	accuracy_policy_5: 0.58047
	loss_value_5: 0.04119
	loss_reward_5: 0.00668
	loss_policy: 0.38994
	loss_value: 0.36857
	loss_reward: 0.02778
[2024-05-08 16:22:41] nn step 50600, lr: 0.1.
	loss_policy_0: 0.15033
	accuracy_policy_0: 0.70449
	loss_value_0: 0.16786
	loss_policy_1: 0.0348
	accuracy_policy_1: 0.66582
	loss_value_1: 0.03479
	loss_reward_1: 0.00488
	loss_policy_2: 0.03677
	accuracy_policy_2: 0.6458
	loss_value_2: 0.03608
	loss_reward_2: 0.00476
	loss_policy_3: 0.03968
	accuracy_policy_3: 0.6275
	loss_value_3: 0.0373
	loss_reward_3: 0.00501
	loss_policy_4: 0.04209
	accuracy_policy_4: 0.61268
	loss_value_4: 0.03856
	loss_reward_4: 0.00571
	loss_policy_5: 0.0446
	accuracy_policy_5: 0.5966
	loss_value_5: 0.03985
	loss_reward_5: 0.00651
	loss_policy: 0.34827
	loss_value: 0.35445
	loss_reward: 0.02687
Optimization_Done 50600
[2024-05-08 16:24:41] [command] train weight_iter_50600.pkl 252 254
[2024-05-08 16:25:23] nn step 50700, lr: 0.1.
	loss_policy_0: 0.15773
	accuracy_policy_0: 0.67635
	loss_value_0: 0.17327
	loss_policy_1: 0.03582
	accuracy_policy_1: 0.63555
	loss_value_1: 0.03606
	loss_reward_1: 0.00502
	loss_policy_2: 0.03844
	accuracy_policy_2: 0.61564
	loss_value_2: 0.03736
	loss_reward_2: 0.00488
	loss_policy_3: 0.041
	accuracy_policy_3: 0.59373
	loss_value_3: 0.03849
	loss_reward_3: 0.00542
	loss_policy_4: 0.04308
	accuracy_policy_4: 0.57852
	loss_value_4: 0.03959
	loss_reward_4: 0.00576
	loss_policy_5: 0.04516
	accuracy_policy_5: 0.56146
	loss_value_5: 0.04065
	loss_reward_5: 0.00665
	loss_policy: 0.36123
	loss_value: 0.36542
	loss_reward: 0.02773
[2024-05-08 16:26:04] nn step 50800, lr: 0.1.
	loss_policy_0: 0.14571
	accuracy_policy_0: 0.71357
	loss_value_0: 0.17913
	loss_policy_1: 0.03424
	accuracy_policy_1: 0.67178
	loss_value_1: 0.03715
	loss_reward_1: 0.00537
	loss_policy_2: 0.037
	accuracy_policy_2: 0.64963
	loss_value_2: 0.03849
	loss_reward_2: 0.00522
	loss_policy_3: 0.03944
	accuracy_policy_3: 0.62984
	loss_value_3: 0.03963
	loss_reward_3: 0.00564
	loss_policy_4: 0.04175
	accuracy_policy_4: 0.61109
	loss_value_4: 0.04077
	loss_reward_4: 0.00614
	loss_policy_5: 0.04476
	accuracy_policy_5: 0.58961
	loss_value_5: 0.04194
	loss_reward_5: 0.00717
	loss_policy: 0.34291
	loss_value: 0.37711
	loss_reward: 0.02954
Optimization_Done 50800
[2024-05-08 16:28:28] [command] train weight_iter_50800.pkl 253 255
[2024-05-08 16:29:10] nn step 50900, lr: 0.1.
	loss_policy_0: 0.14042
	accuracy_policy_0: 0.70879
	loss_value_0: 0.16889
	loss_policy_1: 0.03251
	accuracy_policy_1: 0.6723
	loss_value_1: 0.03515
	loss_reward_1: 0.00511
	loss_policy_2: 0.03503
	accuracy_policy_2: 0.64959
	loss_value_2: 0.03638
	loss_reward_2: 0.0048
	loss_policy_3: 0.0373
	accuracy_policy_3: 0.63445
	loss_value_3: 0.03763
	loss_reward_3: 0.00539
	loss_policy_4: 0.03975
	accuracy_policy_4: 0.61662
	loss_value_4: 0.03882
	loss_reward_4: 0.00579
	loss_policy_5: 0.04233
	accuracy_policy_5: 0.59697
	loss_value_5: 0.03977
	loss_reward_5: 0.00689
	loss_policy: 0.32734
	loss_value: 0.35664
	loss_reward: 0.02797
[2024-05-08 16:29:51] nn step 51000, lr: 0.1.
	loss_policy_0: 0.1258
	accuracy_policy_0: 0.74885
	loss_value_0: 0.17623
	loss_policy_1: 0.0304
	accuracy_policy_1: 0.70076
	loss_value_1: 0.03683
	loss_reward_1: 0.00523
	loss_policy_2: 0.03333
	accuracy_policy_2: 0.67805
	loss_value_2: 0.03818
	loss_reward_2: 0.005
	loss_policy_3: 0.03545
	accuracy_policy_3: 0.66375
	loss_value_3: 0.03929
	loss_reward_3: 0.00527
	loss_policy_4: 0.03812
	accuracy_policy_4: 0.64354
	loss_value_4: 0.04037
	loss_reward_4: 0.0058
	loss_policy_5: 0.04117
	accuracy_policy_5: 0.62107
	loss_value_5: 0.04141
	loss_reward_5: 0.00704
	loss_policy: 0.30428
	loss_value: 0.37232
	loss_reward: 0.02834
Optimization_Done 51000
[2024-05-08 16:32:42] [command] train weight_iter_51000.pkl 254 256
[2024-05-08 16:33:24] nn step 51100, lr: 0.1.
	loss_policy_0: 0.14989
	accuracy_policy_0: 0.6998
	loss_value_0: 0.16642
	loss_policy_1: 0.03479
	accuracy_policy_1: 0.65879
	loss_value_1: 0.03473
	loss_reward_1: 0.00491
	loss_policy_2: 0.03731
	accuracy_policy_2: 0.63973
	loss_value_2: 0.03607
	loss_reward_2: 0.00466
	loss_policy_3: 0.04008
	accuracy_policy_3: 0.61725
	loss_value_3: 0.03754
	loss_reward_3: 0.00512
	loss_policy_4: 0.04288
	accuracy_policy_4: 0.59676
	loss_value_4: 0.03871
	loss_reward_4: 0.00548
	loss_policy_5: 0.04539
	accuracy_policy_5: 0.57893
	loss_value_5: 0.04007
	loss_reward_5: 0.00629
	loss_policy: 0.35034
	loss_value: 0.35353
	loss_reward: 0.02646
[2024-05-08 16:34:05] nn step 51200, lr: 0.1.
	loss_policy_0: 0.1347
	accuracy_policy_0: 0.73527
	loss_value_0: 0.16826
	loss_policy_1: 0.03223
	accuracy_policy_1: 0.68775
	loss_value_1: 0.03513
	loss_reward_1: 0.00487
	loss_policy_2: 0.03507
	accuracy_policy_2: 0.6677
	loss_value_2: 0.03663
	loss_reward_2: 0.00467
	loss_policy_3: 0.03785
	accuracy_policy_3: 0.6435
	loss_value_3: 0.03802
	loss_reward_3: 0.0051
	loss_policy_4: 0.04034
	accuracy_policy_4: 0.62828
	loss_value_4: 0.03925
	loss_reward_4: 0.00555
	loss_policy_5: 0.04325
	accuracy_policy_5: 0.60424
	loss_value_5: 0.04055
	loss_reward_5: 0.00663
	loss_policy: 0.32343
	loss_value: 0.35785
	loss_reward: 0.02682
Optimization_Done 51200
[2024-05-08 16:36:40] [command] train weight_iter_51200.pkl 255 257
[2024-05-08 16:37:21] nn step 51300, lr: 0.1.
	loss_policy_0: 0.15414
	accuracy_policy_0: 0.69018
	loss_value_0: 0.16494
	loss_policy_1: 0.03573
	accuracy_policy_1: 0.6477
	loss_value_1: 0.03453
	loss_reward_1: 0.00467
	loss_policy_2: 0.03832
	accuracy_policy_2: 0.63184
	loss_value_2: 0.03607
	loss_reward_2: 0.0045
	loss_policy_3: 0.04127
	accuracy_policy_3: 0.61043
	loss_value_3: 0.03734
	loss_reward_3: 0.00481
	loss_policy_4: 0.04397
	accuracy_policy_4: 0.59312
	loss_value_4: 0.03857
	loss_reward_4: 0.00527
	loss_policy_5: 0.04621
	accuracy_policy_5: 0.57607
	loss_value_5: 0.03974
	loss_reward_5: 0.00614
	loss_policy: 0.35964
	loss_value: 0.35118
	loss_reward: 0.02539
[2024-05-08 16:38:01] nn step 51400, lr: 0.1.
	loss_policy_0: 0.13582
	accuracy_policy_0: 0.73645
	loss_value_0: 0.17016
	loss_policy_1: 0.03268
	accuracy_policy_1: 0.68979
	loss_value_1: 0.03594
	loss_reward_1: 0.0049
	loss_policy_2: 0.03635
	accuracy_policy_2: 0.66494
	loss_value_2: 0.03755
	loss_reward_2: 0.00477
	loss_policy_3: 0.03902
	accuracy_policy_3: 0.64537
	loss_value_3: 0.03895
	loss_reward_3: 0.00521
	loss_policy_4: 0.04191
	accuracy_policy_4: 0.62617
	loss_value_4: 0.04021
	loss_reward_4: 0.00551
	loss_policy_5: 0.04537
	accuracy_policy_5: 0.60129
	loss_value_5: 0.0415
	loss_reward_5: 0.0065
	loss_policy: 0.33114
	loss_value: 0.3643
	loss_reward: 0.02689
Optimization_Done 51400
[2024-05-08 16:40:40] [command] train weight_iter_51400.pkl 256 258
[2024-05-08 16:41:22] nn step 51500, lr: 0.1.
	loss_policy_0: 0.15768
	accuracy_policy_0: 0.67312
	loss_value_0: 0.16013
	loss_policy_1: 0.0366
	accuracy_policy_1: 0.62908
	loss_value_1: 0.03352
	loss_reward_1: 0.00446
	loss_policy_2: 0.03946
	accuracy_policy_2: 0.60232
	loss_value_2: 0.03494
	loss_reward_2: 0.00438
	loss_policy_3: 0.04211
	accuracy_policy_3: 0.58781
	loss_value_3: 0.03624
	loss_reward_3: 0.00456
	loss_policy_4: 0.04491
	accuracy_policy_4: 0.56725
	loss_value_4: 0.03744
	loss_reward_4: 0.00518
	loss_policy_5: 0.04694
	accuracy_policy_5: 0.55143
	loss_value_5: 0.03859
	loss_reward_5: 0.0057
	loss_policy: 0.3677
	loss_value: 0.34086
	loss_reward: 0.02428
[2024-05-08 16:42:02] nn step 51600, lr: 0.1.
	loss_policy_0: 0.13524
	accuracy_policy_0: 0.71465
	loss_value_0: 0.15643
	loss_policy_1: 0.0327
	accuracy_policy_1: 0.66508
	loss_value_1: 0.03287
	loss_reward_1: 0.00433
	loss_policy_2: 0.03583
	accuracy_policy_2: 0.63512
	loss_value_2: 0.03419
	loss_reward_2: 0.00435
	loss_policy_3: 0.03868
	accuracy_policy_3: 0.61766
	loss_value_3: 0.03532
	loss_reward_3: 0.00444
	loss_policy_4: 0.04065
	accuracy_policy_4: 0.60266
	loss_value_4: 0.03639
	loss_reward_4: 0.00488
	loss_policy_5: 0.04375
	accuracy_policy_5: 0.57992
	loss_value_5: 0.0375
	loss_reward_5: 0.00581
	loss_policy: 0.32685
	loss_value: 0.33271
	loss_reward: 0.0238
Optimization_Done 51600
[2024-05-08 16:44:41] [command] train weight_iter_51600.pkl 257 259
[2024-05-08 16:45:23] nn step 51700, lr: 0.1.
	loss_policy_0: 0.15476
	accuracy_policy_0: 0.65164
	loss_value_0: 0.16366
	loss_policy_1: 0.03562
	accuracy_policy_1: 0.61215
	loss_value_1: 0.03414
	loss_reward_1: 0.00439
	loss_policy_2: 0.03801
	accuracy_policy_2: 0.58895
	loss_value_2: 0.03542
	loss_reward_2: 0.00427
	loss_policy_3: 0.04035
	accuracy_policy_3: 0.56877
	loss_value_3: 0.03657
	loss_reward_3: 0.0047
	loss_policy_4: 0.04231
	accuracy_policy_4: 0.55502
	loss_value_4: 0.03769
	loss_reward_4: 0.00493
	loss_policy_5: 0.04489
	accuracy_policy_5: 0.53754
	loss_value_5: 0.03875
	loss_reward_5: 0.00557
	loss_policy: 0.35594
	loss_value: 0.34622
	loss_reward: 0.02386
[2024-05-08 16:46:03] nn step 51800, lr: 0.1.
	loss_policy_0: 0.13284
	accuracy_policy_0: 0.70117
	loss_value_0: 0.15864
	loss_policy_1: 0.03183
	accuracy_policy_1: 0.65061
	loss_value_1: 0.03336
	loss_reward_1: 0.00434
	loss_policy_2: 0.03444
	accuracy_policy_2: 0.62955
	loss_value_2: 0.0345
	loss_reward_2: 0.00423
	loss_policy_3: 0.03689
	accuracy_policy_3: 0.60902
	loss_value_3: 0.03571
	loss_reward_3: 0.0044
	loss_policy_4: 0.03926
	accuracy_policy_4: 0.59225
	loss_value_4: 0.03676
	loss_reward_4: 0.00474
	loss_policy_5: 0.04176
	accuracy_policy_5: 0.57303
	loss_value_5: 0.03795
	loss_reward_5: 0.00571
	loss_policy: 0.31703
	loss_value: 0.33692
	loss_reward: 0.02341
Optimization_Done 51800
[2024-05-08 16:48:40] [command] train weight_iter_51800.pkl 258 260
[2024-05-08 16:49:22] nn step 51900, lr: 0.1.
	loss_policy_0: 0.15783
	accuracy_policy_0: 0.65975
	loss_value_0: 0.15968
	loss_policy_1: 0.03597
	accuracy_policy_1: 0.61799
	loss_value_1: 0.03317
	loss_reward_1: 0.0048
	loss_policy_2: 0.03842
	accuracy_policy_2: 0.59822
	loss_value_2: 0.0346
	loss_reward_2: 0.00485
	loss_policy_3: 0.04088
	accuracy_policy_3: 0.57678
	loss_value_3: 0.03581
	loss_reward_3: 0.00525
	loss_policy_4: 0.04304
	accuracy_policy_4: 0.55799
	loss_value_4: 0.03693
	loss_reward_4: 0.00558
	loss_policy_5: 0.04509
	accuracy_policy_5: 0.54406
	loss_value_5: 0.03806
	loss_reward_5: 0.00642
	loss_policy: 0.36123
	loss_value: 0.33826
	loss_reward: 0.0269
[2024-05-08 16:50:03] nn step 52000, lr: 0.1.
	loss_policy_0: 0.13941
	accuracy_policy_0: 0.70055
	loss_value_0: 0.15751
	loss_policy_1: 0.03316
	accuracy_policy_1: 0.64967
	loss_value_1: 0.03294
	loss_reward_1: 0.00482
	loss_policy_2: 0.03579
	accuracy_policy_2: 0.62434
	loss_value_2: 0.03432
	loss_reward_2: 0.00476
	loss_policy_3: 0.03816
	accuracy_policy_3: 0.60777
	loss_value_3: 0.03547
	loss_reward_3: 0.00516
	loss_policy_4: 0.04041
	accuracy_policy_4: 0.59215
	loss_value_4: 0.03661
	loss_reward_4: 0.00549
	loss_policy_5: 0.04268
	accuracy_policy_5: 0.57184
	loss_value_5: 0.03779
	loss_reward_5: 0.00628
	loss_policy: 0.3296
	loss_value: 0.33464
	loss_reward: 0.02652
Optimization_Done 52000
[2024-05-08 16:52:40] [command] train weight_iter_52000.pkl 259 261
[2024-05-08 16:53:22] nn step 52100, lr: 0.1.
	loss_policy_0: 0.17051
	accuracy_policy_0: 0.6558
	loss_value_0: 0.16469
	loss_policy_1: 0.03859
	accuracy_policy_1: 0.62248
	loss_value_1: 0.03447
	loss_reward_1: 0.00535
	loss_policy_2: 0.04103
	accuracy_policy_2: 0.60006
	loss_value_2: 0.0359
	loss_reward_2: 0.00509
	loss_policy_3: 0.04397
	accuracy_policy_3: 0.57957
	loss_value_3: 0.03726
	loss_reward_3: 0.00553
	loss_policy_4: 0.0463
	accuracy_policy_4: 0.56027
	loss_value_4: 0.03861
	loss_reward_4: 0.00602
	loss_policy_5: 0.04887
	accuracy_policy_5: 0.54129
	loss_value_5: 0.0399
	loss_reward_5: 0.00686
	loss_policy: 0.38927
	loss_value: 0.35083
	loss_reward: 0.02885
[2024-05-08 16:54:03] nn step 52200, lr: 0.1.
	loss_policy_0: 0.15646
	accuracy_policy_0: 0.69531
	loss_value_0: 0.16805
	loss_policy_1: 0.03679
	accuracy_policy_1: 0.64459
	loss_value_1: 0.03527
	loss_reward_1: 0.00542
	loss_policy_2: 0.03961
	accuracy_policy_2: 0.62463
	loss_value_2: 0.03679
	loss_reward_2: 0.00529
	loss_policy_3: 0.04219
	accuracy_policy_3: 0.60723
	loss_value_3: 0.03818
	loss_reward_3: 0.0056
	loss_policy_4: 0.04481
	accuracy_policy_4: 0.58689
	loss_value_4: 0.03951
	loss_reward_4: 0.00609
	loss_policy_5: 0.04754
	accuracy_policy_5: 0.56783
	loss_value_5: 0.04073
	loss_reward_5: 0.00709
	loss_policy: 0.3674
	loss_value: 0.35852
	loss_reward: 0.02949
Optimization_Done 52200
[2024-05-08 16:56:39] [command] train weight_iter_52200.pkl 260 262
[2024-05-08 16:57:21] nn step 52300, lr: 0.1.
	loss_policy_0: 0.17301
	accuracy_policy_0: 0.65504
	loss_value_0: 0.17686
	loss_policy_1: 0.04083
	accuracy_policy_1: 0.60311
	loss_value_1: 0.03703
	loss_reward_1: 0.00609
	loss_policy_2: 0.04368
	accuracy_policy_2: 0.58035
	loss_value_2: 0.03831
	loss_reward_2: 0.00573
	loss_policy_3: 0.04696
	accuracy_policy_3: 0.55887
	loss_value_3: 0.03945
	loss_reward_3: 0.00627
	loss_policy_4: 0.04953
	accuracy_policy_4: 0.54014
	loss_value_4: 0.04039
	loss_reward_4: 0.00696
	loss_policy_5: 0.05217
	accuracy_policy_5: 0.51811
	loss_value_5: 0.0415
	loss_reward_5: 0.008
	loss_policy: 0.40617
	loss_value: 0.37353
	loss_reward: 0.03306
[2024-05-08 16:58:02] nn step 52400, lr: 0.1.
	loss_policy_0: 0.15812
	accuracy_policy_0: 0.69309
	loss_value_0: 0.17917
	loss_policy_1: 0.03833
	accuracy_policy_1: 0.63203
	loss_value_1: 0.03736
	loss_reward_1: 0.00623
	loss_policy_2: 0.04187
	accuracy_policy_2: 0.60598
	loss_value_2: 0.0388
	loss_reward_2: 0.00598
	loss_policy_3: 0.04503
	accuracy_policy_3: 0.58328
	loss_value_3: 0.03998
	loss_reward_3: 0.00641
	loss_policy_4: 0.04772
	accuracy_policy_4: 0.56773
	loss_value_4: 0.04112
	loss_reward_4: 0.00694
	loss_policy_5: 0.05072
	accuracy_policy_5: 0.54285
	loss_value_5: 0.04226
	loss_reward_5: 0.00823
	loss_policy: 0.38179
	loss_value: 0.37869
	loss_reward: 0.03379
Optimization_Done 52400
[2024-05-08 17:00:12] [command] train weight_iter_52400.pkl 261 263
[2024-05-08 17:00:54] nn step 52500, lr: 0.1.
	loss_policy_0: 0.16835
	accuracy_policy_0: 0.66291
	loss_value_0: 0.17685
	loss_policy_1: 0.03915
	accuracy_policy_1: 0.61389
	loss_value_1: 0.03689
	loss_reward_1: 0.00536
	loss_policy_2: 0.04245
	accuracy_policy_2: 0.58586
	loss_value_2: 0.03812
	loss_reward_2: 0.00521
	loss_policy_3: 0.04522
	accuracy_policy_3: 0.56598
	loss_value_3: 0.03917
	loss_reward_3: 0.0057
	loss_policy_4: 0.04718
	accuracy_policy_4: 0.54975
	loss_value_4: 0.04014
	loss_reward_4: 0.00602
	loss_policy_5: 0.04978
	accuracy_policy_5: 0.52887
	loss_value_5: 0.04102
	loss_reward_5: 0.00727
	loss_policy: 0.39213
	loss_value: 0.37219
	loss_reward: 0.02956
[2024-05-08 17:01:35] nn step 52600, lr: 0.1.
	loss_policy_0: 0.15102
	accuracy_policy_0: 0.70457
	loss_value_0: 0.17701
	loss_policy_1: 0.03736
	accuracy_policy_1: 0.64074
	loss_value_1: 0.03699
	loss_reward_1: 0.0056
	loss_policy_2: 0.04069
	accuracy_policy_2: 0.61348
	loss_value_2: 0.03841
	loss_reward_2: 0.00534
	loss_policy_3: 0.04358
	accuracy_policy_3: 0.59494
	loss_value_3: 0.03956
	loss_reward_3: 0.00597
	loss_policy_4: 0.0462
	accuracy_policy_4: 0.57365
	loss_value_4: 0.04047
	loss_reward_4: 0.00626
	loss_policy_5: 0.04913
	accuracy_policy_5: 0.55098
	loss_value_5: 0.0414
	loss_reward_5: 0.00751
	loss_policy: 0.36798
	loss_value: 0.37384
	loss_reward: 0.03068
Optimization_Done 52600
[2024-05-08 17:04:11] [command] train weight_iter_52600.pkl 262 264
[2024-05-08 17:04:53] nn step 52700, lr: 0.1.
	loss_policy_0: 0.15833
	accuracy_policy_0: 0.6924
	loss_value_0: 0.17235
	loss_policy_1: 0.03827
	accuracy_policy_1: 0.63945
	loss_value_1: 0.036
	loss_reward_1: 0.00544
	loss_policy_2: 0.04133
	accuracy_policy_2: 0.61695
	loss_value_2: 0.03738
	loss_reward_2: 0.00522
	loss_policy_3: 0.04373
	accuracy_policy_3: 0.59838
	loss_value_3: 0.03851
	loss_reward_3: 0.00563
	loss_policy_4: 0.04634
	accuracy_policy_4: 0.58053
	loss_value_4: 0.03961
	loss_reward_4: 0.00608
	loss_policy_5: 0.04914
	accuracy_policy_5: 0.56402
	loss_value_5: 0.0407
	loss_reward_5: 0.0072
	loss_policy: 0.37714
	loss_value: 0.36456
	loss_reward: 0.02957
[2024-05-08 17:05:33] nn step 52800, lr: 0.1.
	loss_policy_0: 0.14205
	accuracy_policy_0: 0.72012
	loss_value_0: 0.16878
	loss_policy_1: 0.03481
	accuracy_policy_1: 0.6634
	loss_value_1: 0.03527
	loss_reward_1: 0.0056
	loss_policy_2: 0.03765
	accuracy_policy_2: 0.64068
	loss_value_2: 0.03655
	loss_reward_2: 0.00509
	loss_policy_3: 0.04078
	accuracy_policy_3: 0.61742
	loss_value_3: 0.03781
	loss_reward_3: 0.00563
	loss_policy_4: 0.04287
	accuracy_policy_4: 0.60389
	loss_value_4: 0.0389
	loss_reward_4: 0.00597
	loss_policy_5: 0.04589
	accuracy_policy_5: 0.58428
	loss_value_5: 0.03999
	loss_reward_5: 0.00718
	loss_policy: 0.34405
	loss_value: 0.3573
	loss_reward: 0.02948
Optimization_Done 52800
[2024-05-08 17:08:01] [command] train weight_iter_52800.pkl 263 265
[2024-05-08 17:08:42] nn step 52900, lr: 0.1.
	loss_policy_0: 0.15836
	accuracy_policy_0: 0.69072
	loss_value_0: 0.17172
	loss_policy_1: 0.03671
	accuracy_policy_1: 0.64416
	loss_value_1: 0.0358
	loss_reward_1: 0.00506
	loss_policy_2: 0.03961
	accuracy_policy_2: 0.62557
	loss_value_2: 0.03735
	loss_reward_2: 0.00469
	loss_policy_3: 0.04264
	accuracy_policy_3: 0.60705
	loss_value_3: 0.0387
	loss_reward_3: 0.00505
	loss_policy_4: 0.04491
	accuracy_policy_4: 0.59354
	loss_value_4: 0.03988
	loss_reward_4: 0.00552
	loss_policy_5: 0.04742
	accuracy_policy_5: 0.57203
	loss_value_5: 0.04109
	loss_reward_5: 0.00646
	loss_policy: 0.36965
	loss_value: 0.36454
	loss_reward: 0.02677
[2024-05-08 17:09:22] nn step 53000, lr: 0.1.
	loss_policy_0: 0.14042
	accuracy_policy_0: 0.7267
	loss_value_0: 0.16985
	loss_policy_1: 0.03345
	accuracy_policy_1: 0.67768
	loss_value_1: 0.0356
	loss_reward_1: 0.00506
	loss_policy_2: 0.03627
	accuracy_policy_2: 0.65574
	loss_value_2: 0.03713
	loss_reward_2: 0.00476
	loss_policy_3: 0.03937
	accuracy_policy_3: 0.63826
	loss_value_3: 0.03826
	loss_reward_3: 0.00506
	loss_policy_4: 0.04191
	accuracy_policy_4: 0.6209
	loss_value_4: 0.0396
	loss_reward_4: 0.00545
	loss_policy_5: 0.0449
	accuracy_policy_5: 0.59887
	loss_value_5: 0.04101
	loss_reward_5: 0.00654
	loss_policy: 0.33631
	loss_value: 0.36145
	loss_reward: 0.02686
Optimization_Done 53000
[2024-05-08 17:11:44] [command] train weight_iter_53000.pkl 264 266
[2024-05-08 17:12:26] nn step 53100, lr: 0.1.
	loss_policy_0: 0.15861
	accuracy_policy_0: 0.66773
	loss_value_0: 0.16877
	loss_policy_1: 0.03659
	accuracy_policy_1: 0.62787
	loss_value_1: 0.0354
	loss_reward_1: 0.00555
	loss_policy_2: 0.03951
	accuracy_policy_2: 0.60418
	loss_value_2: 0.03696
	loss_reward_2: 0.00535
	loss_policy_3: 0.04241
	accuracy_policy_3: 0.58322
	loss_value_3: 0.03842
	loss_reward_3: 0.00586
	loss_policy_4: 0.04541
	accuracy_policy_4: 0.56229
	loss_value_4: 0.0396
	loss_reward_4: 0.00618
	loss_policy_5: 0.04813
	accuracy_policy_5: 0.54494
	loss_value_5: 0.04071
	loss_reward_5: 0.00713
	loss_policy: 0.37066
	loss_value: 0.35985
	loss_reward: 0.03007
[2024-05-08 17:13:06] nn step 53200, lr: 0.1.
	loss_policy_0: 0.13413
	accuracy_policy_0: 0.71314
	loss_value_0: 0.16251
	loss_policy_1: 0.03236
	accuracy_policy_1: 0.66033
	loss_value_1: 0.03405
	loss_reward_1: 0.0053
	loss_policy_2: 0.03521
	accuracy_policy_2: 0.63752
	loss_value_2: 0.03535
	loss_reward_2: 0.00496
	loss_policy_3: 0.03807
	accuracy_policy_3: 0.61436
	loss_value_3: 0.03643
	loss_reward_3: 0.00551
	loss_policy_4: 0.04064
	accuracy_policy_4: 0.59654
	loss_value_4: 0.03763
	loss_reward_4: 0.00577
	loss_policy_5: 0.04362
	accuracy_policy_5: 0.57492
	loss_value_5: 0.03888
	loss_reward_5: 0.00694
	loss_policy: 0.32403
	loss_value: 0.34486
	loss_reward: 0.02847
Optimization_Done 53200
[2024-05-08 17:15:28] [command] train weight_iter_53200.pkl 265 267
[2024-05-08 17:16:10] nn step 53300, lr: 0.1.
	loss_policy_0: 0.13004
	accuracy_policy_0: 0.69902
	loss_value_0: 0.15608
	loss_policy_1: 0.03138
	accuracy_policy_1: 0.64105
	loss_value_1: 0.03235
	loss_reward_1: 0.00448
	loss_policy_2: 0.03393
	accuracy_policy_2: 0.61738
	loss_value_2: 0.03365
	loss_reward_2: 0.00432
	loss_policy_3: 0.03632
	accuracy_policy_3: 0.59977
	loss_value_3: 0.03476
	loss_reward_3: 0.00468
	loss_policy_4: 0.03876
	accuracy_policy_4: 0.57918
	loss_value_4: 0.0357
	loss_reward_4: 0.0049
	loss_policy_5: 0.04159
	accuracy_policy_5: 0.55537
	loss_value_5: 0.03681
	loss_reward_5: 0.00584
	loss_policy: 0.31201
	loss_value: 0.32936
	loss_reward: 0.02423
[2024-05-08 17:16:50] nn step 53400, lr: 0.1.
	loss_policy_0: 0.11507
	accuracy_policy_0: 0.72551
	loss_value_0: 0.1536
	loss_policy_1: 0.02896
	accuracy_policy_1: 0.66389
	loss_value_1: 0.03184
	loss_reward_1: 0.00437
	loss_policy_2: 0.03135
	accuracy_policy_2: 0.64707
	loss_value_2: 0.03303
	loss_reward_2: 0.00426
	loss_policy_3: 0.03372
	accuracy_policy_3: 0.625
	loss_value_3: 0.03416
	loss_reward_3: 0.00456
	loss_policy_4: 0.03627
	accuracy_policy_4: 0.60498
	loss_value_4: 0.03503
	loss_reward_4: 0.00485
	loss_policy_5: 0.0389
	accuracy_policy_5: 0.58273
	loss_value_5: 0.03621
	loss_reward_5: 0.00588
	loss_policy: 0.28427
	loss_value: 0.32387
	loss_reward: 0.02392
Optimization_Done 53400
[2024-05-08 17:19:30] [command] train weight_iter_53400.pkl 266 268
[2024-05-08 17:20:12] nn step 53500, lr: 0.1.
	loss_policy_0: 0.162
	accuracy_policy_0: 0.64104
	loss_value_0: 0.14613
	loss_policy_1: 0.03714
	accuracy_policy_1: 0.59879
	loss_value_1: 0.03028
	loss_reward_1: 0.00438
	loss_policy_2: 0.03999
	accuracy_policy_2: 0.57826
	loss_value_2: 0.03153
	loss_reward_2: 0.00427
	loss_policy_3: 0.04242
	accuracy_policy_3: 0.55562
	loss_value_3: 0.03263
	loss_reward_3: 0.00465
	loss_policy_4: 0.04453
	accuracy_policy_4: 0.53768
	loss_value_4: 0.03375
	loss_reward_4: 0.00494
	loss_policy_5: 0.04665
	accuracy_policy_5: 0.52162
	loss_value_5: 0.03488
	loss_reward_5: 0.00582
	loss_policy: 0.37274
	loss_value: 0.30919
	loss_reward: 0.02406
[2024-05-08 17:20:52] nn step 53600, lr: 0.1.
	loss_policy_0: 0.13907
	accuracy_policy_0: 0.69891
	loss_value_0: 0.14914
	loss_policy_1: 0.03376
	accuracy_policy_1: 0.64418
	loss_value_1: 0.03106
	loss_reward_1: 0.00456
	loss_policy_2: 0.0361
	accuracy_policy_2: 0.62744
	loss_value_2: 0.03226
	loss_reward_2: 0.00436
	loss_policy_3: 0.0392
	accuracy_policy_3: 0.5983
	loss_value_3: 0.03347
	loss_reward_3: 0.00478
	loss_policy_4: 0.04148
	accuracy_policy_4: 0.58391
	loss_value_4: 0.03444
	loss_reward_4: 0.00507
	loss_policy_5: 0.04391
	accuracy_policy_5: 0.56152
	loss_value_5: 0.03552
	loss_reward_5: 0.00612
	loss_policy: 0.33352
	loss_value: 0.31588
	loss_reward: 0.02488
Optimization_Done 53600
[2024-05-08 17:23:26] [command] train weight_iter_53600.pkl 267 269
[2024-05-08 17:24:08] nn step 53700, lr: 0.1.
	loss_policy_0: 0.14547
	accuracy_policy_0: 0.68664
	loss_value_0: 0.14231
	loss_policy_1: 0.03399
	accuracy_policy_1: 0.64416
	loss_value_1: 0.02956
	loss_reward_1: 0.00403
	loss_policy_2: 0.03668
	accuracy_policy_2: 0.62139
	loss_value_2: 0.03071
	loss_reward_2: 0.004
	loss_policy_3: 0.03887
	accuracy_policy_3: 0.60184
	loss_value_3: 0.03179
	loss_reward_3: 0.00426
	loss_policy_4: 0.04076
	accuracy_policy_4: 0.58627
	loss_value_4: 0.03292
	loss_reward_4: 0.00458
	loss_policy_5: 0.04284
	accuracy_policy_5: 0.57307
	loss_value_5: 0.03398
	loss_reward_5: 0.00544
	loss_policy: 0.33861
	loss_value: 0.30127
	loss_reward: 0.02232
[2024-05-08 17:24:49] nn step 53800, lr: 0.1.
	loss_policy_0: 0.12977
	accuracy_policy_0: 0.71986
	loss_value_0: 0.14367
	loss_policy_1: 0.03131
	accuracy_policy_1: 0.67258
	loss_value_1: 0.03006
	loss_reward_1: 0.0042
	loss_policy_2: 0.03361
	accuracy_policy_2: 0.65191
	loss_value_2: 0.03129
	loss_reward_2: 0.00394
	loss_policy_3: 0.03628
	accuracy_policy_3: 0.63441
	loss_value_3: 0.03243
	loss_reward_3: 0.00413
	loss_policy_4: 0.03883
	accuracy_policy_4: 0.61229
	loss_value_4: 0.03346
	loss_reward_4: 0.00469
	loss_policy_5: 0.0409
	accuracy_policy_5: 0.5968
	loss_value_5: 0.0345
	loss_reward_5: 0.00538
	loss_policy: 0.31069
	loss_value: 0.30541
	loss_reward: 0.02234
Optimization_Done 53800
[2024-05-08 17:27:25] [command] train weight_iter_53800.pkl 268 270
[2024-05-08 17:28:07] nn step 53900, lr: 0.1.
	loss_policy_0: 0.16644
	accuracy_policy_0: 0.6818
	loss_value_0: 0.17131
	loss_policy_1: 0.03874
	accuracy_policy_1: 0.63715
	loss_value_1: 0.03571
	loss_reward_1: 0.00577
	loss_policy_2: 0.04162
	accuracy_policy_2: 0.61924
	loss_value_2: 0.03719
	loss_reward_2: 0.00569
	loss_policy_3: 0.04388
	accuracy_policy_3: 0.60369
	loss_value_3: 0.03826
	loss_reward_3: 0.0061
	loss_policy_4: 0.04652
	accuracy_policy_4: 0.58885
	loss_value_4: 0.03949
	loss_reward_4: 0.00665
	loss_policy_5: 0.04903
	accuracy_policy_5: 0.57396
	loss_value_5: 0.04061
	loss_reward_5: 0.00774
	loss_policy: 0.38623
	loss_value: 0.36257
	loss_reward: 0.03195
[2024-05-08 17:28:48] nn step 54000, lr: 0.1.
	loss_policy_0: 0.15214
	accuracy_policy_0: 0.71748
	loss_value_0: 0.17156
	loss_policy_1: 0.0359
	accuracy_policy_1: 0.67148
	loss_value_1: 0.03589
	loss_reward_1: 0.00589
	loss_policy_2: 0.03903
	accuracy_policy_2: 0.65092
	loss_value_2: 0.0373
	loss_reward_2: 0.00558
	loss_policy_3: 0.04228
	accuracy_policy_3: 0.62973
	loss_value_3: 0.03874
	loss_reward_3: 0.00606
	loss_policy_4: 0.04501
	accuracy_policy_4: 0.61334
	loss_value_4: 0.03993
	loss_reward_4: 0.0067
	loss_policy_5: 0.0477
	accuracy_policy_5: 0.59613
	loss_value_5: 0.04112
	loss_reward_5: 0.00771
	loss_policy: 0.36205
	loss_value: 0.36454
	loss_reward: 0.03195
Optimization_Done 54000
[2024-05-08 17:31:23] [command] train weight_iter_54000.pkl 269 271
[2024-05-08 17:32:06] nn step 54100, lr: 0.1.
	loss_policy_0: 0.15427
	accuracy_policy_0: 0.6866
	loss_value_0: 0.1678
	loss_policy_1: 0.03638
	accuracy_policy_1: 0.63688
	loss_value_1: 0.03482
	loss_reward_1: 0.00575
	loss_policy_2: 0.0395
	accuracy_policy_2: 0.61369
	loss_value_2: 0.0363
	loss_reward_2: 0.00573
	loss_policy_3: 0.04229
	accuracy_policy_3: 0.59143
	loss_value_3: 0.03751
	loss_reward_3: 0.0062
	loss_policy_4: 0.04428
	accuracy_policy_4: 0.58061
	loss_value_4: 0.0386
	loss_reward_4: 0.00666
	loss_policy_5: 0.04647
	accuracy_policy_5: 0.56467
	loss_value_5: 0.03963
	loss_reward_5: 0.00768
	loss_policy: 0.36319
	loss_value: 0.35466
	loss_reward: 0.03201
[2024-05-08 17:32:46] nn step 54200, lr: 0.1.
	loss_policy_0: 0.13957
	accuracy_policy_0: 0.72926
	loss_value_0: 0.17329
	loss_policy_1: 0.03423
	accuracy_policy_1: 0.67363
	loss_value_1: 0.03604
	loss_reward_1: 0.00601
	loss_policy_2: 0.03725
	accuracy_policy_2: 0.65182
	loss_value_2: 0.03729
	loss_reward_2: 0.00611
	loss_policy_3: 0.04022
	accuracy_policy_3: 0.6308
	loss_value_3: 0.03845
	loss_reward_3: 0.00641
	loss_policy_4: 0.04252
	accuracy_policy_4: 0.61789
	loss_value_4: 0.03963
	loss_reward_4: 0.0071
	loss_policy_5: 0.04497
	accuracy_policy_5: 0.59947
	loss_value_5: 0.04092
	loss_reward_5: 0.00797
	loss_policy: 0.33877
	loss_value: 0.36562
	loss_reward: 0.03361
Optimization_Done 54200
[2024-05-08 17:34:56] [command] train weight_iter_54200.pkl 270 272
[2024-05-08 17:35:38] nn step 54300, lr: 0.1.
	loss_policy_0: 0.16141
	accuracy_policy_0: 0.68596
	loss_value_0: 0.18322
	loss_policy_1: 0.0387
	accuracy_policy_1: 0.63279
	loss_value_1: 0.03818
	loss_reward_1: 0.00589
	loss_policy_2: 0.0415
	accuracy_policy_2: 0.60936
	loss_value_2: 0.03936
	loss_reward_2: 0.00582
	loss_policy_3: 0.04411
	accuracy_policy_3: 0.5875
	loss_value_3: 0.04062
	loss_reward_3: 0.00616
	loss_policy_4: 0.04657
	accuracy_policy_4: 0.57264
	loss_value_4: 0.04159
	loss_reward_4: 0.00671
	loss_policy_5: 0.04873
	accuracy_policy_5: 0.55395
	loss_value_5: 0.04269
	loss_reward_5: 0.00774
	loss_policy: 0.38102
	loss_value: 0.38566
	loss_reward: 0.03231
[2024-05-08 17:36:19] nn step 54400, lr: 0.1.
	loss_policy_0: 0.14275
	accuracy_policy_0: 0.72277
	loss_value_0: 0.17995
	loss_policy_1: 0.03504
	accuracy_policy_1: 0.66426
	loss_value_1: 0.03755
	loss_reward_1: 0.00576
	loss_policy_2: 0.03801
	accuracy_policy_2: 0.64199
	loss_value_2: 0.0388
	loss_reward_2: 0.00568
	loss_policy_3: 0.04066
	accuracy_policy_3: 0.62281
	loss_value_3: 0.03987
	loss_reward_3: 0.00614
	loss_policy_4: 0.04322
	accuracy_policy_4: 0.60176
	loss_value_4: 0.04099
	loss_reward_4: 0.00667
	loss_policy_5: 0.04519
	accuracy_policy_5: 0.58729
	loss_value_5: 0.04205
	loss_reward_5: 0.00758
	loss_policy: 0.34486
	loss_value: 0.37921
	loss_reward: 0.03183
Optimization_Done 54400
[2024-05-08 17:38:54] [command] train weight_iter_54400.pkl 271 273
[2024-05-08 17:39:35] nn step 54500, lr: 0.1.
	loss_policy_0: 0.15253
	accuracy_policy_0: 0.70635
	loss_value_0: 0.16546
	loss_policy_1: 0.03598
	accuracy_policy_1: 0.65818
	loss_value_1: 0.03444
	loss_reward_1: 0.00519
	loss_policy_2: 0.03922
	accuracy_policy_2: 0.63195
	loss_value_2: 0.03585
	loss_reward_2: 0.00513
	loss_policy_3: 0.04208
	accuracy_policy_3: 0.60768
	loss_value_3: 0.03709
	loss_reward_3: 0.0057
	loss_policy_4: 0.0439
	accuracy_policy_4: 0.59324
	loss_value_4: 0.03812
	loss_reward_4: 0.00595
	loss_policy_5: 0.04658
	accuracy_policy_5: 0.57234
	loss_value_5: 0.03928
	loss_reward_5: 0.00693
	loss_policy: 0.3603
	loss_value: 0.35026
	loss_reward: 0.0289
[2024-05-08 17:40:16] nn step 54600, lr: 0.1.
	loss_policy_0: 0.14263
	accuracy_policy_0: 0.73699
	loss_value_0: 0.1736
	loss_policy_1: 0.03492
	accuracy_policy_1: 0.68664
	loss_value_1: 0.03632
	loss_reward_1: 0.00539
	loss_policy_2: 0.03827
	accuracy_policy_2: 0.65705
	loss_value_2: 0.03777
	loss_reward_2: 0.00535
	loss_policy_3: 0.04118
	accuracy_policy_3: 0.63637
	loss_value_3: 0.03911
	loss_reward_3: 0.00582
	loss_policy_4: 0.04385
	accuracy_policy_4: 0.61787
	loss_value_4: 0.04045
	loss_reward_4: 0.00625
	loss_policy_5: 0.04599
	accuracy_policy_5: 0.59707
	loss_value_5: 0.04159
	loss_reward_5: 0.00725
	loss_policy: 0.34684
	loss_value: 0.36884
	loss_reward: 0.03007
Optimization_Done 54600
[2024-05-08 17:42:56] [command] train weight_iter_54600.pkl 272 274
[2024-05-08 17:43:37] nn step 54700, lr: 0.1.
	loss_policy_0: 0.16262
	accuracy_policy_0: 0.68508
	loss_value_0: 0.16585
	loss_policy_1: 0.03759
	accuracy_policy_1: 0.646
	loss_value_1: 0.03471
	loss_reward_1: 0.0047
	loss_policy_2: 0.04055
	accuracy_policy_2: 0.62238
	loss_value_2: 0.03623
	loss_reward_2: 0.00486
	loss_policy_3: 0.04306
	accuracy_policy_3: 0.60346
	loss_value_3: 0.03744
	loss_reward_3: 0.00515
	loss_policy_4: 0.04561
	accuracy_policy_4: 0.58148
	loss_value_4: 0.03867
	loss_reward_4: 0.00568
	loss_policy_5: 0.04825
	accuracy_policy_5: 0.56529
	loss_value_5: 0.0398
	loss_reward_5: 0.00671
	loss_policy: 0.37769
	loss_value: 0.3527
	loss_reward: 0.0271
[2024-05-08 17:44:17] nn step 54800, lr: 0.1.
	loss_policy_0: 0.15126
	accuracy_policy_0: 0.72121
	loss_value_0: 0.1736
	loss_policy_1: 0.03604
	accuracy_policy_1: 0.67561
	loss_value_1: 0.03644
	loss_reward_1: 0.00506
	loss_policy_2: 0.03908
	accuracy_policy_2: 0.65537
	loss_value_2: 0.03804
	loss_reward_2: 0.00501
	loss_policy_3: 0.04251
	accuracy_policy_3: 0.6308
	loss_value_3: 0.03948
	loss_reward_3: 0.0055
	loss_policy_4: 0.04519
	accuracy_policy_4: 0.60861
	loss_value_4: 0.04076
	loss_reward_4: 0.00592
	loss_policy_5: 0.04823
	accuracy_policy_5: 0.59078
	loss_value_5: 0.04196
	loss_reward_5: 0.00685
	loss_policy: 0.36231
	loss_value: 0.37028
	loss_reward: 0.02835
Optimization_Done 54800
[2024-05-08 17:46:53] [command] train weight_iter_54800.pkl 273 275
[2024-05-08 17:47:34] nn step 54900, lr: 0.1.
	loss_policy_0: 0.15385
	accuracy_policy_0: 0.69404
	loss_value_0: 0.16735
	loss_policy_1: 0.03592
	accuracy_policy_1: 0.65275
	loss_value_1: 0.03515
	loss_reward_1: 0.00508
	loss_policy_2: 0.03885
	accuracy_policy_2: 0.62982
	loss_value_2: 0.03651
	loss_reward_2: 0.00473
	loss_policy_3: 0.04157
	accuracy_policy_3: 0.60871
	loss_value_3: 0.03774
	loss_reward_3: 0.00521
	loss_policy_4: 0.04428
	accuracy_policy_4: 0.58945
	loss_value_4: 0.03898
	loss_reward_4: 0.00576
	loss_policy_5: 0.04652
	accuracy_policy_5: 0.57297
	loss_value_5: 0.04027
	loss_reward_5: 0.00661
	loss_policy: 0.36099
	loss_value: 0.35601
	loss_reward: 0.02739
[2024-05-08 17:48:15] nn step 55000, lr: 0.1.
	loss_policy_0: 0.13737
	accuracy_policy_0: 0.73271
	loss_value_0: 0.16743
	loss_policy_1: 0.03371
	accuracy_policy_1: 0.67916
	loss_value_1: 0.03508
	loss_reward_1: 0.0051
	loss_policy_2: 0.03662
	accuracy_policy_2: 0.65352
	loss_value_2: 0.03645
	loss_reward_2: 0.00484
	loss_policy_3: 0.03931
	accuracy_policy_3: 0.63322
	loss_value_3: 0.03772
	loss_reward_3: 0.0053
	loss_policy_4: 0.04226
	accuracy_policy_4: 0.61404
	loss_value_4: 0.03896
	loss_reward_4: 0.00564
	loss_policy_5: 0.04446
	accuracy_policy_5: 0.59859
	loss_value_5: 0.04003
	loss_reward_5: 0.0066
	loss_policy: 0.33374
	loss_value: 0.35566
	loss_reward: 0.02748
Optimization_Done 55000
[2024-05-08 17:50:38] [command] train weight_iter_55000.pkl 274 276
[2024-05-08 17:51:20] nn step 55100, lr: 0.1.
	loss_policy_0: 0.1563
	accuracy_policy_0: 0.68287
	loss_value_0: 0.16924
	loss_policy_1: 0.03703
	accuracy_policy_1: 0.63133
	loss_value_1: 0.03536
	loss_reward_1: 0.0045
	loss_policy_2: 0.03946
	accuracy_policy_2: 0.61119
	loss_value_2: 0.03665
	loss_reward_2: 0.00447
	loss_policy_3: 0.04229
	accuracy_policy_3: 0.58408
	loss_value_3: 0.03781
	loss_reward_3: 0.00483
	loss_policy_4: 0.04447
	accuracy_policy_4: 0.57129
	loss_value_4: 0.03883
	loss_reward_4: 0.00531
	loss_policy_5: 0.04667
	accuracy_policy_5: 0.55566
	loss_value_5: 0.03984
	loss_reward_5: 0.00598
	loss_policy: 0.36622
	loss_value: 0.35773
	loss_reward: 0.0251
[2024-05-08 17:52:00] nn step 55200, lr: 0.1.
	loss_policy_0: 0.12748
	accuracy_policy_0: 0.72586
	loss_value_0: 0.1593
	loss_policy_1: 0.03159
	accuracy_policy_1: 0.66613
	loss_value_1: 0.03331
	loss_reward_1: 0.00425
	loss_policy_2: 0.03419
	accuracy_policy_2: 0.64393
	loss_value_2: 0.03448
	loss_reward_2: 0.00415
	loss_policy_3: 0.03673
	accuracy_policy_3: 0.62367
	loss_value_3: 0.03557
	loss_reward_3: 0.0045
	loss_policy_4: 0.03915
	accuracy_policy_4: 0.60301
	loss_value_4: 0.03665
	loss_reward_4: 0.00489
	loss_policy_5: 0.04181
	accuracy_policy_5: 0.58412
	loss_value_5: 0.03762
	loss_reward_5: 0.00584
	loss_policy: 0.31096
	loss_value: 0.33693
	loss_reward: 0.02363
Optimization_Done 55200
[2024-05-08 17:54:12] [command] train weight_iter_55200.pkl 275 277
[2024-05-08 17:54:54] nn step 55300, lr: 0.1.
	loss_policy_0: 0.16671
	accuracy_policy_0: 0.64764
	loss_value_0: 0.15255
	loss_policy_1: 0.03754
	accuracy_policy_1: 0.60369
	loss_value_1: 0.03175
	loss_reward_1: 0.00423
	loss_policy_2: 0.04007
	accuracy_policy_2: 0.58172
	loss_value_2: 0.03281
	loss_reward_2: 0.0042
	loss_policy_3: 0.04204
	accuracy_policy_3: 0.56094
	loss_value_3: 0.03388
	loss_reward_3: 0.0046
	loss_policy_4: 0.04399
	accuracy_policy_4: 0.5467
	loss_value_4: 0.03491
	loss_reward_4: 0.00501
	loss_policy_5: 0.04589
	accuracy_policy_5: 0.53102
	loss_value_5: 0.03583
	loss_reward_5: 0.00578
	loss_policy: 0.37624
	loss_value: 0.32173
	loss_reward: 0.02382
[2024-05-08 17:55:35] nn step 55400, lr: 0.1.
	loss_policy_0: 0.14082
	accuracy_policy_0: 0.69609
	loss_value_0: 0.1485
	loss_policy_1: 0.033
	accuracy_policy_1: 0.64543
	loss_value_1: 0.03091
	loss_reward_1: 0.00427
	loss_policy_2: 0.03563
	accuracy_policy_2: 0.62264
	loss_value_2: 0.03201
	loss_reward_2: 0.0042
	loss_policy_3: 0.03792
	accuracy_policy_3: 0.60506
	loss_value_3: 0.03307
	loss_reward_3: 0.00463
	loss_policy_4: 0.03991
	accuracy_policy_4: 0.58648
	loss_value_4: 0.03417
	loss_reward_4: 0.00494
	loss_policy_5: 0.04206
	accuracy_policy_5: 0.56826
	loss_value_5: 0.03515
	loss_reward_5: 0.00588
	loss_policy: 0.32935
	loss_value: 0.31381
	loss_reward: 0.02393
Optimization_Done 55400
[2024-05-08 17:58:13] [command] train weight_iter_55400.pkl 276 278
[2024-05-08 17:58:55] nn step 55500, lr: 0.1.
	loss_policy_0: 0.18408
	accuracy_policy_0: 0.64211
	loss_value_0: 0.15835
	loss_policy_1: 0.04151
	accuracy_policy_1: 0.60023
	loss_value_1: 0.03314
	loss_reward_1: 0.0047
	loss_policy_2: 0.04411
	accuracy_policy_2: 0.57906
	loss_value_2: 0.03458
	loss_reward_2: 0.0046
	loss_policy_3: 0.04649
	accuracy_policy_3: 0.56275
	loss_value_3: 0.0358
	loss_reward_3: 0.00493
	loss_policy_4: 0.04852
	accuracy_policy_4: 0.55027
	loss_value_4: 0.03708
	loss_reward_4: 0.00554
	loss_policy_5: 0.05105
	accuracy_policy_5: 0.53131
	loss_value_5: 0.03826
	loss_reward_5: 0.00636
	loss_policy: 0.41577
	loss_value: 0.3372
	loss_reward: 0.02612
[2024-05-08 17:59:36] nn step 55600, lr: 0.1.
	loss_policy_0: 0.16745
	accuracy_policy_0: 0.67832
	loss_value_0: 0.15992
	loss_policy_1: 0.03829
	accuracy_policy_1: 0.63586
	loss_value_1: 0.03353
	loss_reward_1: 0.00466
	loss_policy_2: 0.04059
	accuracy_policy_2: 0.61912
	loss_value_2: 0.03476
	loss_reward_2: 0.00462
	loss_policy_3: 0.04313
	accuracy_policy_3: 0.59992
	loss_value_3: 0.03592
	loss_reward_3: 0.00489
	loss_policy_4: 0.04584
	accuracy_policy_4: 0.58066
	loss_value_4: 0.03707
	loss_reward_4: 0.00537
	loss_policy_5: 0.0487
	accuracy_policy_5: 0.56234
	loss_value_5: 0.03832
	loss_reward_5: 0.00636
	loss_policy: 0.384
	loss_value: 0.33953
	loss_reward: 0.0259
Optimization_Done 55600
[2024-05-08 18:02:12] [command] train weight_iter_55600.pkl 277 279
[2024-05-08 18:02:53] nn step 55700, lr: 0.1.
	loss_policy_0: 0.17734
	accuracy_policy_0: 0.67512
	loss_value_0: 0.17384
	loss_policy_1: 0.0398
	accuracy_policy_1: 0.6433
	loss_value_1: 0.03623
	loss_reward_1: 0.00616
	loss_policy_2: 0.04202
	accuracy_policy_2: 0.62793
	loss_value_2: 0.03755
	loss_reward_2: 0.00593
	loss_policy_3: 0.04506
	accuracy_policy_3: 0.60756
	loss_value_3: 0.03887
	loss_reward_3: 0.00636
	loss_policy_4: 0.04784
	accuracy_policy_4: 0.59172
	loss_value_4: 0.04018
	loss_reward_4: 0.00718
	loss_policy_5: 0.04969
	accuracy_policy_5: 0.57676
	loss_value_5: 0.04144
	loss_reward_5: 0.00799
	loss_policy: 0.40175
	loss_value: 0.36811
	loss_reward: 0.03362
[2024-05-08 18:03:34] nn step 55800, lr: 0.1.
	loss_policy_0: 0.16428
	accuracy_policy_0: 0.70736
	loss_value_0: 0.1728
	loss_policy_1: 0.03782
	accuracy_policy_1: 0.66316
	loss_value_1: 0.0362
	loss_reward_1: 0.00612
	loss_policy_2: 0.04038
	accuracy_policy_2: 0.64797
	loss_value_2: 0.03753
	loss_reward_2: 0.00593
	loss_policy_3: 0.04344
	accuracy_policy_3: 0.62502
	loss_value_3: 0.03879
	loss_reward_3: 0.00648
	loss_policy_4: 0.04575
	accuracy_policy_4: 0.61352
	loss_value_4: 0.04008
	loss_reward_4: 0.007
	loss_policy_5: 0.04834
	accuracy_policy_5: 0.59393
	loss_value_5: 0.04126
	loss_reward_5: 0.00802
	loss_policy: 0.38002
	loss_value: 0.36666
	loss_reward: 0.03355
Optimization_Done 55800
[2024-05-08 18:05:57] [command] train weight_iter_55800.pkl 278 280
[2024-05-08 18:06:39] nn step 55900, lr: 0.1.
	loss_policy_0: 0.15597
	accuracy_policy_0: 0.67762
	loss_value_0: 0.16767
	loss_policy_1: 0.03625
	accuracy_policy_1: 0.63135
	loss_value_1: 0.03491
	loss_reward_1: 0.00581
	loss_policy_2: 0.03896
	accuracy_policy_2: 0.61096
	loss_value_2: 0.03632
	loss_reward_2: 0.0057
	loss_policy_3: 0.04161
	accuracy_policy_3: 0.58848
	loss_value_3: 0.03751
	loss_reward_3: 0.00605
	loss_policy_4: 0.04352
	accuracy_policy_4: 0.57471
	loss_value_4: 0.03852
	loss_reward_4: 0.00662
	loss_policy_5: 0.04594
	accuracy_policy_5: 0.55811
	loss_value_5: 0.03951
	loss_reward_5: 0.00771
	loss_policy: 0.36225
	loss_value: 0.35445
	loss_reward: 0.03188
[2024-05-08 18:07:20] nn step 56000, lr: 0.1.
	loss_policy_0: 0.15156
	accuracy_policy_0: 0.71514
	loss_value_0: 0.17867
	loss_policy_1: 0.03614
	accuracy_policy_1: 0.66197
	loss_value_1: 0.03706
	loss_reward_1: 0.00619
	loss_policy_2: 0.03912
	accuracy_policy_2: 0.64412
	loss_value_2: 0.03838
	loss_reward_2: 0.0059
	loss_policy_3: 0.04219
	accuracy_policy_3: 0.61936
	loss_value_3: 0.03967
	loss_reward_3: 0.00646
	loss_policy_4: 0.04464
	accuracy_policy_4: 0.60184
	loss_value_4: 0.04072
	loss_reward_4: 0.00692
	loss_policy_5: 0.04708
	accuracy_policy_5: 0.58486
	loss_value_5: 0.042
	loss_reward_5: 0.00814
	loss_policy: 0.36074
	loss_value: 0.3765
	loss_reward: 0.03361
Optimization_Done 56000
[2024-05-08 18:09:43] [command] train weight_iter_56000.pkl 279 281
[2024-05-08 18:10:25] nn step 56100, lr: 0.1.
	loss_policy_0: 0.14808
	accuracy_policy_0: 0.69826
	loss_value_0: 0.17528
	loss_policy_1: 0.03473
	accuracy_policy_1: 0.65205
	loss_value_1: 0.0364
	loss_reward_1: 0.00577
	loss_policy_2: 0.03758
	accuracy_policy_2: 0.62934
	loss_value_2: 0.03766
	loss_reward_2: 0.00576
	loss_policy_3: 0.03996
	accuracy_policy_3: 0.61256
	loss_value_3: 0.03882
	loss_reward_3: 0.0062
	loss_policy_4: 0.04219
	accuracy_policy_4: 0.5983
	loss_value_4: 0.03994
	loss_reward_4: 0.00665
	loss_policy_5: 0.0449
	accuracy_policy_5: 0.57768
	loss_value_5: 0.0411
	loss_reward_5: 0.00763
	loss_policy: 0.34743
	loss_value: 0.3692
	loss_reward: 0.03202
[2024-05-08 18:11:05] nn step 56200, lr: 0.1.
	loss_policy_0: 0.12001
	accuracy_policy_0: 0.73912
	loss_value_0: 0.16327
	loss_policy_1: 0.02952
	accuracy_policy_1: 0.68432
	loss_value_1: 0.034
	loss_reward_1: 0.00545
	loss_policy_2: 0.0321
	accuracy_policy_2: 0.66311
	loss_value_2: 0.0352
	loss_reward_2: 0.00522
	loss_policy_3: 0.03393
	accuracy_policy_3: 0.64621
	loss_value_3: 0.0363
	loss_reward_3: 0.00566
	loss_policy_4: 0.03632
	accuracy_policy_4: 0.6268
	loss_value_4: 0.03734
	loss_reward_4: 0.00625
	loss_policy_5: 0.03876
	accuracy_policy_5: 0.61102
	loss_value_5: 0.03848
	loss_reward_5: 0.00709
	loss_policy: 0.29063
	loss_value: 0.34457
	loss_reward: 0.02967
Optimization_Done 56200
[2024-05-08 18:13:43] [command] train weight_iter_56200.pkl 280 282
[2024-05-08 18:14:24] nn step 56300, lr: 0.1.
	loss_policy_0: 0.16033
	accuracy_policy_0: 0.6783
	loss_value_0: 0.1651
	loss_policy_1: 0.03677
	accuracy_policy_1: 0.63441
	loss_value_1: 0.03456
	loss_reward_1: 0.00506
	loss_policy_2: 0.03916
	accuracy_policy_2: 0.61623
	loss_value_2: 0.03596
	loss_reward_2: 0.00501
	loss_policy_3: 0.042
	accuracy_policy_3: 0.59406
	loss_value_3: 0.03717
	loss_reward_3: 0.00543
	loss_policy_4: 0.04471
	accuracy_policy_4: 0.57354
	loss_value_4: 0.03826
	loss_reward_4: 0.00588
	loss_policy_5: 0.04748
	accuracy_policy_5: 0.55072
	loss_value_5: 0.03937
	loss_reward_5: 0.00687
	loss_policy: 0.37045
	loss_value: 0.35042
	loss_reward: 0.02825
[2024-05-08 18:15:05] nn step 56400, lr: 0.1.
	loss_policy_0: 0.13909
	accuracy_policy_0: 0.72002
	loss_value_0: 0.16378
	loss_policy_1: 0.03318
	accuracy_policy_1: 0.67066
	loss_value_1: 0.03436
	loss_reward_1: 0.00507
	loss_policy_2: 0.03602
	accuracy_policy_2: 0.65334
	loss_value_2: 0.03564
	loss_reward_2: 0.00497
	loss_policy_3: 0.03891
	accuracy_policy_3: 0.62393
	loss_value_3: 0.03685
	loss_reward_3: 0.00535
	loss_policy_4: 0.04127
	accuracy_policy_4: 0.60637
	loss_value_4: 0.03814
	loss_reward_4: 0.00571
	loss_policy_5: 0.04448
	accuracy_policy_5: 0.57861
	loss_value_5: 0.03937
	loss_reward_5: 0.00666
	loss_policy: 0.33294
	loss_value: 0.34814
	loss_reward: 0.02777
Optimization_Done 56400
[2024-05-08 18:17:42] [command] train weight_iter_56400.pkl 281 283
[2024-05-08 18:18:24] nn step 56500, lr: 0.1.
	loss_policy_0: 0.17372
	accuracy_policy_0: 0.67301
	loss_value_0: 0.17221
	loss_policy_1: 0.0392
	accuracy_policy_1: 0.63455
	loss_value_1: 0.03597
	loss_reward_1: 0.00515
	loss_policy_2: 0.04231
	accuracy_policy_2: 0.61021
	loss_value_2: 0.03741
	loss_reward_2: 0.00501
	loss_policy_3: 0.04503
	accuracy_policy_3: 0.58873
	loss_value_3: 0.03877
	loss_reward_3: 0.00547
	loss_policy_4: 0.04826
	accuracy_policy_4: 0.56809
	loss_value_4: 0.04003
	loss_reward_4: 0.00579
	loss_policy_5: 0.05065
	accuracy_policy_5: 0.55193
	loss_value_5: 0.04141
	loss_reward_5: 0.00671
	loss_policy: 0.39918
	loss_value: 0.3658
	loss_reward: 0.02813
[2024-05-08 18:19:04] nn step 56600, lr: 0.1.
	loss_policy_0: 0.14786
	accuracy_policy_0: 0.71512
	loss_value_0: 0.167
	loss_policy_1: 0.03517
	accuracy_policy_1: 0.66438
	loss_value_1: 0.03506
	loss_reward_1: 0.00518
	loss_policy_2: 0.03811
	accuracy_policy_2: 0.6435
	loss_value_2: 0.03645
	loss_reward_2: 0.00495
	loss_policy_3: 0.04104
	accuracy_policy_3: 0.62311
	loss_value_3: 0.0379
	loss_reward_3: 0.00544
	loss_policy_4: 0.04412
	accuracy_policy_4: 0.60193
	loss_value_4: 0.03905
	loss_reward_4: 0.00586
	loss_policy_5: 0.04685
	accuracy_policy_5: 0.58324
	loss_value_5: 0.04025
	loss_reward_5: 0.00679
	loss_policy: 0.35316
	loss_value: 0.3557
	loss_reward: 0.02822
Optimization_Done 56600
[2024-05-08 18:21:39] [command] train weight_iter_56600.pkl 282 284
[2024-05-08 18:22:21] nn step 56700, lr: 0.1.
	loss_policy_0: 0.16015
	accuracy_policy_0: 0.68375
	loss_value_0: 0.16556
	loss_policy_1: 0.03732
	accuracy_policy_1: 0.63457
	loss_value_1: 0.03446
	loss_reward_1: 0.00515
	loss_policy_2: 0.04008
	accuracy_policy_2: 0.61637
	loss_value_2: 0.0359
	loss_reward_2: 0.00499
	loss_policy_3: 0.04267
	accuracy_policy_3: 0.59473
	loss_value_3: 0.03712
	loss_reward_3: 0.00535
	loss_policy_4: 0.04558
	accuracy_policy_4: 0.57541
	loss_value_4: 0.03825
	loss_reward_4: 0.00583
	loss_policy_5: 0.04849
	accuracy_policy_5: 0.55293
	loss_value_5: 0.03938
	loss_reward_5: 0.0068
	loss_policy: 0.37428
	loss_value: 0.35068
	loss_reward: 0.02812
[2024-05-08 18:23:02] nn step 56800, lr: 0.1.
	loss_policy_0: 0.14736
	accuracy_policy_0: 0.7117
	loss_value_0: 0.16542
	loss_policy_1: 0.03487
	accuracy_policy_1: 0.66322
	loss_value_1: 0.03472
	loss_reward_1: 0.00512
	loss_policy_2: 0.03789
	accuracy_policy_2: 0.63922
	loss_value_2: 0.03617
	loss_reward_2: 0.00522
	loss_policy_3: 0.0409
	accuracy_policy_3: 0.61611
	loss_value_3: 0.03743
	loss_reward_3: 0.00544
	loss_policy_4: 0.04374
	accuracy_policy_4: 0.59301
	loss_value_4: 0.03867
	loss_reward_4: 0.00584
	loss_policy_5: 0.04664
	accuracy_policy_5: 0.57264
	loss_value_5: 0.03981
	loss_reward_5: 0.00681
	loss_policy: 0.3514
	loss_value: 0.35222
	loss_reward: 0.02843
Optimization_Done 56800
[2024-05-08 18:25:24] [command] train weight_iter_56800.pkl 283 285
[2024-05-08 18:26:06] nn step 56900, lr: 0.1.
	loss_policy_0: 0.14845
	accuracy_policy_0: 0.68725
	loss_value_0: 0.16227
	loss_policy_1: 0.03486
	accuracy_policy_1: 0.63893
	loss_value_1: 0.03366
	loss_reward_1: 0.00476
	loss_policy_2: 0.03748
	accuracy_policy_2: 0.61713
	loss_value_2: 0.035
	loss_reward_2: 0.00471
	loss_policy_3: 0.03988
	accuracy_policy_3: 0.59729
	loss_value_3: 0.03611
	loss_reward_3: 0.00511
	loss_policy_4: 0.04219
	accuracy_policy_4: 0.57904
	loss_value_4: 0.03718
	loss_reward_4: 0.00545
	loss_policy_5: 0.04499
	accuracy_policy_5: 0.55859
	loss_value_5: 0.03821
	loss_reward_5: 0.0064
	loss_policy: 0.34785
	loss_value: 0.34243
	loss_reward: 0.02643
[2024-05-08 18:26:46] nn step 57000, lr: 0.1.
	loss_policy_0: 0.13374
	accuracy_policy_0: 0.72107
	loss_value_0: 0.16519
	loss_policy_1: 0.03246
	accuracy_policy_1: 0.67119
	loss_value_1: 0.03447
	loss_reward_1: 0.00489
	loss_policy_2: 0.03488
	accuracy_policy_2: 0.6484
	loss_value_2: 0.03562
	loss_reward_2: 0.00471
	loss_policy_3: 0.03768
	accuracy_policy_3: 0.62906
	loss_value_3: 0.03673
	loss_reward_3: 0.00505
	loss_policy_4: 0.0399
	accuracy_policy_4: 0.60855
	loss_value_4: 0.03778
	loss_reward_4: 0.00542
	loss_policy_5: 0.04277
	accuracy_policy_5: 0.58535
	loss_value_5: 0.0389
	loss_reward_5: 0.00639
	loss_policy: 0.32144
	loss_value: 0.34869
	loss_reward: 0.02645
Optimization_Done 57000
[2024-05-08 18:29:38] [command] train weight_iter_57000.pkl 284 286
[2024-05-08 18:30:20] nn step 57100, lr: 0.1.
	loss_policy_0: 0.1641
	accuracy_policy_0: 0.65938
	loss_value_0: 0.1619
	loss_policy_1: 0.03694
	accuracy_policy_1: 0.62061
	loss_value_1: 0.03357
	loss_reward_1: 0.00494
	loss_policy_2: 0.03952
	accuracy_policy_2: 0.60027
	loss_value_2: 0.03493
	loss_reward_2: 0.0048
	loss_policy_3: 0.04166
	accuracy_policy_3: 0.58359
	loss_value_3: 0.03603
	loss_reward_3: 0.00531
	loss_policy_4: 0.04411
	accuracy_policy_4: 0.56754
	loss_value_4: 0.03714
	loss_reward_4: 0.00557
	loss_policy_5: 0.04656
	accuracy_policy_5: 0.54531
	loss_value_5: 0.03818
	loss_reward_5: 0.0064
	loss_policy: 0.3729
	loss_value: 0.34177
	loss_reward: 0.02702
[2024-05-08 18:31:01] nn step 57200, lr: 0.1.
	loss_policy_0: 0.14071
	accuracy_policy_0: 0.71293
	loss_value_0: 0.16439
	loss_policy_1: 0.03294
	accuracy_policy_1: 0.66869
	loss_value_1: 0.03431
	loss_reward_1: 0.00512
	loss_policy_2: 0.03594
	accuracy_policy_2: 0.64662
	loss_value_2: 0.03549
	loss_reward_2: 0.00484
	loss_policy_3: 0.03854
	accuracy_policy_3: 0.62357
	loss_value_3: 0.03667
	loss_reward_3: 0.00531
	loss_policy_4: 0.04141
	accuracy_policy_4: 0.60719
	loss_value_4: 0.03785
	loss_reward_4: 0.00567
	loss_policy_5: 0.0439
	accuracy_policy_5: 0.57945
	loss_value_5: 0.03893
	loss_reward_5: 0.00662
	loss_policy: 0.33343
	loss_value: 0.34764
	loss_reward: 0.02756
Optimization_Done 57200
[2024-05-08 18:33:39] [command] train weight_iter_57200.pkl 285 287
[2024-05-08 18:34:21] nn step 57300, lr: 0.1.
	loss_policy_0: 0.16786
	accuracy_policy_0: 0.67881
	loss_value_0: 0.17489
	loss_policy_1: 0.03844
	accuracy_policy_1: 0.63422
	loss_value_1: 0.03658
	loss_reward_1: 0.00526
	loss_policy_2: 0.04081
	accuracy_policy_2: 0.61959
	loss_value_2: 0.0379
	loss_reward_2: 0.00517
	loss_policy_3: 0.04378
	accuracy_policy_3: 0.59795
	loss_value_3: 0.03923
	loss_reward_3: 0.00565
	loss_policy_4: 0.0462
	accuracy_policy_4: 0.58318
	loss_value_4: 0.04052
	loss_reward_4: 0.00612
	loss_policy_5: 0.04879
	accuracy_policy_5: 0.56176
	loss_value_5: 0.04166
	loss_reward_5: 0.00705
	loss_policy: 0.38587
	loss_value: 0.37077
	loss_reward: 0.02926
[2024-05-08 18:35:01] nn step 57400, lr: 0.1.
	loss_policy_0: 0.14343
	accuracy_policy_0: 0.71688
	loss_value_0: 0.16891
	loss_policy_1: 0.03409
	accuracy_policy_1: 0.66707
	loss_value_1: 0.03523
	loss_reward_1: 0.00507
	loss_policy_2: 0.0367
	accuracy_policy_2: 0.6452
	loss_value_2: 0.03667
	loss_reward_2: 0.00491
	loss_policy_3: 0.03968
	accuracy_policy_3: 0.62574
	loss_value_3: 0.03777
	loss_reward_3: 0.00535
	loss_policy_4: 0.04157
	accuracy_policy_4: 0.61131
	loss_value_4: 0.0389
	loss_reward_4: 0.00584
	loss_policy_5: 0.04435
	accuracy_policy_5: 0.58834
	loss_value_5: 0.04016
	loss_reward_5: 0.00681
	loss_policy: 0.33983
	loss_value: 0.35764
	loss_reward: 0.02798
Optimization_Done 57400
[2024-05-08 18:37:37] [command] train weight_iter_57400.pkl 286 288
[2024-05-08 18:38:18] nn step 57500, lr: 0.1.
	loss_policy_0: 0.15895
	accuracy_policy_0: 0.67727
	loss_value_0: 0.16852
	loss_policy_1: 0.03652
	accuracy_policy_1: 0.63131
	loss_value_1: 0.03499
	loss_reward_1: 0.00538
	loss_policy_2: 0.03918
	accuracy_policy_2: 0.60809
	loss_value_2: 0.03647
	loss_reward_2: 0.00511
	loss_policy_3: 0.04168
	accuracy_policy_3: 0.59197
	loss_value_3: 0.03775
	loss_reward_3: 0.00564
	loss_policy_4: 0.04405
	accuracy_policy_4: 0.57637
	loss_value_4: 0.03894
	loss_reward_4: 0.00624
	loss_policy_5: 0.04635
	accuracy_policy_5: 0.56039
	loss_value_5: 0.04013
	loss_reward_5: 0.00727
	loss_policy: 0.36673
	loss_value: 0.35679
	loss_reward: 0.02965
[2024-05-08 18:38:59] nn step 57600, lr: 0.1.
	loss_policy_0: 0.14838
	accuracy_policy_0: 0.71742
	loss_value_0: 0.1747
	loss_policy_1: 0.03501
	accuracy_policy_1: 0.66807
	loss_value_1: 0.03642
	loss_reward_1: 0.00557
	loss_policy_2: 0.03782
	accuracy_policy_2: 0.64801
	loss_value_2: 0.03786
	loss_reward_2: 0.00549
	loss_policy_3: 0.04088
	accuracy_policy_3: 0.6259
	loss_value_3: 0.0391
	loss_reward_3: 0.00582
	loss_policy_4: 0.04343
	accuracy_policy_4: 0.60615
	loss_value_4: 0.04035
	loss_reward_4: 0.00642
	loss_policy_5: 0.04667
	accuracy_policy_5: 0.58498
	loss_value_5: 0.04153
	loss_reward_5: 0.00754
	loss_policy: 0.3522
	loss_value: 0.36995
	loss_reward: 0.03084
Optimization_Done 57600
[2024-05-08 18:40:58] [command] train weight_iter_57600.pkl 287 289
[2024-05-08 18:41:40] nn step 57700, lr: 0.1.
	loss_policy_0: 0.14751
	accuracy_policy_0: 0.69324
	loss_value_0: 0.17056
	loss_policy_1: 0.03472
	accuracy_policy_1: 0.64367
	loss_value_1: 0.03558
	loss_reward_1: 0.00512
	loss_policy_2: 0.03741
	accuracy_policy_2: 0.6198
	loss_value_2: 0.03682
	loss_reward_2: 0.00476
	loss_policy_3: 0.03986
	accuracy_policy_3: 0.60412
	loss_value_3: 0.03787
	loss_reward_3: 0.00521
	loss_policy_4: 0.04229
	accuracy_policy_4: 0.58326
	loss_value_4: 0.03894
	loss_reward_4: 0.00558
	loss_policy_5: 0.04521
	accuracy_policy_5: 0.5592
	loss_value_5: 0.04
	loss_reward_5: 0.00653
	loss_policy: 0.347
	loss_value: 0.35977
	loss_reward: 0.02719
[2024-05-08 18:42:21] nn step 57800, lr: 0.1.
	loss_policy_0: 0.13345
	accuracy_policy_0: 0.72637
	loss_value_0: 0.17182
	loss_policy_1: 0.03231
	accuracy_policy_1: 0.67221
	loss_value_1: 0.03568
	loss_reward_1: 0.00503
	loss_policy_2: 0.03516
	accuracy_policy_2: 0.65127
	loss_value_2: 0.03687
	loss_reward_2: 0.00484
	loss_policy_3: 0.03744
	accuracy_policy_3: 0.63473
	loss_value_3: 0.03794
	loss_reward_3: 0.00541
	loss_policy_4: 0.04022
	accuracy_policy_4: 0.60984
	loss_value_4: 0.03902
	loss_reward_4: 0.00578
	loss_policy_5: 0.04293
	accuracy_policy_5: 0.59242
	loss_value_5: 0.04003
	loss_reward_5: 0.00675
	loss_policy: 0.3215
	loss_value: 0.36136
	loss_reward: 0.02781
Optimization_Done 57800
[2024-05-08 18:44:58] [command] train weight_iter_57800.pkl 288 290
[2024-05-08 18:45:39] nn step 57900, lr: 0.1.
	loss_policy_0: 0.1654
	accuracy_policy_0: 0.65744
	loss_value_0: 0.16372
	loss_policy_1: 0.03766
	accuracy_policy_1: 0.61721
	loss_value_1: 0.03428
	loss_reward_1: 0.0046
	loss_policy_2: 0.03976
	accuracy_policy_2: 0.60031
	loss_value_2: 0.0355
	loss_reward_2: 0.00451
	loss_policy_3: 0.04225
	accuracy_policy_3: 0.57752
	loss_value_3: 0.03678
	loss_reward_3: 0.00488
	loss_policy_4: 0.04446
	accuracy_policy_4: 0.56314
	loss_value_4: 0.0379
	loss_reward_4: 0.00522
	loss_policy_5: 0.04662
	accuracy_policy_5: 0.54492
	loss_value_5: 0.03897
	loss_reward_5: 0.00603
	loss_policy: 0.37614
	loss_value: 0.34716
	loss_reward: 0.02524
[2024-05-08 18:46:20] nn step 58000, lr: 0.1.
	loss_policy_0: 0.14291
	accuracy_policy_0: 0.70904
	loss_value_0: 0.16344
	loss_policy_1: 0.03346
	accuracy_policy_1: 0.6598
	loss_value_1: 0.03405
	loss_reward_1: 0.00452
	loss_policy_2: 0.03608
	accuracy_policy_2: 0.64076
	loss_value_2: 0.03531
	loss_reward_2: 0.00454
	loss_policy_3: 0.03851
	accuracy_policy_3: 0.62219
	loss_value_3: 0.03645
	loss_reward_3: 0.00489
	loss_policy_4: 0.04106
	accuracy_policy_4: 0.60127
	loss_value_4: 0.03759
	loss_reward_4: 0.00523
	loss_policy_5: 0.04374
	accuracy_policy_5: 0.58475
	loss_value_5: 0.03869
	loss_reward_5: 0.00614
	loss_policy: 0.33576
	loss_value: 0.34553
	loss_reward: 0.02532
Optimization_Done 58000
[2024-05-08 18:48:58] [command] train weight_iter_58000.pkl 289 291
[2024-05-08 18:49:40] nn step 58100, lr: 0.1.
	loss_policy_0: 0.18104
	accuracy_policy_0: 0.66873
	loss_value_0: 0.17929
	loss_policy_1: 0.04039
	accuracy_policy_1: 0.63412
	loss_value_1: 0.03758
	loss_reward_1: 0.00496
	loss_policy_2: 0.04285
	accuracy_policy_2: 0.61666
	loss_value_2: 0.03917
	loss_reward_2: 0.00499
	loss_policy_3: 0.04532
	accuracy_policy_3: 0.59578
	loss_value_3: 0.04068
	loss_reward_3: 0.00528
	loss_policy_4: 0.04781
	accuracy_policy_4: 0.57871
	loss_value_4: 0.04192
	loss_reward_4: 0.00577
	loss_policy_5: 0.05038
	accuracy_policy_5: 0.56082
	loss_value_5: 0.04308
	loss_reward_5: 0.00663
	loss_policy: 0.40781
	loss_value: 0.38171
	loss_reward: 0.02763
[2024-05-08 18:50:20] nn step 58200, lr: 0.1.
	loss_policy_0: 0.15544
	accuracy_policy_0: 0.70959
	loss_value_0: 0.17068
	loss_policy_1: 0.03549
	accuracy_policy_1: 0.66977
	loss_value_1: 0.03559
	loss_reward_1: 0.00487
	loss_policy_2: 0.03785
	accuracy_policy_2: 0.65096
	loss_value_2: 0.0371
	loss_reward_2: 0.00472
	loss_policy_3: 0.04018
	accuracy_policy_3: 0.63129
	loss_value_3: 0.03837
	loss_reward_3: 0.00515
	loss_policy_4: 0.04301
	accuracy_policy_4: 0.61174
	loss_value_4: 0.03956
	loss_reward_4: 0.00551
	loss_policy_5: 0.04557
	accuracy_policy_5: 0.59436
	loss_value_5: 0.04063
	loss_reward_5: 0.00644
	loss_policy: 0.35754
	loss_value: 0.36193
	loss_reward: 0.02669
Optimization_Done 58200
[2024-05-08 18:53:09] [command] train weight_iter_58200.pkl 290 292
[2024-05-08 18:53:51] nn step 58300, lr: 0.1.
	loss_policy_0: 0.17239
	accuracy_policy_0: 0.67438
	loss_value_0: 0.17437
	loss_policy_1: 0.03854
	accuracy_policy_1: 0.64035
	loss_value_1: 0.03656
	loss_reward_1: 0.00541
	loss_policy_2: 0.0411
	accuracy_policy_2: 0.62082
	loss_value_2: 0.03787
	loss_reward_2: 0.00526
	loss_policy_3: 0.04358
	accuracy_policy_3: 0.60254
	loss_value_3: 0.03914
	loss_reward_3: 0.00566
	loss_policy_4: 0.04597
	accuracy_policy_4: 0.58482
	loss_value_4: 0.04042
	loss_reward_4: 0.00619
	loss_policy_5: 0.0486
	accuracy_policy_5: 0.56967
	loss_value_5: 0.04178
	loss_reward_5: 0.00709
	loss_policy: 0.39019
	loss_value: 0.37014
	loss_reward: 0.02962
[2024-05-08 18:54:31] nn step 58400, lr: 0.1.
	loss_policy_0: 0.1524
	accuracy_policy_0: 0.70713
	loss_value_0: 0.17031
	loss_policy_1: 0.03477
	accuracy_policy_1: 0.66879
	loss_value_1: 0.03562
	loss_reward_1: 0.00512
	loss_policy_2: 0.03721
	accuracy_policy_2: 0.64809
	loss_value_2: 0.037
	loss_reward_2: 0.00509
	loss_policy_3: 0.03983
	accuracy_policy_3: 0.63352
	loss_value_3: 0.03821
	loss_reward_3: 0.00549
	loss_policy_4: 0.04233
	accuracy_policy_4: 0.61393
	loss_value_4: 0.03943
	loss_reward_4: 0.0059
	loss_policy_5: 0.04521
	accuracy_policy_5: 0.59428
	loss_value_5: 0.04069
	loss_reward_5: 0.00687
	loss_policy: 0.35174
	loss_value: 0.36126
	loss_reward: 0.02846
Optimization_Done 58400
[2024-05-08 18:56:56] [command] train weight_iter_58400.pkl 291 293
[2024-05-08 18:57:37] nn step 58500, lr: 0.1.
	loss_policy_0: 0.1438
	accuracy_policy_0: 0.69104
	loss_value_0: 0.16726
	loss_policy_1: 0.03292
	accuracy_policy_1: 0.65361
	loss_value_1: 0.0347
	loss_reward_1: 0.0048
	loss_policy_2: 0.03532
	accuracy_policy_2: 0.63613
	loss_value_2: 0.03597
	loss_reward_2: 0.00475
	loss_policy_3: 0.03754
	accuracy_policy_3: 0.61861
	loss_value_3: 0.03694
	loss_reward_3: 0.00515
	loss_policy_4: 0.03992
	accuracy_policy_4: 0.60326
	loss_value_4: 0.03813
	loss_reward_4: 0.00549
	loss_policy_5: 0.04177
	accuracy_policy_5: 0.58594
	loss_value_5: 0.0392
	loss_reward_5: 0.00632
	loss_policy: 0.33127
	loss_value: 0.3522
	loss_reward: 0.02651
[2024-05-08 18:58:18] nn step 58600, lr: 0.1.
	loss_policy_0: 0.12063
	accuracy_policy_0: 0.73188
	loss_value_0: 0.15684
	loss_policy_1: 0.0288
	accuracy_policy_1: 0.68262
	loss_value_1: 0.03262
	loss_reward_1: 0.00459
	loss_policy_2: 0.03076
	accuracy_policy_2: 0.66463
	loss_value_2: 0.03375
	loss_reward_2: 0.00456
	loss_policy_3: 0.03328
	accuracy_policy_3: 0.64541
	loss_value_3: 0.03481
	loss_reward_3: 0.00493
	loss_policy_4: 0.03534
	accuracy_policy_4: 0.63033
	loss_value_4: 0.03584
	loss_reward_4: 0.00531
	loss_policy_5: 0.03778
	accuracy_policy_5: 0.60822
	loss_value_5: 0.03697
	loss_reward_5: 0.00611
	loss_policy: 0.2866
	loss_value: 0.33082
	loss_reward: 0.02549
Optimization_Done 58600
[2024-05-08 19:00:55] [command] train weight_iter_58600.pkl 292 294
[2024-05-08 19:01:36] nn step 58700, lr: 0.1.
	loss_policy_0: 0.14372
	accuracy_policy_0: 0.66453
	loss_value_0: 0.1481
	loss_policy_1: 0.03307
	accuracy_policy_1: 0.62047
	loss_value_1: 0.03086
	loss_reward_1: 0.00411
	loss_policy_2: 0.0357
	accuracy_policy_2: 0.59678
	loss_value_2: 0.03202
	loss_reward_2: 0.004
	loss_policy_3: 0.03779
	accuracy_policy_3: 0.58012
	loss_value_3: 0.03299
	loss_reward_3: 0.0044
	loss_policy_4: 0.0396
	accuracy_policy_4: 0.56477
	loss_value_4: 0.03398
	loss_reward_4: 0.00471
	loss_policy_5: 0.04133
	accuracy_policy_5: 0.55275
	loss_value_5: 0.03497
	loss_reward_5: 0.00556
	loss_policy: 0.33119
	loss_value: 0.31292
	loss_reward: 0.02279
[2024-05-08 19:02:17] nn step 58800, lr: 0.1.
	loss_policy_0: 0.13369
	accuracy_policy_0: 0.71348
	loss_value_0: 0.15713
	loss_policy_1: 0.03177
	accuracy_policy_1: 0.6651
	loss_value_1: 0.03285
	loss_reward_1: 0.00438
	loss_policy_2: 0.03433
	accuracy_policy_2: 0.64469
	loss_value_2: 0.03409
	loss_reward_2: 0.00442
	loss_policy_3: 0.03684
	accuracy_policy_3: 0.62547
	loss_value_3: 0.03523
	loss_reward_3: 0.00487
	loss_policy_4: 0.0391
	accuracy_policy_4: 0.60783
	loss_value_4: 0.03625
	loss_reward_4: 0.00521
	loss_policy_5: 0.04124
	accuracy_policy_5: 0.58867
	loss_value_5: 0.03743
	loss_reward_5: 0.00597
	loss_policy: 0.31696
	loss_value: 0.33298
	loss_reward: 0.02486
Optimization_Done 58800
[2024-05-08 19:05:02] [command] train weight_iter_58800.pkl 293 295
[2024-05-08 19:05:44] nn step 58900, lr: 0.1.
	loss_policy_0: 0.16548
	accuracy_policy_0: 0.64967
	loss_value_0: 0.1447
	loss_policy_1: 0.03726
	accuracy_policy_1: 0.61209
	loss_value_1: 0.03042
	loss_reward_1: 0.00411
	loss_policy_2: 0.03957
	accuracy_policy_2: 0.59316
	loss_value_2: 0.03158
	loss_reward_2: 0.00414
	loss_policy_3: 0.04191
	accuracy_policy_3: 0.57705
	loss_value_3: 0.03285
	loss_reward_3: 0.00458
	loss_policy_4: 0.04413
	accuracy_policy_4: 0.55963
	loss_value_4: 0.03383
	loss_reward_4: 0.0049
	loss_policy_5: 0.04651
	accuracy_policy_5: 0.54283
	loss_value_5: 0.03491
	loss_reward_5: 0.00549
	loss_policy: 0.37487
	loss_value: 0.30829
	loss_reward: 0.02321
[2024-05-08 19:06:24] nn step 59000, lr: 0.1.
	loss_policy_0: 0.1443
	accuracy_policy_0: 0.70572
	loss_value_0: 0.1467
	loss_policy_1: 0.0336
	accuracy_policy_1: 0.65334
	loss_value_1: 0.03061
	loss_reward_1: 0.00423
	loss_policy_2: 0.03623
	accuracy_policy_2: 0.63336
	loss_value_2: 0.03191
	loss_reward_2: 0.0041
	loss_policy_3: 0.03823
	accuracy_policy_3: 0.61643
	loss_value_3: 0.03303
	loss_reward_3: 0.00454
	loss_policy_4: 0.04068
	accuracy_policy_4: 0.60354
	loss_value_4: 0.0341
	loss_reward_4: 0.00477
	loss_policy_5: 0.04315
	accuracy_policy_5: 0.58369
	loss_value_5: 0.03513
	loss_reward_5: 0.00557
	loss_policy: 0.33619
	loss_value: 0.31147
	loss_reward: 0.02321
Optimization_Done 59000
[2024-05-08 19:08:55] [command] train weight_iter_59000.pkl 294 296
[2024-05-08 19:09:37] nn step 59100, lr: 0.1.
	loss_policy_0: 0.16998
	accuracy_policy_0: 0.67141
	loss_value_0: 0.16439
	loss_policy_1: 0.03772
	accuracy_policy_1: 0.64023
	loss_value_1: 0.03419
	loss_reward_1: 0.00506
	loss_policy_2: 0.03978
	accuracy_policy_2: 0.62266
	loss_value_2: 0.03559
	loss_reward_2: 0.00504
	loss_policy_3: 0.04228
	accuracy_policy_3: 0.6058
	loss_value_3: 0.03701
	loss_reward_3: 0.00545
	loss_policy_4: 0.04493
	accuracy_policy_4: 0.59072
	loss_value_4: 0.03819
	loss_reward_4: 0.0059
	loss_policy_5: 0.04701
	accuracy_policy_5: 0.57699
	loss_value_5: 0.0394
	loss_reward_5: 0.00692
	loss_policy: 0.3817
	loss_value: 0.34878
	loss_reward: 0.02836
[2024-05-08 19:10:18] nn step 59200, lr: 0.1.
	loss_policy_0: 0.15918
	accuracy_policy_0: 0.70979
	loss_value_0: 0.16882
	loss_policy_1: 0.03617
	accuracy_policy_1: 0.67
	loss_value_1: 0.03545
	loss_reward_1: 0.00529
	loss_policy_2: 0.03854
	accuracy_policy_2: 0.65311
	loss_value_2: 0.03678
	loss_reward_2: 0.00523
	loss_policy_3: 0.04128
	accuracy_policy_3: 0.63637
	loss_value_3: 0.03822
	loss_reward_3: 0.00573
	loss_policy_4: 0.04424
	accuracy_policy_4: 0.61795
	loss_value_4: 0.03958
	loss_reward_4: 0.00611
	loss_policy_5: 0.04678
	accuracy_policy_5: 0.59689
	loss_value_5: 0.04082
	loss_reward_5: 0.00694
	loss_policy: 0.36619
	loss_value: 0.35967
	loss_reward: 0.0293
Optimization_Done 59200
[2024-05-08 19:12:54] [command] train weight_iter_59200.pkl 295 297
[2024-05-08 19:13:36] nn step 59300, lr: 0.1.
	loss_policy_0: 0.16213
	accuracy_policy_0: 0.68658
	loss_value_0: 0.171
	loss_policy_1: 0.0368
	accuracy_policy_1: 0.65115
	loss_value_1: 0.0357
	loss_reward_1: 0.0054
	loss_policy_2: 0.03884
	accuracy_policy_2: 0.63535
	loss_value_2: 0.03713
	loss_reward_2: 0.00528
	loss_policy_3: 0.04152
	accuracy_policy_3: 0.61934
	loss_value_3: 0.03843
	loss_reward_3: 0.0056
	loss_policy_4: 0.04367
	accuracy_policy_4: 0.59814
	loss_value_4: 0.03958
	loss_reward_4: 0.00589
	loss_policy_5: 0.04573
	accuracy_policy_5: 0.58582
	loss_value_5: 0.04079
	loss_reward_5: 0.00694
	loss_policy: 0.3687
	loss_value: 0.36263
	loss_reward: 0.02911
[2024-05-08 19:14:17] nn step 59400, lr: 0.1.
	loss_policy_0: 0.13931
	accuracy_policy_0: 0.72141
	loss_value_0: 0.16152
	loss_policy_1: 0.03232
	accuracy_policy_1: 0.68021
	loss_value_1: 0.03379
	loss_reward_1: 0.00511
	loss_policy_2: 0.0348
	accuracy_policy_2: 0.66318
	loss_value_2: 0.03504
	loss_reward_2: 0.00501
	loss_policy_3: 0.03691
	accuracy_policy_3: 0.64273
	loss_value_3: 0.03621
	loss_reward_3: 0.00546
	loss_policy_4: 0.03921
	accuracy_policy_4: 0.62566
	loss_value_4: 0.03739
	loss_reward_4: 0.00584
	loss_policy_5: 0.04189
	accuracy_policy_5: 0.60822
	loss_value_5: 0.03852
	loss_reward_5: 0.00683
	loss_policy: 0.32445
	loss_value: 0.34247
	loss_reward: 0.02825
Optimization_Done 59400
[2024-05-08 19:16:43] [command] train weight_iter_59400.pkl 296 298
[2024-05-08 19:17:25] nn step 59500, lr: 0.1.
	loss_policy_0: 0.15284
	accuracy_policy_0: 0.69596
	loss_value_0: 0.17276
	loss_policy_1: 0.03507
	accuracy_policy_1: 0.65562
	loss_value_1: 0.03599
	loss_reward_1: 0.00499
	loss_policy_2: 0.03706
	accuracy_policy_2: 0.6366
	loss_value_2: 0.03734
	loss_reward_2: 0.00491
	loss_policy_3: 0.0396
	accuracy_policy_3: 0.61922
	loss_value_3: 0.03842
	loss_reward_3: 0.00528
	loss_policy_4: 0.04168
	accuracy_policy_4: 0.60562
	loss_value_4: 0.03945
	loss_reward_4: 0.00566
	loss_policy_5: 0.04384
	accuracy_policy_5: 0.58682
	loss_value_5: 0.04039
	loss_reward_5: 0.00668
	loss_policy: 0.35009
	loss_value: 0.36435
	loss_reward: 0.02753
[2024-05-08 19:18:05] nn step 59600, lr: 0.1.
	loss_policy_0: 0.12592
	accuracy_policy_0: 0.73723
	loss_value_0: 0.16404
	loss_policy_1: 0.03001
	accuracy_policy_1: 0.68789
	loss_value_1: 0.03415
	loss_reward_1: 0.00484
	loss_policy_2: 0.03223
	accuracy_policy_2: 0.67264
	loss_value_2: 0.03519
	loss_reward_2: 0.00463
	loss_policy_3: 0.03453
	accuracy_policy_3: 0.65398
	loss_value_3: 0.03643
	loss_reward_3: 0.00496
	loss_policy_4: 0.03644
	accuracy_policy_4: 0.64064
	loss_value_4: 0.03742
	loss_reward_4: 0.00552
	loss_policy_5: 0.03849
	accuracy_policy_5: 0.62094
	loss_value_5: 0.03838
	loss_reward_5: 0.0063
	loss_policy: 0.29762
	loss_value: 0.34562
	loss_reward: 0.02624
Optimization_Done 59600
[2024-05-08 19:20:41] [command] train weight_iter_59600.pkl 297 299
[2024-05-08 19:21:23] nn step 59700, lr: 0.1.
	loss_policy_0: 0.15968
	accuracy_policy_0: 0.67445
	loss_value_0: 0.16329
	loss_policy_1: 0.03717
	accuracy_policy_1: 0.63406
	loss_value_1: 0.0341
	loss_reward_1: 0.00462
	loss_policy_2: 0.03951
	accuracy_policy_2: 0.6159
	loss_value_2: 0.03555
	loss_reward_2: 0.00455
	loss_policy_3: 0.04213
	accuracy_policy_3: 0.59543
	loss_value_3: 0.03678
	loss_reward_3: 0.00505
	loss_policy_4: 0.04486
	accuracy_policy_4: 0.57578
	loss_value_4: 0.03798
	loss_reward_4: 0.00525
	loss_policy_5: 0.04746
	accuracy_policy_5: 0.55494
	loss_value_5: 0.0391
	loss_reward_5: 0.00623
	loss_policy: 0.37081
	loss_value: 0.3468
	loss_reward: 0.02569
[2024-05-08 19:22:04] nn step 59800, lr: 0.1.
	loss_policy_0: 0.13869
	accuracy_policy_0: 0.71463
	loss_value_0: 0.15955
	loss_policy_1: 0.03291
	accuracy_policy_1: 0.66789
	loss_value_1: 0.03337
	loss_reward_1: 0.00452
	loss_policy_2: 0.03566
	accuracy_policy_2: 0.64877
	loss_value_2: 0.03469
	loss_reward_2: 0.00438
	loss_policy_3: 0.03829
	accuracy_policy_3: 0.63145
	loss_value_3: 0.03581
	loss_reward_3: 0.00477
	loss_policy_4: 0.04061
	accuracy_policy_4: 0.61098
	loss_value_4: 0.03696
	loss_reward_4: 0.00515
	loss_policy_5: 0.04333
	accuracy_policy_5: 0.59041
	loss_value_5: 0.03805
	loss_reward_5: 0.00619
	loss_policy: 0.32948
	loss_value: 0.33843
	loss_reward: 0.025
Optimization_Done 59800
[2024-05-08 19:24:14] [command] train weight_iter_59800.pkl 298 300
[2024-05-08 19:24:56] nn step 59900, lr: 0.1.
	loss_policy_0: 0.16455
	accuracy_policy_0: 0.66857
	loss_value_0: 0.1619
	loss_policy_1: 0.03708
	accuracy_policy_1: 0.63309
	loss_value_1: 0.03392
	loss_reward_1: 0.00447
	loss_policy_2: 0.03968
	accuracy_policy_2: 0.61326
	loss_value_2: 0.03526
	loss_reward_2: 0.00451
	loss_policy_3: 0.04294
	accuracy_policy_3: 0.59045
	loss_value_3: 0.03648
	loss_reward_3: 0.00476
	loss_policy_4: 0.04528
	accuracy_policy_4: 0.57404
	loss_value_4: 0.03768
	loss_reward_4: 0.00523
	loss_policy_5: 0.04783
	accuracy_policy_5: 0.55523
	loss_value_5: 0.03884
	loss_reward_5: 0.00599
	loss_policy: 0.37737
	loss_value: 0.34406
	loss_reward: 0.02495
[2024-05-08 19:25:36] nn step 60000, lr: 0.1.
	loss_policy_0: 0.15185
	accuracy_policy_0: 0.70875
	loss_value_0: 0.16898
	loss_policy_1: 0.03548
	accuracy_policy_1: 0.6658
	loss_value_1: 0.03547
	loss_reward_1: 0.00481
	loss_policy_2: 0.03841
	accuracy_policy_2: 0.63861
	loss_value_2: 0.03693
	loss_reward_2: 0.00458
	loss_policy_3: 0.04143
	accuracy_policy_3: 0.61971
	loss_value_3: 0.03815
	loss_reward_3: 0.00501
	loss_policy_4: 0.04424
	accuracy_policy_4: 0.60107
	loss_value_4: 0.03936
	loss_reward_4: 0.00535
	loss_policy_5: 0.047
	accuracy_policy_5: 0.5858
	loss_value_5: 0.04046
	loss_reward_5: 0.00641
	loss_policy: 0.35841
	loss_value: 0.35936
	loss_reward: 0.02616
Optimization_Done 60000
