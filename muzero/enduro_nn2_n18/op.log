A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 08:32:18] [command] train weight_iter_0.pkl 1 1
[2024-05-04 08:32:39] nn step 50, lr: 0.1.
	loss_policy_0: 0.08801
	accuracy_policy_0: 0.69969
	loss_value_0: 0.50378
	loss_policy_1: 0.02248
	accuracy_policy_1: 0.63359
	loss_value_1: 0.09111
	loss_reward_1: 0.02272
	loss_policy_2: 0.02411
	accuracy_policy_2: 0.58359
	loss_value_2: 0.09155
	loss_reward_2: 0.02254
	loss_policy_3: 0.0252
	accuracy_policy_3: 0.56074
	loss_value_3: 0.09179
	loss_reward_3: 0.02254
	loss_policy_4: 0.02559
	accuracy_policy_4: 0.54797
	loss_value_4: 0.09152
	loss_reward_4: 0.02259
	loss_policy_5: 0.02636
	accuracy_policy_5: 0.53914
	loss_value_5: 0.09162
	loss_reward_5: 0.02266
	loss_policy: 0.21176
	loss_value: 0.96138
	loss_reward: 0.11305
[2024-05-04 08:32:54] nn step 100, lr: 0.1.
	loss_policy_0: 0.01167
	accuracy_policy_0: 0.88809
	loss_value_0: 0.23933
	loss_policy_1: 0.00417
	accuracy_policy_1: 0.87812
	loss_value_1: 0.048
	loss_reward_1: 0.0
	loss_policy_2: 0.00449
	accuracy_policy_2: 0.87531
	loss_value_2: 0.048
	loss_reward_2: 0.0
	loss_policy_3: 0.00551
	accuracy_policy_3: 0.8741
	loss_value_3: 0.04802
	loss_reward_3: 0.0
	loss_policy_4: 0.00559
	accuracy_policy_4: 0.8557
	loss_value_4: 0.04809
	loss_reward_4: 0.0
	loss_policy_5: 0.00625
	accuracy_policy_5: 0.83578
	loss_value_5: 0.04808
	loss_reward_5: 0.0
	loss_policy: 0.03767
	loss_value: 0.47952
	loss_reward: 0.0
[2024-05-04 08:33:10] nn step 150, lr: 0.1.
	loss_policy_0: 0.00882
	accuracy_policy_0: 0.89879
	loss_value_0: 0.25925
	loss_policy_1: 0.00305
	accuracy_policy_1: 0.89758
	loss_value_1: 0.05187
	loss_reward_1: 0.0
	loss_policy_2: 0.00258
	accuracy_policy_2: 0.89066
	loss_value_2: 0.05189
	loss_reward_2: 0.0
	loss_policy_3: 0.00297
	accuracy_policy_3: 0.89621
	loss_value_3: 0.05191
	loss_reward_3: 0.0
	loss_policy_4: 0.00269
	accuracy_policy_4: 0.89727
	loss_value_4: 0.05192
	loss_reward_4: 0.0
	loss_policy_5: 0.00316
	accuracy_policy_5: 0.8923
	loss_value_5: 0.05195
	loss_reward_5: 0.0
	loss_policy: 0.02327
	loss_value: 0.51879
	loss_reward: 0.0
[2024-05-04 08:33:25] nn step 200, lr: 0.1.
	loss_policy_0: 0.00805
	accuracy_policy_0: 0.89969
	loss_value_0: 0.25084
	loss_policy_1: 0.00263
	accuracy_policy_1: 0.9027
	loss_value_1: 0.05017
	loss_reward_1: 0.0
	loss_policy_2: 0.00237
	accuracy_policy_2: 0.89508
	loss_value_2: 0.05016
	loss_reward_2: 0.0
	loss_policy_3: 0.00256
	accuracy_policy_3: 0.89359
	loss_value_3: 0.05014
	loss_reward_3: 0.0
	loss_policy_4: 0.0023
	accuracy_policy_4: 0.89902
	loss_value_4: 0.05013
	loss_reward_4: 0.0
	loss_policy_5: 0.00258
	accuracy_policy_5: 0.89598
	loss_value_5: 0.05014
	loss_reward_5: 0.0
	loss_policy: 0.02048
	loss_value: 0.50158
	loss_reward: 0.0
Optimization_Done 200
[2024-05-04 08:35:23] [command] train weight_iter_200.pkl 1 2
[2024-05-04 08:35:40] nn step 250, lr: 0.1.
	loss_policy_0: 0.05243
	accuracy_policy_0: 0.7707
	loss_value_0: 0.21797
	loss_policy_1: 0.00969
	accuracy_policy_1: 0.77727
	loss_value_1: 0.04352
	loss_reward_1: 0.0
	loss_policy_2: 0.0086
	accuracy_policy_2: 0.78141
	loss_value_2: 0.04347
	loss_reward_2: 0.0
	loss_policy_3: 0.00869
	accuracy_policy_3: 0.75863
	loss_value_3: 0.04344
	loss_reward_3: 0.0
	loss_policy_4: 0.00951
	accuracy_policy_4: 0.73988
	loss_value_4: 0.04341
	loss_reward_4: 0.0
	loss_policy_5: 0.00999
	accuracy_policy_5: 0.72461
	loss_value_5: 0.04339
	loss_reward_5: 0.0
	loss_policy: 0.0989
	loss_value: 0.43519
	loss_reward: 0.0
[2024-05-04 08:35:55] nn step 300, lr: 0.1.
	loss_policy_0: 0.01167
	accuracy_policy_0: 0.90141
	loss_value_0: 0.21545
	loss_policy_1: 0.00363
	accuracy_policy_1: 0.90434
	loss_value_1: 0.04309
	loss_reward_1: 0.0
	loss_policy_2: 0.00318
	accuracy_policy_2: 0.90609
	loss_value_2: 0.04303
	loss_reward_2: 0.0
	loss_policy_3: 0.00357
	accuracy_policy_3: 0.89469
	loss_value_3: 0.04299
	loss_reward_3: 0.0
	loss_policy_4: 0.00352
	accuracy_policy_4: 0.8907
	loss_value_4: 0.04295
	loss_reward_4: 0.0
	loss_policy_5: 0.00355
	accuracy_policy_5: 0.88988
	loss_value_5: 0.04292
	loss_reward_5: 0.0
	loss_policy: 0.02912
	loss_value: 0.43044
	loss_reward: 0.0
[2024-05-04 08:36:10] nn step 350, lr: 0.1.
	loss_policy_0: 0.00982
	accuracy_policy_0: 0.91109
	loss_value_0: 0.2217
	loss_policy_1: 0.00328
	accuracy_policy_1: 0.90828
	loss_value_1: 0.04433
	loss_reward_1: 0.0
	loss_policy_2: 0.00301
	accuracy_policy_2: 0.90816
	loss_value_2: 0.04426
	loss_reward_2: 0.0
	loss_policy_3: 0.00335
	accuracy_policy_3: 0.9007
	loss_value_3: 0.04421
	loss_reward_3: 0.0
	loss_policy_4: 0.00326
	accuracy_policy_4: 0.89723
	loss_value_4: 0.04417
	loss_reward_4: 0.0
	loss_policy_5: 0.00352
	accuracy_policy_5: 0.89031
	loss_value_5: 0.04413
	loss_reward_5: 0.0
	loss_policy: 0.02624
	loss_value: 0.44281
	loss_reward: 0.0
[2024-05-04 08:36:26] nn step 400, lr: 0.1.
	loss_policy_0: 0.00682
	accuracy_policy_0: 0.91766
	loss_value_0: 0.2117
	loss_policy_1: 0.00271
	accuracy_policy_1: 0.91477
	loss_value_1: 0.04232
	loss_reward_1: 0.0
	loss_policy_2: 0.00242
	accuracy_policy_2: 0.91516
	loss_value_2: 0.04226
	loss_reward_2: 0.0
	loss_policy_3: 0.00282
	accuracy_policy_3: 0.9057
	loss_value_3: 0.04221
	loss_reward_3: 0.0
	loss_policy_4: 0.00271
	accuracy_policy_4: 0.90645
	loss_value_4: 0.04215
	loss_reward_4: 0.0
	loss_policy_5: 0.00291
	accuracy_policy_5: 0.90328
	loss_value_5: 0.04211
	loss_reward_5: 0.0
	loss_policy: 0.0204
	loss_value: 0.42275
	loss_reward: 0.0
Optimization_Done 400
[2024-05-04 08:38:26] [command] train weight_iter_400.pkl 1 3
[2024-05-04 08:38:42] nn step 450, lr: 0.1.
	loss_policy_0: 0.01152
	accuracy_policy_0: 0.89031
	loss_value_0: 0.21466
	loss_policy_1: 0.00411
	accuracy_policy_1: 0.88406
	loss_value_1: 0.0429
	loss_reward_1: 0.00196
	loss_policy_2: 0.00422
	accuracy_policy_2: 0.87895
	loss_value_2: 0.04274
	loss_reward_2: 0.00158
	loss_policy_3: 0.00497
	accuracy_policy_3: 0.86031
	loss_value_3: 0.04264
	loss_reward_3: 0.00179
	loss_policy_4: 0.00505
	accuracy_policy_4: 0.8584
	loss_value_4: 0.04253
	loss_reward_4: 0.00166
	loss_policy_5: 0.00556
	accuracy_policy_5: 0.83117
	loss_value_5: 0.04239
	loss_reward_5: 0.0014
	loss_policy: 0.03542
	loss_value: 0.42787
	loss_reward: 0.0084
[2024-05-04 08:38:58] nn step 500, lr: 0.1.
	loss_policy_0: 0.00572
	accuracy_policy_0: 0.92586
	loss_value_0: 0.20306
	loss_policy_1: 0.00278
	accuracy_policy_1: 0.89266
	loss_value_1: 0.04054
	loss_reward_1: 0.00092
	loss_policy_2: 0.00271
	accuracy_policy_2: 0.895
	loss_value_2: 0.04044
	loss_reward_2: 0.00084
	loss_policy_3: 0.00332
	accuracy_policy_3: 0.88012
	loss_value_3: 0.04032
	loss_reward_3: 0.00093
	loss_policy_4: 0.00324
	accuracy_policy_4: 0.87719
	loss_value_4: 0.04018
	loss_reward_4: 0.00093
	loss_policy_5: 0.00368
	accuracy_policy_5: 0.86094
	loss_value_5: 0.0401
	loss_reward_5: 0.00081
	loss_policy: 0.02146
	loss_value: 0.40464
	loss_reward: 0.00443
[2024-05-04 08:39:13] nn step 550, lr: 0.1.
	loss_policy_0: 0.00523
	accuracy_policy_0: 0.93258
	loss_value_0: 0.19622
	loss_policy_1: 0.0023
	accuracy_policy_1: 0.89926
	loss_value_1: 0.03916
	loss_reward_1: 0.00087
	loss_policy_2: 0.00233
	accuracy_policy_2: 0.89215
	loss_value_2: 0.03905
	loss_reward_2: 0.00077
	loss_policy_3: 0.00275
	accuracy_policy_3: 0.88945
	loss_value_3: 0.03894
	loss_reward_3: 0.00093
	loss_policy_4: 0.00271
	accuracy_policy_4: 0.89066
	loss_value_4: 0.0388
	loss_reward_4: 0.00092
	loss_policy_5: 0.00311
	accuracy_policy_5: 0.86836
	loss_value_5: 0.03866
	loss_reward_5: 0.00086
	loss_policy: 0.01844
	loss_value: 0.39083
	loss_reward: 0.00434
[2024-05-04 08:39:29] nn step 600, lr: 0.1.
	loss_policy_0: 0.00558
	accuracy_policy_0: 0.9293
	loss_value_0: 0.20068
	loss_policy_1: 0.00223
	accuracy_policy_1: 0.90164
	loss_value_1: 0.04006
	loss_reward_1: 0.00096
	loss_policy_2: 0.00237
	accuracy_policy_2: 0.89922
	loss_value_2: 0.03992
	loss_reward_2: 0.00087
	loss_policy_3: 0.00287
	accuracy_policy_3: 0.8841
	loss_value_3: 0.03979
	loss_reward_3: 0.00091
	loss_policy_4: 0.00287
	accuracy_policy_4: 0.88191
	loss_value_4: 0.03965
	loss_reward_4: 0.0009
	loss_policy_5: 0.00318
	accuracy_policy_5: 0.86492
	loss_value_5: 0.03952
	loss_reward_5: 0.00076
	loss_policy: 0.0191
	loss_value: 0.39963
	loss_reward: 0.0044
Optimization_Done 600
[2024-05-04 08:41:26] [command] train weight_iter_600.pkl 1 4
[2024-05-04 08:41:43] nn step 650, lr: 0.1.
	loss_policy_0: 0.01382
	accuracy_policy_0: 0.92895
	loss_value_0: 0.21289
	loss_policy_1: 0.00385
	accuracy_policy_1: 0.91559
	loss_value_1: 0.0425
	loss_reward_1: 0.00087
	loss_policy_2: 0.00376
	accuracy_policy_2: 0.91637
	loss_value_2: 0.04234
	loss_reward_2: 0.00086
	loss_policy_3: 0.00418
	accuracy_policy_3: 0.9018
	loss_value_3: 0.04223
	loss_reward_3: 0.00081
	loss_policy_4: 0.00411
	accuracy_policy_4: 0.90258
	loss_value_4: 0.04211
	loss_reward_4: 0.00082
	loss_policy_5: 0.00456
	accuracy_policy_5: 0.89023
	loss_value_5: 0.04192
	loss_reward_5: 0.00087
	loss_policy: 0.03429
	loss_value: 0.42399
	loss_reward: 0.00423
[2024-05-04 08:41:59] nn step 700, lr: 0.1.
	loss_policy_0: 0.01035
	accuracy_policy_0: 0.94582
	loss_value_0: 0.20656
	loss_policy_1: 0.00289
	accuracy_policy_1: 0.92648
	loss_value_1: 0.04122
	loss_reward_1: 0.0008
	loss_policy_2: 0.00286
	accuracy_policy_2: 0.92113
	loss_value_2: 0.0411
	loss_reward_2: 0.00072
	loss_policy_3: 0.00307
	accuracy_policy_3: 0.91582
	loss_value_3: 0.04098
	loss_reward_3: 0.00079
	loss_policy_4: 0.00302
	accuracy_policy_4: 0.91426
	loss_value_4: 0.04083
	loss_reward_4: 0.00083
	loss_policy_5: 0.00331
	accuracy_policy_5: 0.90082
	loss_value_5: 0.04069
	loss_reward_5: 0.00079
	loss_policy: 0.0255
	loss_value: 0.41138
	loss_reward: 0.00393
[2024-05-04 08:42:14] nn step 750, lr: 0.1.
	loss_policy_0: 0.01001
	accuracy_policy_0: 0.94309
	loss_value_0: 0.19359
	loss_policy_1: 0.00261
	accuracy_policy_1: 0.92254
	loss_value_1: 0.03862
	loss_reward_1: 0.00073
	loss_policy_2: 0.00261
	accuracy_policy_2: 0.92184
	loss_value_2: 0.0385
	loss_reward_2: 0.00077
	loss_policy_3: 0.00282
	accuracy_policy_3: 0.91414
	loss_value_3: 0.03838
	loss_reward_3: 0.00077
	loss_policy_4: 0.00276
	accuracy_policy_4: 0.91184
	loss_value_4: 0.03822
	loss_reward_4: 0.00076
	loss_policy_5: 0.00304
	accuracy_policy_5: 0.89898
	loss_value_5: 0.03807
	loss_reward_5: 0.00074
	loss_policy: 0.02385
	loss_value: 0.38538
	loss_reward: 0.00376
[2024-05-04 08:42:29] nn step 800, lr: 0.1.
	loss_policy_0: 0.00925
	accuracy_policy_0: 0.94867
	loss_value_0: 0.19177
	loss_policy_1: 0.0024
	accuracy_policy_1: 0.92719
	loss_value_1: 0.03828
	loss_reward_1: 0.00067
	loss_policy_2: 0.00242
	accuracy_policy_2: 0.92566
	loss_value_2: 0.03814
	loss_reward_2: 0.00073
	loss_policy_3: 0.0026
	accuracy_policy_3: 0.91734
	loss_value_3: 0.03801
	loss_reward_3: 0.00072
	loss_policy_4: 0.00259
	accuracy_policy_4: 0.9168
	loss_value_4: 0.03789
	loss_reward_4: 0.00063
	loss_policy_5: 0.00271
	accuracy_policy_5: 0.90566
	loss_value_5: 0.03776
	loss_reward_5: 0.00066
	loss_policy: 0.02197
	loss_value: 0.38186
	loss_reward: 0.00341
Optimization_Done 800
[2024-05-04 08:44:19] [command] train weight_iter_800.pkl 1 5
[2024-05-04 08:44:37] nn step 850, lr: 0.1.
	loss_policy_0: 0.01788
	accuracy_policy_0: 0.89609
	loss_value_0: 0.19019
	loss_policy_1: 0.00572
	accuracy_policy_1: 0.89289
	loss_value_1: 0.03798
	loss_reward_1: 0.00061
	loss_policy_2: 0.00569
	accuracy_policy_2: 0.89234
	loss_value_2: 0.03784
	loss_reward_2: 0.0005
	loss_policy_3: 0.00675
	accuracy_policy_3: 0.8832
	loss_value_3: 0.03775
	loss_reward_3: 0.00053
	loss_policy_4: 0.00706
	accuracy_policy_4: 0.87113
	loss_value_4: 0.03763
	loss_reward_4: 0.00052
	loss_policy_5: 0.00803
	accuracy_policy_5: 0.86004
	loss_value_5: 0.03747
	loss_reward_5: 0.00068
	loss_policy: 0.05113
	loss_value: 0.37886
	loss_reward: 0.00284
[2024-05-04 08:44:52] nn step 900, lr: 0.1.
	loss_policy_0: 0.01472
	accuracy_policy_0: 0.91656
	loss_value_0: 0.18405
	loss_policy_1: 0.00462
	accuracy_policy_1: 0.90574
	loss_value_1: 0.03676
	loss_reward_1: 0.00049
	loss_policy_2: 0.00455
	accuracy_policy_2: 0.89949
	loss_value_2: 0.03664
	loss_reward_2: 0.00051
	loss_policy_3: 0.00529
	accuracy_policy_3: 0.89203
	loss_value_3: 0.03653
	loss_reward_3: 0.00055
	loss_policy_4: 0.00533
	accuracy_policy_4: 0.88273
	loss_value_4: 0.03642
	loss_reward_4: 0.0005
	loss_policy_5: 0.00584
	accuracy_policy_5: 0.87152
	loss_value_5: 0.0363
	loss_reward_5: 0.00055
	loss_policy: 0.04034
	loss_value: 0.36669
	loss_reward: 0.00261
[2024-05-04 08:45:08] nn step 950, lr: 0.1.
	loss_policy_0: 0.01262
	accuracy_policy_0: 0.92719
	loss_value_0: 0.17782
	loss_policy_1: 0.00401
	accuracy_policy_1: 0.91488
	loss_value_1: 0.0355
	loss_reward_1: 0.0005
	loss_policy_2: 0.00392
	accuracy_policy_2: 0.90797
	loss_value_2: 0.03537
	loss_reward_2: 0.00051
	loss_policy_3: 0.00455
	accuracy_policy_3: 0.89766
	loss_value_3: 0.03527
	loss_reward_3: 0.00049
	loss_policy_4: 0.00442
	accuracy_policy_4: 0.88773
	loss_value_4: 0.03516
	loss_reward_4: 0.00052
	loss_policy_5: 0.00487
	accuracy_policy_5: 0.88008
	loss_value_5: 0.03503
	loss_reward_5: 0.00056
	loss_policy: 0.0344
	loss_value: 0.35415
	loss_reward: 0.00258
[2024-05-04 08:45:23] nn step 1000, lr: 0.1.
	loss_policy_0: 0.01236
	accuracy_policy_0: 0.92562
	loss_value_0: 0.17548
	loss_policy_1: 0.00376
	accuracy_policy_1: 0.91355
	loss_value_1: 0.03506
	loss_reward_1: 0.00051
	loss_policy_2: 0.00385
	accuracy_policy_2: 0.90543
	loss_value_2: 0.03496
	loss_reward_2: 0.00053
	loss_policy_3: 0.00417
	accuracy_policy_3: 0.90004
	loss_value_3: 0.03483
	loss_reward_3: 0.00049
	loss_policy_4: 0.00426
	accuracy_policy_4: 0.8852
	loss_value_4: 0.0347
	loss_reward_4: 0.0005
	loss_policy_5: 0.00449
	accuracy_policy_5: 0.8775
	loss_value_5: 0.03458
	loss_reward_5: 0.00048
	loss_policy: 0.03288
	loss_value: 0.34961
	loss_reward: 0.00251
Optimization_Done 1000
[2024-05-04 08:47:21] [command] train weight_iter_1000.pkl 2 6
[2024-05-04 08:47:40] nn step 1050, lr: 0.1.
	loss_policy_0: 0.01615
	accuracy_policy_0: 0.9298
	loss_value_0: 0.18411
	loss_policy_1: 0.00513
	accuracy_policy_1: 0.9118
	loss_value_1: 0.03679
	loss_reward_1: 0.00049
	loss_policy_2: 0.00532
	accuracy_policy_2: 0.89141
	loss_value_2: 0.03668
	loss_reward_2: 0.00049
	loss_policy_3: 0.00585
	accuracy_policy_3: 0.88805
	loss_value_3: 0.03654
	loss_reward_3: 0.00049
	loss_policy_4: 0.00594
	accuracy_policy_4: 0.8775
	loss_value_4: 0.03641
	loss_reward_4: 0.00054
	loss_policy_5: 0.00661
	accuracy_policy_5: 0.86988
	loss_value_5: 0.03629
	loss_reward_5: 0.00047
	loss_policy: 0.04499
	loss_value: 0.36682
	loss_reward: 0.00248
[2024-05-04 08:47:56] nn step 1100, lr: 0.1.
	loss_policy_0: 0.01338
	accuracy_policy_0: 0.93996
	loss_value_0: 0.17847
	loss_policy_1: 0.00385
	accuracy_policy_1: 0.91648
	loss_value_1: 0.03563
	loss_reward_1: 0.00046
	loss_policy_2: 0.00439
	accuracy_policy_2: 0.90246
	loss_value_2: 0.03552
	loss_reward_2: 0.00047
	loss_policy_3: 0.00454
	accuracy_policy_3: 0.89805
	loss_value_3: 0.03539
	loss_reward_3: 0.00046
	loss_policy_4: 0.00467
	accuracy_policy_4: 0.88859
	loss_value_4: 0.03526
	loss_reward_4: 0.00048
	loss_policy_5: 0.00483
	accuracy_policy_5: 0.8827
	loss_value_5: 0.03514
	loss_reward_5: 0.00046
	loss_policy: 0.03565
	loss_value: 0.35542
	loss_reward: 0.00232
[2024-05-04 08:48:11] nn step 1150, lr: 0.1.
	loss_policy_0: 0.01274
	accuracy_policy_0: 0.94418
	loss_value_0: 0.17932
	loss_policy_1: 0.0035
	accuracy_policy_1: 0.92016
	loss_value_1: 0.0358
	loss_reward_1: 0.00049
	loss_policy_2: 0.00414
	accuracy_policy_2: 0.90926
	loss_value_2: 0.03568
	loss_reward_2: 0.00047
	loss_policy_3: 0.00413
	accuracy_policy_3: 0.90492
	loss_value_3: 0.03556
	loss_reward_3: 0.00046
	loss_policy_4: 0.00438
	accuracy_policy_4: 0.88984
	loss_value_4: 0.03543
	loss_reward_4: 0.00048
	loss_policy_5: 0.00452
	accuracy_policy_5: 0.88395
	loss_value_5: 0.03529
	loss_reward_5: 0.00048
	loss_policy: 0.03341
	loss_value: 0.35708
	loss_reward: 0.00238
[2024-05-04 08:48:26] nn step 1200, lr: 0.1.
	loss_policy_0: 0.01231
	accuracy_policy_0: 0.94332
	loss_value_0: 0.17444
	loss_policy_1: 0.00332
	accuracy_policy_1: 0.92277
	loss_value_1: 0.03485
	loss_reward_1: 0.00048
	loss_policy_2: 0.00394
	accuracy_policy_2: 0.91047
	loss_value_2: 0.03471
	loss_reward_2: 0.0005
	loss_policy_3: 0.00383
	accuracy_policy_3: 0.90441
	loss_value_3: 0.0346
	loss_reward_3: 0.00039
	loss_policy_4: 0.00403
	accuracy_policy_4: 0.8968
	loss_value_4: 0.03446
	loss_reward_4: 0.00053
	loss_policy_5: 0.00411
	accuracy_policy_5: 0.89207
	loss_value_5: 0.03433
	loss_reward_5: 0.00047
	loss_policy: 0.03153
	loss_value: 0.34739
	loss_reward: 0.00237
Optimization_Done 1200
[2024-05-04 08:50:25] [command] train weight_iter_1200.pkl 3 7
[2024-05-04 08:50:41] nn step 1250, lr: 0.1.
	loss_policy_0: 0.02835
	accuracy_policy_0: 0.90762
	loss_value_0: 0.18537
	loss_policy_1: 0.00681
	accuracy_policy_1: 0.8827
	loss_value_1: 0.03703
	loss_reward_1: 0.00049
	loss_policy_2: 0.00708
	accuracy_policy_2: 0.87766
	loss_value_2: 0.03686
	loss_reward_2: 0.0005
	loss_policy_3: 0.00719
	accuracy_policy_3: 0.86395
	loss_value_3: 0.03671
	loss_reward_3: 0.00057
	loss_policy_4: 0.00733
	accuracy_policy_4: 0.86422
	loss_value_4: 0.03657
	loss_reward_4: 0.00048
	loss_policy_5: 0.00783
	accuracy_policy_5: 0.83875
	loss_value_5: 0.03641
	loss_reward_5: 0.0005
	loss_policy: 0.06459
	loss_value: 0.36894
	loss_reward: 0.00254
[2024-05-04 08:50:57] nn step 1300, lr: 0.1.
	loss_policy_0: 0.02147
	accuracy_policy_0: 0.91012
	loss_value_0: 0.1651
	loss_policy_1: 0.00503
	accuracy_policy_1: 0.88262
	loss_value_1: 0.03297
	loss_reward_1: 0.00045
	loss_policy_2: 0.00532
	accuracy_policy_2: 0.88387
	loss_value_2: 0.03284
	loss_reward_2: 0.00045
	loss_policy_3: 0.0054
	accuracy_policy_3: 0.86812
	loss_value_3: 0.03271
	loss_reward_3: 0.00045
	loss_policy_4: 0.00552
	accuracy_policy_4: 0.86352
	loss_value_4: 0.03259
	loss_reward_4: 0.0005
	loss_policy_5: 0.00561
	accuracy_policy_5: 0.84324
	loss_value_5: 0.03247
	loss_reward_5: 0.00042
	loss_policy: 0.04836
	loss_value: 0.32868
	loss_reward: 0.00227
[2024-05-04 08:51:12] nn step 1350, lr: 0.1.
	loss_policy_0: 0.02005
	accuracy_policy_0: 0.9134
	loss_value_0: 0.16144
	loss_policy_1: 0.00467
	accuracy_policy_1: 0.88328
	loss_value_1: 0.03223
	loss_reward_1: 0.00041
	loss_policy_2: 0.00505
	accuracy_policy_2: 0.88
	loss_value_2: 0.0321
	loss_reward_2: 0.00043
	loss_policy_3: 0.00503
	accuracy_policy_3: 0.8673
	loss_value_3: 0.03196
	loss_reward_3: 0.00041
	loss_policy_4: 0.0052
	accuracy_policy_4: 0.86469
	loss_value_4: 0.03183
	loss_reward_4: 0.00049
	loss_policy_5: 0.00529
	accuracy_policy_5: 0.8459
	loss_value_5: 0.0317
	loss_reward_5: 0.00044
	loss_policy: 0.0453
	loss_value: 0.32126
	loss_reward: 0.00217
[2024-05-04 08:51:28] nn step 1400, lr: 0.1.
	loss_policy_0: 0.02181
	accuracy_policy_0: 0.90617
	loss_value_0: 0.17633
	loss_policy_1: 0.00513
	accuracy_policy_1: 0.87809
	loss_value_1: 0.03518
	loss_reward_1: 0.0005
	loss_policy_2: 0.00545
	accuracy_policy_2: 0.87484
	loss_value_2: 0.03502
	loss_reward_2: 0.00049
	loss_policy_3: 0.00544
	accuracy_policy_3: 0.86293
	loss_value_3: 0.03487
	loss_reward_3: 0.00044
	loss_policy_4: 0.00545
	accuracy_policy_4: 0.86379
	loss_value_4: 0.03472
	loss_reward_4: 0.00045
	loss_policy_5: 0.00573
	accuracy_policy_5: 0.84062
	loss_value_5: 0.03457
	loss_reward_5: 0.00047
	loss_policy: 0.049
	loss_value: 0.35068
	loss_reward: 0.00234
Optimization_Done 1400
[2024-05-04 08:53:25] [command] train weight_iter_1400.pkl 4 8
[2024-05-04 08:53:42] nn step 1450, lr: 0.1.
	loss_policy_0: 0.02936
	accuracy_policy_0: 0.87578
	loss_value_0: 0.16466
	loss_policy_1: 0.00682
	accuracy_policy_1: 0.84359
	loss_value_1: 0.03291
	loss_reward_1: 5e-05
	loss_policy_2: 0.00705
	accuracy_policy_2: 0.84895
	loss_value_2: 0.03282
	loss_reward_2: 8e-05
	loss_policy_3: 0.00722
	accuracy_policy_3: 0.83484
	loss_value_3: 0.03274
	loss_reward_3: 5e-05
	loss_policy_4: 0.0073
	accuracy_policy_4: 0.82516
	loss_value_4: 0.03263
	loss_reward_4: 6e-05
	loss_policy_5: 0.00745
	accuracy_policy_5: 0.81762
	loss_value_5: 0.03255
	loss_reward_5: 6e-05
	loss_policy: 0.06521
	loss_value: 0.3283
	loss_reward: 0.00029
[2024-05-04 08:53:58] nn step 1500, lr: 0.1.
	loss_policy_0: 0.02639
	accuracy_policy_0: 0.88734
	loss_value_0: 0.1743
	loss_policy_1: 0.00611
	accuracy_policy_1: 0.86098
	loss_value_1: 0.03481
	loss_reward_1: 7e-05
	loss_policy_2: 0.0066
	accuracy_policy_2: 0.85918
	loss_value_2: 0.03473
	loss_reward_2: 5e-05
	loss_policy_3: 0.00652
	accuracy_policy_3: 0.84586
	loss_value_3: 0.03463
	loss_reward_3: 7e-05
	loss_policy_4: 0.00675
	accuracy_policy_4: 0.83988
	loss_value_4: 0.03453
	loss_reward_4: 5e-05
	loss_policy_5: 0.00674
	accuracy_policy_5: 0.83484
	loss_value_5: 0.03442
	loss_reward_5: 7e-05
	loss_policy: 0.05912
	loss_value: 0.34743
	loss_reward: 0.00031
[2024-05-04 08:54:13] nn step 1550, lr: 0.1.
	loss_policy_0: 0.02496
	accuracy_policy_0: 0.88828
	loss_value_0: 0.16763
	loss_policy_1: 0.00576
	accuracy_policy_1: 0.86652
	loss_value_1: 0.03351
	loss_reward_1: 4e-05
	loss_policy_2: 0.00623
	accuracy_policy_2: 0.86328
	loss_value_2: 0.0334
	loss_reward_2: 6e-05
	loss_policy_3: 0.00621
	accuracy_policy_3: 0.84789
	loss_value_3: 0.03331
	loss_reward_3: 5e-05
	loss_policy_4: 0.00632
	accuracy_policy_4: 0.83926
	loss_value_4: 0.03322
	loss_reward_4: 5e-05
	loss_policy_5: 0.00634
	accuracy_policy_5: 0.83879
	loss_value_5: 0.03311
	loss_reward_5: 7e-05
	loss_policy: 0.05582
	loss_value: 0.33418
	loss_reward: 0.00028
[2024-05-04 08:54:29] nn step 1600, lr: 0.1.
	loss_policy_0: 0.02593
	accuracy_policy_0: 0.88621
	loss_value_0: 0.17548
	loss_policy_1: 0.00597
	accuracy_policy_1: 0.86609
	loss_value_1: 0.03508
	loss_reward_1: 6e-05
	loss_policy_2: 0.00651
	accuracy_policy_2: 0.85738
	loss_value_2: 0.03497
	loss_reward_2: 8e-05
	loss_policy_3: 0.00637
	accuracy_policy_3: 0.8507
	loss_value_3: 0.03486
	loss_reward_3: 5e-05
	loss_policy_4: 0.00646
	accuracy_policy_4: 0.84277
	loss_value_4: 0.03478
	loss_reward_4: 7e-05
	loss_policy_5: 0.00646
	accuracy_policy_5: 0.8402
	loss_value_5: 0.03469
	loss_reward_5: 7e-05
	loss_policy: 0.0577
	loss_value: 0.34987
	loss_reward: 0.00031
Optimization_Done 1600
[2024-05-04 08:56:23] [command] train weight_iter_1600.pkl 5 9
[2024-05-04 08:56:40] nn step 1650, lr: 0.1.
	loss_policy_0: 0.03506
	accuracy_policy_0: 0.76195
	loss_value_0: 0.15506
	loss_policy_1: 0.00779
	accuracy_policy_1: 0.76477
	loss_value_1: 0.03098
	loss_reward_1: 0.0
	loss_policy_2: 0.00861
	accuracy_policy_2: 0.74324
	loss_value_2: 0.03088
	loss_reward_2: 0.0
	loss_policy_3: 0.00867
	accuracy_policy_3: 0.74277
	loss_value_3: 0.03076
	loss_reward_3: 0.0
	loss_policy_4: 0.00901
	accuracy_policy_4: 0.72656
	loss_value_4: 0.03067
	loss_reward_4: 0.0
	loss_policy_5: 0.00912
	accuracy_policy_5: 0.72855
	loss_value_5: 0.03058
	loss_reward_5: 0.0
	loss_policy: 0.07827
	loss_value: 0.30894
	loss_reward: 1e-05
[2024-05-04 08:56:56] nn step 1700, lr: 0.1.
	loss_policy_0: 0.02689
	accuracy_policy_0: 0.79309
	loss_value_0: 0.15306
	loss_policy_1: 0.0063
	accuracy_policy_1: 0.78879
	loss_value_1: 0.03059
	loss_reward_1: 0.0
	loss_policy_2: 0.00714
	accuracy_policy_2: 0.76922
	loss_value_2: 0.0305
	loss_reward_2: 0.0
	loss_policy_3: 0.00705
	accuracy_policy_3: 0.76629
	loss_value_3: 0.0304
	loss_reward_3: 0.0
	loss_policy_4: 0.00742
	accuracy_policy_4: 0.75223
	loss_value_4: 0.03031
	loss_reward_4: 0.0
	loss_policy_5: 0.00738
	accuracy_policy_5: 0.75348
	loss_value_5: 0.0302
	loss_reward_5: 0.0
	loss_policy: 0.06217
	loss_value: 0.30505
	loss_reward: 0.0
[2024-05-04 08:57:12] nn step 1750, lr: 0.1.
	loss_policy_0: 0.02744
	accuracy_policy_0: 0.80066
	loss_value_0: 0.16246
	loss_policy_1: 0.00636
	accuracy_policy_1: 0.79602
	loss_value_1: 0.03246
	loss_reward_1: 0.0
	loss_policy_2: 0.00729
	accuracy_policy_2: 0.77414
	loss_value_2: 0.03236
	loss_reward_2: 0.0
	loss_policy_3: 0.00714
	accuracy_policy_3: 0.77211
	loss_value_3: 0.03224
	loss_reward_3: 0.0
	loss_policy_4: 0.00743
	accuracy_policy_4: 0.75562
	loss_value_4: 0.03211
	loss_reward_4: 0.0
	loss_policy_5: 0.00736
	accuracy_policy_5: 0.76293
	loss_value_5: 0.03203
	loss_reward_5: 0.0
	loss_policy: 0.06302
	loss_value: 0.32367
	loss_reward: 0.0
[2024-05-04 08:57:28] nn step 1800, lr: 0.1.
	loss_policy_0: 0.02394
	accuracy_policy_0: 0.80277
	loss_value_0: 0.14495
	loss_policy_1: 0.00564
	accuracy_policy_1: 0.79766
	loss_value_1: 0.02895
	loss_reward_1: 0.0
	loss_policy_2: 0.00655
	accuracy_policy_2: 0.77414
	loss_value_2: 0.02887
	loss_reward_2: 0.0
	loss_policy_3: 0.00635
	accuracy_policy_3: 0.77242
	loss_value_3: 0.02877
	loss_reward_3: 0.0
	loss_policy_4: 0.00665
	accuracy_policy_4: 0.75676
	loss_value_4: 0.02867
	loss_reward_4: 0.0
	loss_policy_5: 0.00648
	accuracy_policy_5: 0.75461
	loss_value_5: 0.02858
	loss_reward_5: 0.0
	loss_policy: 0.05561
	loss_value: 0.2888
	loss_reward: 0.0
Optimization_Done 1800
[2024-05-04 08:59:30] [command] train weight_iter_1800.pkl 6 10
[2024-05-04 08:59:47] nn step 1850, lr: 0.1.
	loss_policy_0: 0.02785
	accuracy_policy_0: 0.75766
	loss_value_0: 0.16354
	loss_policy_1: 0.0059
	accuracy_policy_1: 0.76
	loss_value_1: 0.03269
	loss_reward_1: 0.0
	loss_policy_2: 0.00661
	accuracy_policy_2: 0.73066
	loss_value_2: 0.0326
	loss_reward_2: 0.0
	loss_policy_3: 0.00644
	accuracy_policy_3: 0.73852
	loss_value_3: 0.03251
	loss_reward_3: 0.0
	loss_policy_4: 0.00673
	accuracy_policy_4: 0.72418
	loss_value_4: 0.03242
	loss_reward_4: 0.0
	loss_policy_5: 0.00675
	accuracy_policy_5: 0.73211
	loss_value_5: 0.03232
	loss_reward_5: 0.0
	loss_policy: 0.06027
	loss_value: 0.32607
	loss_reward: 0.0
[2024-05-04 09:00:03] nn step 1900, lr: 0.1.
	loss_policy_0: 0.02105
	accuracy_policy_0: 0.79949
	loss_value_0: 0.15973
	loss_policy_1: 0.00488
	accuracy_policy_1: 0.7882
	loss_value_1: 0.03193
	loss_reward_1: 0.0
	loss_policy_2: 0.00551
	accuracy_policy_2: 0.76227
	loss_value_2: 0.03186
	loss_reward_2: 0.0
	loss_policy_3: 0.00536
	accuracy_policy_3: 0.76031
	loss_value_3: 0.03177
	loss_reward_3: 0.0
	loss_policy_4: 0.00549
	accuracy_policy_4: 0.75961
	loss_value_4: 0.03169
	loss_reward_4: 0.0
	loss_policy_5: 0.0056
	accuracy_policy_5: 0.7552
	loss_value_5: 0.03161
	loss_reward_5: 0.0
	loss_policy: 0.04789
	loss_value: 0.31859
	loss_reward: 0.0
[2024-05-04 09:00:18] nn step 1950, lr: 0.1.
	loss_policy_0: 0.01989
	accuracy_policy_0: 0.81426
	loss_value_0: 0.16719
	loss_policy_1: 0.00485
	accuracy_policy_1: 0.79547
	loss_value_1: 0.0334
	loss_reward_1: 0.0
	loss_policy_2: 0.0055
	accuracy_policy_2: 0.76891
	loss_value_2: 0.03331
	loss_reward_2: 0.0
	loss_policy_3: 0.00533
	accuracy_policy_3: 0.7702
	loss_value_3: 0.03321
	loss_reward_3: 0.0
	loss_policy_4: 0.00548
	accuracy_policy_4: 0.76363
	loss_value_4: 0.03314
	loss_reward_4: 0.0
	loss_policy_5: 0.00551
	accuracy_policy_5: 0.76309
	loss_value_5: 0.03303
	loss_reward_5: 0.0
	loss_policy: 0.04656
	loss_value: 0.33328
	loss_reward: 0.0
[2024-05-04 09:00:34] nn step 2000, lr: 0.1.
	loss_policy_0: 0.01851
	accuracy_policy_0: 0.8132
	loss_value_0: 0.16107
	loss_policy_1: 0.00441
	accuracy_policy_1: 0.8091
	loss_value_1: 0.03219
	loss_reward_1: 0.0
	loss_policy_2: 0.00507
	accuracy_policy_2: 0.78398
	loss_value_2: 0.03211
	loss_reward_2: 0.0
	loss_policy_3: 0.00504
	accuracy_policy_3: 0.78055
	loss_value_3: 0.03202
	loss_reward_3: 0.0
	loss_policy_4: 0.00513
	accuracy_policy_4: 0.7807
	loss_value_4: 0.03193
	loss_reward_4: 0.0
	loss_policy_5: 0.00521
	accuracy_policy_5: 0.7773
	loss_value_5: 0.03183
	loss_reward_5: 0.0
	loss_policy: 0.04337
	loss_value: 0.32114
	loss_reward: 0.0
Optimization_Done 2000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 09:03:35] [command] train weight_iter_2000.pkl 7 11
[2024-05-04 09:04:04] nn step 2050, lr: 0.1.
	loss_policy_0: 0.02664
	accuracy_policy_0: 0.72113
	loss_value_0: 0.15931
	loss_policy_1: 0.0057
	accuracy_policy_1: 0.72516
	loss_value_1: 0.03184
	loss_reward_1: 0.0
	loss_policy_2: 0.0061
	accuracy_policy_2: 0.70402
	loss_value_2: 0.03174
	loss_reward_2: 0.0
	loss_policy_3: 0.00613
	accuracy_policy_3: 0.71496
	loss_value_3: 0.03166
	loss_reward_3: 0.0
	loss_policy_4: 0.00652
	accuracy_policy_4: 0.69047
	loss_value_4: 0.03158
	loss_reward_4: 0.0
	loss_policy_5: 0.00652
	accuracy_policy_5: 0.71016
	loss_value_5: 0.03151
	loss_reward_5: 0.0
	loss_policy: 0.05761
	loss_value: 0.31764
	loss_reward: 0.0
[2024-05-04 09:04:20] nn step 2100, lr: 0.1.
	loss_policy_0: 0.0187
	accuracy_policy_0: 0.7757
	loss_value_0: 0.16622
	loss_policy_1: 0.00454
	accuracy_policy_1: 0.75746
	loss_value_1: 0.03321
	loss_reward_1: 0.0
	loss_policy_2: 0.00534
	accuracy_policy_2: 0.73023
	loss_value_2: 0.03312
	loss_reward_2: 0.0
	loss_policy_3: 0.00548
	accuracy_policy_3: 0.73727
	loss_value_3: 0.03302
	loss_reward_3: 0.0
	loss_policy_4: 0.00573
	accuracy_policy_4: 0.72047
	loss_value_4: 0.03292
	loss_reward_4: 0.0
	loss_policy_5: 0.0058
	accuracy_policy_5: 0.72402
	loss_value_5: 0.03285
	loss_reward_5: 0.0
	loss_policy: 0.04559
	loss_value: 0.33134
	loss_reward: 0.0
[2024-05-04 09:04:36] nn step 2150, lr: 0.1.
	loss_policy_0: 0.0157
	accuracy_policy_0: 0.79078
	loss_value_0: 0.15661
	loss_policy_1: 0.00394
	accuracy_policy_1: 0.77176
	loss_value_1: 0.03131
	loss_reward_1: 0.0
	loss_policy_2: 0.00465
	accuracy_policy_2: 0.7441
	loss_value_2: 0.03122
	loss_reward_2: 0.0
	loss_policy_3: 0.00472
	accuracy_policy_3: 0.75074
	loss_value_3: 0.03112
	loss_reward_3: 0.0
	loss_policy_4: 0.00502
	accuracy_policy_4: 0.74043
	loss_value_4: 0.03102
	loss_reward_4: 0.0
	loss_policy_5: 0.00506
	accuracy_policy_5: 0.74656
	loss_value_5: 0.03094
	loss_reward_5: 0.0
	loss_policy: 0.03909
	loss_value: 0.31222
	loss_reward: 0.0
[2024-05-04 09:04:52] nn step 2200, lr: 0.1.
	loss_policy_0: 0.01675
	accuracy_policy_0: 0.78988
	loss_value_0: 0.16608
	loss_policy_1: 0.00422
	accuracy_policy_1: 0.77238
	loss_value_1: 0.03319
	loss_reward_1: 0.0
	loss_policy_2: 0.00484
	accuracy_policy_2: 0.74926
	loss_value_2: 0.0331
	loss_reward_2: 0.0
	loss_policy_3: 0.005
	accuracy_policy_3: 0.75219
	loss_value_3: 0.03301
	loss_reward_3: 0.0
	loss_policy_4: 0.00525
	accuracy_policy_4: 0.73914
	loss_value_4: 0.03291
	loss_reward_4: 0.0
	loss_policy_5: 0.00526
	accuracy_policy_5: 0.74633
	loss_value_5: 0.03281
	loss_reward_5: 0.0
	loss_policy: 0.04131
	loss_value: 0.3311
	loss_reward: 0.0
Optimization_Done 2200
[2024-05-04 09:06:57] [command] train weight_iter_2200.pkl 8 12
[2024-05-04 09:07:14] nn step 2250, lr: 0.1.
	loss_policy_0: 0.01627
	accuracy_policy_0: 0.82133
	loss_value_0: 0.16274
	loss_policy_1: 0.00415
	accuracy_policy_1: 0.7998
	loss_value_1: 0.03252
	loss_reward_1: 0.0
	loss_policy_2: 0.00464
	accuracy_policy_2: 0.78473
	loss_value_2: 0.03244
	loss_reward_2: 0.0
	loss_policy_3: 0.00485
	accuracy_policy_3: 0.77348
	loss_value_3: 0.03235
	loss_reward_3: 0.0
	loss_policy_4: 0.00507
	accuracy_policy_4: 0.76672
	loss_value_4: 0.03227
	loss_reward_4: 0.0
	loss_policy_5: 0.00514
	accuracy_policy_5: 0.77426
	loss_value_5: 0.03218
	loss_reward_5: 0.0
	loss_policy: 0.04012
	loss_value: 0.3245
	loss_reward: 0.0
[2024-05-04 09:07:30] nn step 2300, lr: 0.1.
	loss_policy_0: 0.01318
	accuracy_policy_0: 0.84695
	loss_value_0: 0.1646
	loss_policy_1: 0.00358
	accuracy_policy_1: 0.82523
	loss_value_1: 0.03288
	loss_reward_1: 0.0
	loss_policy_2: 0.00414
	accuracy_policy_2: 0.80145
	loss_value_2: 0.03278
	loss_reward_2: 0.0
	loss_policy_3: 0.00428
	accuracy_policy_3: 0.79559
	loss_value_3: 0.03268
	loss_reward_3: 0.0
	loss_policy_4: 0.00464
	accuracy_policy_4: 0.78215
	loss_value_4: 0.0326
	loss_reward_4: 0.0
	loss_policy_5: 0.00459
	accuracy_policy_5: 0.78969
	loss_value_5: 0.03249
	loss_reward_5: 0.0
	loss_policy: 0.03441
	loss_value: 0.32803
	loss_reward: 0.0
[2024-05-04 09:07:46] nn step 2350, lr: 0.1.
	loss_policy_0: 0.01257
	accuracy_policy_0: 0.84328
	loss_value_0: 0.15629
	loss_policy_1: 0.00326
	accuracy_policy_1: 0.83355
	loss_value_1: 0.03122
	loss_reward_1: 0.0
	loss_policy_2: 0.00375
	accuracy_policy_2: 0.80906
	loss_value_2: 0.03113
	loss_reward_2: 0.0
	loss_policy_3: 0.00393
	accuracy_policy_3: 0.80461
	loss_value_3: 0.03104
	loss_reward_3: 0.0
	loss_policy_4: 0.00413
	accuracy_policy_4: 0.79664
	loss_value_4: 0.03096
	loss_reward_4: 0.0
	loss_policy_5: 0.00427
	accuracy_policy_5: 0.79809
	loss_value_5: 0.03087
	loss_reward_5: 0.0
	loss_policy: 0.03191
	loss_value: 0.31151
	loss_reward: 0.0
[2024-05-04 09:08:02] nn step 2400, lr: 0.1.
	loss_policy_0: 0.01176
	accuracy_policy_0: 0.85055
	loss_value_0: 0.15206
	loss_policy_1: 0.00312
	accuracy_policy_1: 0.83984
	loss_value_1: 0.03038
	loss_reward_1: 0.0
	loss_policy_2: 0.00362
	accuracy_policy_2: 0.81594
	loss_value_2: 0.03028
	loss_reward_2: 0.0
	loss_policy_3: 0.0038
	accuracy_policy_3: 0.80922
	loss_value_3: 0.03021
	loss_reward_3: 0.0
	loss_policy_4: 0.00399
	accuracy_policy_4: 0.79891
	loss_value_4: 0.03012
	loss_reward_4: 0.0
	loss_policy_5: 0.00403
	accuracy_policy_5: 0.80555
	loss_value_5: 0.03002
	loss_reward_5: 0.0
	loss_policy: 0.03032
	loss_value: 0.30307
	loss_reward: 0.0
Optimization_Done 2400
[2024-05-04 09:10:03] [command] train weight_iter_2400.pkl 9 13
[2024-05-04 09:10:20] nn step 2450, lr: 0.1.
	loss_policy_0: 0.01877
	accuracy_policy_0: 0.80137
	loss_value_0: 0.15104
	loss_policy_1: 0.00482
	accuracy_policy_1: 0.77324
	loss_value_1: 0.03018
	loss_reward_1: 6e-05
	loss_policy_2: 0.00554
	accuracy_policy_2: 0.74945
	loss_value_2: 0.03008
	loss_reward_2: 2e-05
	loss_policy_3: 0.00574
	accuracy_policy_3: 0.74812
	loss_value_3: 0.02998
	loss_reward_3: 3e-05
	loss_policy_4: 0.0062
	accuracy_policy_4: 0.72848
	loss_value_4: 0.02989
	loss_reward_4: 3e-05
	loss_policy_5: 0.00645
	accuracy_policy_5: 0.73477
	loss_value_5: 0.02979
	loss_reward_5: 2e-05
	loss_policy: 0.04751
	loss_value: 0.30095
	loss_reward: 0.00016
[2024-05-04 09:10:36] nn step 2500, lr: 0.1.
	loss_policy_0: 0.01347
	accuracy_policy_0: 0.82973
	loss_value_0: 0.15118
	loss_policy_1: 0.00382
	accuracy_policy_1: 0.80117
	loss_value_1: 0.0302
	loss_reward_1: 3e-05
	loss_policy_2: 0.0044
	accuracy_policy_2: 0.77836
	loss_value_2: 0.03012
	loss_reward_2: 3e-05
	loss_policy_3: 0.00468
	accuracy_policy_3: 0.76832
	loss_value_3: 0.03003
	loss_reward_3: 3e-05
	loss_policy_4: 0.00488
	accuracy_policy_4: 0.76602
	loss_value_4: 0.02992
	loss_reward_4: 4e-05
	loss_policy_5: 0.0051
	accuracy_policy_5: 0.7677
	loss_value_5: 0.02983
	loss_reward_5: 5e-05
	loss_policy: 0.03637
	loss_value: 0.30129
	loss_reward: 0.00017
[2024-05-04 09:10:53] nn step 2550, lr: 0.1.
	loss_policy_0: 0.01348
	accuracy_policy_0: 0.83195
	loss_value_0: 0.15295
	loss_policy_1: 0.00367
	accuracy_policy_1: 0.80688
	loss_value_1: 0.03056
	loss_reward_1: 2e-05
	loss_policy_2: 0.00423
	accuracy_policy_2: 0.78008
	loss_value_2: 0.03047
	loss_reward_2: 3e-05
	loss_policy_3: 0.00446
	accuracy_policy_3: 0.78086
	loss_value_3: 0.03038
	loss_reward_3: 3e-05
	loss_policy_4: 0.00468
	accuracy_policy_4: 0.7634
	loss_value_4: 0.03028
	loss_reward_4: 4e-05
	loss_policy_5: 0.00482
	accuracy_policy_5: 0.76828
	loss_value_5: 0.03021
	loss_reward_5: 2e-05
	loss_policy: 0.03534
	loss_value: 0.30486
	loss_reward: 0.00014
[2024-05-04 09:11:09] nn step 2600, lr: 0.1.
	loss_policy_0: 0.01206
	accuracy_policy_0: 0.83895
	loss_value_0: 0.14633
	loss_policy_1: 0.00342
	accuracy_policy_1: 0.81227
	loss_value_1: 0.02924
	loss_reward_1: 2e-05
	loss_policy_2: 0.00394
	accuracy_policy_2: 0.78531
	loss_value_2: 0.02916
	loss_reward_2: 4e-05
	loss_policy_3: 0.00416
	accuracy_policy_3: 0.78195
	loss_value_3: 0.02905
	loss_reward_3: 4e-05
	loss_policy_4: 0.00439
	accuracy_policy_4: 0.7752
	loss_value_4: 0.02896
	loss_reward_4: 3e-05
	loss_policy_5: 0.00449
	accuracy_policy_5: 0.78023
	loss_value_5: 0.02886
	loss_reward_5: 4e-05
	loss_policy: 0.03246
	loss_value: 0.29161
	loss_reward: 0.00015
Optimization_Done 2600
[2024-05-04 09:13:09] [command] train weight_iter_2600.pkl 10 14
[2024-05-04 09:13:26] nn step 2650, lr: 0.1.
	loss_policy_0: 0.01219
	accuracy_policy_0: 0.85121
	loss_value_0: 0.1506
	loss_policy_1: 0.00353
	accuracy_policy_1: 0.8193
	loss_value_1: 0.03009
	loss_reward_1: 3e-05
	loss_policy_2: 0.00379
	accuracy_policy_2: 0.7952
	loss_value_2: 0.02998
	loss_reward_2: 3e-05
	loss_policy_3: 0.00404
	accuracy_policy_3: 0.79496
	loss_value_3: 0.0299
	loss_reward_3: 4e-05
	loss_policy_4: 0.00427
	accuracy_policy_4: 0.78496
	loss_value_4: 0.02981
	loss_reward_4: 3e-05
	loss_policy_5: 0.00449
	accuracy_policy_5: 0.78898
	loss_value_5: 0.02971
	loss_reward_5: 2e-05
	loss_policy: 0.03231
	loss_value: 0.30008
	loss_reward: 0.00015
[2024-05-04 09:13:42] nn step 2700, lr: 0.1.
	loss_policy_0: 0.01077
	accuracy_policy_0: 0.86465
	loss_value_0: 0.16191
	loss_policy_1: 0.00311
	accuracy_policy_1: 0.83395
	loss_value_1: 0.03235
	loss_reward_1: 4e-05
	loss_policy_2: 0.00351
	accuracy_policy_2: 0.81133
	loss_value_2: 0.03226
	loss_reward_2: 3e-05
	loss_policy_3: 0.00372
	accuracy_policy_3: 0.81641
	loss_value_3: 0.03215
	loss_reward_3: 3e-05
	loss_policy_4: 0.00399
	accuracy_policy_4: 0.80547
	loss_value_4: 0.03205
	loss_reward_4: 3e-05
	loss_policy_5: 0.00405
	accuracy_policy_5: 0.81633
	loss_value_5: 0.03195
	loss_reward_5: 3e-05
	loss_policy: 0.02915
	loss_value: 0.32267
	loss_reward: 0.00016
[2024-05-04 09:13:58] nn step 2750, lr: 0.1.
	loss_policy_0: 0.00949
	accuracy_policy_0: 0.86324
	loss_value_0: 0.14825
	loss_policy_1: 0.00284
	accuracy_policy_1: 0.83492
	loss_value_1: 0.02962
	loss_reward_1: 4e-05
	loss_policy_2: 0.00321
	accuracy_policy_2: 0.81828
	loss_value_2: 0.02954
	loss_reward_2: 4e-05
	loss_policy_3: 0.00351
	accuracy_policy_3: 0.81488
	loss_value_3: 0.02944
	loss_reward_3: 2e-05
	loss_policy_4: 0.00374
	accuracy_policy_4: 0.79961
	loss_value_4: 0.02932
	loss_reward_4: 3e-05
	loss_policy_5: 0.00392
	accuracy_policy_5: 0.80758
	loss_value_5: 0.02924
	loss_reward_5: 2e-05
	loss_policy: 0.02672
	loss_value: 0.2954
	loss_reward: 0.00015
[2024-05-04 09:14:14] nn step 2800, lr: 0.1.
	loss_policy_0: 0.00997
	accuracy_policy_0: 0.8684
	loss_value_0: 0.15923
	loss_policy_1: 0.00294
	accuracy_policy_1: 0.84316
	loss_value_1: 0.0318
	loss_reward_1: 5e-05
	loss_policy_2: 0.00333
	accuracy_policy_2: 0.82359
	loss_value_2: 0.0317
	loss_reward_2: 2e-05
	loss_policy_3: 0.00356
	accuracy_policy_3: 0.82617
	loss_value_3: 0.0316
	loss_reward_3: 3e-05
	loss_policy_4: 0.00377
	accuracy_policy_4: 0.81301
	loss_value_4: 0.0315
	loss_reward_4: 3e-05
	loss_policy_5: 0.00394
	accuracy_policy_5: 0.81984
	loss_value_5: 0.03138
	loss_reward_5: 3e-05
	loss_policy: 0.02751
	loss_value: 0.31722
	loss_reward: 0.00016
Optimization_Done 2800
[2024-05-04 09:16:08] [command] train weight_iter_2800.pkl 11 15
[2024-05-04 09:16:25] nn step 2850, lr: 0.1.
	loss_policy_0: 0.0113
	accuracy_policy_0: 0.8491
	loss_value_0: 0.14648
	loss_policy_1: 0.00361
	accuracy_policy_1: 0.81965
	loss_value_1: 0.02924
	loss_reward_1: 4e-05
	loss_policy_2: 0.00405
	accuracy_policy_2: 0.79934
	loss_value_2: 0.02916
	loss_reward_2: 3e-05
	loss_policy_3: 0.00426
	accuracy_policy_3: 0.7952
	loss_value_3: 0.02904
	loss_reward_3: 3e-05
	loss_policy_4: 0.00446
	accuracy_policy_4: 0.78754
	loss_value_4: 0.02894
	loss_reward_4: 3e-05
	loss_policy_5: 0.00477
	accuracy_policy_5: 0.78336
	loss_value_5: 0.02885
	loss_reward_5: 3e-05
	loss_policy: 0.03246
	loss_value: 0.29171
	loss_reward: 0.00015
[2024-05-04 09:16:41] nn step 2900, lr: 0.1.
	loss_policy_0: 0.00932
	accuracy_policy_0: 0.86645
	loss_value_0: 0.14813
	loss_policy_1: 0.00312
	accuracy_policy_1: 0.83383
	loss_value_1: 0.0296
	loss_reward_1: 3e-05
	loss_policy_2: 0.00363
	accuracy_policy_2: 0.81207
	loss_value_2: 0.0295
	loss_reward_2: 4e-05
	loss_policy_3: 0.00395
	accuracy_policy_3: 0.80867
	loss_value_3: 0.0294
	loss_reward_3: 3e-05
	loss_policy_4: 0.00407
	accuracy_policy_4: 0.79598
	loss_value_4: 0.0293
	loss_reward_4: 3e-05
	loss_policy_5: 0.00422
	accuracy_policy_5: 0.79867
	loss_value_5: 0.02919
	loss_reward_5: 3e-05
	loss_policy: 0.02831
	loss_value: 0.29512
	loss_reward: 0.00016
[2024-05-04 09:16:58] nn step 2950, lr: 0.1.
	loss_policy_0: 0.00824
	accuracy_policy_0: 0.87188
	loss_value_0: 0.13866
	loss_policy_1: 0.00275
	accuracy_policy_1: 0.84453
	loss_value_1: 0.0277
	loss_reward_1: 3e-05
	loss_policy_2: 0.0032
	accuracy_policy_2: 0.81934
	loss_value_2: 0.0276
	loss_reward_2: 3e-05
	loss_policy_3: 0.00341
	accuracy_policy_3: 0.81305
	loss_value_3: 0.0275
	loss_reward_3: 3e-05
	loss_policy_4: 0.0036
	accuracy_policy_4: 0.80352
	loss_value_4: 0.0274
	loss_reward_4: 3e-05
	loss_policy_5: 0.00378
	accuracy_policy_5: 0.80488
	loss_value_5: 0.02729
	loss_reward_5: 3e-05
	loss_policy: 0.02498
	loss_value: 0.27615
	loss_reward: 0.00015
[2024-05-04 09:17:14] nn step 3000, lr: 0.1.
	loss_policy_0: 0.0078
	accuracy_policy_0: 0.87836
	loss_value_0: 0.1411
	loss_policy_1: 0.00273
	accuracy_policy_1: 0.84723
	loss_value_1: 0.02817
	loss_reward_1: 3e-05
	loss_policy_2: 0.00326
	accuracy_policy_2: 0.82086
	loss_value_2: 0.02808
	loss_reward_2: 3e-05
	loss_policy_3: 0.00352
	accuracy_policy_3: 0.81543
	loss_value_3: 0.02798
	loss_reward_3: 3e-05
	loss_policy_4: 0.00364
	accuracy_policy_4: 0.80664
	loss_value_4: 0.02789
	loss_reward_4: 2e-05
	loss_policy_5: 0.00389
	accuracy_policy_5: 0.80473
	loss_value_5: 0.02779
	loss_reward_5: 3e-05
	loss_policy: 0.02485
	loss_value: 0.28103
	loss_reward: 0.00015
Optimization_Done 3000
[2024-05-04 09:19:14] [command] train weight_iter_3000.pkl 12 16
[2024-05-04 09:19:31] nn step 3050, lr: 0.1.
	loss_policy_0: 0.00938
	accuracy_policy_0: 0.8725
	loss_value_0: 0.14791
	loss_policy_1: 0.00304
	accuracy_policy_1: 0.84574
	loss_value_1: 0.02955
	loss_reward_1: 2e-05
	loss_policy_2: 0.00352
	accuracy_policy_2: 0.82773
	loss_value_2: 0.02943
	loss_reward_2: 2e-05
	loss_policy_3: 0.00371
	accuracy_policy_3: 0.82328
	loss_value_3: 0.02932
	loss_reward_3: 2e-05
	loss_policy_4: 0.00386
	accuracy_policy_4: 0.81512
	loss_value_4: 0.02921
	loss_reward_4: 2e-05
	loss_policy_5: 0.00416
	accuracy_policy_5: 0.81109
	loss_value_5: 0.02911
	loss_reward_5: 3e-05
	loss_policy: 0.02768
	loss_value: 0.29453
	loss_reward: 0.00012
[2024-05-04 09:19:47] nn step 3100, lr: 0.1.
	loss_policy_0: 0.008
	accuracy_policy_0: 0.8841
	loss_value_0: 0.14583
	loss_policy_1: 0.00276
	accuracy_policy_1: 0.8534
	loss_value_1: 0.02913
	loss_reward_1: 2e-05
	loss_policy_2: 0.00321
	accuracy_policy_2: 0.83926
	loss_value_2: 0.02904
	loss_reward_2: 4e-05
	loss_policy_3: 0.00332
	accuracy_policy_3: 0.83605
	loss_value_3: 0.02893
	loss_reward_3: 3e-05
	loss_policy_4: 0.00349
	accuracy_policy_4: 0.82125
	loss_value_4: 0.02882
	loss_reward_4: 3e-05
	loss_policy_5: 0.00379
	accuracy_policy_5: 0.81906
	loss_value_5: 0.02872
	loss_reward_5: 2e-05
	loss_policy: 0.02457
	loss_value: 0.29048
	loss_reward: 0.00014
[2024-05-04 09:20:03] nn step 3150, lr: 0.1.
	loss_policy_0: 0.0079
	accuracy_policy_0: 0.88418
	loss_value_0: 0.14735
	loss_policy_1: 0.00268
	accuracy_policy_1: 0.85902
	loss_value_1: 0.02944
	loss_reward_1: 3e-05
	loss_policy_2: 0.0032
	accuracy_policy_2: 0.84012
	loss_value_2: 0.02935
	loss_reward_2: 3e-05
	loss_policy_3: 0.0034
	accuracy_policy_3: 0.83496
	loss_value_3: 0.02926
	loss_reward_3: 3e-05
	loss_policy_4: 0.00353
	accuracy_policy_4: 0.82695
	loss_value_4: 0.02914
	loss_reward_4: 3e-05
	loss_policy_5: 0.00381
	accuracy_policy_5: 0.82297
	loss_value_5: 0.02904
	loss_reward_5: 3e-05
	loss_policy: 0.02453
	loss_value: 0.29357
	loss_reward: 0.00015
[2024-05-04 09:20:19] nn step 3200, lr: 0.1.
	loss_policy_0: 0.00772
	accuracy_policy_0: 0.88484
	loss_value_0: 0.14615
	loss_policy_1: 0.00263
	accuracy_policy_1: 0.86055
	loss_value_1: 0.0292
	loss_reward_1: 3e-05
	loss_policy_2: 0.00312
	accuracy_policy_2: 0.84488
	loss_value_2: 0.02912
	loss_reward_2: 2e-05
	loss_policy_3: 0.00336
	accuracy_policy_3: 0.83852
	loss_value_3: 0.029
	loss_reward_3: 4e-05
	loss_policy_4: 0.00352
	accuracy_policy_4: 0.8275
	loss_value_4: 0.02888
	loss_reward_4: 3e-05
	loss_policy_5: 0.00379
	accuracy_policy_5: 0.82199
	loss_value_5: 0.02877
	loss_reward_5: 2e-05
	loss_policy: 0.02415
	loss_value: 0.29111
	loss_reward: 0.00014
Optimization_Done 3200
[2024-05-04 09:22:21] [command] train weight_iter_3200.pkl 13 17
[2024-05-04 09:22:38] nn step 3250, lr: 0.1.
	loss_policy_0: 0.01291
	accuracy_policy_0: 0.82609
	loss_value_0: 0.13881
	loss_policy_1: 0.00384
	accuracy_policy_1: 0.79953
	loss_value_1: 0.02771
	loss_reward_1: 5e-05
	loss_policy_2: 0.00421
	accuracy_policy_2: 0.79062
	loss_value_2: 0.02762
	loss_reward_2: 3e-05
	loss_policy_3: 0.00461
	accuracy_policy_3: 0.78184
	loss_value_3: 0.02752
	loss_reward_3: 4e-05
	loss_policy_4: 0.00479
	accuracy_policy_4: 0.76684
	loss_value_4: 0.02742
	loss_reward_4: 3e-05
	loss_policy_5: 0.00522
	accuracy_policy_5: 0.76664
	loss_value_5: 0.02734
	loss_reward_5: 3e-05
	loss_policy: 0.03559
	loss_value: 0.27641
	loss_reward: 0.00018
[2024-05-04 09:22:54] nn step 3300, lr: 0.1.
	loss_policy_0: 0.00932
	accuracy_policy_0: 0.85391
	loss_value_0: 0.12948
	loss_policy_1: 0.00302
	accuracy_policy_1: 0.82578
	loss_value_1: 0.02586
	loss_reward_1: 3e-05
	loss_policy_2: 0.0035
	accuracy_policy_2: 0.80543
	loss_value_2: 0.02577
	loss_reward_2: 3e-05
	loss_policy_3: 0.00378
	accuracy_policy_3: 0.80145
	loss_value_3: 0.02565
	loss_reward_3: 3e-05
	loss_policy_4: 0.00391
	accuracy_policy_4: 0.78957
	loss_value_4: 0.02555
	loss_reward_4: 3e-05
	loss_policy_5: 0.00424
	accuracy_policy_5: 0.78355
	loss_value_5: 0.02546
	loss_reward_5: 2e-05
	loss_policy: 0.02777
	loss_value: 0.25778
	loss_reward: 0.00013
[2024-05-04 09:23:11] nn step 3350, lr: 0.1.
	loss_policy_0: 0.00905
	accuracy_policy_0: 0.86133
	loss_value_0: 0.13511
	loss_policy_1: 0.00289
	accuracy_policy_1: 0.8343
	loss_value_1: 0.027
	loss_reward_1: 3e-05
	loss_policy_2: 0.00331
	accuracy_policy_2: 0.81828
	loss_value_2: 0.02692
	loss_reward_2: 4e-05
	loss_policy_3: 0.00368
	accuracy_policy_3: 0.80828
	loss_value_3: 0.02683
	loss_reward_3: 4e-05
	loss_policy_4: 0.00383
	accuracy_policy_4: 0.79484
	loss_value_4: 0.02673
	loss_reward_4: 2e-05
	loss_policy_5: 0.0041
	accuracy_policy_5: 0.79609
	loss_value_5: 0.02663
	loss_reward_5: 3e-05
	loss_policy: 0.02686
	loss_value: 0.26921
	loss_reward: 0.00015
[2024-05-04 09:23:27] nn step 3400, lr: 0.1.
	loss_policy_0: 0.00793
	accuracy_policy_0: 0.86656
	loss_value_0: 0.12818
	loss_policy_1: 0.00266
	accuracy_policy_1: 0.84023
	loss_value_1: 0.02562
	loss_reward_1: 2e-05
	loss_policy_2: 0.00311
	accuracy_policy_2: 0.81961
	loss_value_2: 0.02551
	loss_reward_2: 4e-05
	loss_policy_3: 0.0034
	accuracy_policy_3: 0.80926
	loss_value_3: 0.02541
	loss_reward_3: 3e-05
	loss_policy_4: 0.00349
	accuracy_policy_4: 0.7998
	loss_value_4: 0.0253
	loss_reward_4: 4e-05
	loss_policy_5: 0.00385
	accuracy_policy_5: 0.79746
	loss_value_5: 0.02521
	loss_reward_5: 2e-05
	loss_policy: 0.02444
	loss_value: 0.25524
	loss_reward: 0.00015
Optimization_Done 3400
[2024-05-04 09:25:26] [command] train weight_iter_3400.pkl 14 18
[2024-05-04 09:25:43] nn step 3450, lr: 0.1.
	loss_policy_0: 0.00983
	accuracy_policy_0: 0.85871
	loss_value_0: 0.13075
	loss_policy_1: 0.00303
	accuracy_policy_1: 0.82703
	loss_value_1: 0.02613
	loss_reward_1: 4e-05
	loss_policy_2: 0.00347
	accuracy_policy_2: 0.79773
	loss_value_2: 0.02602
	loss_reward_2: 4e-05
	loss_policy_3: 0.00372
	accuracy_policy_3: 0.80086
	loss_value_3: 0.02591
	loss_reward_3: 4e-05
	loss_policy_4: 0.00379
	accuracy_policy_4: 0.78555
	loss_value_4: 0.0258
	loss_reward_4: 5e-05
	loss_policy_5: 0.00405
	accuracy_policy_5: 0.78508
	loss_value_5: 0.0257
	loss_reward_5: 5e-05
	loss_policy: 0.02789
	loss_value: 0.26031
	loss_reward: 0.00021
[2024-05-04 09:25:59] nn step 3500, lr: 0.1.
	loss_policy_0: 0.00784
	accuracy_policy_0: 0.87664
	loss_value_0: 0.13111
	loss_policy_1: 0.00269
	accuracy_policy_1: 0.84172
	loss_value_1: 0.02618
	loss_reward_1: 3e-05
	loss_policy_2: 0.00317
	accuracy_policy_2: 0.81145
	loss_value_2: 0.0261
	loss_reward_2: 2e-05
	loss_policy_3: 0.00326
	accuracy_policy_3: 0.81121
	loss_value_3: 0.026
	loss_reward_3: 4e-05
	loss_policy_4: 0.0033
	accuracy_policy_4: 0.80121
	loss_value_4: 0.02591
	loss_reward_4: 5e-05
	loss_policy_5: 0.00358
	accuracy_policy_5: 0.79906
	loss_value_5: 0.0258
	loss_reward_5: 4e-05
	loss_policy: 0.02384
	loss_value: 0.2611
	loss_reward: 0.00018
[2024-05-04 09:26:15] nn step 3550, lr: 0.1.
	loss_policy_0: 0.00738
	accuracy_policy_0: 0.8823
	loss_value_0: 0.12862
	loss_policy_1: 0.00249
	accuracy_policy_1: 0.8434
	loss_value_1: 0.02568
	loss_reward_1: 4e-05
	loss_policy_2: 0.00298
	accuracy_policy_2: 0.81758
	loss_value_2: 0.0256
	loss_reward_2: 3e-05
	loss_policy_3: 0.00318
	accuracy_policy_3: 0.81406
	loss_value_3: 0.0255
	loss_reward_3: 4e-05
	loss_policy_4: 0.0032
	accuracy_policy_4: 0.81156
	loss_value_4: 0.02541
	loss_reward_4: 4e-05
	loss_policy_5: 0.00342
	accuracy_policy_5: 0.80555
	loss_value_5: 0.02531
	loss_reward_5: 4e-05
	loss_policy: 0.02265
	loss_value: 0.25611
	loss_reward: 0.00019
[2024-05-04 09:26:32] nn step 3600, lr: 0.1.
	loss_policy_0: 0.00667
	accuracy_policy_0: 0.88961
	loss_value_0: 0.12628
	loss_policy_1: 0.00241
	accuracy_policy_1: 0.84645
	loss_value_1: 0.02522
	loss_reward_1: 3e-05
	loss_policy_2: 0.00283
	accuracy_policy_2: 0.8218
	loss_value_2: 0.02512
	loss_reward_2: 4e-05
	loss_policy_3: 0.00307
	accuracy_policy_3: 0.81945
	loss_value_3: 0.02502
	loss_reward_3: 4e-05
	loss_policy_4: 0.00315
	accuracy_policy_4: 0.80973
	loss_value_4: 0.02492
	loss_reward_4: 3e-05
	loss_policy_5: 0.00339
	accuracy_policy_5: 0.8023
	loss_value_5: 0.02481
	loss_reward_5: 4e-05
	loss_policy: 0.02151
	loss_value: 0.25138
	loss_reward: 0.00018
Optimization_Done 3600
[2024-05-04 09:28:26] [command] train weight_iter_3600.pkl 15 19
[2024-05-04 09:28:43] nn step 3650, lr: 0.1.
	loss_policy_0: 0.00603
	accuracy_policy_0: 0.88445
	loss_value_0: 0.11734
	loss_policy_1: 0.00241
	accuracy_policy_1: 0.84332
	loss_value_1: 0.02342
	loss_reward_1: 3e-05
	loss_policy_2: 0.00302
	accuracy_policy_2: 0.81332
	loss_value_2: 0.02331
	loss_reward_2: 3e-05
	loss_policy_3: 0.00321
	accuracy_policy_3: 0.8084
	loss_value_3: 0.02322
	loss_reward_3: 4e-05
	loss_policy_4: 0.00329
	accuracy_policy_4: 0.79098
	loss_value_4: 0.02311
	loss_reward_4: 3e-05
	loss_policy_5: 0.00359
	accuracy_policy_5: 0.79039
	loss_value_5: 0.02302
	loss_reward_5: 3e-05
	loss_policy: 0.02155
	loss_value: 0.23343
	loss_reward: 0.00016
[2024-05-04 09:28:59] nn step 3700, lr: 0.1.
	loss_policy_0: 0.00576
	accuracy_policy_0: 0.88469
	loss_value_0: 0.12029
	loss_policy_1: 0.00239
	accuracy_policy_1: 0.8466
	loss_value_1: 0.024
	loss_reward_1: 5e-05
	loss_policy_2: 0.00286
	accuracy_policy_2: 0.82293
	loss_value_2: 0.0239
	loss_reward_2: 4e-05
	loss_policy_3: 0.00314
	accuracy_policy_3: 0.81445
	loss_value_3: 0.0238
	loss_reward_3: 5e-05
	loss_policy_4: 0.0032
	accuracy_policy_4: 0.80285
	loss_value_4: 0.02368
	loss_reward_4: 4e-05
	loss_policy_5: 0.00344
	accuracy_policy_5: 0.79805
	loss_value_5: 0.02359
	loss_reward_5: 3e-05
	loss_policy: 0.02078
	loss_value: 0.23925
	loss_reward: 0.0002
[2024-05-04 09:29:15] nn step 3750, lr: 0.1.
	loss_policy_0: 0.00607
	accuracy_policy_0: 0.88617
	loss_value_0: 0.1194
	loss_policy_1: 0.00234
	accuracy_policy_1: 0.84844
	loss_value_1: 0.02384
	loss_reward_1: 4e-05
	loss_policy_2: 0.00294
	accuracy_policy_2: 0.81883
	loss_value_2: 0.02374
	loss_reward_2: 3e-05
	loss_policy_3: 0.00314
	accuracy_policy_3: 0.81809
	loss_value_3: 0.02364
	loss_reward_3: 4e-05
	loss_policy_4: 0.00322
	accuracy_policy_4: 0.80211
	loss_value_4: 0.02356
	loss_reward_4: 4e-05
	loss_policy_5: 0.00343
	accuracy_policy_5: 0.80062
	loss_value_5: 0.02345
	loss_reward_5: 4e-05
	loss_policy: 0.02115
	loss_value: 0.23763
	loss_reward: 0.00019
[2024-05-04 09:29:32] nn step 3800, lr: 0.1.
	loss_policy_0: 0.00562
	accuracy_policy_0: 0.89023
	loss_value_0: 0.11156
	loss_policy_1: 0.00222
	accuracy_policy_1: 0.84914
	loss_value_1: 0.02227
	loss_reward_1: 3e-05
	loss_policy_2: 0.00275
	accuracy_policy_2: 0.82035
	loss_value_2: 0.02219
	loss_reward_2: 3e-05
	loss_policy_3: 0.00292
	accuracy_policy_3: 0.81293
	loss_value_3: 0.02207
	loss_reward_3: 3e-05
	loss_policy_4: 0.00302
	accuracy_policy_4: 0.80496
	loss_value_4: 0.02199
	loss_reward_4: 3e-05
	loss_policy_5: 0.00331
	accuracy_policy_5: 0.79402
	loss_value_5: 0.0219
	loss_reward_5: 3e-05
	loss_policy: 0.01984
	loss_value: 0.22199
	loss_reward: 0.00015
Optimization_Done 3800
[2024-05-04 09:31:34] [command] train weight_iter_3800.pkl 16 20
[2024-05-04 09:31:51] nn step 3850, lr: 0.1.
	loss_policy_0: 0.01093
	accuracy_policy_0: 0.85727
	loss_value_0: 0.11187
	loss_policy_1: 0.00294
	accuracy_policy_1: 0.83234
	loss_value_1: 0.02185
	loss_reward_1: 0.0018
	loss_policy_2: 0.0033
	accuracy_policy_2: 0.8132
	loss_value_2: 0.02123
	loss_reward_2: 0.00178
	loss_policy_3: 0.00335
	accuracy_policy_3: 0.81316
	loss_value_3: 0.02075
	loss_reward_3: 0.00169
	loss_policy_4: 0.00357
	accuracy_policy_4: 0.80605
	loss_value_4: 0.02024
	loss_reward_4: 0.00172
	loss_policy_5: 0.00373
	accuracy_policy_5: 0.8034
	loss_value_5: 0.01969
	loss_reward_5: 0.00176
	loss_policy: 0.02782
	loss_value: 0.21564
	loss_reward: 0.00875
[2024-05-04 09:32:07] nn step 3900, lr: 0.1.
	loss_policy_0: 0.00736
	accuracy_policy_0: 0.88531
	loss_value_0: 0.10824
	loss_policy_1: 0.00208
	accuracy_policy_1: 0.86566
	loss_value_1: 0.02128
	loss_reward_1: 0.00144
	loss_policy_2: 0.00247
	accuracy_policy_2: 0.84434
	loss_value_2: 0.02083
	loss_reward_2: 0.00133
	loss_policy_3: 0.00259
	accuracy_policy_3: 0.8368
	loss_value_3: 0.02042
	loss_reward_3: 0.00134
	loss_policy_4: 0.00278
	accuracy_policy_4: 0.82945
	loss_value_4: 0.01996
	loss_reward_4: 0.00138
	loss_policy_5: 0.00282
	accuracy_policy_5: 0.82664
	loss_value_5: 0.01956
	loss_reward_5: 0.00136
	loss_policy: 0.0201
	loss_value: 0.21029
	loss_reward: 0.00686
[2024-05-04 09:32:24] nn step 3950, lr: 0.1.
	loss_policy_0: 0.00574
	accuracy_policy_0: 0.90129
	loss_value_0: 0.10381
	loss_policy_1: 0.00181
	accuracy_policy_1: 0.87684
	loss_value_1: 0.02039
	loss_reward_1: 0.0012
	loss_policy_2: 0.00222
	accuracy_policy_2: 0.85266
	loss_value_2: 0.01994
	loss_reward_2: 0.00122
	loss_policy_3: 0.00242
	accuracy_policy_3: 0.8473
	loss_value_3: 0.01953
	loss_reward_3: 0.00114
	loss_policy_4: 0.00247
	accuracy_policy_4: 0.84164
	loss_value_4: 0.0191
	loss_reward_4: 0.00115
	loss_policy_5: 0.00257
	accuracy_policy_5: 0.83621
	loss_value_5: 0.01871
	loss_reward_5: 0.00112
	loss_policy: 0.01722
	loss_value: 0.20148
	loss_reward: 0.00583
[2024-05-04 09:32:40] nn step 4000, lr: 0.1.
	loss_policy_0: 0.00546
	accuracy_policy_0: 0.90938
	loss_value_0: 0.10308
	loss_policy_1: 0.0017
	accuracy_policy_1: 0.88203
	loss_value_1: 0.02016
	loss_reward_1: 0.00116
	loss_policy_2: 0.002
	accuracy_policy_2: 0.8657
	loss_value_2: 0.01971
	loss_reward_2: 0.00116
	loss_policy_3: 0.00221
	accuracy_policy_3: 0.85699
	loss_value_3: 0.0193
	loss_reward_3: 0.00107
	loss_policy_4: 0.00231
	accuracy_policy_4: 0.85359
	loss_value_4: 0.01886
	loss_reward_4: 0.00113
	loss_policy_5: 0.00244
	accuracy_policy_5: 0.84953
	loss_value_5: 0.01844
	loss_reward_5: 0.00108
	loss_policy: 0.01612
	loss_value: 0.19955
	loss_reward: 0.0056
Optimization_Done 4000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 09:36:39] [command] train weight_iter_4000.pkl 17 21
[2024-05-04 09:37:07] nn step 4050, lr: 0.08.
	loss_policy_0: 0.01069
	accuracy_policy_0: 0.85184
	loss_value_0: 0.11437
	loss_policy_1: 0.00319
	accuracy_policy_1: 0.82543
	loss_value_1: 0.02258
	loss_reward_1: 0.0011
	loss_policy_2: 0.0037
	accuracy_policy_2: 0.80191
	loss_value_2: 0.0222
	loss_reward_2: 0.00111
	loss_policy_3: 0.00419
	accuracy_policy_3: 0.80059
	loss_value_3: 0.02184
	loss_reward_3: 0.00111
	loss_policy_4: 0.00436
	accuracy_policy_4: 0.77836
	loss_value_4: 0.02142
	loss_reward_4: 0.00114
	loss_policy_5: 0.00456
	accuracy_policy_5: 0.78277
	loss_value_5: 0.021
	loss_reward_5: 0.00112
	loss_policy: 0.0307
	loss_value: 0.2234
	loss_reward: 0.00558
[2024-05-04 09:37:22] nn step 4100, lr: 0.08.
	loss_policy_0: 0.00798
	accuracy_policy_0: 0.87121
	loss_value_0: 0.10834
	loss_policy_1: 0.00249
	accuracy_policy_1: 0.84336
	loss_value_1: 0.02138
	loss_reward_1: 0.00107
	loss_policy_2: 0.00291
	accuracy_policy_2: 0.8218
	loss_value_2: 0.02099
	loss_reward_2: 0.00109
	loss_policy_3: 0.0033
	accuracy_policy_3: 0.81758
	loss_value_3: 0.02061
	loss_reward_3: 0.00109
	loss_policy_4: 0.00342
	accuracy_policy_4: 0.79574
	loss_value_4: 0.02023
	loss_reward_4: 0.00105
	loss_policy_5: 0.00351
	accuracy_policy_5: 0.80172
	loss_value_5: 0.0198
	loss_reward_5: 0.00114
	loss_policy: 0.02361
	loss_value: 0.21136
	loss_reward: 0.00543
[2024-05-04 09:37:38] nn step 4150, lr: 0.08.
	loss_policy_0: 0.00746
	accuracy_policy_0: 0.87898
	loss_value_0: 0.10975
	loss_policy_1: 0.00234
	accuracy_policy_1: 0.84938
	loss_value_1: 0.02165
	loss_reward_1: 0.00111
	loss_policy_2: 0.00275
	accuracy_policy_2: 0.83023
	loss_value_2: 0.02123
	loss_reward_2: 0.00114
	loss_policy_3: 0.00313
	accuracy_policy_3: 0.82355
	loss_value_3: 0.02083
	loss_reward_3: 0.00108
	loss_policy_4: 0.0032
	accuracy_policy_4: 0.80727
	loss_value_4: 0.02042
	loss_reward_4: 0.00112
	loss_policy_5: 0.00332
	accuracy_policy_5: 0.81293
	loss_value_5: 0.01999
	loss_reward_5: 0.00108
	loss_policy: 0.02219
	loss_value: 0.21387
	loss_reward: 0.00552
[2024-05-04 09:37:53] nn step 4200, lr: 0.08.
	loss_policy_0: 0.0072
	accuracy_policy_0: 0.88238
	loss_value_0: 0.10727
	loss_policy_1: 0.00224
	accuracy_policy_1: 0.8577
	loss_value_1: 0.02111
	loss_reward_1: 0.00109
	loss_policy_2: 0.00266
	accuracy_policy_2: 0.83527
	loss_value_2: 0.02069
	loss_reward_2: 0.00111
	loss_policy_3: 0.00298
	accuracy_policy_3: 0.82527
	loss_value_3: 0.02031
	loss_reward_3: 0.00102
	loss_policy_4: 0.00309
	accuracy_policy_4: 0.81363
	loss_value_4: 0.01988
	loss_reward_4: 0.0011
	loss_policy_5: 0.0032
	accuracy_policy_5: 0.81676
	loss_value_5: 0.01937
	loss_reward_5: 0.00111
	loss_policy: 0.02138
	loss_value: 0.20863
	loss_reward: 0.00543
Optimization_Done 4200
[2024-05-04 09:39:56] [command] train weight_iter_4200.pkl 18 22
[2024-05-04 09:40:13] nn step 4250, lr: 0.08.
	loss_policy_0: 0.02305
	accuracy_policy_0: 0.70211
	loss_value_0: 0.09502
	loss_policy_1: 0.005
	accuracy_policy_1: 0.71348
	loss_value_1: 0.01834
	loss_reward_1: 0.00301
	loss_policy_2: 0.00516
	accuracy_policy_2: 0.70312
	loss_value_2: 0.0175
	loss_reward_2: 0.00301
	loss_policy_3: 0.00532
	accuracy_policy_3: 0.70223
	loss_value_3: 0.01666
	loss_reward_3: 0.00299
	loss_policy_4: 0.00559
	accuracy_policy_4: 0.68832
	loss_value_4: 0.01577
	loss_reward_4: 0.00299
	loss_policy_5: 0.00577
	accuracy_policy_5: 0.68543
	loss_value_5: 0.01485
	loss_reward_5: 0.00299
	loss_policy: 0.04989
	loss_value: 0.17813
	loss_reward: 0.01499
[2024-05-04 09:40:29] nn step 4300, lr: 0.08.
	loss_policy_0: 0.01535
	accuracy_policy_0: 0.74812
	loss_value_0: 0.09194
	loss_policy_1: 0.0035
	accuracy_policy_1: 0.75367
	loss_value_1: 0.01768
	loss_reward_1: 0.00274
	loss_policy_2: 0.00366
	accuracy_policy_2: 0.7598
	loss_value_2: 0.0169
	loss_reward_2: 0.00271
	loss_policy_3: 0.00379
	accuracy_policy_3: 0.76676
	loss_value_3: 0.01604
	loss_reward_3: 0.00271
	loss_policy_4: 0.00379
	accuracy_policy_4: 0.76508
	loss_value_4: 0.01514
	loss_reward_4: 0.0028
	loss_policy_5: 0.00388
	accuracy_policy_5: 0.76617
	loss_value_5: 0.0142
	loss_reward_5: 0.0028
	loss_policy: 0.03396
	loss_value: 0.17191
	loss_reward: 0.01377
[2024-05-04 09:40:45] nn step 4350, lr: 0.08.
	loss_policy_0: 0.01409
	accuracy_policy_0: 0.7691
	loss_value_0: 0.09176
	loss_policy_1: 0.00311
	accuracy_policy_1: 0.78219
	loss_value_1: 0.01763
	loss_reward_1: 0.00279
	loss_policy_2: 0.00325
	accuracy_policy_2: 0.78
	loss_value_2: 0.0168
	loss_reward_2: 0.00272
	loss_policy_3: 0.00335
	accuracy_policy_3: 0.78656
	loss_value_3: 0.01596
	loss_reward_3: 0.00271
	loss_policy_4: 0.00336
	accuracy_policy_4: 0.78805
	loss_value_4: 0.01505
	loss_reward_4: 0.00267
	loss_policy_5: 0.00335
	accuracy_policy_5: 0.79531
	loss_value_5: 0.01406
	loss_reward_5: 0.0028
	loss_policy: 0.03052
	loss_value: 0.17126
	loss_reward: 0.01368
[2024-05-04 09:41:01] nn step 4400, lr: 0.08.
	loss_policy_0: 0.01264
	accuracy_policy_0: 0.80016
	loss_value_0: 0.09322
	loss_policy_1: 0.00286
	accuracy_policy_1: 0.81293
	loss_value_1: 0.01794
	loss_reward_1: 0.00274
	loss_policy_2: 0.00294
	accuracy_policy_2: 0.81523
	loss_value_2: 0.01713
	loss_reward_2: 0.0027
	loss_policy_3: 0.00305
	accuracy_policy_3: 0.82695
	loss_value_3: 0.01626
	loss_reward_3: 0.00272
	loss_policy_4: 0.00308
	accuracy_policy_4: 0.82367
	loss_value_4: 0.01535
	loss_reward_4: 0.00278
	loss_policy_5: 0.00304
	accuracy_policy_5: 0.82988
	loss_value_5: 0.01438
	loss_reward_5: 0.00282
	loss_policy: 0.02761
	loss_value: 0.17428
	loss_reward: 0.01376
Optimization_Done 4400
[2024-05-04 09:43:03] [command] train weight_iter_4400.pkl 19 23
[2024-05-04 09:43:20] nn step 4450, lr: 0.08.
	loss_policy_0: 0.01478
	accuracy_policy_0: 0.81922
	loss_value_0: 0.09173
	loss_policy_1: 0.00345
	accuracy_policy_1: 0.82031
	loss_value_1: 0.01774
	loss_reward_1: 0.00256
	loss_policy_2: 0.00366
	accuracy_policy_2: 0.82406
	loss_value_2: 0.01693
	loss_reward_2: 0.00255
	loss_policy_3: 0.00387
	accuracy_policy_3: 0.82129
	loss_value_3: 0.01614
	loss_reward_3: 0.00256
	loss_policy_4: 0.00389
	accuracy_policy_4: 0.82711
	loss_value_4: 0.01532
	loss_reward_4: 0.00255
	loss_policy_5: 0.00401
	accuracy_policy_5: 0.82445
	loss_value_5: 0.01443
	loss_reward_5: 0.00265
	loss_policy: 0.03365
	loss_value: 0.17229
	loss_reward: 0.01288
[2024-05-04 09:43:36] nn step 4500, lr: 0.08.
	loss_policy_0: 0.01224
	accuracy_policy_0: 0.84387
	loss_value_0: 0.09635
	loss_policy_1: 0.00292
	accuracy_policy_1: 0.84207
	loss_value_1: 0.01859
	loss_reward_1: 0.00269
	loss_policy_2: 0.00316
	accuracy_policy_2: 0.84098
	loss_value_2: 0.0177
	loss_reward_2: 0.0027
	loss_policy_3: 0.00334
	accuracy_policy_3: 0.84199
	loss_value_3: 0.01687
	loss_reward_3: 0.00268
	loss_policy_4: 0.00342
	accuracy_policy_4: 0.84223
	loss_value_4: 0.01598
	loss_reward_4: 0.00264
	loss_policy_5: 0.00352
	accuracy_policy_5: 0.8384
	loss_value_5: 0.01499
	loss_reward_5: 0.00276
	loss_policy: 0.02861
	loss_value: 0.18048
	loss_reward: 0.01348
[2024-05-04 09:43:52] nn step 4550, lr: 0.08.
	loss_policy_0: 0.01145
	accuracy_policy_0: 0.86258
	loss_value_0: 0.09729
	loss_policy_1: 0.00276
	accuracy_policy_1: 0.85363
	loss_value_1: 0.0188
	loss_reward_1: 0.0027
	loss_policy_2: 0.003
	accuracy_policy_2: 0.85094
	loss_value_2: 0.01795
	loss_reward_2: 0.00271
	loss_policy_3: 0.00318
	accuracy_policy_3: 0.85012
	loss_value_3: 0.01714
	loss_reward_3: 0.00271
	loss_policy_4: 0.00328
	accuracy_policy_4: 0.85336
	loss_value_4: 0.01631
	loss_reward_4: 0.00261
	loss_policy_5: 0.00326
	accuracy_policy_5: 0.85059
	loss_value_5: 0.01539
	loss_reward_5: 0.00279
	loss_policy: 0.02692
	loss_value: 0.18288
	loss_reward: 0.01352
[2024-05-04 09:44:08] nn step 4600, lr: 0.08.
	loss_policy_0: 0.01041
	accuracy_policy_0: 0.86203
	loss_value_0: 0.09167
	loss_policy_1: 0.00251
	accuracy_policy_1: 0.85262
	loss_value_1: 0.01777
	loss_reward_1: 0.00247
	loss_policy_2: 0.00272
	accuracy_policy_2: 0.85449
	loss_value_2: 0.01705
	loss_reward_2: 0.00246
	loss_policy_3: 0.00287
	accuracy_policy_3: 0.85047
	loss_value_3: 0.01625
	loss_reward_3: 0.00255
	loss_policy_4: 0.00293
	accuracy_policy_4: 0.85613
	loss_value_4: 0.01545
	loss_reward_4: 0.0025
	loss_policy_5: 0.00299
	accuracy_policy_5: 0.8532
	loss_value_5: 0.0146
	loss_reward_5: 0.00254
	loss_policy: 0.02443
	loss_value: 0.17279
	loss_reward: 0.01252
Optimization_Done 4600
[2024-05-04 09:46:07] [command] train weight_iter_4600.pkl 20 24
[2024-05-04 09:46:25] nn step 4650, lr: 0.08.
	loss_policy_0: 0.01148
	accuracy_policy_0: 0.84426
	loss_value_0: 0.09139
	loss_policy_1: 0.00279
	accuracy_policy_1: 0.8325
	loss_value_1: 0.0177
	loss_reward_1: 0.00238
	loss_policy_2: 0.00302
	accuracy_policy_2: 0.83711
	loss_value_2: 0.01703
	loss_reward_2: 0.00231
	loss_policy_3: 0.00314
	accuracy_policy_3: 0.83305
	loss_value_3: 0.01642
	loss_reward_3: 0.00227
	loss_policy_4: 0.00319
	accuracy_policy_4: 0.83898
	loss_value_4: 0.01567
	loss_reward_4: 0.00238
	loss_policy_5: 0.00322
	accuracy_policy_5: 0.83852
	loss_value_5: 0.0149
	loss_reward_5: 0.00236
	loss_policy: 0.02684
	loss_value: 0.17311
	loss_reward: 0.0117
[2024-05-04 09:46:41] nn step 4700, lr: 0.08.
	loss_policy_0: 0.01065
	accuracy_policy_0: 0.8566
	loss_value_0: 0.09209
	loss_policy_1: 0.00263
	accuracy_policy_1: 0.84391
	loss_value_1: 0.01796
	loss_reward_1: 0.00226
	loss_policy_2: 0.0028
	accuracy_policy_2: 0.84234
	loss_value_2: 0.01736
	loss_reward_2: 0.00226
	loss_policy_3: 0.00295
	accuracy_policy_3: 0.84293
	loss_value_3: 0.01671
	loss_reward_3: 0.00233
	loss_policy_4: 0.003
	accuracy_policy_4: 0.84426
	loss_value_4: 0.01606
	loss_reward_4: 0.00227
	loss_policy_5: 0.00302
	accuracy_policy_5: 0.84082
	loss_value_5: 0.01537
	loss_reward_5: 0.00234
	loss_policy: 0.02505
	loss_value: 0.17555
	loss_reward: 0.01147
[2024-05-04 09:46:57] nn step 4750, lr: 0.08.
	loss_policy_0: 0.01119
	accuracy_policy_0: 0.85371
	loss_value_0: 0.09777
	loss_policy_1: 0.00269
	accuracy_policy_1: 0.8493
	loss_value_1: 0.01908
	loss_reward_1: 0.00239
	loss_policy_2: 0.00289
	accuracy_policy_2: 0.8443
	loss_value_2: 0.01846
	loss_reward_2: 0.0024
	loss_policy_3: 0.00304
	accuracy_policy_3: 0.8432
	loss_value_3: 0.01781
	loss_reward_3: 0.0024
	loss_policy_4: 0.00308
	accuracy_policy_4: 0.84633
	loss_value_4: 0.0172
	loss_reward_4: 0.00239
	loss_policy_5: 0.00314
	accuracy_policy_5: 0.84355
	loss_value_5: 0.01646
	loss_reward_5: 0.00242
	loss_policy: 0.02603
	loss_value: 0.18679
	loss_reward: 0.012
[2024-05-04 09:47:13] nn step 4800, lr: 0.08.
	loss_policy_0: 0.00973
	accuracy_policy_0: 0.86531
	loss_value_0: 0.09219
	loss_policy_1: 0.00253
	accuracy_policy_1: 0.84973
	loss_value_1: 0.01798
	loss_reward_1: 0.00217
	loss_policy_2: 0.00268
	accuracy_policy_2: 0.8475
	loss_value_2: 0.01739
	loss_reward_2: 0.00228
	loss_policy_3: 0.0028
	accuracy_policy_3: 0.85113
	loss_value_3: 0.01689
	loss_reward_3: 0.0021
	loss_policy_4: 0.00287
	accuracy_policy_4: 0.85152
	loss_value_4: 0.0163
	loss_reward_4: 0.00218
	loss_policy_5: 0.00296
	accuracy_policy_5: 0.84441
	loss_value_5: 0.01558
	loss_reward_5: 0.00234
	loss_policy: 0.02358
	loss_value: 0.17632
	loss_reward: 0.01108
Optimization_Done 4800
[2024-05-04 09:49:05] [command] train weight_iter_4800.pkl 21 25
[2024-05-04 09:49:23] nn step 4850, lr: 0.08.
	loss_policy_0: 0.01308
	accuracy_policy_0: 0.86488
	loss_value_0: 0.09597
	loss_policy_1: 0.00338
	accuracy_policy_1: 0.84445
	loss_value_1: 0.01877
	loss_reward_1: 0.00194
	loss_policy_2: 0.00363
	accuracy_policy_2: 0.84316
	loss_value_2: 0.01833
	loss_reward_2: 0.0019
	loss_policy_3: 0.00377
	accuracy_policy_3: 0.84008
	loss_value_3: 0.01784
	loss_reward_3: 0.00198
	loss_policy_4: 0.0039
	accuracy_policy_4: 0.83484
	loss_value_4: 0.01742
	loss_reward_4: 0.00181
	loss_policy_5: 0.00401
	accuracy_policy_5: 0.82977
	loss_value_5: 0.01694
	loss_reward_5: 0.0019
	loss_policy: 0.03178
	loss_value: 0.18528
	loss_reward: 0.00952
[2024-05-04 09:49:39] nn step 4900, lr: 0.08.
	loss_policy_0: 0.01067
	accuracy_policy_0: 0.87793
	loss_value_0: 0.09505
	loss_policy_1: 0.00291
	accuracy_policy_1: 0.85863
	loss_value_1: 0.01866
	loss_reward_1: 0.00187
	loss_policy_2: 0.00322
	accuracy_policy_2: 0.84492
	loss_value_2: 0.01827
	loss_reward_2: 0.00183
	loss_policy_3: 0.00336
	accuracy_policy_3: 0.84465
	loss_value_3: 0.01787
	loss_reward_3: 0.00176
	loss_policy_4: 0.00348
	accuracy_policy_4: 0.8398
	loss_value_4: 0.01743
	loss_reward_4: 0.00184
	loss_policy_5: 0.00348
	accuracy_policy_5: 0.83926
	loss_value_5: 0.01701
	loss_reward_5: 0.00182
	loss_policy: 0.02711
	loss_value: 0.18428
	loss_reward: 0.00912
[2024-05-04 09:49:55] nn step 4950, lr: 0.08.
	loss_policy_0: 0.00975
	accuracy_policy_0: 0.88777
	loss_value_0: 0.09536
	loss_policy_1: 0.0028
	accuracy_policy_1: 0.85969
	loss_value_1: 0.01881
	loss_reward_1: 0.0018
	loss_policy_2: 0.0031
	accuracy_policy_2: 0.85148
	loss_value_2: 0.01846
	loss_reward_2: 0.00173
	loss_policy_3: 0.00322
	accuracy_policy_3: 0.8516
	loss_value_3: 0.0181
	loss_reward_3: 0.00175
	loss_policy_4: 0.00334
	accuracy_policy_4: 0.84508
	loss_value_4: 0.01776
	loss_reward_4: 0.00174
	loss_policy_5: 0.00341
	accuracy_policy_5: 0.83934
	loss_value_5: 0.01738
	loss_reward_5: 0.00185
	loss_policy: 0.02561
	loss_value: 0.18587
	loss_reward: 0.00887
[2024-05-04 09:50:11] nn step 5000, lr: 0.08.
	loss_policy_0: 0.01011
	accuracy_policy_0: 0.88387
	loss_value_0: 0.09735
	loss_policy_1: 0.00286
	accuracy_policy_1: 0.8582
	loss_value_1: 0.01929
	loss_reward_1: 0.00168
	loss_policy_2: 0.00321
	accuracy_policy_2: 0.84539
	loss_value_2: 0.019
	loss_reward_2: 0.00167
	loss_policy_3: 0.00333
	accuracy_policy_3: 0.84863
	loss_value_3: 0.0187
	loss_reward_3: 0.00175
	loss_policy_4: 0.00343
	accuracy_policy_4: 0.84125
	loss_value_4: 0.01837
	loss_reward_4: 0.00172
	loss_policy_5: 0.00347
	accuracy_policy_5: 0.84195
	loss_value_5: 0.01802
	loss_reward_5: 0.00177
	loss_policy: 0.02642
	loss_value: 0.19074
	loss_reward: 0.00859
Optimization_Done 5000
[2024-05-04 09:52:11] [command] train weight_iter_5000.pkl 22 26
[2024-05-04 09:52:28] nn step 5050, lr: 0.08.
	loss_policy_0: 0.04921
	accuracy_policy_0: 0.72516
	loss_value_0: 0.1057
	loss_policy_1: 0.00968
	accuracy_policy_1: 0.73043
	loss_value_1: 0.02091
	loss_reward_1: 0.00149
	loss_policy_2: 0.00945
	accuracy_policy_2: 0.73219
	loss_value_2: 0.02056
	loss_reward_2: 0.00152
	loss_policy_3: 0.0093
	accuracy_policy_3: 0.74617
	loss_value_3: 0.02027
	loss_reward_3: 0.00145
	loss_policy_4: 0.00945
	accuracy_policy_4: 0.73809
	loss_value_4: 0.01999
	loss_reward_4: 0.00153
	loss_policy_5: 0.00958
	accuracy_policy_5: 0.73105
	loss_value_5: 0.01972
	loss_reward_5: 0.00161
	loss_policy: 0.09665
	loss_value: 0.20716
	loss_reward: 0.00759
[2024-05-04 09:52:44] nn step 5100, lr: 0.08.
	loss_policy_0: 0.03272
	accuracy_policy_0: 0.79949
	loss_value_0: 0.10333
	loss_policy_1: 0.00662
	accuracy_policy_1: 0.8023
	loss_value_1: 0.02045
	loss_reward_1: 0.00149
	loss_policy_2: 0.00651
	accuracy_policy_2: 0.80207
	loss_value_2: 0.02015
	loss_reward_2: 0.00148
	loss_policy_3: 0.00654
	accuracy_policy_3: 0.80109
	loss_value_3: 0.01981
	loss_reward_3: 0.00155
	loss_policy_4: 0.00669
	accuracy_policy_4: 0.79863
	loss_value_4: 0.01951
	loss_reward_4: 0.00149
	loss_policy_5: 0.00668
	accuracy_policy_5: 0.79598
	loss_value_5: 0.01922
	loss_reward_5: 0.00151
	loss_policy: 0.06575
	loss_value: 0.20247
	loss_reward: 0.00753
[2024-05-04 09:53:01] nn step 5150, lr: 0.08.
	loss_policy_0: 0.02408
	accuracy_policy_0: 0.83016
	loss_value_0: 0.09879
	loss_policy_1: 0.0053
	accuracy_policy_1: 0.82121
	loss_value_1: 0.01962
	loss_reward_1: 0.00145
	loss_policy_2: 0.00548
	accuracy_policy_2: 0.81395
	loss_value_2: 0.01937
	loss_reward_2: 0.00144
	loss_policy_3: 0.00544
	accuracy_policy_3: 0.82348
	loss_value_3: 0.0191
	loss_reward_3: 0.00143
	loss_policy_4: 0.00557
	accuracy_policy_4: 0.82059
	loss_value_4: 0.01885
	loss_reward_4: 0.00145
	loss_policy_5: 0.00556
	accuracy_policy_5: 0.81977
	loss_value_5: 0.01855
	loss_reward_5: 0.00146
	loss_policy: 0.05143
	loss_value: 0.19427
	loss_reward: 0.00723
[2024-05-04 09:53:17] nn step 5200, lr: 0.08.
	loss_policy_0: 0.02322
	accuracy_policy_0: 0.83934
	loss_value_0: 0.10286
	loss_policy_1: 0.00514
	accuracy_policy_1: 0.83047
	loss_value_1: 0.02045
	loss_reward_1: 0.00154
	loss_policy_2: 0.00532
	accuracy_policy_2: 0.82289
	loss_value_2: 0.02025
	loss_reward_2: 0.00145
	loss_policy_3: 0.00532
	accuracy_policy_3: 0.82684
	loss_value_3: 0.02005
	loss_reward_3: 0.00141
	loss_policy_4: 0.00529
	accuracy_policy_4: 0.8266
	loss_value_4: 0.0198
	loss_reward_4: 0.00148
	loss_policy_5: 0.00537
	accuracy_policy_5: 0.82277
	loss_value_5: 0.01956
	loss_reward_5: 0.00153
	loss_policy: 0.04965
	loss_value: 0.20296
	loss_reward: 0.0074
Optimization_Done 5200
[2024-05-04 09:55:20] [command] train weight_iter_5200.pkl 23 27
[2024-05-04 09:55:37] nn step 5250, lr: 0.08.
	loss_policy_0: 0.04476
	accuracy_policy_0: 0.70754
	loss_value_0: 0.10853
	loss_policy_1: 0.01028
	accuracy_policy_1: 0.68223
	loss_value_1: 0.02168
	loss_reward_1: 5e-05
	loss_policy_2: 0.01021
	accuracy_policy_2: 0.67891
	loss_value_2: 0.02159
	loss_reward_2: 3e-05
	loss_policy_3: 0.01056
	accuracy_policy_3: 0.68492
	loss_value_3: 0.02153
	loss_reward_3: 3e-05
	loss_policy_4: 0.01048
	accuracy_policy_4: 0.67715
	loss_value_4: 0.02149
	loss_reward_4: 4e-05
	loss_policy_5: 0.01087
	accuracy_policy_5: 0.67559
	loss_value_5: 0.02141
	loss_reward_5: 3e-05
	loss_policy: 0.09716
	loss_value: 0.21623
	loss_reward: 0.00019
[2024-05-04 09:55:53] nn step 5300, lr: 0.08.
	loss_policy_0: 0.03455
	accuracy_policy_0: 0.75137
	loss_value_0: 0.11086
	loss_policy_1: 0.00832
	accuracy_policy_1: 0.72648
	loss_value_1: 0.02215
	loss_reward_1: 2e-05
	loss_policy_2: 0.00844
	accuracy_policy_2: 0.71766
	loss_value_2: 0.02208
	loss_reward_2: 2e-05
	loss_policy_3: 0.00853
	accuracy_policy_3: 0.72844
	loss_value_3: 0.02201
	loss_reward_3: 3e-05
	loss_policy_4: 0.00849
	accuracy_policy_4: 0.71832
	loss_value_4: 0.02195
	loss_reward_4: 3e-05
	loss_policy_5: 0.00872
	accuracy_policy_5: 0.71387
	loss_value_5: 0.0219
	loss_reward_5: 3e-05
	loss_policy: 0.07706
	loss_value: 0.22095
	loss_reward: 0.00013
[2024-05-04 09:56:09] nn step 5350, lr: 0.08.
	loss_policy_0: 0.03051
	accuracy_policy_0: 0.76883
	loss_value_0: 0.11403
	loss_policy_1: 0.00747
	accuracy_policy_1: 0.75004
	loss_value_1: 0.02278
	loss_reward_1: 3e-05
	loss_policy_2: 0.00783
	accuracy_policy_2: 0.73371
	loss_value_2: 0.0227
	loss_reward_2: 2e-05
	loss_policy_3: 0.00813
	accuracy_policy_3: 0.72836
	loss_value_3: 0.02265
	loss_reward_3: 4e-05
	loss_policy_4: 0.00804
	accuracy_policy_4: 0.7318
	loss_value_4: 0.02258
	loss_reward_4: 2e-05
	loss_policy_5: 0.00814
	accuracy_policy_5: 0.72652
	loss_value_5: 0.02252
	loss_reward_5: 3e-05
	loss_policy: 0.07012
	loss_value: 0.22725
	loss_reward: 0.00013
[2024-05-04 09:56:26] nn step 5400, lr: 0.08.
	loss_policy_0: 0.02806
	accuracy_policy_0: 0.77281
	loss_value_0: 0.11302
	loss_policy_1: 0.00714
	accuracy_policy_1: 0.74891
	loss_value_1: 0.02255
	loss_reward_1: 2e-05
	loss_policy_2: 0.00744
	accuracy_policy_2: 0.73617
	loss_value_2: 0.02246
	loss_reward_2: 3e-05
	loss_policy_3: 0.00782
	accuracy_policy_3: 0.73316
	loss_value_3: 0.02239
	loss_reward_3: 2e-05
	loss_policy_4: 0.00784
	accuracy_policy_4: 0.73555
	loss_value_4: 0.02232
	loss_reward_4: 2e-05
	loss_policy_5: 0.008
	accuracy_policy_5: 0.72641
	loss_value_5: 0.02225
	loss_reward_5: 3e-05
	loss_policy: 0.06629
	loss_value: 0.22499
	loss_reward: 0.00012
Optimization_Done 5400
[2024-05-04 09:58:26] [command] train weight_iter_5400.pkl 24 28
[2024-05-04 09:58:43] nn step 5450, lr: 0.08.
	loss_policy_0: 0.02561
	accuracy_policy_0: 0.78906
	loss_value_0: 0.11653
	loss_policy_1: 0.00687
	accuracy_policy_1: 0.75035
	loss_value_1: 0.02327
	loss_reward_1: 0.0
	loss_policy_2: 0.00752
	accuracy_policy_2: 0.72461
	loss_value_2: 0.02319
	loss_reward_2: 0.0
	loss_policy_3: 0.00761
	accuracy_policy_3: 0.74039
	loss_value_3: 0.02311
	loss_reward_3: 0.0
	loss_policy_4: 0.00737
	accuracy_policy_4: 0.74047
	loss_value_4: 0.02304
	loss_reward_4: 0.0
	loss_policy_5: 0.00754
	accuracy_policy_5: 0.74422
	loss_value_5: 0.02296
	loss_reward_5: 0.0
	loss_policy: 0.06252
	loss_value: 0.23209
	loss_reward: 2e-05
[2024-05-04 09:59:00] nn step 5500, lr: 0.08.
	loss_policy_0: 0.02364
	accuracy_policy_0: 0.7891
	loss_value_0: 0.11675
	loss_policy_1: 0.00619
	accuracy_policy_1: 0.76863
	loss_value_1: 0.0233
	loss_reward_1: 0.0
	loss_policy_2: 0.0068
	accuracy_policy_2: 0.74652
	loss_value_2: 0.02322
	loss_reward_2: 0.0
	loss_policy_3: 0.00692
	accuracy_policy_3: 0.75355
	loss_value_3: 0.02313
	loss_reward_3: 0.0
	loss_policy_4: 0.00677
	accuracy_policy_4: 0.75184
	loss_value_4: 0.02304
	loss_reward_4: 0.0
	loss_policy_5: 0.00686
	accuracy_policy_5: 0.75316
	loss_value_5: 0.02296
	loss_reward_5: 0.0
	loss_policy: 0.0572
	loss_value: 0.2324
	loss_reward: 1e-05
[2024-05-04 09:59:16] nn step 5550, lr: 0.08.
	loss_policy_0: 0.02072
	accuracy_policy_0: 0.80668
	loss_value_0: 0.10883
	loss_policy_1: 0.00548
	accuracy_policy_1: 0.77457
	loss_value_1: 0.02172
	loss_reward_1: 0.0
	loss_policy_2: 0.00616
	accuracy_policy_2: 0.74891
	loss_value_2: 0.02164
	loss_reward_2: 0.0
	loss_policy_3: 0.00627
	accuracy_policy_3: 0.75371
	loss_value_3: 0.02159
	loss_reward_3: 0.0
	loss_policy_4: 0.00626
	accuracy_policy_4: 0.75203
	loss_value_4: 0.02153
	loss_reward_4: 0.0
	loss_policy_5: 0.00643
	accuracy_policy_5: 0.7523
	loss_value_5: 0.02145
	loss_reward_5: 0.0
	loss_policy: 0.05133
	loss_value: 0.21677
	loss_reward: 1e-05
[2024-05-04 09:59:32] nn step 5600, lr: 0.08.
	loss_policy_0: 0.02021
	accuracy_policy_0: 0.81016
	loss_value_0: 0.11007
	loss_policy_1: 0.00541
	accuracy_policy_1: 0.77766
	loss_value_1: 0.02198
	loss_reward_1: 0.0
	loss_policy_2: 0.00608
	accuracy_policy_2: 0.74625
	loss_value_2: 0.02192
	loss_reward_2: 0.0
	loss_policy_3: 0.00621
	accuracy_policy_3: 0.76012
	loss_value_3: 0.02186
	loss_reward_3: 0.0
	loss_policy_4: 0.00614
	accuracy_policy_4: 0.75953
	loss_value_4: 0.02176
	loss_reward_4: 0.0
	loss_policy_5: 0.00627
	accuracy_policy_5: 0.76562
	loss_value_5: 0.02169
	loss_reward_5: 0.0
	loss_policy: 0.05033
	loss_value: 0.21929
	loss_reward: 1e-05
Optimization_Done 5600
[2024-05-04 10:01:23] [command] train weight_iter_5600.pkl 25 29
[2024-05-04 10:01:40] nn step 5650, lr: 0.08.
	loss_policy_0: 0.0212
	accuracy_policy_0: 0.78711
	loss_value_0: 0.1019
	loss_policy_1: 0.00554
	accuracy_policy_1: 0.75977
	loss_value_1: 0.02034
	loss_reward_1: 0.0
	loss_policy_2: 0.00614
	accuracy_policy_2: 0.74082
	loss_value_2: 0.02025
	loss_reward_2: 0.0
	loss_policy_3: 0.00639
	accuracy_policy_3: 0.74762
	loss_value_3: 0.02019
	loss_reward_3: 0.0
	loss_policy_4: 0.00646
	accuracy_policy_4: 0.73336
	loss_value_4: 0.0201
	loss_reward_4: 0.0
	loss_policy_5: 0.00647
	accuracy_policy_5: 0.73785
	loss_value_5: 0.02003
	loss_reward_5: 0.0
	loss_policy: 0.0522
	loss_value: 0.2028
	loss_reward: 1e-05
[2024-05-04 10:01:57] nn step 5700, lr: 0.08.
	loss_policy_0: 0.02027
	accuracy_policy_0: 0.7991
	loss_value_0: 0.10611
	loss_policy_1: 0.00544
	accuracy_policy_1: 0.76414
	loss_value_1: 0.02118
	loss_reward_1: 0.0
	loss_policy_2: 0.00618
	accuracy_policy_2: 0.74961
	loss_value_2: 0.02111
	loss_reward_2: 0.0
	loss_policy_3: 0.00632
	accuracy_policy_3: 0.74969
	loss_value_3: 0.02105
	loss_reward_3: 0.0
	loss_policy_4: 0.00638
	accuracy_policy_4: 0.74898
	loss_value_4: 0.02096
	loss_reward_4: 0.0
	loss_policy_5: 0.00649
	accuracy_policy_5: 0.7482
	loss_value_5: 0.02089
	loss_reward_5: 0.0
	loss_policy: 0.05109
	loss_value: 0.21129
	loss_reward: 1e-05
[2024-05-04 10:02:13] nn step 5750, lr: 0.08.
	loss_policy_0: 0.0179
	accuracy_policy_0: 0.80961
	loss_value_0: 0.10108
	loss_policy_1: 0.0049
	accuracy_policy_1: 0.77383
	loss_value_1: 0.02019
	loss_reward_1: 0.0
	loss_policy_2: 0.00571
	accuracy_policy_2: 0.75031
	loss_value_2: 0.02012
	loss_reward_2: 0.0
	loss_policy_3: 0.00591
	accuracy_policy_3: 0.75688
	loss_value_3: 0.02005
	loss_reward_3: 0.0
	loss_policy_4: 0.00605
	accuracy_policy_4: 0.74766
	loss_value_4: 0.01998
	loss_reward_4: 0.0
	loss_policy_5: 0.0061
	accuracy_policy_5: 0.75035
	loss_value_5: 0.01992
	loss_reward_5: 0.0
	loss_policy: 0.04655
	loss_value: 0.20135
	loss_reward: 1e-05
[2024-05-04 10:02:29] nn step 5800, lr: 0.08.
	loss_policy_0: 0.01808
	accuracy_policy_0: 0.81336
	loss_value_0: 0.10124
	loss_policy_1: 0.00506
	accuracy_policy_1: 0.77027
	loss_value_1: 0.02022
	loss_reward_1: 0.0
	loss_policy_2: 0.00571
	accuracy_policy_2: 0.75586
	loss_value_2: 0.02013
	loss_reward_2: 0.0
	loss_policy_3: 0.00597
	accuracy_policy_3: 0.75605
	loss_value_3: 0.02007
	loss_reward_3: 0.0
	loss_policy_4: 0.00602
	accuracy_policy_4: 0.75117
	loss_value_4: 0.02001
	loss_reward_4: 0.0
	loss_policy_5: 0.00606
	accuracy_policy_5: 0.75113
	loss_value_5: 0.01994
	loss_reward_5: 0.0
	loss_policy: 0.0469
	loss_value: 0.20161
	loss_reward: 1e-05
Optimization_Done 5800
[2024-05-04 10:04:28] [command] train weight_iter_5800.pkl 26 30
[2024-05-04 10:04:45] nn step 5850, lr: 0.08.
	loss_policy_0: 0.02902
	accuracy_policy_0: 0.76523
	loss_value_0: 0.11237
	loss_policy_1: 0.00728
	accuracy_policy_1: 0.73109
	loss_value_1: 0.02245
	loss_reward_1: 0.0
	loss_policy_2: 0.00812
	accuracy_policy_2: 0.70586
	loss_value_2: 0.02237
	loss_reward_2: 0.0
	loss_policy_3: 0.00844
	accuracy_policy_3: 0.70527
	loss_value_3: 0.0223
	loss_reward_3: 0.0
	loss_policy_4: 0.00863
	accuracy_policy_4: 0.69695
	loss_value_4: 0.02223
	loss_reward_4: 0.0
	loss_policy_5: 0.0086
	accuracy_policy_5: 0.70199
	loss_value_5: 0.02216
	loss_reward_5: 0.0
	loss_policy: 0.07009
	loss_value: 0.22388
	loss_reward: 1e-05
[2024-05-04 10:05:01] nn step 5900, lr: 0.08.
	loss_policy_0: 0.02115
	accuracy_policy_0: 0.78852
	loss_value_0: 0.09854
	loss_policy_1: 0.00547
	accuracy_policy_1: 0.75234
	loss_value_1: 0.01969
	loss_reward_1: 0.0
	loss_policy_2: 0.00623
	accuracy_policy_2: 0.7268
	loss_value_2: 0.01961
	loss_reward_2: 0.0
	loss_policy_3: 0.00647
	accuracy_policy_3: 0.7234
	loss_value_3: 0.01954
	loss_reward_3: 0.0
	loss_policy_4: 0.00649
	accuracy_policy_4: 0.72426
	loss_value_4: 0.01946
	loss_reward_4: 0.0
	loss_policy_5: 0.00654
	accuracy_policy_5: 0.72738
	loss_value_5: 0.01941
	loss_reward_5: 0.0
	loss_policy: 0.05236
	loss_value: 0.19625
	loss_reward: 1e-05
[2024-05-04 10:05:17] nn step 5950, lr: 0.08.
	loss_policy_0: 0.02006
	accuracy_policy_0: 0.79668
	loss_value_0: 0.0984
	loss_policy_1: 0.00514
	accuracy_policy_1: 0.75836
	loss_value_1: 0.01967
	loss_reward_1: 0.0
	loss_policy_2: 0.00581
	accuracy_policy_2: 0.73828
	loss_value_2: 0.01959
	loss_reward_2: 0.0
	loss_policy_3: 0.00599
	accuracy_policy_3: 0.73348
	loss_value_3: 0.01953
	loss_reward_3: 0.0
	loss_policy_4: 0.00603
	accuracy_policy_4: 0.73062
	loss_value_4: 0.01946
	loss_reward_4: 0.0
	loss_policy_5: 0.00613
	accuracy_policy_5: 0.73254
	loss_value_5: 0.0194
	loss_reward_5: 0.0
	loss_policy: 0.04917
	loss_value: 0.19605
	loss_reward: 1e-05
[2024-05-04 10:05:34] nn step 6000, lr: 0.08.
	loss_policy_0: 0.01738
	accuracy_policy_0: 0.8023
	loss_value_0: 0.09343
	loss_policy_1: 0.0046
	accuracy_policy_1: 0.7618
	loss_value_1: 0.01867
	loss_reward_1: 0.0
	loss_policy_2: 0.00526
	accuracy_policy_2: 0.73855
	loss_value_2: 0.01858
	loss_reward_2: 0.0
	loss_policy_3: 0.0055
	accuracy_policy_3: 0.73547
	loss_value_3: 0.01852
	loss_reward_3: 0.0
	loss_policy_4: 0.00562
	accuracy_policy_4: 0.73234
	loss_value_4: 0.01846
	loss_reward_4: 0.0
	loss_policy_5: 0.00562
	accuracy_policy_5: 0.73391
	loss_value_5: 0.0184
	loss_reward_5: 0.0
	loss_policy: 0.04396
	loss_value: 0.18605
	loss_reward: 1e-05
Optimization_Done 6000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 10:09:52] [command] train weight_iter_6000.pkl 27 31
[2024-05-04 10:10:20] nn step 6050, lr: 0.07.
	loss_policy_0: 0.01637
	accuracy_policy_0: 0.77297
	loss_value_0: 0.10601
	loss_policy_1: 0.00451
	accuracy_policy_1: 0.74109
	loss_value_1: 0.02113
	loss_reward_1: 0.0
	loss_policy_2: 0.00525
	accuracy_policy_2: 0.70418
	loss_value_2: 0.02105
	loss_reward_2: 0.0
	loss_policy_3: 0.00563
	accuracy_policy_3: 0.70695
	loss_value_3: 0.02099
	loss_reward_3: 0.0
	loss_policy_4: 0.00576
	accuracy_policy_4: 0.69355
	loss_value_4: 0.02091
	loss_reward_4: 0.0
	loss_policy_5: 0.00591
	accuracy_policy_5: 0.69438
	loss_value_5: 0.02085
	loss_reward_5: 0.0
	loss_policy: 0.04342
	loss_value: 0.21095
	loss_reward: 1e-05
[2024-05-04 10:10:37] nn step 6100, lr: 0.07.
	loss_policy_0: 0.01322
	accuracy_policy_0: 0.79578
	loss_value_0: 0.10373
	loss_policy_1: 0.00374
	accuracy_policy_1: 0.76461
	loss_value_1: 0.02069
	loss_reward_1: 0.0
	loss_policy_2: 0.00444
	accuracy_policy_2: 0.73215
	loss_value_2: 0.02063
	loss_reward_2: 0.0
	loss_policy_3: 0.00494
	accuracy_policy_3: 0.7259
	loss_value_3: 0.02056
	loss_reward_3: 0.0
	loss_policy_4: 0.00495
	accuracy_policy_4: 0.71922
	loss_value_4: 0.02048
	loss_reward_4: 0.0
	loss_policy_5: 0.00504
	accuracy_policy_5: 0.72562
	loss_value_5: 0.02042
	loss_reward_5: 0.0
	loss_policy: 0.03633
	loss_value: 0.20652
	loss_reward: 1e-05
[2024-05-04 10:10:53] nn step 6150, lr: 0.07.
	loss_policy_0: 0.01241
	accuracy_policy_0: 0.80305
	loss_value_0: 0.10582
	loss_policy_1: 0.00364
	accuracy_policy_1: 0.76793
	loss_value_1: 0.02111
	loss_reward_1: 0.0
	loss_policy_2: 0.00436
	accuracy_policy_2: 0.73871
	loss_value_2: 0.02102
	loss_reward_2: 0.0
	loss_policy_3: 0.0048
	accuracy_policy_3: 0.73285
	loss_value_3: 0.02095
	loss_reward_3: 0.0
	loss_policy_4: 0.0048
	accuracy_policy_4: 0.72906
	loss_value_4: 0.02086
	loss_reward_4: 0.0
	loss_policy_5: 0.00487
	accuracy_policy_5: 0.72988
	loss_value_5: 0.02079
	loss_reward_5: 0.0
	loss_policy: 0.03489
	loss_value: 0.21053
	loss_reward: 0.0
[2024-05-04 10:11:09] nn step 6200, lr: 0.07.
	loss_policy_0: 0.01214
	accuracy_policy_0: 0.80418
	loss_value_0: 0.0999
	loss_policy_1: 0.00344
	accuracy_policy_1: 0.7677
	loss_value_1: 0.01995
	loss_reward_1: 0.0
	loss_policy_2: 0.00411
	accuracy_policy_2: 0.74238
	loss_value_2: 0.01988
	loss_reward_2: 0.0
	loss_policy_3: 0.00452
	accuracy_policy_3: 0.73246
	loss_value_3: 0.01981
	loss_reward_3: 0.0
	loss_policy_4: 0.00457
	accuracy_policy_4: 0.72496
	loss_value_4: 0.01974
	loss_reward_4: 0.0
	loss_policy_5: 0.0046
	accuracy_policy_5: 0.73039
	loss_value_5: 0.01967
	loss_reward_5: 0.0
	loss_policy: 0.03337
	loss_value: 0.19896
	loss_reward: 0.0
Optimization_Done 6200
[2024-05-04 10:13:13] [command] train weight_iter_6200.pkl 28 32
[2024-05-04 10:13:30] nn step 6250, lr: 0.07.
	loss_policy_0: 0.01502
	accuracy_policy_0: 0.77148
	loss_value_0: 0.10113
	loss_policy_1: 0.00423
	accuracy_policy_1: 0.72598
	loss_value_1: 0.02019
	loss_reward_1: 0.0
	loss_policy_2: 0.00479
	accuracy_policy_2: 0.71461
	loss_value_2: 0.0201
	loss_reward_2: 0.0
	loss_policy_3: 0.00527
	accuracy_policy_3: 0.69543
	loss_value_3: 0.02004
	loss_reward_3: 0.0
	loss_policy_4: 0.0054
	accuracy_policy_4: 0.7002
	loss_value_4: 0.01999
	loss_reward_4: 0.0
	loss_policy_5: 0.00557
	accuracy_policy_5: 0.69344
	loss_value_5: 0.01992
	loss_reward_5: 0.0
	loss_policy: 0.04027
	loss_value: 0.20138
	loss_reward: 1e-05
[2024-05-04 10:13:46] nn step 6300, lr: 0.07.
	loss_policy_0: 0.01095
	accuracy_policy_0: 0.80328
	loss_value_0: 0.09998
	loss_policy_1: 0.00334
	accuracy_policy_1: 0.75496
	loss_value_1: 0.01995
	loss_reward_1: 0.0
	loss_policy_2: 0.00393
	accuracy_policy_2: 0.73961
	loss_value_2: 0.01988
	loss_reward_2: 0.0
	loss_policy_3: 0.00431
	accuracy_policy_3: 0.72398
	loss_value_3: 0.0198
	loss_reward_3: 0.0
	loss_policy_4: 0.00453
	accuracy_policy_4: 0.72215
	loss_value_4: 0.01973
	loss_reward_4: 0.0
	loss_policy_5: 0.00461
	accuracy_policy_5: 0.71527
	loss_value_5: 0.01966
	loss_reward_5: 0.0
	loss_policy: 0.03167
	loss_value: 0.19901
	loss_reward: 0.0
[2024-05-04 10:14:02] nn step 6350, lr: 0.07.
	loss_policy_0: 0.01032
	accuracy_policy_0: 0.80902
	loss_value_0: 0.09673
	loss_policy_1: 0.00308
	accuracy_policy_1: 0.765
	loss_value_1: 0.01931
	loss_reward_1: 0.0
	loss_policy_2: 0.00364
	accuracy_policy_2: 0.74871
	loss_value_2: 0.01925
	loss_reward_2: 0.0
	loss_policy_3: 0.00405
	accuracy_policy_3: 0.72445
	loss_value_3: 0.01918
	loss_reward_3: 0.0
	loss_policy_4: 0.00415
	accuracy_policy_4: 0.7266
	loss_value_4: 0.01911
	loss_reward_4: 0.0
	loss_policy_5: 0.00422
	accuracy_policy_5: 0.72266
	loss_value_5: 0.01904
	loss_reward_5: 0.0
	loss_policy: 0.02946
	loss_value: 0.19262
	loss_reward: 0.0
[2024-05-04 10:14:18] nn step 6400, lr: 0.07.
	loss_policy_0: 0.00973
	accuracy_policy_0: 0.80887
	loss_value_0: 0.09273
	loss_policy_1: 0.00287
	accuracy_policy_1: 0.76703
	loss_value_1: 0.01851
	loss_reward_1: 0.0
	loss_policy_2: 0.00338
	accuracy_policy_2: 0.7491
	loss_value_2: 0.01845
	loss_reward_2: 0.0
	loss_policy_3: 0.00371
	accuracy_policy_3: 0.73145
	loss_value_3: 0.01839
	loss_reward_3: 0.0
	loss_policy_4: 0.00377
	accuracy_policy_4: 0.73691
	loss_value_4: 0.01834
	loss_reward_4: 0.0
	loss_policy_5: 0.00389
	accuracy_policy_5: 0.72766
	loss_value_5: 0.01828
	loss_reward_5: 0.0
	loss_policy: 0.02735
	loss_value: 0.18469
	loss_reward: 0.0
Optimization_Done 6400
[2024-05-04 10:16:15] [command] train weight_iter_6400.pkl 29 33
[2024-05-04 10:16:32] nn step 6450, lr: 0.07.
	loss_policy_0: 0.01284
	accuracy_policy_0: 0.76883
	loss_value_0: 0.09285
	loss_policy_1: 0.00355
	accuracy_policy_1: 0.73004
	loss_value_1: 0.01855
	loss_reward_1: 0.0
	loss_policy_2: 0.00397
	accuracy_policy_2: 0.71988
	loss_value_2: 0.01848
	loss_reward_2: 0.0
	loss_policy_3: 0.00454
	accuracy_policy_3: 0.68922
	loss_value_3: 0.01842
	loss_reward_3: 0.0
	loss_policy_4: 0.00476
	accuracy_policy_4: 0.68957
	loss_value_4: 0.01835
	loss_reward_4: 0.0
	loss_policy_5: 0.00485
	accuracy_policy_5: 0.68547
	loss_value_5: 0.01827
	loss_reward_5: 0.0
	loss_policy: 0.03451
	loss_value: 0.18492
	loss_reward: 0.0
[2024-05-04 10:16:48] nn step 6500, lr: 0.07.
	loss_policy_0: 0.01002
	accuracy_policy_0: 0.79773
	loss_value_0: 0.08887
	loss_policy_1: 0.003
	accuracy_policy_1: 0.75309
	loss_value_1: 0.01775
	loss_reward_1: 0.0
	loss_policy_2: 0.00345
	accuracy_policy_2: 0.73297
	loss_value_2: 0.01768
	loss_reward_2: 0.0
	loss_policy_3: 0.00383
	accuracy_policy_3: 0.71785
	loss_value_3: 0.0176
	loss_reward_3: 0.0
	loss_policy_4: 0.00413
	accuracy_policy_4: 0.70926
	loss_value_4: 0.01753
	loss_reward_4: 0.0
	loss_policy_5: 0.00422
	accuracy_policy_5: 0.7048
	loss_value_5: 0.01746
	loss_reward_5: 0.0
	loss_policy: 0.02865
	loss_value: 0.1769
	loss_reward: 0.0
[2024-05-04 10:17:04] nn step 6550, lr: 0.07.
	loss_policy_0: 0.00937
	accuracy_policy_0: 0.7991
	loss_value_0: 0.08824
	loss_policy_1: 0.00286
	accuracy_policy_1: 0.75391
	loss_value_1: 0.01762
	loss_reward_1: 0.0
	loss_policy_2: 0.00335
	accuracy_policy_2: 0.74406
	loss_value_2: 0.01754
	loss_reward_2: 0.0
	loss_policy_3: 0.00364
	accuracy_policy_3: 0.72102
	loss_value_3: 0.01747
	loss_reward_3: 0.0
	loss_policy_4: 0.00392
	accuracy_policy_4: 0.71621
	loss_value_4: 0.01741
	loss_reward_4: 0.0
	loss_policy_5: 0.00407
	accuracy_policy_5: 0.70828
	loss_value_5: 0.01734
	loss_reward_5: 0.0
	loss_policy: 0.02721
	loss_value: 0.17561
	loss_reward: 0.0
[2024-05-04 10:17:20] nn step 6600, lr: 0.07.
	loss_policy_0: 0.00948
	accuracy_policy_0: 0.8027
	loss_value_0: 0.08974
	loss_policy_1: 0.00285
	accuracy_policy_1: 0.75699
	loss_value_1: 0.01793
	loss_reward_1: 0.0
	loss_policy_2: 0.00332
	accuracy_policy_2: 0.74711
	loss_value_2: 0.01786
	loss_reward_2: 0.0
	loss_policy_3: 0.00372
	accuracy_policy_3: 0.72375
	loss_value_3: 0.01778
	loss_reward_3: 0.0
	loss_policy_4: 0.00397
	accuracy_policy_4: 0.71617
	loss_value_4: 0.01772
	loss_reward_4: 0.0
	loss_policy_5: 0.00414
	accuracy_policy_5: 0.71004
	loss_value_5: 0.01767
	loss_reward_5: 0.0
	loss_policy: 0.02747
	loss_value: 0.1787
	loss_reward: 0.0
Optimization_Done 6600
[2024-05-04 10:19:16] [command] train weight_iter_6600.pkl 30 34
[2024-05-04 10:19:34] nn step 6650, lr: 0.07.
	loss_policy_0: 0.00981
	accuracy_policy_0: 0.7902
	loss_value_0: 0.09411
	loss_policy_1: 0.00299
	accuracy_policy_1: 0.73316
	loss_value_1: 0.01879
	loss_reward_1: 0.0
	loss_policy_2: 0.00334
	accuracy_policy_2: 0.72133
	loss_value_2: 0.01871
	loss_reward_2: 0.0
	loss_policy_3: 0.00364
	accuracy_policy_3: 0.70875
	loss_value_3: 0.01864
	loss_reward_3: 0.0
	loss_policy_4: 0.00383
	accuracy_policy_4: 0.70258
	loss_value_4: 0.01857
	loss_reward_4: 0.0
	loss_policy_5: 0.00405
	accuracy_policy_5: 0.69836
	loss_value_5: 0.01852
	loss_reward_5: 0.0
	loss_policy: 0.02766
	loss_value: 0.18733
	loss_reward: 0.0
[2024-05-04 10:19:50] nn step 6700, lr: 0.07.
	loss_policy_0: 0.00944
	accuracy_policy_0: 0.80078
	loss_value_0: 0.10045
	loss_policy_1: 0.00296
	accuracy_policy_1: 0.75109
	loss_value_1: 0.02007
	loss_reward_1: 0.0
	loss_policy_2: 0.00331
	accuracy_policy_2: 0.73637
	loss_value_2: 0.02
	loss_reward_2: 0.0
	loss_policy_3: 0.00365
	accuracy_policy_3: 0.71824
	loss_value_3: 0.01993
	loss_reward_3: 0.0
	loss_policy_4: 0.00386
	accuracy_policy_4: 0.71945
	loss_value_4: 0.01985
	loss_reward_4: 0.0
	loss_policy_5: 0.004
	accuracy_policy_5: 0.71203
	loss_value_5: 0.01977
	loss_reward_5: 0.0
	loss_policy: 0.02722
	loss_value: 0.20006
	loss_reward: 0.0
[2024-05-04 10:20:06] nn step 6750, lr: 0.07.
	loss_policy_0: 0.00863
	accuracy_policy_0: 0.80105
	loss_value_0: 0.09435
	loss_policy_1: 0.00267
	accuracy_policy_1: 0.75105
	loss_value_1: 0.01884
	loss_reward_1: 0.0
	loss_policy_2: 0.00308
	accuracy_policy_2: 0.73727
	loss_value_2: 0.01877
	loss_reward_2: 0.0
	loss_policy_3: 0.00335
	accuracy_policy_3: 0.72004
	loss_value_3: 0.01871
	loss_reward_3: 0.0
	loss_policy_4: 0.00354
	accuracy_policy_4: 0.71734
	loss_value_4: 0.01865
	loss_reward_4: 0.0
	loss_policy_5: 0.00371
	accuracy_policy_5: 0.72191
	loss_value_5: 0.01858
	loss_reward_5: 0.0
	loss_policy: 0.02499
	loss_value: 0.1879
	loss_reward: 0.0
[2024-05-04 10:20:22] nn step 6800, lr: 0.07.
	loss_policy_0: 0.00815
	accuracy_policy_0: 0.80727
	loss_value_0: 0.08956
	loss_policy_1: 0.00253
	accuracy_policy_1: 0.75398
	loss_value_1: 0.01788
	loss_reward_1: 0.0
	loss_policy_2: 0.0028
	accuracy_policy_2: 0.74434
	loss_value_2: 0.01783
	loss_reward_2: 0.0
	loss_policy_3: 0.0031
	accuracy_policy_3: 0.72996
	loss_value_3: 0.01777
	loss_reward_3: 0.0
	loss_policy_4: 0.00331
	accuracy_policy_4: 0.72273
	loss_value_4: 0.01771
	loss_reward_4: 0.0
	loss_policy_5: 0.00345
	accuracy_policy_5: 0.72508
	loss_value_5: 0.01763
	loss_reward_5: 0.0
	loss_policy: 0.02334
	loss_value: 0.17838
	loss_reward: 0.0
Optimization_Done 6800
[2024-05-04 10:22:14] [command] train weight_iter_6800.pkl 31 35
[2024-05-04 10:22:31] nn step 6850, lr: 0.07.
	loss_policy_0: 0.00812
	accuracy_policy_0: 0.79867
	loss_value_0: 0.08601
	loss_policy_1: 0.0026
	accuracy_policy_1: 0.75301
	loss_value_1: 0.01718
	loss_reward_1: 0.0
	loss_policy_2: 0.00291
	accuracy_policy_2: 0.73926
	loss_value_2: 0.01711
	loss_reward_2: 0.0
	loss_policy_3: 0.00319
	accuracy_policy_3: 0.72742
	loss_value_3: 0.01704
	loss_reward_3: 0.0
	loss_policy_4: 0.00333
	accuracy_policy_4: 0.72551
	loss_value_4: 0.01698
	loss_reward_4: 0.0
	loss_policy_5: 0.00342
	accuracy_policy_5: 0.72188
	loss_value_5: 0.01691
	loss_reward_5: 0.0
	loss_policy: 0.02357
	loss_value: 0.17123
	loss_reward: 0.0
[2024-05-04 10:22:48] nn step 6900, lr: 0.07.
	loss_policy_0: 0.00719
	accuracy_policy_0: 0.81242
	loss_value_0: 0.08675
	loss_policy_1: 0.00236
	accuracy_policy_1: 0.76535
	loss_value_1: 0.01733
	loss_reward_1: 0.0
	loss_policy_2: 0.00271
	accuracy_policy_2: 0.7507
	loss_value_2: 0.01728
	loss_reward_2: 0.0
	loss_policy_3: 0.003
	accuracy_policy_3: 0.73691
	loss_value_3: 0.01722
	loss_reward_3: 0.0
	loss_policy_4: 0.0031
	accuracy_policy_4: 0.73645
	loss_value_4: 0.01715
	loss_reward_4: 0.0
	loss_policy_5: 0.00321
	accuracy_policy_5: 0.73309
	loss_value_5: 0.01707
	loss_reward_5: 0.0
	loss_policy: 0.02156
	loss_value: 0.17279
	loss_reward: 0.0
[2024-05-04 10:23:04] nn step 6950, lr: 0.07.
	loss_policy_0: 0.00652
	accuracy_policy_0: 0.81129
	loss_value_0: 0.08035
	loss_policy_1: 0.0022
	accuracy_policy_1: 0.76809
	loss_value_1: 0.01605
	loss_reward_1: 0.0
	loss_policy_2: 0.00251
	accuracy_policy_2: 0.75344
	loss_value_2: 0.01599
	loss_reward_2: 0.0
	loss_policy_3: 0.00271
	accuracy_policy_3: 0.7398
	loss_value_3: 0.01593
	loss_reward_3: 0.0
	loss_policy_4: 0.00282
	accuracy_policy_4: 0.7357
	loss_value_4: 0.01589
	loss_reward_4: 0.0
	loss_policy_5: 0.00289
	accuracy_policy_5: 0.73375
	loss_value_5: 0.01581
	loss_reward_5: 0.0
	loss_policy: 0.01963
	loss_value: 0.16001
	loss_reward: 0.0
[2024-05-04 10:23:21] nn step 7000, lr: 0.07.
	loss_policy_0: 0.00665
	accuracy_policy_0: 0.81633
	loss_value_0: 0.08435
	loss_policy_1: 0.00226
	accuracy_policy_1: 0.77117
	loss_value_1: 0.01685
	loss_reward_1: 0.0
	loss_policy_2: 0.00259
	accuracy_policy_2: 0.75578
	loss_value_2: 0.01679
	loss_reward_2: 0.0
	loss_policy_3: 0.00279
	accuracy_policy_3: 0.74102
	loss_value_3: 0.01674
	loss_reward_3: 0.0
	loss_policy_4: 0.00293
	accuracy_policy_4: 0.7368
	loss_value_4: 0.01667
	loss_reward_4: 0.0
	loss_policy_5: 0.00301
	accuracy_policy_5: 0.72801
	loss_value_5: 0.01662
	loss_reward_5: 0.0
	loss_policy: 0.02023
	loss_value: 0.16803
	loss_reward: 0.0
Optimization_Done 7000
[2024-05-04 10:25:19] [command] train weight_iter_7000.pkl 32 36
[2024-05-04 10:25:36] nn step 7050, lr: 0.07.
	loss_policy_0: 0.02049
	accuracy_policy_0: 0.66359
	loss_value_0: 0.07942
	loss_policy_1: 0.00444
	accuracy_policy_1: 0.64289
	loss_value_1: 0.01553
	loss_reward_1: 0.00173
	loss_policy_2: 0.00432
	accuracy_policy_2: 0.64914
	loss_value_2: 0.01492
	loss_reward_2: 0.00177
	loss_policy_3: 0.00457
	accuracy_policy_3: 0.64957
	loss_value_3: 0.01425
	loss_reward_3: 0.00171
	loss_policy_4: 0.00473
	accuracy_policy_4: 0.64172
	loss_value_4: 0.01363
	loss_reward_4: 0.00161
	loss_policy_5: 0.00482
	accuracy_policy_5: 0.63406
	loss_value_5: 0.01294
	loss_reward_5: 0.00171
	loss_policy: 0.04336
	loss_value: 0.1507
	loss_reward: 0.00852
[2024-05-04 10:25:52] nn step 7100, lr: 0.07.
	loss_policy_0: 0.0085
	accuracy_policy_0: 0.7548
	loss_value_0: 0.07509
	loss_policy_1: 0.00231
	accuracy_policy_1: 0.73039
	loss_value_1: 0.01468
	loss_reward_1: 0.00134
	loss_policy_2: 0.00243
	accuracy_policy_2: 0.73109
	loss_value_2: 0.0142
	loss_reward_2: 0.00136
	loss_policy_3: 0.0026
	accuracy_policy_3: 0.72605
	loss_value_3: 0.01373
	loss_reward_3: 0.00133
	loss_policy_4: 0.00266
	accuracy_policy_4: 0.72527
	loss_value_4: 0.01322
	loss_reward_4: 0.00132
	loss_policy_5: 0.0028
	accuracy_policy_5: 0.70707
	loss_value_5: 0.01272
	loss_reward_5: 0.00135
	loss_policy: 0.02131
	loss_value: 0.14362
	loss_reward: 0.00668
[2024-05-04 10:26:09] nn step 7150, lr: 0.07.
	loss_policy_0: 0.0067
	accuracy_policy_0: 0.77707
	loss_value_0: 0.07217
	loss_policy_1: 0.00196
	accuracy_policy_1: 0.75672
	loss_value_1: 0.01418
	loss_reward_1: 0.00121
	loss_policy_2: 0.00217
	accuracy_policy_2: 0.74262
	loss_value_2: 0.01377
	loss_reward_2: 0.00119
	loss_policy_3: 0.0023
	accuracy_policy_3: 0.74633
	loss_value_3: 0.01337
	loss_reward_3: 0.00117
	loss_policy_4: 0.0024
	accuracy_policy_4: 0.73613
	loss_value_4: 0.01298
	loss_reward_4: 0.00116
	loss_policy_5: 0.00247
	accuracy_policy_5: 0.72734
	loss_value_5: 0.01257
	loss_reward_5: 0.00118
	loss_policy: 0.018
	loss_value: 0.13904
	loss_reward: 0.00591
[2024-05-04 10:26:25] nn step 7200, lr: 0.07.
	loss_policy_0: 0.00635
	accuracy_policy_0: 0.77863
	loss_value_0: 0.07032
	loss_policy_1: 0.00182
	accuracy_policy_1: 0.7593
	loss_value_1: 0.01383
	loss_reward_1: 0.00111
	loss_policy_2: 0.00201
	accuracy_policy_2: 0.74602
	loss_value_2: 0.01352
	loss_reward_2: 0.00109
	loss_policy_3: 0.00212
	accuracy_policy_3: 0.7502
	loss_value_3: 0.01319
	loss_reward_3: 0.00106
	loss_policy_4: 0.00224
	accuracy_policy_4: 0.74332
	loss_value_4: 0.01281
	loss_reward_4: 0.00108
	loss_policy_5: 0.00233
	accuracy_policy_5: 0.72961
	loss_value_5: 0.01247
	loss_reward_5: 0.00104
	loss_policy: 0.01686
	loss_value: 0.13615
	loss_reward: 0.00538
Optimization_Done 7200
[2024-05-04 10:28:26] [command] train weight_iter_7200.pkl 33 37
[2024-05-04 10:28:43] nn step 7250, lr: 0.07.
	loss_policy_0: 0.00705
	accuracy_policy_0: 0.77426
	loss_value_0: 0.07502
	loss_policy_1: 0.00214
	accuracy_policy_1: 0.74266
	loss_value_1: 0.01476
	loss_reward_1: 0.00133
	loss_policy_2: 0.00239
	accuracy_policy_2: 0.72668
	loss_value_2: 0.01439
	loss_reward_2: 0.00136
	loss_policy_3: 0.00261
	accuracy_policy_3: 0.72324
	loss_value_3: 0.01402
	loss_reward_3: 0.00128
	loss_policy_4: 0.00274
	accuracy_policy_4: 0.71918
	loss_value_4: 0.01361
	loss_reward_4: 0.00128
	loss_policy_5: 0.0029
	accuracy_policy_5: 0.70852
	loss_value_5: 0.01319
	loss_reward_5: 0.00132
	loss_policy: 0.01984
	loss_value: 0.14499
	loss_reward: 0.00659
[2024-05-04 10:28:59] nn step 7300, lr: 0.07.
	loss_policy_0: 0.00566
	accuracy_policy_0: 0.80559
	loss_value_0: 0.0716
	loss_policy_1: 0.00178
	accuracy_policy_1: 0.76344
	loss_value_1: 0.01411
	loss_reward_1: 0.0012
	loss_policy_2: 0.00206
	accuracy_policy_2: 0.74617
	loss_value_2: 0.01381
	loss_reward_2: 0.00116
	loss_policy_3: 0.00225
	accuracy_policy_3: 0.7393
	loss_value_3: 0.01351
	loss_reward_3: 0.00114
	loss_policy_4: 0.00238
	accuracy_policy_4: 0.73191
	loss_value_4: 0.01316
	loss_reward_4: 0.0012
	loss_policy_5: 0.0025
	accuracy_policy_5: 0.72688
	loss_value_5: 0.01284
	loss_reward_5: 0.00119
	loss_policy: 0.01663
	loss_value: 0.13903
	loss_reward: 0.00588
[2024-05-04 10:29:15] nn step 7350, lr: 0.07.
	loss_policy_0: 0.00519
	accuracy_policy_0: 0.80496
	loss_value_0: 0.06705
	loss_policy_1: 0.00163
	accuracy_policy_1: 0.76832
	loss_value_1: 0.0133
	loss_reward_1: 0.00108
	loss_policy_2: 0.00188
	accuracy_policy_2: 0.75602
	loss_value_2: 0.01305
	loss_reward_2: 0.00106
	loss_policy_3: 0.002
	accuracy_policy_3: 0.74973
	loss_value_3: 0.01279
	loss_reward_3: 0.00103
	loss_policy_4: 0.00214
	accuracy_policy_4: 0.74176
	loss_value_4: 0.01248
	loss_reward_4: 0.00114
	loss_policy_5: 0.00227
	accuracy_policy_5: 0.73387
	loss_value_5: 0.01216
	loss_reward_5: 0.0011
	loss_policy: 0.01511
	loss_value: 0.13084
	loss_reward: 0.00541
[2024-05-04 10:29:31] nn step 7400, lr: 0.07.
	loss_policy_0: 0.00531
	accuracy_policy_0: 0.81387
	loss_value_0: 0.0696
	loss_policy_1: 0.00167
	accuracy_policy_1: 0.77641
	loss_value_1: 0.0138
	loss_reward_1: 0.00108
	loss_policy_2: 0.00192
	accuracy_policy_2: 0.75223
	loss_value_2: 0.01356
	loss_reward_2: 0.00109
	loss_policy_3: 0.00208
	accuracy_policy_3: 0.75184
	loss_value_3: 0.01335
	loss_reward_3: 0.00099
	loss_policy_4: 0.00219
	accuracy_policy_4: 0.74867
	loss_value_4: 0.01305
	loss_reward_4: 0.00108
	loss_policy_5: 0.00229
	accuracy_policy_5: 0.73957
	loss_value_5: 0.01278
	loss_reward_5: 0.0011
	loss_policy: 0.01546
	loss_value: 0.13615
	loss_reward: 0.00534
Optimization_Done 7400
[2024-05-04 10:31:28] [command] train weight_iter_7400.pkl 34 38
[2024-05-04 10:31:45] nn step 7450, lr: 0.07.
	loss_policy_0: 0.01057
	accuracy_policy_0: 0.79426
	loss_value_0: 0.07626
	loss_policy_1: 0.00267
	accuracy_policy_1: 0.78656
	loss_value_1: 0.01417
	loss_reward_1: 0.00392
	loss_policy_2: 0.00264
	accuracy_policy_2: 0.7952
	loss_value_2: 0.01337
	loss_reward_2: 0.0038
	loss_policy_3: 0.00286
	accuracy_policy_3: 0.79
	loss_value_3: 0.01242
	loss_reward_3: 0.0038
	loss_policy_4: 0.00302
	accuracy_policy_4: 0.80133
	loss_value_4: 0.01138
	loss_reward_4: 0.00384
	loss_policy_5: 0.00327
	accuracy_policy_5: 0.78785
	loss_value_5: 0.01038
	loss_reward_5: 0.00385
	loss_policy: 0.02504
	loss_value: 0.13798
	loss_reward: 0.01922
[2024-05-04 10:32:01] nn step 7500, lr: 0.07.
	loss_policy_0: 0.00563
	accuracy_policy_0: 0.86523
	loss_value_0: 0.07223
	loss_policy_1: 0.00166
	accuracy_policy_1: 0.83164
	loss_value_1: 0.01411
	loss_reward_1: 0.00336
	loss_policy_2: 0.00173
	accuracy_policy_2: 0.83016
	loss_value_2: 0.01369
	loss_reward_2: 0.00335
	loss_policy_3: 0.00186
	accuracy_policy_3: 0.8325
	loss_value_3: 0.0131
	loss_reward_3: 0.00334
	loss_policy_4: 0.00194
	accuracy_policy_4: 0.83102
	loss_value_4: 0.01234
	loss_reward_4: 0.0035
	loss_policy_5: 0.00206
	accuracy_policy_5: 0.82184
	loss_value_5: 0.01156
	loss_reward_5: 0.00344
	loss_policy: 0.01488
	loss_value: 0.13702
	loss_reward: 0.01699
[2024-05-04 10:32:18] nn step 7550, lr: 0.07.
	loss_policy_0: 0.00509
	accuracy_policy_0: 0.84859
	loss_value_0: 0.07256
	loss_policy_1: 0.00153
	accuracy_policy_1: 0.81207
	loss_value_1: 0.01443
	loss_reward_1: 0.00272
	loss_policy_2: 0.00164
	accuracy_policy_2: 0.80789
	loss_value_2: 0.01416
	loss_reward_2: 0.00272
	loss_policy_3: 0.00181
	accuracy_policy_3: 0.81227
	loss_value_3: 0.01378
	loss_reward_3: 0.00276
	loss_policy_4: 0.00184
	accuracy_policy_4: 0.80957
	loss_value_4: 0.01327
	loss_reward_4: 0.00291
	loss_policy_5: 0.00197
	accuracy_policy_5: 0.79566
	loss_value_5: 0.01259
	loss_reward_5: 0.00308
	loss_policy: 0.01388
	loss_value: 0.14078
	loss_reward: 0.0142
[2024-05-04 10:32:34] nn step 7600, lr: 0.07.
	loss_policy_0: 0.00436
	accuracy_policy_0: 0.85004
	loss_value_0: 0.06946
	loss_policy_1: 0.00138
	accuracy_policy_1: 0.81625
	loss_value_1: 0.01393
	loss_reward_1: 0.00244
	loss_policy_2: 0.00153
	accuracy_policy_2: 0.80777
	loss_value_2: 0.01375
	loss_reward_2: 0.00236
	loss_policy_3: 0.00162
	accuracy_policy_3: 0.81262
	loss_value_3: 0.01349
	loss_reward_3: 0.00236
	loss_policy_4: 0.0017
	accuracy_policy_4: 0.80383
	loss_value_4: 0.01306
	loss_reward_4: 0.00262
	loss_policy_5: 0.00176
	accuracy_policy_5: 0.79496
	loss_value_5: 0.01248
	loss_reward_5: 0.00273
	loss_policy: 0.01235
	loss_value: 0.13617
	loss_reward: 0.01252
Optimization_Done 7600
[2024-05-04 10:34:25] [command] train weight_iter_7600.pkl 35 39
[2024-05-04 10:34:43] nn step 7650, lr: 0.07.
	loss_policy_0: 0.00678
	accuracy_policy_0: 0.82582
	loss_value_0: 0.07684
	loss_policy_1: 0.00194
	accuracy_policy_1: 0.78449
	loss_value_1: 0.01543
	loss_reward_1: 0.00284
	loss_policy_2: 0.00212
	accuracy_policy_2: 0.7843
	loss_value_2: 0.01522
	loss_reward_2: 0.00281
	loss_policy_3: 0.00227
	accuracy_policy_3: 0.78031
	loss_value_3: 0.01489
	loss_reward_3: 0.0028
	loss_policy_4: 0.00233
	accuracy_policy_4: 0.77066
	loss_value_4: 0.01442
	loss_reward_4: 0.00301
	loss_policy_5: 0.00246
	accuracy_policy_5: 0.76531
	loss_value_5: 0.01386
	loss_reward_5: 0.00313
	loss_policy: 0.01791
	loss_value: 0.15066
	loss_reward: 0.01459
[2024-05-04 10:34:59] nn step 7700, lr: 0.07.
	loss_policy_0: 0.00605
	accuracy_policy_0: 0.8316
	loss_value_0: 0.07256
	loss_policy_1: 0.00169
	accuracy_policy_1: 0.79508
	loss_value_1: 0.0146
	loss_reward_1: 0.00255
	loss_policy_2: 0.00182
	accuracy_policy_2: 0.78961
	loss_value_2: 0.01441
	loss_reward_2: 0.00246
	loss_policy_3: 0.00199
	accuracy_policy_3: 0.7898
	loss_value_3: 0.01419
	loss_reward_3: 0.00252
	loss_policy_4: 0.00206
	accuracy_policy_4: 0.77914
	loss_value_4: 0.01385
	loss_reward_4: 0.00268
	loss_policy_5: 0.00217
	accuracy_policy_5: 0.7777
	loss_value_5: 0.01335
	loss_reward_5: 0.00289
	loss_policy: 0.01578
	loss_value: 0.14296
	loss_reward: 0.01311
[2024-05-04 10:35:15] nn step 7750, lr: 0.07.
	loss_policy_0: 0.0058
	accuracy_policy_0: 0.83797
	loss_value_0: 0.07212
	loss_policy_1: 0.00159
	accuracy_policy_1: 0.79949
	loss_value_1: 0.01461
	loss_reward_1: 0.00256
	loss_policy_2: 0.00176
	accuracy_policy_2: 0.79941
	loss_value_2: 0.01448
	loss_reward_2: 0.0025
	loss_policy_3: 0.00191
	accuracy_policy_3: 0.79359
	loss_value_3: 0.0143
	loss_reward_3: 0.00248
	loss_policy_4: 0.00201
	accuracy_policy_4: 0.78727
	loss_value_4: 0.01399
	loss_reward_4: 0.00265
	loss_policy_5: 0.00207
	accuracy_policy_5: 0.78242
	loss_value_5: 0.01359
	loss_reward_5: 0.00282
	loss_policy: 0.01514
	loss_value: 0.14309
	loss_reward: 0.013
[2024-05-04 10:35:31] nn step 7800, lr: 0.07.
	loss_policy_0: 0.00556
	accuracy_policy_0: 0.83691
	loss_value_0: 0.06962
	loss_policy_1: 0.0015
	accuracy_policy_1: 0.81375
	loss_value_1: 0.01413
	loss_reward_1: 0.00234
	loss_policy_2: 0.00163
	accuracy_policy_2: 0.80371
	loss_value_2: 0.01405
	loss_reward_2: 0.0023
	loss_policy_3: 0.00179
	accuracy_policy_3: 0.8034
	loss_value_3: 0.01388
	loss_reward_3: 0.00231
	loss_policy_4: 0.00191
	accuracy_policy_4: 0.79031
	loss_value_4: 0.01362
	loss_reward_4: 0.00245
	loss_policy_5: 0.00194
	accuracy_policy_5: 0.78926
	loss_value_5: 0.01328
	loss_reward_5: 0.00274
	loss_policy: 0.01433
	loss_value: 0.13858
	loss_reward: 0.01214
Optimization_Done 7800
[2024-05-04 10:37:28] [command] train weight_iter_7800.pkl 36 40
[2024-05-04 10:37:45] nn step 7850, lr: 0.07.
	loss_policy_0: 0.02762
	accuracy_policy_0: 0.6659
	loss_value_0: 0.08133
	loss_policy_1: 0.0061
	accuracy_policy_1: 0.59777
	loss_value_1: 0.01631
	loss_reward_1: 0.0027
	loss_policy_2: 0.00621
	accuracy_policy_2: 0.62133
	loss_value_2: 0.01621
	loss_reward_2: 0.00262
	loss_policy_3: 0.00618
	accuracy_policy_3: 0.60875
	loss_value_3: 0.0161
	loss_reward_3: 0.00263
	loss_policy_4: 0.00625
	accuracy_policy_4: 0.60379
	loss_value_4: 0.01588
	loss_reward_4: 0.00296
	loss_policy_5: 0.00633
	accuracy_policy_5: 0.59203
	loss_value_5: 0.01556
	loss_reward_5: 0.00303
	loss_policy: 0.0587
	loss_value: 0.1614
	loss_reward: 0.01394
[2024-05-04 10:38:01] nn step 7900, lr: 0.07.
	loss_policy_0: 0.02133
	accuracy_policy_0: 0.73844
	loss_value_0: 0.08197
	loss_policy_1: 0.00461
	accuracy_policy_1: 0.71672
	loss_value_1: 0.01664
	loss_reward_1: 0.00276
	loss_policy_2: 0.00473
	accuracy_policy_2: 0.71508
	loss_value_2: 0.01657
	loss_reward_2: 0.00275
	loss_policy_3: 0.00472
	accuracy_policy_3: 0.71926
	loss_value_3: 0.01651
	loss_reward_3: 0.00278
	loss_policy_4: 0.00483
	accuracy_policy_4: 0.71012
	loss_value_4: 0.0163
	loss_reward_4: 0.0029
	loss_policy_5: 0.00497
	accuracy_policy_5: 0.69793
	loss_value_5: 0.01596
	loss_reward_5: 0.00312
	loss_policy: 0.04519
	loss_value: 0.16395
	loss_reward: 0.01431
[2024-05-04 10:38:17] nn step 7950, lr: 0.07.
	loss_policy_0: 0.01944
	accuracy_policy_0: 0.75133
	loss_value_0: 0.07916
	loss_policy_1: 0.00416
	accuracy_policy_1: 0.73879
	loss_value_1: 0.01615
	loss_reward_1: 0.00271
	loss_policy_2: 0.00426
	accuracy_policy_2: 0.73137
	loss_value_2: 0.01624
	loss_reward_2: 0.00267
	loss_policy_3: 0.00421
	accuracy_policy_3: 0.73664
	loss_value_3: 0.01622
	loss_reward_3: 0.0026
	loss_policy_4: 0.00429
	accuracy_policy_4: 0.72945
	loss_value_4: 0.01604
	loss_reward_4: 0.0029
	loss_policy_5: 0.00435
	accuracy_policy_5: 0.7182
	loss_value_5: 0.01575
	loss_reward_5: 0.00298
	loss_policy: 0.0407
	loss_value: 0.15957
	loss_reward: 0.01387
[2024-05-04 10:38:34] nn step 8000, lr: 0.07.
	loss_policy_0: 0.01777
	accuracy_policy_0: 0.75734
	loss_value_0: 0.07656
	loss_policy_1: 0.00381
	accuracy_policy_1: 0.73465
	loss_value_1: 0.01564
	loss_reward_1: 0.00258
	loss_policy_2: 0.00381
	accuracy_policy_2: 0.73867
	loss_value_2: 0.01572
	loss_reward_2: 0.00252
	loss_policy_3: 0.00388
	accuracy_policy_3: 0.73387
	loss_value_3: 0.01572
	loss_reward_3: 0.00239
	loss_policy_4: 0.00389
	accuracy_policy_4: 0.73359
	loss_value_4: 0.01557
	loss_reward_4: 0.00272
	loss_policy_5: 0.00413
	accuracy_policy_5: 0.71855
	loss_value_5: 0.01534
	loss_reward_5: 0.00297
	loss_policy: 0.03729
	loss_value: 0.15456
	loss_reward: 0.01317
Optimization_Done 8000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 10:44:53] [command] train weight_iter_8000.pkl 37 41
[2024-05-04 10:45:22] nn step 8050, lr: 0.06.
	loss_policy_0: 0.0162
	accuracy_policy_0: 0.7909
	loss_value_0: 0.07718
	loss_policy_1: 0.00373
	accuracy_policy_1: 0.76852
	loss_value_1: 0.01558
	loss_reward_1: 0.00297
	loss_policy_2: 0.00399
	accuracy_policy_2: 0.76285
	loss_value_2: 0.01557
	loss_reward_2: 0.00292
	loss_policy_3: 0.00404
	accuracy_policy_3: 0.76598
	loss_value_3: 0.01542
	loss_reward_3: 0.00281
	loss_policy_4: 0.0042
	accuracy_policy_4: 0.75117
	loss_value_4: 0.01505
	loss_reward_4: 0.00317
	loss_policy_5: 0.00437
	accuracy_policy_5: 0.73785
	loss_value_5: 0.01463
	loss_reward_5: 0.00325
	loss_policy: 0.03654
	loss_value: 0.15343
	loss_reward: 0.01512
[2024-05-04 10:45:38] nn step 8100, lr: 0.06.
	loss_policy_0: 0.01615
	accuracy_policy_0: 0.76945
	loss_value_0: 0.07769
	loss_policy_1: 0.00373
	accuracy_policy_1: 0.74719
	loss_value_1: 0.01581
	loss_reward_1: 0.00252
	loss_policy_2: 0.00401
	accuracy_policy_2: 0.74059
	loss_value_2: 0.01583
	loss_reward_2: 0.0024
	loss_policy_3: 0.00408
	accuracy_policy_3: 0.74195
	loss_value_3: 0.01575
	loss_reward_3: 0.00242
	loss_policy_4: 0.00426
	accuracy_policy_4: 0.73461
	loss_value_4: 0.01552
	loss_reward_4: 0.0027
	loss_policy_5: 0.0044
	accuracy_policy_5: 0.72117
	loss_value_5: 0.01514
	loss_reward_5: 0.00304
	loss_policy: 0.03663
	loss_value: 0.15574
	loss_reward: 0.01307
[2024-05-04 10:45:54] nn step 8150, lr: 0.06.
	loss_policy_0: 0.0157
	accuracy_policy_0: 0.77645
	loss_value_0: 0.08079
	loss_policy_1: 0.00375
	accuracy_policy_1: 0.74676
	loss_value_1: 0.01649
	loss_reward_1: 0.00255
	loss_policy_2: 0.00397
	accuracy_policy_2: 0.74633
	loss_value_2: 0.01652
	loss_reward_2: 0.00247
	loss_policy_3: 0.00409
	accuracy_policy_3: 0.74172
	loss_value_3: 0.01643
	loss_reward_3: 0.00248
	loss_policy_4: 0.00427
	accuracy_policy_4: 0.7366
	loss_value_4: 0.01626
	loss_reward_4: 0.00276
	loss_policy_5: 0.00448
	accuracy_policy_5: 0.7252
	loss_value_5: 0.01593
	loss_reward_5: 0.003
	loss_policy: 0.03626
	loss_value: 0.16242
	loss_reward: 0.01326
[2024-05-04 10:46:10] nn step 8200, lr: 0.06.
	loss_policy_0: 0.01514
	accuracy_policy_0: 0.78004
	loss_value_0: 0.07784
	loss_policy_1: 0.00344
	accuracy_policy_1: 0.76008
	loss_value_1: 0.01591
	loss_reward_1: 0.00251
	loss_policy_2: 0.00377
	accuracy_policy_2: 0.74504
	loss_value_2: 0.016
	loss_reward_2: 0.00232
	loss_policy_3: 0.00383
	accuracy_policy_3: 0.74098
	loss_value_3: 0.01596
	loss_reward_3: 0.00237
	loss_policy_4: 0.00401
	accuracy_policy_4: 0.73738
	loss_value_4: 0.0158
	loss_reward_4: 0.00272
	loss_policy_5: 0.00417
	accuracy_policy_5: 0.71965
	loss_value_5: 0.01552
	loss_reward_5: 0.00294
	loss_policy: 0.03436
	loss_value: 0.15702
	loss_reward: 0.01285
Optimization_Done 8200
[2024-05-04 10:48:20] [command] train weight_iter_8200.pkl 38 42
[2024-05-04 10:48:38] nn step 8250, lr: 0.06.
	loss_policy_0: 0.04655
	accuracy_policy_0: 0.67492
	loss_value_0: 0.09113
	loss_policy_1: 0.01007
	accuracy_policy_1: 0.65277
	loss_value_1: 0.01855
	loss_reward_1: 0.00323
	loss_policy_2: 0.01025
	accuracy_policy_2: 0.64172
	loss_value_2: 0.01874
	loss_reward_2: 0.00298
	loss_policy_3: 0.01044
	accuracy_policy_3: 0.63938
	loss_value_3: 0.01887
	loss_reward_3: 0.00309
	loss_policy_4: 0.01082
	accuracy_policy_4: 0.62379
	loss_value_4: 0.01877
	loss_reward_4: 0.00346
	loss_policy_5: 0.01122
	accuracy_policy_5: 0.61426
	loss_value_5: 0.01857
	loss_reward_5: 0.00371
	loss_policy: 0.09935
	loss_value: 0.18464
	loss_reward: 0.01646
[2024-05-04 10:48:54] nn step 8300, lr: 0.06.
	loss_policy_0: 0.03549
	accuracy_policy_0: 0.72707
	loss_value_0: 0.08696
	loss_policy_1: 0.00774
	accuracy_policy_1: 0.70711
	loss_value_1: 0.01778
	loss_reward_1: 0.00303
	loss_policy_2: 0.00798
	accuracy_policy_2: 0.69668
	loss_value_2: 0.01801
	loss_reward_2: 0.00279
	loss_policy_3: 0.00824
	accuracy_policy_3: 0.68902
	loss_value_3: 0.01818
	loss_reward_3: 0.00297
	loss_policy_4: 0.00854
	accuracy_policy_4: 0.68637
	loss_value_4: 0.01816
	loss_reward_4: 0.00327
	loss_policy_5: 0.00887
	accuracy_policy_5: 0.66879
	loss_value_5: 0.01803
	loss_reward_5: 0.00357
	loss_policy: 0.07685
	loss_value: 0.17712
	loss_reward: 0.01564
[2024-05-04 10:49:09] nn step 8350, lr: 0.06.
	loss_policy_0: 0.03304
	accuracy_policy_0: 0.73645
	loss_value_0: 0.08711
	loss_policy_1: 0.00712
	accuracy_policy_1: 0.71914
	loss_value_1: 0.01784
	loss_reward_1: 0.00306
	loss_policy_2: 0.00741
	accuracy_policy_2: 0.70832
	loss_value_2: 0.01807
	loss_reward_2: 0.00289
	loss_policy_3: 0.00777
	accuracy_policy_3: 0.70113
	loss_value_3: 0.01816
	loss_reward_3: 0.00295
	loss_policy_4: 0.00803
	accuracy_policy_4: 0.69332
	loss_value_4: 0.01817
	loss_reward_4: 0.00334
	loss_policy_5: 0.0084
	accuracy_policy_5: 0.68242
	loss_value_5: 0.01808
	loss_reward_5: 0.00334
	loss_policy: 0.07177
	loss_value: 0.17744
	loss_reward: 0.01557
[2024-05-04 10:49:26] nn step 8400, lr: 0.06.
	loss_policy_0: 0.03256
	accuracy_policy_0: 0.74398
	loss_value_0: 0.09078
	loss_policy_1: 0.00725
	accuracy_policy_1: 0.72102
	loss_value_1: 0.01863
	loss_reward_1: 0.00306
	loss_policy_2: 0.00753
	accuracy_policy_2: 0.72371
	loss_value_2: 0.0189
	loss_reward_2: 0.00307
	loss_policy_3: 0.00774
	accuracy_policy_3: 0.70895
	loss_value_3: 0.01905
	loss_reward_3: 0.00306
	loss_policy_4: 0.00804
	accuracy_policy_4: 0.69996
	loss_value_4: 0.01915
	loss_reward_4: 0.00333
	loss_policy_5: 0.00843
	accuracy_policy_5: 0.69586
	loss_value_5: 0.01906
	loss_reward_5: 0.00359
	loss_policy: 0.07154
	loss_value: 0.18558
	loss_reward: 0.01611
Optimization_Done 8400
[2024-05-04 10:51:30] [command] train weight_iter_8400.pkl 39 43
[2024-05-04 10:51:47] nn step 8450, lr: 0.06.
	loss_policy_0: 0.037
	accuracy_policy_0: 0.69496
	loss_value_0: 0.08743
	loss_policy_1: 0.0083
	accuracy_policy_1: 0.66953
	loss_value_1: 0.01778
	loss_reward_1: 0.00251
	loss_policy_2: 0.00861
	accuracy_policy_2: 0.66062
	loss_value_2: 0.01817
	loss_reward_2: 0.00242
	loss_policy_3: 0.00897
	accuracy_policy_3: 0.65793
	loss_value_3: 0.01845
	loss_reward_3: 0.00238
	loss_policy_4: 0.00935
	accuracy_policy_4: 0.6432
	loss_value_4: 0.01864
	loss_reward_4: 0.00263
	loss_policy_5: 0.0097
	accuracy_policy_5: 0.63461
	loss_value_5: 0.01877
	loss_reward_5: 0.0027
	loss_policy: 0.08194
	loss_value: 0.17924
	loss_reward: 0.01263
[2024-05-04 10:52:03] nn step 8500, lr: 0.06.
	loss_policy_0: 0.03397
	accuracy_policy_0: 0.71117
	loss_value_0: 0.08578
	loss_policy_1: 0.00771
	accuracy_policy_1: 0.68195
	loss_value_1: 0.01745
	loss_reward_1: 0.00249
	loss_policy_2: 0.00799
	accuracy_policy_2: 0.67914
	loss_value_2: 0.01774
	loss_reward_2: 0.00229
	loss_policy_3: 0.00815
	accuracy_policy_3: 0.66953
	loss_value_3: 0.01805
	loss_reward_3: 0.00238
	loss_policy_4: 0.00857
	accuracy_policy_4: 0.66012
	loss_value_4: 0.01826
	loss_reward_4: 0.00247
	loss_policy_5: 0.00891
	accuracy_policy_5: 0.65254
	loss_value_5: 0.01839
	loss_reward_5: 0.00269
	loss_policy: 0.07529
	loss_value: 0.17567
	loss_reward: 0.01231
[2024-05-04 10:52:19] nn step 8550, lr: 0.06.
	loss_policy_0: 0.03056
	accuracy_policy_0: 0.71082
	loss_value_0: 0.07963
	loss_policy_1: 0.00691
	accuracy_policy_1: 0.68262
	loss_value_1: 0.01625
	loss_reward_1: 0.00231
	loss_policy_2: 0.00724
	accuracy_policy_2: 0.68273
	loss_value_2: 0.01649
	loss_reward_2: 0.00214
	loss_policy_3: 0.00754
	accuracy_policy_3: 0.67258
	loss_value_3: 0.0167
	loss_reward_3: 0.0022
	loss_policy_4: 0.0078
	accuracy_policy_4: 0.66094
	loss_value_4: 0.01697
	loss_reward_4: 0.00225
	loss_policy_5: 0.0082
	accuracy_policy_5: 0.65312
	loss_value_5: 0.01716
	loss_reward_5: 0.00252
	loss_policy: 0.06826
	loss_value: 0.1632
	loss_reward: 0.01142
[2024-05-04 10:52:35] nn step 8600, lr: 0.06.
	loss_policy_0: 0.03193
	accuracy_policy_0: 0.71387
	loss_value_0: 0.0838
	loss_policy_1: 0.00722
	accuracy_policy_1: 0.68418
	loss_value_1: 0.01709
	loss_reward_1: 0.00235
	loss_policy_2: 0.00752
	accuracy_policy_2: 0.68156
	loss_value_2: 0.0174
	loss_reward_2: 0.00219
	loss_policy_3: 0.00776
	accuracy_policy_3: 0.66918
	loss_value_3: 0.01769
	loss_reward_3: 0.00221
	loss_policy_4: 0.00824
	accuracy_policy_4: 0.66188
	loss_value_4: 0.0179
	loss_reward_4: 0.00242
	loss_policy_5: 0.00846
	accuracy_policy_5: 0.6527
	loss_value_5: 0.01809
	loss_reward_5: 0.00263
	loss_policy: 0.07114
	loss_value: 0.17196
	loss_reward: 0.01179
Optimization_Done 8600
[2024-05-04 10:54:38] [command] train weight_iter_8600.pkl 40 44
[2024-05-04 10:54:56] nn step 8650, lr: 0.06.
	loss_policy_0: 0.03697
	accuracy_policy_0: 0.7382
	loss_value_0: 0.09224
	loss_policy_1: 0.00865
	accuracy_policy_1: 0.70711
	loss_value_1: 0.01867
	loss_reward_1: 0.00259
	loss_policy_2: 0.00922
	accuracy_policy_2: 0.69223
	loss_value_2: 0.01887
	loss_reward_2: 0.0023
	loss_policy_3: 0.00961
	accuracy_policy_3: 0.68465
	loss_value_3: 0.01902
	loss_reward_3: 0.00243
	loss_policy_4: 0.01007
	accuracy_policy_4: 0.6791
	loss_value_4: 0.01917
	loss_reward_4: 0.00257
	loss_policy_5: 0.01047
	accuracy_policy_5: 0.66469
	loss_value_5: 0.01922
	loss_reward_5: 0.00284
	loss_policy: 0.08498
	loss_value: 0.18719
	loss_reward: 0.01273
[2024-05-04 10:55:12] nn step 8700, lr: 0.06.
	loss_policy_0: 0.03011
	accuracy_policy_0: 0.75367
	loss_value_0: 0.08548
	loss_policy_1: 0.00727
	accuracy_policy_1: 0.72285
	loss_value_1: 0.01734
	loss_reward_1: 0.00235
	loss_policy_2: 0.00776
	accuracy_policy_2: 0.71355
	loss_value_2: 0.01757
	loss_reward_2: 0.00214
	loss_policy_3: 0.00815
	accuracy_policy_3: 0.70746
	loss_value_3: 0.01781
	loss_reward_3: 0.00223
	loss_policy_4: 0.00845
	accuracy_policy_4: 0.7016
	loss_value_4: 0.01798
	loss_reward_4: 0.00247
	loss_policy_5: 0.00877
	accuracy_policy_5: 0.69367
	loss_value_5: 0.01815
	loss_reward_5: 0.00258
	loss_policy: 0.07052
	loss_value: 0.17435
	loss_reward: 0.01177
[2024-05-04 10:55:28] nn step 8750, lr: 0.06.
	loss_policy_0: 0.0273
	accuracy_policy_0: 0.76445
	loss_value_0: 0.0822
	loss_policy_1: 0.00659
	accuracy_policy_1: 0.72324
	loss_value_1: 0.01672
	loss_reward_1: 0.0023
	loss_policy_2: 0.00715
	accuracy_policy_2: 0.71695
	loss_value_2: 0.01695
	loss_reward_2: 0.00217
	loss_policy_3: 0.00754
	accuracy_policy_3: 0.71062
	loss_value_3: 0.01712
	loss_reward_3: 0.00209
	loss_policy_4: 0.00786
	accuracy_policy_4: 0.70379
	loss_value_4: 0.01733
	loss_reward_4: 0.00239
	loss_policy_5: 0.00816
	accuracy_policy_5: 0.6927
	loss_value_5: 0.01744
	loss_reward_5: 0.00265
	loss_policy: 0.0646
	loss_value: 0.16775
	loss_reward: 0.01159
[2024-05-04 10:55:44] nn step 8800, lr: 0.06.
	loss_policy_0: 0.02893
	accuracy_policy_0: 0.7577
	loss_value_0: 0.08781
	loss_policy_1: 0.00703
	accuracy_policy_1: 0.72156
	loss_value_1: 0.01783
	loss_reward_1: 0.00246
	loss_policy_2: 0.00754
	accuracy_policy_2: 0.7152
	loss_value_2: 0.01807
	loss_reward_2: 0.00231
	loss_policy_3: 0.00791
	accuracy_policy_3: 0.70875
	loss_value_3: 0.01822
	loss_reward_3: 0.00229
	loss_policy_4: 0.0082
	accuracy_policy_4: 0.70355
	loss_value_4: 0.01843
	loss_reward_4: 0.00245
	loss_policy_5: 0.00852
	accuracy_policy_5: 0.69898
	loss_value_5: 0.01861
	loss_reward_5: 0.00279
	loss_policy: 0.06813
	loss_value: 0.17897
	loss_reward: 0.0123
Optimization_Done 8800
[2024-05-04 10:57:39] [command] train weight_iter_8800.pkl 41 45
[2024-05-04 10:57:57] nn step 8850, lr: 0.06.
	loss_policy_0: 0.02381
	accuracy_policy_0: 0.78594
	loss_value_0: 0.08249
	loss_policy_1: 0.00616
	accuracy_policy_1: 0.74516
	loss_value_1: 0.01674
	loss_reward_1: 0.00222
	loss_policy_2: 0.00687
	accuracy_policy_2: 0.73414
	loss_value_2: 0.01697
	loss_reward_2: 0.00205
	loss_policy_3: 0.00732
	accuracy_policy_3: 0.72688
	loss_value_3: 0.0171
	loss_reward_3: 0.00213
	loss_policy_4: 0.00766
	accuracy_policy_4: 0.72129
	loss_value_4: 0.01726
	loss_reward_4: 0.00234
	loss_policy_5: 0.00807
	accuracy_policy_5: 0.70715
	loss_value_5: 0.01736
	loss_reward_5: 0.00241
	loss_policy: 0.05989
	loss_value: 0.16792
	loss_reward: 0.01115
[2024-05-04 10:58:13] nn step 8900, lr: 0.06.
	loss_policy_0: 0.02207
	accuracy_policy_0: 0.79023
	loss_value_0: 0.08395
	loss_policy_1: 0.00585
	accuracy_policy_1: 0.75754
	loss_value_1: 0.01696
	loss_reward_1: 0.00221
	loss_policy_2: 0.00655
	accuracy_policy_2: 0.74094
	loss_value_2: 0.01722
	loss_reward_2: 0.00217
	loss_policy_3: 0.00708
	accuracy_policy_3: 0.73086
	loss_value_3: 0.01738
	loss_reward_3: 0.00219
	loss_policy_4: 0.00735
	accuracy_policy_4: 0.72777
	loss_value_4: 0.01755
	loss_reward_4: 0.00238
	loss_policy_5: 0.00773
	accuracy_policy_5: 0.71645
	loss_value_5: 0.01762
	loss_reward_5: 0.00248
	loss_policy: 0.05664
	loss_value: 0.17067
	loss_reward: 0.01143
[2024-05-04 10:58:29] nn step 8950, lr: 0.06.
	loss_policy_0: 0.021
	accuracy_policy_0: 0.80535
	loss_value_0: 0.08635
	loss_policy_1: 0.00586
	accuracy_policy_1: 0.75992
	loss_value_1: 0.01752
	loss_reward_1: 0.00236
	loss_policy_2: 0.00647
	accuracy_policy_2: 0.74766
	loss_value_2: 0.01768
	loss_reward_2: 0.00226
	loss_policy_3: 0.007
	accuracy_policy_3: 0.73719
	loss_value_3: 0.01786
	loss_reward_3: 0.0022
	loss_policy_4: 0.00743
	accuracy_policy_4: 0.7357
	loss_value_4: 0.01798
	loss_reward_4: 0.00235
	loss_policy_5: 0.00779
	accuracy_policy_5: 0.7166
	loss_value_5: 0.01804
	loss_reward_5: 0.00267
	loss_policy: 0.05555
	loss_value: 0.17542
	loss_reward: 0.01184
[2024-05-04 10:58:46] nn step 9000, lr: 0.06.
	loss_policy_0: 0.02032
	accuracy_policy_0: 0.80125
	loss_value_0: 0.08374
	loss_policy_1: 0.00559
	accuracy_policy_1: 0.76098
	loss_value_1: 0.01697
	loss_reward_1: 0.00229
	loss_policy_2: 0.00619
	accuracy_policy_2: 0.74934
	loss_value_2: 0.01713
	loss_reward_2: 0.00214
	loss_policy_3: 0.00658
	accuracy_policy_3: 0.74031
	loss_value_3: 0.01729
	loss_reward_3: 0.00212
	loss_policy_4: 0.00701
	accuracy_policy_4: 0.72977
	loss_value_4: 0.01739
	loss_reward_4: 0.00219
	loss_policy_5: 0.00726
	accuracy_policy_5: 0.72527
	loss_value_5: 0.01749
	loss_reward_5: 0.00242
	loss_policy: 0.05296
	loss_value: 0.17001
	loss_reward: 0.01117
Optimization_Done 9000
[2024-05-04 11:00:50] [command] train weight_iter_9000.pkl 42 46
[2024-05-04 11:01:07] nn step 9050, lr: 0.06.
	loss_policy_0: 0.04114
	accuracy_policy_0: 0.77359
	loss_value_0: 0.09393
	loss_policy_1: 0.00936
	accuracy_policy_1: 0.75031
	loss_value_1: 0.01909
	loss_reward_1: 0.00272
	loss_policy_2: 0.00991
	accuracy_policy_2: 0.73762
	loss_value_2: 0.01936
	loss_reward_2: 0.0026
	loss_policy_3: 0.01032
	accuracy_policy_3: 0.73586
	loss_value_3: 0.01962
	loss_reward_3: 0.00261
	loss_policy_4: 0.01079
	accuracy_policy_4: 0.72391
	loss_value_4: 0.01978
	loss_reward_4: 0.00274
	loss_policy_5: 0.01114
	accuracy_policy_5: 0.72395
	loss_value_5: 0.01994
	loss_reward_5: 0.00304
	loss_policy: 0.09267
	loss_value: 0.19173
	loss_reward: 0.01371
[2024-05-04 11:01:23] nn step 9100, lr: 0.06.
	loss_policy_0: 0.0337
	accuracy_policy_0: 0.80098
	loss_value_0: 0.09224
	loss_policy_1: 0.00792
	accuracy_policy_1: 0.77598
	loss_value_1: 0.01871
	loss_reward_1: 0.00275
	loss_policy_2: 0.00856
	accuracy_policy_2: 0.76566
	loss_value_2: 0.01897
	loss_reward_2: 0.00264
	loss_policy_3: 0.00905
	accuracy_policy_3: 0.7582
	loss_value_3: 0.01915
	loss_reward_3: 0.00268
	loss_policy_4: 0.0094
	accuracy_policy_4: 0.75047
	loss_value_4: 0.01941
	loss_reward_4: 0.00274
	loss_policy_5: 0.00988
	accuracy_policy_5: 0.74668
	loss_value_5: 0.01955
	loss_reward_5: 0.00293
	loss_policy: 0.0785
	loss_value: 0.18802
	loss_reward: 0.01373
[2024-05-04 11:01:40] nn step 9150, lr: 0.06.
	loss_policy_0: 0.0329
	accuracy_policy_0: 0.80645
	loss_value_0: 0.09227
	loss_policy_1: 0.00788
	accuracy_policy_1: 0.77496
	loss_value_1: 0.01878
	loss_reward_1: 0.00278
	loss_policy_2: 0.00837
	accuracy_policy_2: 0.76824
	loss_value_2: 0.01906
	loss_reward_2: 0.00266
	loss_policy_3: 0.00885
	accuracy_policy_3: 0.76
	loss_value_3: 0.01927
	loss_reward_3: 0.00269
	loss_policy_4: 0.00914
	accuracy_policy_4: 0.75203
	loss_value_4: 0.01947
	loss_reward_4: 0.00274
	loss_policy_5: 0.00959
	accuracy_policy_5: 0.74344
	loss_value_5: 0.01966
	loss_reward_5: 0.00301
	loss_policy: 0.07673
	loss_value: 0.18851
	loss_reward: 0.01388
[2024-05-04 11:01:56] nn step 9200, lr: 0.06.
	loss_policy_0: 0.03273
	accuracy_policy_0: 0.80949
	loss_value_0: 0.09618
	loss_policy_1: 0.00801
	accuracy_policy_1: 0.77344
	loss_value_1: 0.01956
	loss_reward_1: 0.00284
	loss_policy_2: 0.00848
	accuracy_policy_2: 0.76906
	loss_value_2: 0.01985
	loss_reward_2: 0.00282
	loss_policy_3: 0.00892
	accuracy_policy_3: 0.76027
	loss_value_3: 0.02008
	loss_reward_3: 0.00269
	loss_policy_4: 0.00923
	accuracy_policy_4: 0.75582
	loss_value_4: 0.02027
	loss_reward_4: 0.00291
	loss_policy_5: 0.00975
	accuracy_policy_5: 0.74656
	loss_value_5: 0.02041
	loss_reward_5: 0.00313
	loss_policy: 0.07712
	loss_value: 0.19636
	loss_reward: 0.01438
Optimization_Done 9200
[2024-05-04 11:04:00] [command] train weight_iter_9200.pkl 43 47
[2024-05-04 11:04:17] nn step 9250, lr: 0.06.
	loss_policy_0: 0.02743
	accuracy_policy_0: 0.81109
	loss_value_0: 0.08039
	loss_policy_1: 0.00679
	accuracy_policy_1: 0.77539
	loss_value_1: 0.01623
	loss_reward_1: 0.00214
	loss_policy_2: 0.00727
	accuracy_policy_2: 0.76844
	loss_value_2: 0.01632
	loss_reward_2: 0.00204
	loss_policy_3: 0.00772
	accuracy_policy_3: 0.76551
	loss_value_3: 0.01646
	loss_reward_3: 0.00209
	loss_policy_4: 0.00809
	accuracy_policy_4: 0.7534
	loss_value_4: 0.01658
	loss_reward_4: 0.00222
	loss_policy_5: 0.00842
	accuracy_policy_5: 0.74746
	loss_value_5: 0.01664
	loss_reward_5: 0.00234
	loss_policy: 0.06571
	loss_value: 0.16262
	loss_reward: 0.01083
[2024-05-04 11:04:33] nn step 9300, lr: 0.06.
	loss_policy_0: 0.02497
	accuracy_policy_0: 0.82898
	loss_value_0: 0.08585
	loss_policy_1: 0.0062
	accuracy_policy_1: 0.80121
	loss_value_1: 0.01738
	loss_reward_1: 0.00233
	loss_policy_2: 0.00698
	accuracy_policy_2: 0.78461
	loss_value_2: 0.01759
	loss_reward_2: 0.00229
	loss_policy_3: 0.00734
	accuracy_policy_3: 0.78312
	loss_value_3: 0.01772
	loss_reward_3: 0.00218
	loss_policy_4: 0.00783
	accuracy_policy_4: 0.77137
	loss_value_4: 0.01774
	loss_reward_4: 0.00239
	loss_policy_5: 0.00819
	accuracy_policy_5: 0.76652
	loss_value_5: 0.01782
	loss_reward_5: 0.00257
	loss_policy: 0.06151
	loss_value: 0.17409
	loss_reward: 0.01176
[2024-05-04 11:04:50] nn step 9350, lr: 0.06.
	loss_policy_0: 0.02125
	accuracy_policy_0: 0.83883
	loss_value_0: 0.08275
	loss_policy_1: 0.00567
	accuracy_policy_1: 0.80641
	loss_value_1: 0.01671
	loss_reward_1: 0.0022
	loss_policy_2: 0.00622
	accuracy_policy_2: 0.79363
	loss_value_2: 0.01685
	loss_reward_2: 0.00218
	loss_policy_3: 0.0066
	accuracy_policy_3: 0.79016
	loss_value_3: 0.01694
	loss_reward_3: 0.00211
	loss_policy_4: 0.00719
	accuracy_policy_4: 0.7782
	loss_value_4: 0.017
	loss_reward_4: 0.00239
	loss_policy_5: 0.00742
	accuracy_policy_5: 0.77707
	loss_value_5: 0.01702
	loss_reward_5: 0.00242
	loss_policy: 0.05435
	loss_value: 0.16727
	loss_reward: 0.0113
[2024-05-04 11:05:06] nn step 9400, lr: 0.06.
	loss_policy_0: 0.02061
	accuracy_policy_0: 0.84488
	loss_value_0: 0.08196
	loss_policy_1: 0.00567
	accuracy_policy_1: 0.80648
	loss_value_1: 0.01653
	loss_reward_1: 0.00226
	loss_policy_2: 0.00623
	accuracy_policy_2: 0.79531
	loss_value_2: 0.01671
	loss_reward_2: 0.00218
	loss_policy_3: 0.00654
	accuracy_policy_3: 0.78852
	loss_value_3: 0.0168
	loss_reward_3: 0.00213
	loss_policy_4: 0.00718
	accuracy_policy_4: 0.78016
	loss_value_4: 0.01684
	loss_reward_4: 0.00228
	loss_policy_5: 0.00722
	accuracy_policy_5: 0.77879
	loss_value_5: 0.0169
	loss_reward_5: 0.0024
	loss_policy: 0.05345
	loss_value: 0.16575
	loss_reward: 0.01126
Optimization_Done 9400
[2024-05-04 11:07:08] [command] train weight_iter_9400.pkl 44 48
[2024-05-04 11:07:25] nn step 9450, lr: 0.06.
	loss_policy_0: 0.03791
	accuracy_policy_0: 0.83242
	loss_value_0: 0.08869
	loss_policy_1: 0.00873
	accuracy_policy_1: 0.81246
	loss_value_1: 0.01798
	loss_reward_1: 0.0033
	loss_policy_2: 0.00914
	accuracy_policy_2: 0.80641
	loss_value_2: 0.01805
	loss_reward_2: 0.00318
	loss_policy_3: 0.00933
	accuracy_policy_3: 0.80738
	loss_value_3: 0.01807
	loss_reward_3: 0.00322
	loss_policy_4: 0.00956
	accuracy_policy_4: 0.80957
	loss_value_4: 0.01798
	loss_reward_4: 0.00331
	loss_policy_5: 0.0099
	accuracy_policy_5: 0.80852
	loss_value_5: 0.01797
	loss_reward_5: 0.00359
	loss_policy: 0.08456
	loss_value: 0.17873
	loss_reward: 0.0166
[2024-05-04 11:07:42] nn step 9500, lr: 0.06.
	loss_policy_0: 0.02483
	accuracy_policy_0: 0.87219
	loss_value_0: 0.08549
	loss_policy_1: 0.00614
	accuracy_policy_1: 0.85359
	loss_value_1: 0.01729
	loss_reward_1: 0.003
	loss_policy_2: 0.00674
	accuracy_policy_2: 0.845
	loss_value_2: 0.01734
	loss_reward_2: 0.00298
	loss_policy_3: 0.00688
	accuracy_policy_3: 0.84484
	loss_value_3: 0.01728
	loss_reward_3: 0.00293
	loss_policy_4: 0.00724
	accuracy_policy_4: 0.84109
	loss_value_4: 0.0171
	loss_reward_4: 0.00307
	loss_policy_5: 0.0073
	accuracy_policy_5: 0.84203
	loss_value_5: 0.017
	loss_reward_5: 0.00327
	loss_policy: 0.05913
	loss_value: 0.1715
	loss_reward: 0.01524
[2024-05-04 11:07:58] nn step 9550, lr: 0.06.
	loss_policy_0: 0.02341
	accuracy_policy_0: 0.87645
	loss_value_0: 0.08639
	loss_policy_1: 0.00588
	accuracy_policy_1: 0.85449
	loss_value_1: 0.01751
	loss_reward_1: 0.00305
	loss_policy_2: 0.00638
	accuracy_policy_2: 0.85078
	loss_value_2: 0.01752
	loss_reward_2: 0.00299
	loss_policy_3: 0.00658
	accuracy_policy_3: 0.84707
	loss_value_3: 0.01744
	loss_reward_3: 0.00294
	loss_policy_4: 0.00692
	accuracy_policy_4: 0.84426
	loss_value_4: 0.0174
	loss_reward_4: 0.00306
	loss_policy_5: 0.00705
	accuracy_policy_5: 0.8459
	loss_value_5: 0.01726
	loss_reward_5: 0.00319
	loss_policy: 0.05623
	loss_value: 0.17352
	loss_reward: 0.01523
[2024-05-04 11:08:14] nn step 9600, lr: 0.06.
	loss_policy_0: 0.02251
	accuracy_policy_0: 0.88512
	loss_value_0: 0.08986
	loss_policy_1: 0.00576
	accuracy_policy_1: 0.86406
	loss_value_1: 0.01815
	loss_reward_1: 0.00306
	loss_policy_2: 0.00629
	accuracy_policy_2: 0.85566
	loss_value_2: 0.01816
	loss_reward_2: 0.00297
	loss_policy_3: 0.00659
	accuracy_policy_3: 0.85203
	loss_value_3: 0.01805
	loss_reward_3: 0.00306
	loss_policy_4: 0.0069
	accuracy_policy_4: 0.84988
	loss_value_4: 0.01796
	loss_reward_4: 0.00317
	loss_policy_5: 0.00689
	accuracy_policy_5: 0.85324
	loss_value_5: 0.01777
	loss_reward_5: 0.0033
	loss_policy: 0.05495
	loss_value: 0.17994
	loss_reward: 0.01557
Optimization_Done 9600
[2024-05-04 11:10:09] [command] train weight_iter_9600.pkl 45 49
[2024-05-04 11:10:26] nn step 9650, lr: 0.06.
	loss_policy_0: 0.0156
	accuracy_policy_0: 0.90984
	loss_value_0: 0.08052
	loss_policy_1: 0.00436
	accuracy_policy_1: 0.88609
	loss_value_1: 0.01634
	loss_reward_1: 0.0028
	loss_policy_2: 0.00482
	accuracy_policy_2: 0.87863
	loss_value_2: 0.01635
	loss_reward_2: 0.00276
	loss_policy_3: 0.00501
	accuracy_policy_3: 0.87578
	loss_value_3: 0.01623
	loss_reward_3: 0.00279
	loss_policy_4: 0.0052
	accuracy_policy_4: 0.87414
	loss_value_4: 0.01611
	loss_reward_4: 0.00284
	loss_policy_5: 0.00546
	accuracy_policy_5: 0.87258
	loss_value_5: 0.01603
	loss_reward_5: 0.00306
	loss_policy: 0.04045
	loss_value: 0.16159
	loss_reward: 0.01425
[2024-05-04 11:10:42] nn step 9700, lr: 0.06.
	loss_policy_0: 0.01491
	accuracy_policy_0: 0.9141
	loss_value_0: 0.08535
	loss_policy_1: 0.00435
	accuracy_policy_1: 0.89473
	loss_value_1: 0.0173
	loss_reward_1: 0.00298
	loss_policy_2: 0.00482
	accuracy_policy_2: 0.88488
	loss_value_2: 0.01735
	loss_reward_2: 0.00296
	loss_policy_3: 0.00509
	accuracy_policy_3: 0.8766
	loss_value_3: 0.01725
	loss_reward_3: 0.00291
	loss_policy_4: 0.00551
	accuracy_policy_4: 0.87117
	loss_value_4: 0.01713
	loss_reward_4: 0.00311
	loss_policy_5: 0.00555
	accuracy_policy_5: 0.87445
	loss_value_5: 0.01711
	loss_reward_5: 0.00321
	loss_policy: 0.04023
	loss_value: 0.1715
	loss_reward: 0.01517
[2024-05-04 11:10:59] nn step 9750, lr: 0.06.
	loss_policy_0: 0.01396
	accuracy_policy_0: 0.91676
	loss_value_0: 0.08339
	loss_policy_1: 0.00424
	accuracy_policy_1: 0.89254
	loss_value_1: 0.0169
	loss_reward_1: 0.00292
	loss_policy_2: 0.00462
	accuracy_policy_2: 0.88426
	loss_value_2: 0.01695
	loss_reward_2: 0.00292
	loss_policy_3: 0.0049
	accuracy_policy_3: 0.88
	loss_value_3: 0.01694
	loss_reward_3: 0.00269
	loss_policy_4: 0.00522
	accuracy_policy_4: 0.87434
	loss_value_4: 0.01688
	loss_reward_4: 0.003
	loss_policy_5: 0.00532
	accuracy_policy_5: 0.87555
	loss_value_5: 0.01683
	loss_reward_5: 0.00314
	loss_policy: 0.03826
	loss_value: 0.16789
	loss_reward: 0.01467
[2024-05-04 11:11:15] nn step 9800, lr: 0.06.
	loss_policy_0: 0.01304
	accuracy_policy_0: 0.92039
	loss_value_0: 0.08159
	loss_policy_1: 0.00394
	accuracy_policy_1: 0.89719
	loss_value_1: 0.01655
	loss_reward_1: 0.00294
	loss_policy_2: 0.00428
	accuracy_policy_2: 0.89223
	loss_value_2: 0.01667
	loss_reward_2: 0.00285
	loss_policy_3: 0.00479
	accuracy_policy_3: 0.88164
	loss_value_3: 0.01677
	loss_reward_3: 0.00269
	loss_policy_4: 0.00501
	accuracy_policy_4: 0.87738
	loss_value_4: 0.01685
	loss_reward_4: 0.00293
	loss_policy_5: 0.00523
	accuracy_policy_5: 0.87641
	loss_value_5: 0.0169
	loss_reward_5: 0.00308
	loss_policy: 0.03629
	loss_value: 0.16534
	loss_reward: 0.01448
Optimization_Done 9800
[2024-05-04 11:13:16] [command] train weight_iter_9800.pkl 46 50
[2024-05-04 11:13:34] nn step 9850, lr: 0.06.
	loss_policy_0: 0.03855
	accuracy_policy_0: 0.81762
	loss_value_0: 0.08673
	loss_policy_1: 0.00875
	accuracy_policy_1: 0.79605
	loss_value_1: 0.01766
	loss_reward_1: 0.00292
	loss_policy_2: 0.00912
	accuracy_policy_2: 0.79121
	loss_value_2: 0.0179
	loss_reward_2: 0.00282
	loss_policy_3: 0.00965
	accuracy_policy_3: 0.7832
	loss_value_3: 0.01815
	loss_reward_3: 0.00277
	loss_policy_4: 0.01016
	accuracy_policy_4: 0.77863
	loss_value_4: 0.01826
	loss_reward_4: 0.0029
	loss_policy_5: 0.01064
	accuracy_policy_5: 0.76898
	loss_value_5: 0.01835
	loss_reward_5: 0.0031
	loss_policy: 0.08686
	loss_value: 0.17705
	loss_reward: 0.0145
[2024-05-04 11:13:50] nn step 9900, lr: 0.06.
	loss_policy_0: 0.03217
	accuracy_policy_0: 0.84441
	loss_value_0: 0.08755
	loss_policy_1: 0.00773
	accuracy_policy_1: 0.82059
	loss_value_1: 0.01787
	loss_reward_1: 0.00306
	loss_policy_2: 0.008
	accuracy_policy_2: 0.81637
	loss_value_2: 0.01819
	loss_reward_2: 0.00297
	loss_policy_3: 0.00833
	accuracy_policy_3: 0.81406
	loss_value_3: 0.0184
	loss_reward_3: 0.00279
	loss_policy_4: 0.00899
	accuracy_policy_4: 0.80262
	loss_value_4: 0.01862
	loss_reward_4: 0.00299
	loss_policy_5: 0.0095
	accuracy_policy_5: 0.79695
	loss_value_5: 0.01881
	loss_reward_5: 0.00328
	loss_policy: 0.07471
	loss_value: 0.17945
	loss_reward: 0.01509
[2024-05-04 11:14:06] nn step 9950, lr: 0.06.
	loss_policy_0: 0.02955
	accuracy_policy_0: 0.85355
	loss_value_0: 0.08611
	loss_policy_1: 0.00701
	accuracy_policy_1: 0.83461
	loss_value_1: 0.01758
	loss_reward_1: 0.00298
	loss_policy_2: 0.00743
	accuracy_policy_2: 0.82812
	loss_value_2: 0.01788
	loss_reward_2: 0.00284
	loss_policy_3: 0.0078
	accuracy_policy_3: 0.82266
	loss_value_3: 0.01803
	loss_reward_3: 0.00281
	loss_policy_4: 0.00823
	accuracy_policy_4: 0.81633
	loss_value_4: 0.01827
	loss_reward_4: 0.00297
	loss_policy_5: 0.00883
	accuracy_policy_5: 0.80848
	loss_value_5: 0.01847
	loss_reward_5: 0.0031
	loss_policy: 0.06885
	loss_value: 0.17634
	loss_reward: 0.0147
[2024-05-04 11:14:23] nn step 10000, lr: 0.06.
	loss_policy_0: 0.03094
	accuracy_policy_0: 0.85598
	loss_value_0: 0.09505
	loss_policy_1: 0.00753
	accuracy_policy_1: 0.83559
	loss_value_1: 0.01942
	loss_reward_1: 0.0033
	loss_policy_2: 0.00801
	accuracy_policy_2: 0.82637
	loss_value_2: 0.01969
	loss_reward_2: 0.00316
	loss_policy_3: 0.00837
	accuracy_policy_3: 0.82117
	loss_value_3: 0.01996
	loss_reward_3: 0.003
	loss_policy_4: 0.00893
	accuracy_policy_4: 0.81379
	loss_value_4: 0.02023
	loss_reward_4: 0.0032
	loss_policy_5: 0.00945
	accuracy_policy_5: 0.80559
	loss_value_5: 0.02042
	loss_reward_5: 0.00343
	loss_policy: 0.07323
	loss_value: 0.19477
	loss_reward: 0.01609
Optimization_Done 10000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 11:30:17] [command] train weight_iter_10000.pkl 47 51
[2024-05-04 11:30:52] nn step 10050, lr: 0.06.
	loss_policy_0: 0.02665
	accuracy_policy_0: 0.84723
	loss_value_0: 0.07665
	loss_policy_1: 0.0066
	accuracy_policy_1: 0.8277
	loss_value_1: 0.01561
	loss_reward_1: 0.00281
	loss_policy_2: 0.00705
	accuracy_policy_2: 0.81562
	loss_value_2: 0.0159
	loss_reward_2: 0.00263
	loss_policy_3: 0.00722
	accuracy_policy_3: 0.81289
	loss_value_3: 0.01615
	loss_reward_3: 0.00278
	loss_policy_4: 0.0077
	accuracy_policy_4: 0.80082
	loss_value_4: 0.01637
	loss_reward_4: 0.00276
	loss_policy_5: 0.00811
	accuracy_policy_5: 0.79055
	loss_value_5: 0.01645
	loss_reward_5: 0.00292
	loss_policy: 0.06333
	loss_value: 0.15713
	loss_reward: 0.0139
[2024-05-04 11:31:08] nn step 10100, lr: 0.06.
	loss_policy_0: 0.025
	accuracy_policy_0: 0.85332
	loss_value_0: 0.08114
	loss_policy_1: 0.00625
	accuracy_policy_1: 0.83645
	loss_value_1: 0.01654
	loss_reward_1: 0.00271
	loss_policy_2: 0.00666
	accuracy_policy_2: 0.82457
	loss_value_2: 0.01682
	loss_reward_2: 0.00266
	loss_policy_3: 0.00691
	accuracy_policy_3: 0.8227
	loss_value_3: 0.01701
	loss_reward_3: 0.00265
	loss_policy_4: 0.00744
	accuracy_policy_4: 0.81156
	loss_value_4: 0.01721
	loss_reward_4: 0.00276
	loss_policy_5: 0.00792
	accuracy_policy_5: 0.80391
	loss_value_5: 0.0174
	loss_reward_5: 0.00282
	loss_policy: 0.06018
	loss_value: 0.16613
	loss_reward: 0.0136
[2024-05-04 11:31:24] nn step 10150, lr: 0.06.
	loss_policy_0: 0.02347
	accuracy_policy_0: 0.85965
	loss_value_0: 0.08259
	loss_policy_1: 0.00601
	accuracy_policy_1: 0.84309
	loss_value_1: 0.01681
	loss_reward_1: 0.0028
	loss_policy_2: 0.00649
	accuracy_policy_2: 0.83074
	loss_value_2: 0.01708
	loss_reward_2: 0.0027
	loss_policy_3: 0.00682
	accuracy_policy_3: 0.82695
	loss_value_3: 0.0173
	loss_reward_3: 0.00267
	loss_policy_4: 0.00735
	accuracy_policy_4: 0.81914
	loss_value_4: 0.01751
	loss_reward_4: 0.00272
	loss_policy_5: 0.0077
	accuracy_policy_5: 0.81219
	loss_value_5: 0.01775
	loss_reward_5: 0.00288
	loss_policy: 0.05785
	loss_value: 0.16904
	loss_reward: 0.01377
[2024-05-04 11:31:40] nn step 10200, lr: 0.06.
	loss_policy_0: 0.02215
	accuracy_policy_0: 0.86781
	loss_value_0: 0.08078
	loss_policy_1: 0.00588
	accuracy_policy_1: 0.84238
	loss_value_1: 0.01645
	loss_reward_1: 0.00271
	loss_policy_2: 0.00625
	accuracy_policy_2: 0.8307
	loss_value_2: 0.01668
	loss_reward_2: 0.00259
	loss_policy_3: 0.00664
	accuracy_policy_3: 0.82758
	loss_value_3: 0.0169
	loss_reward_3: 0.00243
	loss_policy_4: 0.007
	accuracy_policy_4: 0.82199
	loss_value_4: 0.01713
	loss_reward_4: 0.00271
	loss_policy_5: 0.00739
	accuracy_policy_5: 0.81762
	loss_value_5: 0.01734
	loss_reward_5: 0.00284
	loss_policy: 0.05532
	loss_value: 0.1653
	loss_reward: 0.01327
Optimization_Done 10200
[2024-05-04 11:33:47] [command] train weight_iter_10200.pkl 48 52
[2024-05-04 11:34:04] nn step 10250, lr: 0.06.
	loss_policy_0: 0.04404
	accuracy_policy_0: 0.79363
	loss_value_0: 0.09704
	loss_policy_1: 0.01039
	accuracy_policy_1: 0.77098
	loss_value_1: 0.01978
	loss_reward_1: 0.00357
	loss_policy_2: 0.01088
	accuracy_policy_2: 0.76047
	loss_value_2: 0.02005
	loss_reward_2: 0.00326
	loss_policy_3: 0.01152
	accuracy_policy_3: 0.75598
	loss_value_3: 0.02023
	loss_reward_3: 0.0032
	loss_policy_4: 0.0121
	accuracy_policy_4: 0.74395
	loss_value_4: 0.02042
	loss_reward_4: 0.00323
	loss_policy_5: 0.01264
	accuracy_policy_5: 0.74023
	loss_value_5: 0.02053
	loss_reward_5: 0.00345
	loss_policy: 0.10157
	loss_value: 0.19805
	loss_reward: 0.01671
[2024-05-04 11:34:20] nn step 10300, lr: 0.06.
	loss_policy_0: 0.03241
	accuracy_policy_0: 0.82785
	loss_value_0: 0.08913
	loss_policy_1: 0.00819
	accuracy_policy_1: 0.8032
	loss_value_1: 0.01816
	loss_reward_1: 0.00325
	loss_policy_2: 0.00865
	accuracy_policy_2: 0.79039
	loss_value_2: 0.01841
	loss_reward_2: 0.00297
	loss_policy_3: 0.00907
	accuracy_policy_3: 0.78844
	loss_value_3: 0.01869
	loss_reward_3: 0.00292
	loss_policy_4: 0.0095
	accuracy_policy_4: 0.77996
	loss_value_4: 0.01893
	loss_reward_4: 0.00306
	loss_policy_5: 0.01003
	accuracy_policy_5: 0.76879
	loss_value_5: 0.01921
	loss_reward_5: 0.00318
	loss_policy: 0.07784
	loss_value: 0.18253
	loss_reward: 0.01537
[2024-05-04 11:34:36] nn step 10350, lr: 0.06.
	loss_policy_0: 0.03134
	accuracy_policy_0: 0.83262
	loss_value_0: 0.09115
	loss_policy_1: 0.00779
	accuracy_policy_1: 0.80773
	loss_value_1: 0.01855
	loss_reward_1: 0.00318
	loss_policy_2: 0.00829
	accuracy_policy_2: 0.8
	loss_value_2: 0.01887
	loss_reward_2: 0.00305
	loss_policy_3: 0.00864
	accuracy_policy_3: 0.7973
	loss_value_3: 0.01911
	loss_reward_3: 0.00294
	loss_policy_4: 0.00914
	accuracy_policy_4: 0.7866
	loss_value_4: 0.01932
	loss_reward_4: 0.00315
	loss_policy_5: 0.00972
	accuracy_policy_5: 0.77547
	loss_value_5: 0.01956
	loss_reward_5: 0.00334
	loss_policy: 0.07491
	loss_value: 0.18656
	loss_reward: 0.01566
[2024-05-04 11:34:52] nn step 10400, lr: 0.06.
	loss_policy_0: 0.03106
	accuracy_policy_0: 0.83223
	loss_value_0: 0.092
	loss_policy_1: 0.00764
	accuracy_policy_1: 0.81184
	loss_value_1: 0.01863
	loss_reward_1: 0.00319
	loss_policy_2: 0.00809
	accuracy_policy_2: 0.80457
	loss_value_2: 0.01899
	loss_reward_2: 0.00296
	loss_policy_3: 0.00869
	accuracy_policy_3: 0.79445
	loss_value_3: 0.01924
	loss_reward_3: 0.00279
	loss_policy_4: 0.0092
	accuracy_policy_4: 0.78555
	loss_value_4: 0.01949
	loss_reward_4: 0.00305
	loss_policy_5: 0.00975
	accuracy_policy_5: 0.77637
	loss_value_5: 0.01977
	loss_reward_5: 0.00334
	loss_policy: 0.07442
	loss_value: 0.18811
	loss_reward: 0.01532
Optimization_Done 10400
[2024-05-04 11:36:55] [command] train weight_iter_10400.pkl 49 53
[2024-05-04 11:37:12] nn step 10450, lr: 0.06.
	loss_policy_0: 0.04722
	accuracy_policy_0: 0.75391
	loss_value_0: 0.09226
	loss_policy_1: 0.01113
	accuracy_policy_1: 0.72477
	loss_value_1: 0.0187
	loss_reward_1: 0.00305
	loss_policy_2: 0.0117
	accuracy_policy_2: 0.71258
	loss_value_2: 0.01891
	loss_reward_2: 0.0028
	loss_policy_3: 0.01236
	accuracy_policy_3: 0.70535
	loss_value_3: 0.01911
	loss_reward_3: 0.00283
	loss_policy_4: 0.01308
	accuracy_policy_4: 0.69336
	loss_value_4: 0.01929
	loss_reward_4: 0.00295
	loss_policy_5: 0.01379
	accuracy_policy_5: 0.68262
	loss_value_5: 0.01946
	loss_reward_5: 0.00316
	loss_policy: 0.10929
	loss_value: 0.18774
	loss_reward: 0.01479
[2024-05-04 11:37:28] nn step 10500, lr: 0.06.
	loss_policy_0: 0.04054
	accuracy_policy_0: 0.77711
	loss_value_0: 0.09391
	loss_policy_1: 0.00987
	accuracy_policy_1: 0.75227
	loss_value_1: 0.01913
	loss_reward_1: 0.00297
	loss_policy_2: 0.01034
	accuracy_policy_2: 0.74074
	loss_value_2: 0.01931
	loss_reward_2: 0.00289
	loss_policy_3: 0.011
	accuracy_policy_3: 0.72922
	loss_value_3: 0.01953
	loss_reward_3: 0.00278
	loss_policy_4: 0.01156
	accuracy_policy_4: 0.72332
	loss_value_4: 0.0197
	loss_reward_4: 0.00298
	loss_policy_5: 0.01231
	accuracy_policy_5: 0.71117
	loss_value_5: 0.01979
	loss_reward_5: 0.00307
	loss_policy: 0.09562
	loss_value: 0.19137
	loss_reward: 0.01469
[2024-05-04 11:37:44] nn step 10550, lr: 0.06.
	loss_policy_0: 0.04092
	accuracy_policy_0: 0.77852
	loss_value_0: 0.09612
	loss_policy_1: 0.00988
	accuracy_policy_1: 0.74867
	loss_value_1: 0.01943
	loss_reward_1: 0.00297
	loss_policy_2: 0.01048
	accuracy_policy_2: 0.74254
	loss_value_2: 0.01968
	loss_reward_2: 0.00283
	loss_policy_3: 0.01105
	accuracy_policy_3: 0.72988
	loss_value_3: 0.01991
	loss_reward_3: 0.00288
	loss_policy_4: 0.01166
	accuracy_policy_4: 0.71992
	loss_value_4: 0.02013
	loss_reward_4: 0.00301
	loss_policy_5: 0.0126
	accuracy_policy_5: 0.70633
	loss_value_5: 0.02033
	loss_reward_5: 0.00328
	loss_policy: 0.09659
	loss_value: 0.1956
	loss_reward: 0.01496
[2024-05-04 11:38:00] nn step 10600, lr: 0.06.
	loss_policy_0: 0.03693
	accuracy_policy_0: 0.78707
	loss_value_0: 0.09155
	loss_policy_1: 0.00924
	accuracy_policy_1: 0.75449
	loss_value_1: 0.01858
	loss_reward_1: 0.00292
	loss_policy_2: 0.00963
	accuracy_policy_2: 0.74812
	loss_value_2: 0.01882
	loss_reward_2: 0.00279
	loss_policy_3: 0.01012
	accuracy_policy_3: 0.73965
	loss_value_3: 0.01905
	loss_reward_3: 0.00265
	loss_policy_4: 0.01082
	accuracy_policy_4: 0.73105
	loss_value_4: 0.01924
	loss_reward_4: 0.00282
	loss_policy_5: 0.01154
	accuracy_policy_5: 0.71578
	loss_value_5: 0.0194
	loss_reward_5: 0.00312
	loss_policy: 0.08827
	loss_value: 0.18664
	loss_reward: 0.0143
Optimization_Done 10600
[2024-05-04 11:40:04] [command] train weight_iter_10600.pkl 50 54
[2024-05-04 11:40:22] nn step 10650, lr: 0.06.
	loss_policy_0: 0.0498
	accuracy_policy_0: 0.74969
	loss_value_0: 0.1018
	loss_policy_1: 0.01203
	accuracy_policy_1: 0.71086
	loss_value_1: 0.02069
	loss_reward_1: 0.00343
	loss_policy_2: 0.01254
	accuracy_policy_2: 0.70723
	loss_value_2: 0.02099
	loss_reward_2: 0.00327
	loss_policy_3: 0.01323
	accuracy_policy_3: 0.69898
	loss_value_3: 0.02127
	loss_reward_3: 0.00326
	loss_policy_4: 0.01397
	accuracy_policy_4: 0.67898
	loss_value_4: 0.02147
	loss_reward_4: 0.00346
	loss_policy_5: 0.01498
	accuracy_policy_5: 0.66613
	loss_value_5: 0.02163
	loss_reward_5: 0.00366
	loss_policy: 0.11655
	loss_value: 0.20784
	loss_reward: 0.01707
[2024-05-04 11:40:38] nn step 10700, lr: 0.06.
	loss_policy_0: 0.04218
	accuracy_policy_0: 0.77906
	loss_value_0: 0.10068
	loss_policy_1: 0.01052
	accuracy_policy_1: 0.74062
	loss_value_1: 0.02044
	loss_reward_1: 0.00331
	loss_policy_2: 0.01111
	accuracy_policy_2: 0.72793
	loss_value_2: 0.02076
	loss_reward_2: 0.0032
	loss_policy_3: 0.01195
	accuracy_policy_3: 0.72039
	loss_value_3: 0.02102
	loss_reward_3: 0.00311
	loss_policy_4: 0.01267
	accuracy_policy_4: 0.70586
	loss_value_4: 0.02127
	loss_reward_4: 0.00333
	loss_policy_5: 0.0133
	accuracy_policy_5: 0.68992
	loss_value_5: 0.0214
	loss_reward_5: 0.00363
	loss_policy: 0.10172
	loss_value: 0.20558
	loss_reward: 0.01659
[2024-05-04 11:40:54] nn step 10750, lr: 0.06.
	loss_policy_0: 0.04144
	accuracy_policy_0: 0.7834
	loss_value_0: 0.10365
	loss_policy_1: 0.01054
	accuracy_policy_1: 0.74613
	loss_value_1: 0.0211
	loss_reward_1: 0.00347
	loss_policy_2: 0.01105
	accuracy_policy_2: 0.73211
	loss_value_2: 0.02134
	loss_reward_2: 0.00324
	loss_policy_3: 0.01176
	accuracy_policy_3: 0.72246
	loss_value_3: 0.02161
	loss_reward_3: 0.00313
	loss_policy_4: 0.01247
	accuracy_policy_4: 0.70809
	loss_value_4: 0.02186
	loss_reward_4: 0.00337
	loss_policy_5: 0.01347
	accuracy_policy_5: 0.69543
	loss_value_5: 0.02203
	loss_reward_5: 0.0036
	loss_policy: 0.10074
	loss_value: 0.21158
	loss_reward: 0.01681
[2024-05-04 11:41:10] nn step 10800, lr: 0.06.
	loss_policy_0: 0.0437
	accuracy_policy_0: 0.78566
	loss_value_0: 0.10845
	loss_policy_1: 0.01103
	accuracy_policy_1: 0.74656
	loss_value_1: 0.02208
	loss_reward_1: 0.00363
	loss_policy_2: 0.01174
	accuracy_policy_2: 0.73398
	loss_value_2: 0.02238
	loss_reward_2: 0.00335
	loss_policy_3: 0.01241
	accuracy_policy_3: 0.72633
	loss_value_3: 0.02266
	loss_reward_3: 0.00332
	loss_policy_4: 0.01325
	accuracy_policy_4: 0.7107
	loss_value_4: 0.0229
	loss_reward_4: 0.0034
	loss_policy_5: 0.01401
	accuracy_policy_5: 0.70207
	loss_value_5: 0.02317
	loss_reward_5: 0.00371
	loss_policy: 0.10613
	loss_value: 0.22164
	loss_reward: 0.0174
Optimization_Done 10800
[2024-05-04 11:43:06] [command] train weight_iter_10800.pkl 51 55
[2024-05-04 11:43:23] nn step 10850, lr: 0.06.
	loss_policy_0: 0.03661
	accuracy_policy_0: 0.7893
	loss_value_0: 0.09764
	loss_policy_1: 0.00971
	accuracy_policy_1: 0.74723
	loss_value_1: 0.01983
	loss_reward_1: 0.00323
	loss_policy_2: 0.0103
	accuracy_policy_2: 0.73727
	loss_value_2: 0.02005
	loss_reward_2: 0.00311
	loss_policy_3: 0.01099
	accuracy_policy_3: 0.72824
	loss_value_3: 0.02023
	loss_reward_3: 0.00293
	loss_policy_4: 0.01154
	accuracy_policy_4: 0.71566
	loss_value_4: 0.02039
	loss_reward_4: 0.00309
	loss_policy_5: 0.01227
	accuracy_policy_5: 0.7057
	loss_value_5: 0.02052
	loss_reward_5: 0.00338
	loss_policy: 0.09141
	loss_value: 0.19865
	loss_reward: 0.01575
[2024-05-04 11:43:40] nn step 10900, lr: 0.06.
	loss_policy_0: 0.0327
	accuracy_policy_0: 0.80867
	loss_value_0: 0.09963
	loss_policy_1: 0.00906
	accuracy_policy_1: 0.76426
	loss_value_1: 0.02019
	loss_reward_1: 0.00327
	loss_policy_2: 0.00983
	accuracy_policy_2: 0.74785
	loss_value_2: 0.02045
	loss_reward_2: 0.00303
	loss_policy_3: 0.01057
	accuracy_policy_3: 0.73984
	loss_value_3: 0.02066
	loss_reward_3: 0.00305
	loss_policy_4: 0.0112
	accuracy_policy_4: 0.72621
	loss_value_4: 0.02086
	loss_reward_4: 0.00322
	loss_policy_5: 0.01192
	accuracy_policy_5: 0.71566
	loss_value_5: 0.02109
	loss_reward_5: 0.0034
	loss_policy: 0.08529
	loss_value: 0.20287
	loss_reward: 0.01597
[2024-05-04 11:43:56] nn step 10950, lr: 0.06.
	loss_policy_0: 0.03352
	accuracy_policy_0: 0.80652
	loss_value_0: 0.10339
	loss_policy_1: 0.00913
	accuracy_policy_1: 0.77012
	loss_value_1: 0.02097
	loss_reward_1: 0.00344
	loss_policy_2: 0.00983
	accuracy_policy_2: 0.75316
	loss_value_2: 0.02123
	loss_reward_2: 0.0032
	loss_policy_3: 0.01061
	accuracy_policy_3: 0.74441
	loss_value_3: 0.02148
	loss_reward_3: 0.00306
	loss_policy_4: 0.01135
	accuracy_policy_4: 0.72559
	loss_value_4: 0.02161
	loss_reward_4: 0.00334
	loss_policy_5: 0.01212
	accuracy_policy_5: 0.71891
	loss_value_5: 0.02183
	loss_reward_5: 0.00355
	loss_policy: 0.08656
	loss_value: 0.21052
	loss_reward: 0.01659
[2024-05-04 11:44:12] nn step 11000, lr: 0.06.
	loss_policy_0: 0.03319
	accuracy_policy_0: 0.81094
	loss_value_0: 0.10365
	loss_policy_1: 0.00925
	accuracy_policy_1: 0.76758
	loss_value_1: 0.02099
	loss_reward_1: 0.00347
	loss_policy_2: 0.0098
	accuracy_policy_2: 0.75879
	loss_value_2: 0.02126
	loss_reward_2: 0.00325
	loss_policy_3: 0.01049
	accuracy_policy_3: 0.74582
	loss_value_3: 0.0216
	loss_reward_3: 0.00319
	loss_policy_4: 0.01123
	accuracy_policy_4: 0.7318
	loss_value_4: 0.02182
	loss_reward_4: 0.0033
	loss_policy_5: 0.0119
	accuracy_policy_5: 0.72039
	loss_value_5: 0.02202
	loss_reward_5: 0.00365
	loss_policy: 0.08586
	loss_value: 0.21133
	loss_reward: 0.01686
Optimization_Done 11000
[2024-05-04 11:46:16] [command] train weight_iter_11000.pkl 52 56
[2024-05-04 11:46:34] nn step 11050, lr: 0.06.
	loss_policy_0: 0.0584
	accuracy_policy_0: 0.73391
	loss_value_0: 0.11376
	loss_policy_1: 0.01348
	accuracy_policy_1: 0.71375
	loss_value_1: 0.02314
	loss_reward_1: 0.00387
	loss_policy_2: 0.01412
	accuracy_policy_2: 0.70539
	loss_value_2: 0.02349
	loss_reward_2: 0.00368
	loss_policy_3: 0.01475
	accuracy_policy_3: 0.69176
	loss_value_3: 0.02387
	loss_reward_3: 0.00355
	loss_policy_4: 0.01541
	accuracy_policy_4: 0.68043
	loss_value_4: 0.02414
	loss_reward_4: 0.00383
	loss_policy_5: 0.01621
	accuracy_policy_5: 0.66508
	loss_value_5: 0.02436
	loss_reward_5: 0.00401
	loss_policy: 0.13236
	loss_value: 0.23275
	loss_reward: 0.01896
[2024-05-04 11:46:50] nn step 11100, lr: 0.06.
	loss_policy_0: 0.04483
	accuracy_policy_0: 0.7668
	loss_value_0: 0.10496
	loss_policy_1: 0.01131
	accuracy_policy_1: 0.73281
	loss_value_1: 0.02127
	loss_reward_1: 0.00376
	loss_policy_2: 0.01198
	accuracy_policy_2: 0.72328
	loss_value_2: 0.0216
	loss_reward_2: 0.00342
	loss_policy_3: 0.01248
	accuracy_policy_3: 0.71828
	loss_value_3: 0.02192
	loss_reward_3: 0.0035
	loss_policy_4: 0.01312
	accuracy_policy_4: 0.70535
	loss_value_4: 0.02218
	loss_reward_4: 0.00343
	loss_policy_5: 0.0139
	accuracy_policy_5: 0.68867
	loss_value_5: 0.02244
	loss_reward_5: 0.0038
	loss_policy: 0.10762
	loss_value: 0.21437
	loss_reward: 0.01791
[2024-05-04 11:47:06] nn step 11150, lr: 0.06.
	loss_policy_0: 0.04489
	accuracy_policy_0: 0.77566
	loss_value_0: 0.10654
	loss_policy_1: 0.01101
	accuracy_policy_1: 0.74469
	loss_value_1: 0.02159
	loss_reward_1: 0.00365
	loss_policy_2: 0.01178
	accuracy_policy_2: 0.73145
	loss_value_2: 0.02195
	loss_reward_2: 0.00347
	loss_policy_3: 0.01238
	accuracy_policy_3: 0.71898
	loss_value_3: 0.02232
	loss_reward_3: 0.00351
	loss_policy_4: 0.01299
	accuracy_policy_4: 0.70699
	loss_value_4: 0.02256
	loss_reward_4: 0.00368
	loss_policy_5: 0.01375
	accuracy_policy_5: 0.69629
	loss_value_5: 0.02278
	loss_reward_5: 0.00376
	loss_policy: 0.1068
	loss_value: 0.21774
	loss_reward: 0.01808
[2024-05-04 11:47:22] nn step 11200, lr: 0.06.
	loss_policy_0: 0.04396
	accuracy_policy_0: 0.77559
	loss_value_0: 0.10783
	loss_policy_1: 0.01092
	accuracy_policy_1: 0.74465
	loss_value_1: 0.02194
	loss_reward_1: 0.00381
	loss_policy_2: 0.01164
	accuracy_policy_2: 0.73281
	loss_value_2: 0.02231
	loss_reward_2: 0.00344
	loss_policy_3: 0.01221
	accuracy_policy_3: 0.72316
	loss_value_3: 0.02265
	loss_reward_3: 0.00345
	loss_policy_4: 0.01304
	accuracy_policy_4: 0.71582
	loss_value_4: 0.02293
	loss_reward_4: 0.00368
	loss_policy_5: 0.01385
	accuracy_policy_5: 0.6968
	loss_value_5: 0.02322
	loss_reward_5: 0.00369
	loss_policy: 0.10562
	loss_value: 0.22087
	loss_reward: 0.01806
Optimization_Done 11200
[2024-05-04 11:49:26] [command] train weight_iter_11200.pkl 53 57
[2024-05-04 11:49:43] nn step 11250, lr: 0.06.
	loss_policy_0: 0.06107
	accuracy_policy_0: 0.74426
	loss_value_0: 0.11084
	loss_policy_1: 0.01449
	accuracy_policy_1: 0.70906
	loss_value_1: 0.02247
	loss_reward_1: 0.00409
	loss_policy_2: 0.01493
	accuracy_policy_2: 0.69957
	loss_value_2: 0.0229
	loss_reward_2: 0.00382
	loss_policy_3: 0.01552
	accuracy_policy_3: 0.69312
	loss_value_3: 0.02319
	loss_reward_3: 0.00381
	loss_policy_4: 0.01608
	accuracy_policy_4: 0.68078
	loss_value_4: 0.02353
	loss_reward_4: 0.00398
	loss_policy_5: 0.01677
	accuracy_policy_5: 0.66734
	loss_value_5: 0.02381
	loss_reward_5: 0.0042
	loss_policy: 0.13886
	loss_value: 0.22675
	loss_reward: 0.0199
[2024-05-04 11:49:59] nn step 11300, lr: 0.06.
	loss_policy_0: 0.05105
	accuracy_policy_0: 0.77086
	loss_value_0: 0.10998
	loss_policy_1: 0.01281
	accuracy_policy_1: 0.73344
	loss_value_1: 0.0223
	loss_reward_1: 0.004
	loss_policy_2: 0.01356
	accuracy_policy_2: 0.72031
	loss_value_2: 0.02266
	loss_reward_2: 0.00375
	loss_policy_3: 0.01421
	accuracy_policy_3: 0.71258
	loss_value_3: 0.02299
	loss_reward_3: 0.00372
	loss_policy_4: 0.0147
	accuracy_policy_4: 0.69941
	loss_value_4: 0.02336
	loss_reward_4: 0.00392
	loss_policy_5: 0.01537
	accuracy_policy_5: 0.68789
	loss_value_5: 0.02364
	loss_reward_5: 0.0041
	loss_policy: 0.1217
	loss_value: 0.22493
	loss_reward: 0.01948
[2024-05-04 11:50:15] nn step 11350, lr: 0.06.
	loss_policy_0: 0.04924
	accuracy_policy_0: 0.78309
	loss_value_0: 0.11193
	loss_policy_1: 0.01257
	accuracy_policy_1: 0.73695
	loss_value_1: 0.02268
	loss_reward_1: 0.00418
	loss_policy_2: 0.01313
	accuracy_policy_2: 0.73258
	loss_value_2: 0.02306
	loss_reward_2: 0.00379
	loss_policy_3: 0.01401
	accuracy_policy_3: 0.71738
	loss_value_3: 0.02349
	loss_reward_3: 0.00376
	loss_policy_4: 0.01452
	accuracy_policy_4: 0.70516
	loss_value_4: 0.02382
	loss_reward_4: 0.00399
	loss_policy_5: 0.01527
	accuracy_policy_5: 0.69504
	loss_value_5: 0.0242
	loss_reward_5: 0.00408
	loss_policy: 0.11874
	loss_value: 0.22918
	loss_reward: 0.0198
[2024-05-04 11:50:31] nn step 11400, lr: 0.06.
	loss_policy_0: 0.04642
	accuracy_policy_0: 0.77941
	loss_value_0: 0.10628
	loss_policy_1: 0.01156
	accuracy_policy_1: 0.74125
	loss_value_1: 0.02148
	loss_reward_1: 0.00393
	loss_policy_2: 0.01221
	accuracy_policy_2: 0.73094
	loss_value_2: 0.02183
	loss_reward_2: 0.00358
	loss_policy_3: 0.01279
	accuracy_policy_3: 0.72395
	loss_value_3: 0.02215
	loss_reward_3: 0.00355
	loss_policy_4: 0.01335
	accuracy_policy_4: 0.71711
	loss_value_4: 0.02255
	loss_reward_4: 0.00371
	loss_policy_5: 0.01428
	accuracy_policy_5: 0.70227
	loss_value_5: 0.02284
	loss_reward_5: 0.00387
	loss_policy: 0.11061
	loss_value: 0.21713
	loss_reward: 0.01864
Optimization_Done 11400
[2024-05-04 11:52:34] [command] train weight_iter_11400.pkl 54 58
[2024-05-04 11:52:51] nn step 11450, lr: 0.06.
	loss_policy_0: 0.05958
	accuracy_policy_0: 0.76027
	loss_value_0: 0.12275
	loss_policy_1: 0.0143
	accuracy_policy_1: 0.72137
	loss_value_1: 0.02485
	loss_reward_1: 0.00491
	loss_policy_2: 0.01493
	accuracy_policy_2: 0.71785
	loss_value_2: 0.02531
	loss_reward_2: 0.00468
	loss_policy_3: 0.01549
	accuracy_policy_3: 0.70684
	loss_value_3: 0.02568
	loss_reward_3: 0.00463
	loss_policy_4: 0.01589
	accuracy_policy_4: 0.69977
	loss_value_4: 0.02604
	loss_reward_4: 0.00461
	loss_policy_5: 0.01655
	accuracy_policy_5: 0.68602
	loss_value_5: 0.02627
	loss_reward_5: 0.00508
	loss_policy: 0.13675
	loss_value: 0.2509
	loss_reward: 0.02391
[2024-05-04 11:53:07] nn step 11500, lr: 0.06.
	loss_policy_0: 0.05298
	accuracy_policy_0: 0.7818
	loss_value_0: 0.12283
	loss_policy_1: 0.01347
	accuracy_policy_1: 0.74426
	loss_value_1: 0.02486
	loss_reward_1: 0.0049
	loss_policy_2: 0.0141
	accuracy_policy_2: 0.73355
	loss_value_2: 0.0252
	loss_reward_2: 0.00451
	loss_policy_3: 0.01435
	accuracy_policy_3: 0.72602
	loss_value_3: 0.02556
	loss_reward_3: 0.00448
	loss_policy_4: 0.0151
	accuracy_policy_4: 0.71781
	loss_value_4: 0.02597
	loss_reward_4: 0.00458
	loss_policy_5: 0.01575
	accuracy_policy_5: 0.71133
	loss_value_5: 0.02632
	loss_reward_5: 0.00488
	loss_policy: 0.12576
	loss_value: 0.25074
	loss_reward: 0.02336
[2024-05-04 11:53:24] nn step 11550, lr: 0.06.
	loss_policy_0: 0.04975
	accuracy_policy_0: 0.77516
	loss_value_0: 0.11497
	loss_policy_1: 0.01241
	accuracy_policy_1: 0.74348
	loss_value_1: 0.02328
	loss_reward_1: 0.00458
	loss_policy_2: 0.01297
	accuracy_policy_2: 0.73625
	loss_value_2: 0.02379
	loss_reward_2: 0.00425
	loss_policy_3: 0.01364
	accuracy_policy_3: 0.7275
	loss_value_3: 0.02417
	loss_reward_3: 0.00427
	loss_policy_4: 0.01417
	accuracy_policy_4: 0.71926
	loss_value_4: 0.02456
	loss_reward_4: 0.00425
	loss_policy_5: 0.01469
	accuracy_policy_5: 0.71242
	loss_value_5: 0.02484
	loss_reward_5: 0.0048
	loss_policy: 0.11763
	loss_value: 0.23561
	loss_reward: 0.02214
[2024-05-04 11:53:40] nn step 11600, lr: 0.06.
	loss_policy_0: 0.05237
	accuracy_policy_0: 0.78117
	loss_value_0: 0.12175
	loss_policy_1: 0.01298
	accuracy_policy_1: 0.74238
	loss_value_1: 0.02473
	loss_reward_1: 0.00499
	loss_policy_2: 0.01339
	accuracy_policy_2: 0.74082
	loss_value_2: 0.0251
	loss_reward_2: 0.00452
	loss_policy_3: 0.01405
	accuracy_policy_3: 0.73484
	loss_value_3: 0.02552
	loss_reward_3: 0.00444
	loss_policy_4: 0.01462
	accuracy_policy_4: 0.72277
	loss_value_4: 0.02594
	loss_reward_4: 0.00463
	loss_policy_5: 0.01541
	accuracy_policy_5: 0.7127
	loss_value_5: 0.0263
	loss_reward_5: 0.00474
	loss_policy: 0.12283
	loss_value: 0.24934
	loss_reward: 0.02333
Optimization_Done 11600
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 12:01:08] [command] train weight_iter_11600.pkl 55 59
[2024-05-04 12:01:51] nn step 11650, lr: 0.06.
	loss_policy_0: 0.04725
	accuracy_policy_0: 0.81156
	loss_value_0: 0.11734
	loss_policy_1: 0.01201
	accuracy_policy_1: 0.77488
	loss_value_1: 0.0237
	loss_reward_1: 0.00437
	loss_policy_2: 0.01253
	accuracy_policy_2: 0.7648
	loss_value_2: 0.02414
	loss_reward_2: 0.00402
	loss_policy_3: 0.01324
	accuracy_policy_3: 0.75383
	loss_value_3: 0.02453
	loss_reward_3: 0.00423
	loss_policy_4: 0.01384
	accuracy_policy_4: 0.74598
	loss_value_4: 0.02489
	loss_reward_4: 0.00422
	loss_policy_5: 0.015
	accuracy_policy_5: 0.72828
	loss_value_5: 0.02516
	loss_reward_5: 0.00445
	loss_policy: 0.11387
	loss_value: 0.23976
	loss_reward: 0.02129
[2024-05-04 12:02:07] nn step 11700, lr: 0.06.
	loss_policy_0: 0.04351
	accuracy_policy_0: 0.81965
	loss_value_0: 0.11742
	loss_policy_1: 0.01142
	accuracy_policy_1: 0.78586
	loss_value_1: 0.02375
	loss_reward_1: 0.00463
	loss_policy_2: 0.0119
	accuracy_policy_2: 0.78102
	loss_value_2: 0.02424
	loss_reward_2: 0.00425
	loss_policy_3: 0.01263
	accuracy_policy_3: 0.77016
	loss_value_3: 0.02467
	loss_reward_3: 0.0041
	loss_policy_4: 0.01317
	accuracy_policy_4: 0.7652
	loss_value_4: 0.02509
	loss_reward_4: 0.00429
	loss_policy_5: 0.01426
	accuracy_policy_5: 0.74488
	loss_value_5: 0.02552
	loss_reward_5: 0.00443
	loss_policy: 0.10688
	loss_value: 0.24068
	loss_reward: 0.02169
[2024-05-04 12:02:23] nn step 11750, lr: 0.06.
	loss_policy_0: 0.04185
	accuracy_policy_0: 0.82613
	loss_value_0: 0.11518
	loss_policy_1: 0.01087
	accuracy_policy_1: 0.7909
	loss_value_1: 0.02346
	loss_reward_1: 0.00437
	loss_policy_2: 0.01142
	accuracy_policy_2: 0.78504
	loss_value_2: 0.0239
	loss_reward_2: 0.00409
	loss_policy_3: 0.01219
	accuracy_policy_3: 0.76891
	loss_value_3: 0.02438
	loss_reward_3: 0.00404
	loss_policy_4: 0.01269
	accuracy_policy_4: 0.76664
	loss_value_4: 0.02471
	loss_reward_4: 0.0042
	loss_policy_5: 0.01363
	accuracy_policy_5: 0.74859
	loss_value_5: 0.02503
	loss_reward_5: 0.00442
	loss_policy: 0.10265
	loss_value: 0.23665
	loss_reward: 0.02111
[2024-05-04 12:02:39] nn step 11800, lr: 0.06.
	loss_policy_0: 0.03992
	accuracy_policy_0: 0.82527
	loss_value_0: 0.11169
	loss_policy_1: 0.01054
	accuracy_policy_1: 0.79055
	loss_value_1: 0.02257
	loss_reward_1: 0.00418
	loss_policy_2: 0.01096
	accuracy_policy_2: 0.78531
	loss_value_2: 0.02306
	loss_reward_2: 0.00387
	loss_policy_3: 0.01165
	accuracy_policy_3: 0.77551
	loss_value_3: 0.02358
	loss_reward_3: 0.00386
	loss_policy_4: 0.01232
	accuracy_policy_4: 0.76254
	loss_value_4: 0.02395
	loss_reward_4: 0.00409
	loss_policy_5: 0.01273
	accuracy_policy_5: 0.75625
	loss_value_5: 0.02429
	loss_reward_5: 0.00425
	loss_policy: 0.09813
	loss_value: 0.22913
	loss_reward: 0.02026
Optimization_Done 11800
[2024-05-04 12:04:56] [command] train weight_iter_11800.pkl 56 60
[2024-05-04 12:05:13] nn step 11850, lr: 0.06.
	loss_policy_0: 0.06439
	accuracy_policy_0: 0.79234
	loss_value_0: 0.14468
	loss_policy_1: 0.01549
	accuracy_policy_1: 0.76203
	loss_value_1: 0.02932
	loss_reward_1: 0.00534
	loss_policy_2: 0.01621
	accuracy_policy_2: 0.7557
	loss_value_2: 0.02998
	loss_reward_2: 0.00474
	loss_policy_3: 0.01675
	accuracy_policy_3: 0.7493
	loss_value_3: 0.03062
	loss_reward_3: 0.00488
	loss_policy_4: 0.01772
	accuracy_policy_4: 0.74105
	loss_value_4: 0.03123
	loss_reward_4: 0.00501
	loss_policy_5: 0.01831
	accuracy_policy_5: 0.73484
	loss_value_5: 0.03165
	loss_reward_5: 0.00549
	loss_policy: 0.14888
	loss_value: 0.29747
	loss_reward: 0.02546
[2024-05-04 12:05:29] nn step 11900, lr: 0.06.
	loss_policy_0: 0.06333
	accuracy_policy_0: 0.80102
	loss_value_0: 0.15197
	loss_policy_1: 0.01523
	accuracy_policy_1: 0.77555
	loss_value_1: 0.03072
	loss_reward_1: 0.00551
	loss_policy_2: 0.01603
	accuracy_policy_2: 0.76652
	loss_value_2: 0.03142
	loss_reward_2: 0.00508
	loss_policy_3: 0.01685
	accuracy_policy_3: 0.75523
	loss_value_3: 0.0321
	loss_reward_3: 0.00496
	loss_policy_4: 0.01778
	accuracy_policy_4: 0.74965
	loss_value_4: 0.03274
	loss_reward_4: 0.00544
	loss_policy_5: 0.0185
	accuracy_policy_5: 0.74023
	loss_value_5: 0.03321
	loss_reward_5: 0.0058
	loss_policy: 0.14772
	loss_value: 0.31217
	loss_reward: 0.02678
[2024-05-04 12:05:45] nn step 11950, lr: 0.06.
	loss_policy_0: 0.0626
	accuracy_policy_0: 0.80332
	loss_value_0: 0.1508
	loss_policy_1: 0.01508
	accuracy_policy_1: 0.78227
	loss_value_1: 0.03059
	loss_reward_1: 0.00563
	loss_policy_2: 0.0161
	accuracy_policy_2: 0.77242
	loss_value_2: 0.03119
	loss_reward_2: 0.00523
	loss_policy_3: 0.01676
	accuracy_policy_3: 0.76098
	loss_value_3: 0.0319
	loss_reward_3: 0.00508
	loss_policy_4: 0.0174
	accuracy_policy_4: 0.75355
	loss_value_4: 0.03242
	loss_reward_4: 0.00549
	loss_policy_5: 0.01839
	accuracy_policy_5: 0.7448
	loss_value_5: 0.03287
	loss_reward_5: 0.00572
	loss_policy: 0.14632
	loss_value: 0.30977
	loss_reward: 0.02715
[2024-05-04 12:06:01] nn step 12000, lr: 0.06.
	loss_policy_0: 0.05835
	accuracy_policy_0: 0.80492
	loss_value_0: 0.14154
	loss_policy_1: 0.01419
	accuracy_policy_1: 0.78059
	loss_value_1: 0.02882
	loss_reward_1: 0.00544
	loss_policy_2: 0.01461
	accuracy_policy_2: 0.77371
	loss_value_2: 0.02937
	loss_reward_2: 0.00491
	loss_policy_3: 0.01538
	accuracy_policy_3: 0.76441
	loss_value_3: 0.02985
	loss_reward_3: 0.00475
	loss_policy_4: 0.01619
	accuracy_policy_4: 0.75781
	loss_value_4: 0.03047
	loss_reward_4: 0.00518
	loss_policy_5: 0.01691
	accuracy_policy_5: 0.75301
	loss_value_5: 0.03084
	loss_reward_5: 0.00554
	loss_policy: 0.13562
	loss_value: 0.29088
	loss_reward: 0.02582
Optimization_Done 12000
[2024-05-04 12:08:07] [command] train weight_iter_12000.pkl 57 61
[2024-05-04 12:08:25] nn step 12050, lr: 0.06.
	loss_policy_0: 0.06999
	accuracy_policy_0: 0.80277
	loss_value_0: 0.15263
	loss_policy_1: 0.01633
	accuracy_policy_1: 0.77836
	loss_value_1: 0.03085
	loss_reward_1: 0.00579
	loss_policy_2: 0.017
	accuracy_policy_2: 0.76965
	loss_value_2: 0.03152
	loss_reward_2: 0.0052
	loss_policy_3: 0.01773
	accuracy_policy_3: 0.7602
	loss_value_3: 0.03214
	loss_reward_3: 0.00526
	loss_policy_4: 0.01859
	accuracy_policy_4: 0.7525
	loss_value_4: 0.03276
	loss_reward_4: 0.0057
	loss_policy_5: 0.019
	accuracy_policy_5: 0.74738
	loss_value_5: 0.03315
	loss_reward_5: 0.00587
	loss_policy: 0.15864
	loss_value: 0.31306
	loss_reward: 0.02781
[2024-05-04 12:08:41] nn step 12100, lr: 0.06.
	loss_policy_0: 0.06074
	accuracy_policy_0: 0.81332
	loss_value_0: 0.14494
	loss_policy_1: 0.01478
	accuracy_policy_1: 0.78582
	loss_value_1: 0.02952
	loss_reward_1: 0.00563
	loss_policy_2: 0.01551
	accuracy_policy_2: 0.77793
	loss_value_2: 0.03013
	loss_reward_2: 0.00522
	loss_policy_3: 0.01578
	accuracy_policy_3: 0.77891
	loss_value_3: 0.03071
	loss_reward_3: 0.00522
	loss_policy_4: 0.01653
	accuracy_policy_4: 0.77
	loss_value_4: 0.03121
	loss_reward_4: 0.00545
	loss_policy_5: 0.0173
	accuracy_policy_5: 0.76297
	loss_value_5: 0.0317
	loss_reward_5: 0.00563
	loss_policy: 0.14065
	loss_value: 0.29821
	loss_reward: 0.02714
[2024-05-04 12:08:57] nn step 12150, lr: 0.06.
	loss_policy_0: 0.05967
	accuracy_policy_0: 0.81484
	loss_value_0: 0.14747
	loss_policy_1: 0.0145
	accuracy_policy_1: 0.78617
	loss_value_1: 0.02999
	loss_reward_1: 0.00556
	loss_policy_2: 0.01527
	accuracy_policy_2: 0.78262
	loss_value_2: 0.03045
	loss_reward_2: 0.00518
	loss_policy_3: 0.0157
	accuracy_policy_3: 0.77965
	loss_value_3: 0.03103
	loss_reward_3: 0.00502
	loss_policy_4: 0.01647
	accuracy_policy_4: 0.77094
	loss_value_4: 0.0316
	loss_reward_4: 0.00556
	loss_policy_5: 0.01726
	accuracy_policy_5: 0.7634
	loss_value_5: 0.03201
	loss_reward_5: 0.00568
	loss_policy: 0.13886
	loss_value: 0.30257
	loss_reward: 0.02701
[2024-05-04 12:09:13] nn step 12200, lr: 0.06.
	loss_policy_0: 0.06239
	accuracy_policy_0: 0.81863
	loss_value_0: 0.15363
	loss_policy_1: 0.01497
	accuracy_policy_1: 0.78926
	loss_value_1: 0.03118
	loss_reward_1: 0.00596
	loss_policy_2: 0.01571
	accuracy_policy_2: 0.77988
	loss_value_2: 0.03176
	loss_reward_2: 0.00542
	loss_policy_3: 0.01631
	accuracy_policy_3: 0.77457
	loss_value_3: 0.0323
	loss_reward_3: 0.00537
	loss_policy_4: 0.01686
	accuracy_policy_4: 0.77008
	loss_value_4: 0.0328
	loss_reward_4: 0.00562
	loss_policy_5: 0.01757
	accuracy_policy_5: 0.76793
	loss_value_5: 0.0334
	loss_reward_5: 0.00591
	loss_policy: 0.1438
	loss_value: 0.31507
	loss_reward: 0.02828
Optimization_Done 12200
[2024-05-04 12:11:19] [command] train weight_iter_12200.pkl 58 62
[2024-05-04 12:11:37] nn step 12250, lr: 0.06.
	loss_policy_0: 0.0686
	accuracy_policy_0: 0.79555
	loss_value_0: 0.15679
	loss_policy_1: 0.01639
	accuracy_policy_1: 0.76875
	loss_value_1: 0.0319
	loss_reward_1: 0.0063
	loss_policy_2: 0.0171
	accuracy_policy_2: 0.75977
	loss_value_2: 0.03258
	loss_reward_2: 0.00573
	loss_policy_3: 0.01774
	accuracy_policy_3: 0.75777
	loss_value_3: 0.03323
	loss_reward_3: 0.00578
	loss_policy_4: 0.01837
	accuracy_policy_4: 0.74867
	loss_value_4: 0.03384
	loss_reward_4: 0.00628
	loss_policy_5: 0.01934
	accuracy_policy_5: 0.74336
	loss_value_5: 0.03428
	loss_reward_5: 0.00642
	loss_policy: 0.15754
	loss_value: 0.32262
	loss_reward: 0.03051
[2024-05-04 12:11:53] nn step 12300, lr: 0.06.
	loss_policy_0: 0.06594
	accuracy_policy_0: 0.8152
	loss_value_0: 0.16676
	loss_policy_1: 0.01622
	accuracy_policy_1: 0.78465
	loss_value_1: 0.03404
	loss_reward_1: 0.00667
	loss_policy_2: 0.01662
	accuracy_policy_2: 0.7777
	loss_value_2: 0.03471
	loss_reward_2: 0.00614
	loss_policy_3: 0.01762
	accuracy_policy_3: 0.77117
	loss_value_3: 0.03524
	loss_reward_3: 0.00622
	loss_policy_4: 0.01858
	accuracy_policy_4: 0.7652
	loss_value_4: 0.0359
	loss_reward_4: 0.00639
	loss_policy_5: 0.01945
	accuracy_policy_5: 0.7584
	loss_value_5: 0.0364
	loss_reward_5: 0.00697
	loss_policy: 0.15442
	loss_value: 0.34304
	loss_reward: 0.03239
[2024-05-04 12:12:09] nn step 12350, lr: 0.06.
	loss_policy_0: 0.06069
	accuracy_policy_0: 0.81578
	loss_value_0: 0.15151
	loss_policy_1: 0.01481
	accuracy_policy_1: 0.78297
	loss_value_1: 0.03084
	loss_reward_1: 0.00611
	loss_policy_2: 0.01533
	accuracy_policy_2: 0.77926
	loss_value_2: 0.03146
	loss_reward_2: 0.00565
	loss_policy_3: 0.01611
	accuracy_policy_3: 0.76734
	loss_value_3: 0.0321
	loss_reward_3: 0.00553
	loss_policy_4: 0.01686
	accuracy_policy_4: 0.76793
	loss_value_4: 0.03257
	loss_reward_4: 0.00572
	loss_policy_5: 0.01779
	accuracy_policy_5: 0.75723
	loss_value_5: 0.03308
	loss_reward_5: 0.00635
	loss_policy: 0.14159
	loss_value: 0.31154
	loss_reward: 0.02936
[2024-05-04 12:12:25] nn step 12400, lr: 0.06.
	loss_policy_0: 0.05684
	accuracy_policy_0: 0.82195
	loss_value_0: 0.14969
	loss_policy_1: 0.01392
	accuracy_policy_1: 0.79285
	loss_value_1: 0.03053
	loss_reward_1: 0.00592
	loss_policy_2: 0.01438
	accuracy_policy_2: 0.79004
	loss_value_2: 0.03119
	loss_reward_2: 0.00564
	loss_policy_3: 0.01523
	accuracy_policy_3: 0.78234
	loss_value_3: 0.03179
	loss_reward_3: 0.00527
	loss_policy_4: 0.01596
	accuracy_policy_4: 0.77523
	loss_value_4: 0.03243
	loss_reward_4: 0.00571
	loss_policy_5: 0.01688
	accuracy_policy_5: 0.76363
	loss_value_5: 0.03288
	loss_reward_5: 0.00623
	loss_policy: 0.13323
	loss_value: 0.30852
	loss_reward: 0.02877
Optimization_Done 12400
[2024-05-04 12:14:23] [command] train weight_iter_12400.pkl 59 63
[2024-05-04 12:14:41] nn step 12450, lr: 0.06.
	loss_policy_0: 0.0724
	accuracy_policy_0: 0.81164
	loss_value_0: 0.16582
	loss_policy_1: 0.01688
	accuracy_policy_1: 0.78453
	loss_value_1: 0.03365
	loss_reward_1: 0.00607
	loss_policy_2: 0.018
	accuracy_policy_2: 0.77199
	loss_value_2: 0.03426
	loss_reward_2: 0.00556
	loss_policy_3: 0.01913
	accuracy_policy_3: 0.76336
	loss_value_3: 0.03492
	loss_reward_3: 0.00554
	loss_policy_4: 0.02017
	accuracy_policy_4: 0.75375
	loss_value_4: 0.03551
	loss_reward_4: 0.00588
	loss_policy_5: 0.02115
	accuracy_policy_5: 0.74801
	loss_value_5: 0.03596
	loss_reward_5: 0.0063
	loss_policy: 0.16772
	loss_value: 0.34013
	loss_reward: 0.02937
[2024-05-04 12:14:57] nn step 12500, lr: 0.06.
	loss_policy_0: 0.06306
	accuracy_policy_0: 0.82484
	loss_value_0: 0.16287
	loss_policy_1: 0.016
	accuracy_policy_1: 0.7943
	loss_value_1: 0.03322
	loss_reward_1: 0.00599
	loss_policy_2: 0.01649
	accuracy_policy_2: 0.78676
	loss_value_2: 0.03382
	loss_reward_2: 0.00551
	loss_policy_3: 0.01752
	accuracy_policy_3: 0.77723
	loss_value_3: 0.03439
	loss_reward_3: 0.00576
	loss_policy_4: 0.01846
	accuracy_policy_4: 0.76777
	loss_value_4: 0.03493
	loss_reward_4: 0.00597
	loss_policy_5: 0.01934
	accuracy_policy_5: 0.76328
	loss_value_5: 0.03542
	loss_reward_5: 0.00643
	loss_policy: 0.15087
	loss_value: 0.33465
	loss_reward: 0.02965
[2024-05-04 12:15:13] nn step 12550, lr: 0.06.
	loss_policy_0: 0.06202
	accuracy_policy_0: 0.83375
	loss_value_0: 0.16723
	loss_policy_1: 0.01603
	accuracy_policy_1: 0.79719
	loss_value_1: 0.03393
	loss_reward_1: 0.0062
	loss_policy_2: 0.01698
	accuracy_policy_2: 0.79062
	loss_value_2: 0.03453
	loss_reward_2: 0.0057
	loss_policy_3: 0.01774
	accuracy_policy_3: 0.78168
	loss_value_3: 0.03508
	loss_reward_3: 0.00587
	loss_policy_4: 0.01867
	accuracy_policy_4: 0.77281
	loss_value_4: 0.03557
	loss_reward_4: 0.00619
	loss_policy_5: 0.01943
	accuracy_policy_5: 0.77012
	loss_value_5: 0.03613
	loss_reward_5: 0.00655
	loss_policy: 0.15087
	loss_value: 0.34247
	loss_reward: 0.03051
[2024-05-04 12:15:30] nn step 12600, lr: 0.06.
	loss_policy_0: 0.05818
	accuracy_policy_0: 0.83641
	loss_value_0: 0.16367
	loss_policy_1: 0.01513
	accuracy_policy_1: 0.80637
	loss_value_1: 0.03332
	loss_reward_1: 0.00621
	loss_policy_2: 0.01591
	accuracy_policy_2: 0.7991
	loss_value_2: 0.0339
	loss_reward_2: 0.00559
	loss_policy_3: 0.01681
	accuracy_policy_3: 0.78816
	loss_value_3: 0.03439
	loss_reward_3: 0.00567
	loss_policy_4: 0.01767
	accuracy_policy_4: 0.77816
	loss_value_4: 0.03492
	loss_reward_4: 0.00597
	loss_policy_5: 0.01871
	accuracy_policy_5: 0.77375
	loss_value_5: 0.03546
	loss_reward_5: 0.00621
	loss_policy: 0.1424
	loss_value: 0.33566
	loss_reward: 0.02965
Optimization_Done 12600
[2024-05-04 12:17:37] [command] train weight_iter_12600.pkl 60 64
[2024-05-04 12:17:54] nn step 12650, lr: 0.06.
	loss_policy_0: 0.07573
	accuracy_policy_0: 0.81719
	loss_value_0: 0.17591
	loss_policy_1: 0.01851
	accuracy_policy_1: 0.79102
	loss_value_1: 0.03569
	loss_reward_1: 0.00633
	loss_policy_2: 0.01917
	accuracy_policy_2: 0.78387
	loss_value_2: 0.03643
	loss_reward_2: 0.00578
	loss_policy_3: 0.02005
	accuracy_policy_3: 0.775
	loss_value_3: 0.03715
	loss_reward_3: 0.00583
	loss_policy_4: 0.02087
	accuracy_policy_4: 0.76914
	loss_value_4: 0.0377
	loss_reward_4: 0.00611
	loss_policy_5: 0.02207
	accuracy_policy_5: 0.75898
	loss_value_5: 0.03833
	loss_reward_5: 0.00662
	loss_policy: 0.1764
	loss_value: 0.36121
	loss_reward: 0.03067
[2024-05-04 12:18:11] nn step 12700, lr: 0.06.
	loss_policy_0: 0.07108
	accuracy_policy_0: 0.82559
	loss_value_0: 0.17718
	loss_policy_1: 0.01737
	accuracy_policy_1: 0.79719
	loss_value_1: 0.0359
	loss_reward_1: 0.00639
	loss_policy_2: 0.01812
	accuracy_policy_2: 0.79305
	loss_value_2: 0.03667
	loss_reward_2: 0.00605
	loss_policy_3: 0.01929
	accuracy_policy_3: 0.7827
	loss_value_3: 0.03736
	loss_reward_3: 0.00585
	loss_policy_4: 0.02004
	accuracy_policy_4: 0.77719
	loss_value_4: 0.03796
	loss_reward_4: 0.0063
	loss_policy_5: 0.02111
	accuracy_policy_5: 0.77152
	loss_value_5: 0.03836
	loss_reward_5: 0.00691
	loss_policy: 0.16701
	loss_value: 0.36344
	loss_reward: 0.0315
[2024-05-04 12:18:27] nn step 12750, lr: 0.06.
	loss_policy_0: 0.06636
	accuracy_policy_0: 0.83465
	loss_value_0: 0.17704
	loss_policy_1: 0.0169
	accuracy_policy_1: 0.80492
	loss_value_1: 0.03592
	loss_reward_1: 0.00659
	loss_policy_2: 0.01806
	accuracy_policy_2: 0.79219
	loss_value_2: 0.03659
	loss_reward_2: 0.00584
	loss_policy_3: 0.01906
	accuracy_policy_3: 0.7857
	loss_value_3: 0.03727
	loss_reward_3: 0.0057
	loss_policy_4: 0.01971
	accuracy_policy_4: 0.77617
	loss_value_4: 0.03784
	loss_reward_4: 0.00649
	loss_policy_5: 0.02077
	accuracy_policy_5: 0.77559
	loss_value_5: 0.03845
	loss_reward_5: 0.00686
	loss_policy: 0.16086
	loss_value: 0.36311
	loss_reward: 0.03149
[2024-05-04 12:18:43] nn step 12800, lr: 0.06.
	loss_policy_0: 0.06682
	accuracy_policy_0: 0.82973
	loss_value_0: 0.17357
	loss_policy_1: 0.01696
	accuracy_policy_1: 0.79969
	loss_value_1: 0.03526
	loss_reward_1: 0.0064
	loss_policy_2: 0.01769
	accuracy_policy_2: 0.79602
	loss_value_2: 0.03602
	loss_reward_2: 0.00584
	loss_policy_3: 0.0185
	accuracy_policy_3: 0.78766
	loss_value_3: 0.03662
	loss_reward_3: 0.00591
	loss_policy_4: 0.01939
	accuracy_policy_4: 0.77961
	loss_value_4: 0.03729
	loss_reward_4: 0.00622
	loss_policy_5: 0.02037
	accuracy_policy_5: 0.77098
	loss_value_5: 0.03784
	loss_reward_5: 0.00673
	loss_policy: 0.15974
	loss_value: 0.3566
	loss_reward: 0.0311
Optimization_Done 12800
[2024-05-04 12:20:50] [command] train weight_iter_12800.pkl 61 65
[2024-05-04 12:21:07] nn step 12850, lr: 0.06.
	loss_policy_0: 0.06895
	accuracy_policy_0: 0.83234
	loss_value_0: 0.17723
	loss_policy_1: 0.01618
	accuracy_policy_1: 0.81293
	loss_value_1: 0.03598
	loss_reward_1: 0.00626
	loss_policy_2: 0.01696
	accuracy_policy_2: 0.80371
	loss_value_2: 0.03655
	loss_reward_2: 0.0057
	loss_policy_3: 0.01801
	accuracy_policy_3: 0.79863
	loss_value_3: 0.03712
	loss_reward_3: 0.00587
	loss_policy_4: 0.01872
	accuracy_policy_4: 0.7898
	loss_value_4: 0.03767
	loss_reward_4: 0.00622
	loss_policy_5: 0.01951
	accuracy_policy_5: 0.78668
	loss_value_5: 0.03817
	loss_reward_5: 0.00643
	loss_policy: 0.15832
	loss_value: 0.36272
	loss_reward: 0.03048
[2024-05-04 12:21:23] nn step 12900, lr: 0.06.
	loss_policy_0: 0.06089
	accuracy_policy_0: 0.84535
	loss_value_0: 0.17264
	loss_policy_1: 0.015
	accuracy_policy_1: 0.82145
	loss_value_1: 0.03501
	loss_reward_1: 0.00627
	loss_policy_2: 0.0158
	accuracy_policy_2: 0.81785
	loss_value_2: 0.03561
	loss_reward_2: 0.0056
	loss_policy_3: 0.01666
	accuracy_policy_3: 0.80883
	loss_value_3: 0.03614
	loss_reward_3: 0.00561
	loss_policy_4: 0.01751
	accuracy_policy_4: 0.79887
	loss_value_4: 0.03673
	loss_reward_4: 0.00611
	loss_policy_5: 0.01874
	accuracy_policy_5: 0.78941
	loss_value_5: 0.03717
	loss_reward_5: 0.00667
	loss_policy: 0.14461
	loss_value: 0.3533
	loss_reward: 0.03025
[2024-05-04 12:21:39] nn step 12950, lr: 0.06.
	loss_policy_0: 0.05792
	accuracy_policy_0: 0.84918
	loss_value_0: 0.16756
	loss_policy_1: 0.0145
	accuracy_policy_1: 0.81969
	loss_value_1: 0.03406
	loss_reward_1: 0.00604
	loss_policy_2: 0.01498
	accuracy_policy_2: 0.81414
	loss_value_2: 0.03455
	loss_reward_2: 0.0054
	loss_policy_3: 0.01643
	accuracy_policy_3: 0.80055
	loss_value_3: 0.03515
	loss_reward_3: 0.00559
	loss_policy_4: 0.0171
	accuracy_policy_4: 0.79781
	loss_value_4: 0.03574
	loss_reward_4: 0.0061
	loss_policy_5: 0.01791
	accuracy_policy_5: 0.7916
	loss_value_5: 0.03625
	loss_reward_5: 0.00628
	loss_policy: 0.13884
	loss_value: 0.34331
	loss_reward: 0.02941
[2024-05-04 12:21:56] nn step 13000, lr: 0.06.
	loss_policy_0: 0.05413
	accuracy_policy_0: 0.8566
	loss_value_0: 0.16657
	loss_policy_1: 0.01393
	accuracy_policy_1: 0.82633
	loss_value_1: 0.03378
	loss_reward_1: 0.00595
	loss_policy_2: 0.01476
	accuracy_policy_2: 0.81703
	loss_value_2: 0.03423
	loss_reward_2: 0.00547
	loss_policy_3: 0.01579
	accuracy_policy_3: 0.8075
	loss_value_3: 0.0348
	loss_reward_3: 0.00548
	loss_policy_4: 0.01654
	accuracy_policy_4: 0.80078
	loss_value_4: 0.03535
	loss_reward_4: 0.00591
	loss_policy_5: 0.01725
	accuracy_policy_5: 0.80043
	loss_value_5: 0.03579
	loss_reward_5: 0.00632
	loss_policy: 0.13238
	loss_value: 0.34053
	loss_reward: 0.02913
Optimization_Done 13000
[2024-05-04 12:24:02] [command] train weight_iter_13000.pkl 62 66
[2024-05-04 12:24:20] nn step 13050, lr: 0.06.
	loss_policy_0: 0.06171
	accuracy_policy_0: 0.85305
	loss_value_0: 0.17882
	loss_policy_1: 0.01541
	accuracy_policy_1: 0.82719
	loss_value_1: 0.03621
	loss_reward_1: 0.00645
	loss_policy_2: 0.01623
	accuracy_policy_2: 0.82168
	loss_value_2: 0.0369
	loss_reward_2: 0.00573
	loss_policy_3: 0.01701
	accuracy_policy_3: 0.81262
	loss_value_3: 0.03746
	loss_reward_3: 0.00584
	loss_policy_4: 0.01802
	accuracy_policy_4: 0.80766
	loss_value_4: 0.03802
	loss_reward_4: 0.00636
	loss_policy_5: 0.01879
	accuracy_policy_5: 0.80266
	loss_value_5: 0.03852
	loss_reward_5: 0.00683
	loss_policy: 0.14718
	loss_value: 0.36593
	loss_reward: 0.03121
[2024-05-04 12:24:36] nn step 13100, lr: 0.06.
	loss_policy_0: 0.0609
	accuracy_policy_0: 0.86176
	loss_value_0: 0.1856
	loss_policy_1: 0.01526
	accuracy_policy_1: 0.83297
	loss_value_1: 0.03754
	loss_reward_1: 0.00677
	loss_policy_2: 0.0163
	accuracy_policy_2: 0.82652
	loss_value_2: 0.03821
	loss_reward_2: 0.00626
	loss_policy_3: 0.01728
	accuracy_policy_3: 0.8202
	loss_value_3: 0.03882
	loss_reward_3: 0.00603
	loss_policy_4: 0.01811
	accuracy_policy_4: 0.81344
	loss_value_4: 0.0395
	loss_reward_4: 0.00659
	loss_policy_5: 0.01898
	accuracy_policy_5: 0.81133
	loss_value_5: 0.04001
	loss_reward_5: 0.0072
	loss_policy: 0.14685
	loss_value: 0.37968
	loss_reward: 0.03285
[2024-05-04 12:24:52] nn step 13150, lr: 0.06.
	loss_policy_0: 0.05918
	accuracy_policy_0: 0.85281
	loss_value_0: 0.18093
	loss_policy_1: 0.01475
	accuracy_policy_1: 0.83367
	loss_value_1: 0.03659
	loss_reward_1: 0.00663
	loss_policy_2: 0.01546
	accuracy_policy_2: 0.82945
	loss_value_2: 0.03726
	loss_reward_2: 0.00593
	loss_policy_3: 0.01653
	accuracy_policy_3: 0.81945
	loss_value_3: 0.03775
	loss_reward_3: 0.00604
	loss_policy_4: 0.01721
	accuracy_policy_4: 0.81535
	loss_value_4: 0.03828
	loss_reward_4: 0.00642
	loss_policy_5: 0.01847
	accuracy_policy_5: 0.80918
	loss_value_5: 0.03879
	loss_reward_5: 0.00691
	loss_policy: 0.14159
	loss_value: 0.36959
	loss_reward: 0.03193
[2024-05-04 12:25:08] nn step 13200, lr: 0.06.
	loss_policy_0: 0.05663
	accuracy_policy_0: 0.86344
	loss_value_0: 0.18081
	loss_policy_1: 0.01454
	accuracy_policy_1: 0.84156
	loss_value_1: 0.03663
	loss_reward_1: 0.00645
	loss_policy_2: 0.01558
	accuracy_policy_2: 0.82781
	loss_value_2: 0.03724
	loss_reward_2: 0.00611
	loss_policy_3: 0.01646
	accuracy_policy_3: 0.82281
	loss_value_3: 0.03775
	loss_reward_3: 0.00583
	loss_policy_4: 0.01709
	accuracy_policy_4: 0.81707
	loss_value_4: 0.03851
	loss_reward_4: 0.00631
	loss_policy_5: 0.01815
	accuracy_policy_5: 0.81242
	loss_value_5: 0.03908
	loss_reward_5: 0.00712
	loss_policy: 0.13845
	loss_value: 0.37002
	loss_reward: 0.03182
Optimization_Done 13200
[2024-05-04 12:27:06] [command] train weight_iter_13200.pkl 63 67
[2024-05-04 12:27:24] nn step 13250, lr: 0.06.
	loss_policy_0: 0.07346
	accuracy_policy_0: 0.83305
	loss_value_0: 0.17258
	loss_policy_1: 0.01781
	accuracy_policy_1: 0.81133
	loss_value_1: 0.03484
	loss_reward_1: 0.00577
	loss_policy_2: 0.01835
	accuracy_policy_2: 0.8077
	loss_value_2: 0.03523
	loss_reward_2: 0.00538
	loss_policy_3: 0.01965
	accuracy_policy_3: 0.80016
	loss_value_3: 0.03569
	loss_reward_3: 0.0054
	loss_policy_4: 0.02046
	accuracy_policy_4: 0.79461
	loss_value_4: 0.03609
	loss_reward_4: 0.00597
	loss_policy_5: 0.02151
	accuracy_policy_5: 0.78781
	loss_value_5: 0.03644
	loss_reward_5: 0.00639
	loss_policy: 0.17124
	loss_value: 0.35086
	loss_reward: 0.02891
[2024-05-04 12:27:40] nn step 13300, lr: 0.06.
	loss_policy_0: 0.06815
	accuracy_policy_0: 0.84422
	loss_value_0: 0.17655
	loss_policy_1: 0.01678
	accuracy_policy_1: 0.82078
	loss_value_1: 0.03562
	loss_reward_1: 0.00597
	loss_policy_2: 0.01769
	accuracy_policy_2: 0.81555
	loss_value_2: 0.03605
	loss_reward_2: 0.00555
	loss_policy_3: 0.01892
	accuracy_policy_3: 0.80586
	loss_value_3: 0.03639
	loss_reward_3: 0.00566
	loss_policy_4: 0.01989
	accuracy_policy_4: 0.79715
	loss_value_4: 0.03676
	loss_reward_4: 0.00597
	loss_policy_5: 0.02072
	accuracy_policy_5: 0.78949
	loss_value_5: 0.03713
	loss_reward_5: 0.0064
	loss_policy: 0.16215
	loss_value: 0.3585
	loss_reward: 0.02954
[2024-05-04 12:27:56] nn step 13350, lr: 0.06.
	loss_policy_0: 0.06238
	accuracy_policy_0: 0.84719
	loss_value_0: 0.1662
	loss_policy_1: 0.01553
	accuracy_policy_1: 0.82312
	loss_value_1: 0.03352
	loss_reward_1: 0.00578
	loss_policy_2: 0.01658
	accuracy_policy_2: 0.81211
	loss_value_2: 0.03385
	loss_reward_2: 0.00532
	loss_policy_3: 0.01733
	accuracy_policy_3: 0.80852
	loss_value_3: 0.03427
	loss_reward_3: 0.00535
	loss_policy_4: 0.01828
	accuracy_policy_4: 0.80484
	loss_value_4: 0.03459
	loss_reward_4: 0.00565
	loss_policy_5: 0.01927
	accuracy_policy_5: 0.79879
	loss_value_5: 0.03501
	loss_reward_5: 0.0061
	loss_policy: 0.14936
	loss_value: 0.33744
	loss_reward: 0.02821
[2024-05-04 12:28:13] nn step 13400, lr: 0.06.
	loss_policy_0: 0.06029
	accuracy_policy_0: 0.85234
	loss_value_0: 0.17078
	loss_policy_1: 0.0156
	accuracy_policy_1: 0.82543
	loss_value_1: 0.03458
	loss_reward_1: 0.0058
	loss_policy_2: 0.01618
	accuracy_policy_2: 0.82152
	loss_value_2: 0.03491
	loss_reward_2: 0.00553
	loss_policy_3: 0.017
	accuracy_policy_3: 0.81797
	loss_value_3: 0.03536
	loss_reward_3: 0.00537
	loss_policy_4: 0.01833
	accuracy_policy_4: 0.80883
	loss_value_4: 0.03575
	loss_reward_4: 0.00586
	loss_policy_5: 0.01911
	accuracy_policy_5: 0.80398
	loss_value_5: 0.0362
	loss_reward_5: 0.00618
	loss_policy: 0.14652
	loss_value: 0.34758
	loss_reward: 0.02873
Optimization_Done 13400
[2024-05-04 12:30:19] [command] train weight_iter_13400.pkl 64 68
[2024-05-04 12:30:37] nn step 13450, lr: 0.06.
	loss_policy_0: 0.07162
	accuracy_policy_0: 0.82906
	loss_value_0: 0.17489
	loss_policy_1: 0.01755
	accuracy_policy_1: 0.80453
	loss_value_1: 0.03527
	loss_reward_1: 0.00568
	loss_policy_2: 0.0186
	accuracy_policy_2: 0.79793
	loss_value_2: 0.03556
	loss_reward_2: 0.00527
	loss_policy_3: 0.01998
	accuracy_policy_3: 0.78836
	loss_value_3: 0.03595
	loss_reward_3: 0.00513
	loss_policy_4: 0.02163
	accuracy_policy_4: 0.77832
	loss_value_4: 0.03637
	loss_reward_4: 0.00573
	loss_policy_5: 0.0223
	accuracy_policy_5: 0.77875
	loss_value_5: 0.03666
	loss_reward_5: 0.00611
	loss_policy: 0.17167
	loss_value: 0.3547
	loss_reward: 0.02792
[2024-05-04 12:30:53] nn step 13500, lr: 0.06.
	loss_policy_0: 0.06608
	accuracy_policy_0: 0.84312
	loss_value_0: 0.17706
	loss_policy_1: 0.0168
	accuracy_policy_1: 0.81645
	loss_value_1: 0.0357
	loss_reward_1: 0.00586
	loss_policy_2: 0.01801
	accuracy_policy_2: 0.80508
	loss_value_2: 0.036
	loss_reward_2: 0.00542
	loss_policy_3: 0.01922
	accuracy_policy_3: 0.79996
	loss_value_3: 0.03639
	loss_reward_3: 0.00521
	loss_policy_4: 0.02032
	accuracy_policy_4: 0.79312
	loss_value_4: 0.03679
	loss_reward_4: 0.00572
	loss_policy_5: 0.02119
	accuracy_policy_5: 0.78855
	loss_value_5: 0.03717
	loss_reward_5: 0.00629
	loss_policy: 0.16161
	loss_value: 0.35911
	loss_reward: 0.02849
[2024-05-04 12:31:09] nn step 13550, lr: 0.06.
	loss_policy_0: 0.06154
	accuracy_policy_0: 0.85047
	loss_value_0: 0.17715
	loss_policy_1: 0.01602
	accuracy_policy_1: 0.8225
	loss_value_1: 0.03576
	loss_reward_1: 0.00594
	loss_policy_2: 0.01731
	accuracy_policy_2: 0.81086
	loss_value_2: 0.03612
	loss_reward_2: 0.00532
	loss_policy_3: 0.01851
	accuracy_policy_3: 0.80465
	loss_value_3: 0.03649
	loss_reward_3: 0.00528
	loss_policy_4: 0.01956
	accuracy_policy_4: 0.79742
	loss_value_4: 0.03686
	loss_reward_4: 0.00571
	loss_policy_5: 0.02111
	accuracy_policy_5: 0.78781
	loss_value_5: 0.03722
	loss_reward_5: 0.00614
	loss_policy: 0.15405
	loss_value: 0.3596
	loss_reward: 0.02839
[2024-05-04 12:31:25] nn step 13600, lr: 0.06.
	loss_policy_0: 0.05801
	accuracy_policy_0: 0.84797
	loss_value_0: 0.16575
	loss_policy_1: 0.01496
	accuracy_policy_1: 0.82223
	loss_value_1: 0.03339
	loss_reward_1: 0.00549
	loss_policy_2: 0.01602
	accuracy_policy_2: 0.81477
	loss_value_2: 0.03369
	loss_reward_2: 0.00521
	loss_policy_3: 0.01726
	accuracy_policy_3: 0.80449
	loss_value_3: 0.03403
	loss_reward_3: 0.00503
	loss_policy_4: 0.01819
	accuracy_policy_4: 0.7991
	loss_value_4: 0.03428
	loss_reward_4: 0.0055
	loss_policy_5: 0.01914
	accuracy_policy_5: 0.79387
	loss_value_5: 0.03467
	loss_reward_5: 0.00568
	loss_policy: 0.14358
	loss_value: 0.33581
	loss_reward: 0.02691
Optimization_Done 13600
[2024-05-04 12:33:01] [command] train weight_iter_13600.pkl 65 69
[2024-05-04 12:33:19] nn step 13650, lr: 0.06.
	loss_policy_0: 0.06431
	accuracy_policy_0: 0.82316
	loss_value_0: 0.15771
	loss_policy_1: 0.01639
	accuracy_policy_1: 0.79449
	loss_value_1: 0.03177
	loss_reward_1: 0.00504
	loss_policy_2: 0.01747
	accuracy_policy_2: 0.78344
	loss_value_2: 0.0319
	loss_reward_2: 0.00455
	loss_policy_3: 0.01864
	accuracy_policy_3: 0.77238
	loss_value_3: 0.03209
	loss_reward_3: 0.00459
	loss_policy_4: 0.01971
	accuracy_policy_4: 0.76469
	loss_value_4: 0.03239
	loss_reward_4: 0.0051
	loss_policy_5: 0.021
	accuracy_policy_5: 0.7566
	loss_value_5: 0.03255
	loss_reward_5: 0.00536
	loss_policy: 0.15753
	loss_value: 0.31842
	loss_reward: 0.02463
[2024-05-04 12:33:35] nn step 13700, lr: 0.06.
	loss_policy_0: 0.06088
	accuracy_policy_0: 0.83473
	loss_value_0: 0.15911
	loss_policy_1: 0.01584
	accuracy_policy_1: 0.8016
	loss_value_1: 0.03196
	loss_reward_1: 0.00512
	loss_policy_2: 0.01689
	accuracy_policy_2: 0.79348
	loss_value_2: 0.03231
	loss_reward_2: 0.00486
	loss_policy_3: 0.01834
	accuracy_policy_3: 0.7848
	loss_value_3: 0.03256
	loss_reward_3: 0.00488
	loss_policy_4: 0.01963
	accuracy_policy_4: 0.77621
	loss_value_4: 0.03273
	loss_reward_4: 0.00513
	loss_policy_5: 0.02043
	accuracy_policy_5: 0.77188
	loss_value_5: 0.03307
	loss_reward_5: 0.00545
	loss_policy: 0.152
	loss_value: 0.32174
	loss_reward: 0.02544
[2024-05-04 12:33:52] nn step 13750, lr: 0.06.
	loss_policy_0: 0.05943
	accuracy_policy_0: 0.83305
	loss_value_0: 0.16552
	loss_policy_1: 0.01573
	accuracy_policy_1: 0.80312
	loss_value_1: 0.03321
	loss_reward_1: 0.00516
	loss_policy_2: 0.01709
	accuracy_policy_2: 0.79633
	loss_value_2: 0.03339
	loss_reward_2: 0.00492
	loss_policy_3: 0.01836
	accuracy_policy_3: 0.78914
	loss_value_3: 0.0336
	loss_reward_3: 0.00493
	loss_policy_4: 0.01966
	accuracy_policy_4: 0.77836
	loss_value_4: 0.03375
	loss_reward_4: 0.00518
	loss_policy_5: 0.02064
	accuracy_policy_5: 0.7725
	loss_value_5: 0.03409
	loss_reward_5: 0.00569
	loss_policy: 0.1509
	loss_value: 0.33357
	loss_reward: 0.02588
[2024-05-04 12:34:08] nn step 13800, lr: 0.06.
	loss_policy_0: 0.05727
	accuracy_policy_0: 0.84285
	loss_value_0: 0.16381
	loss_policy_1: 0.01536
	accuracy_policy_1: 0.81336
	loss_value_1: 0.03287
	loss_reward_1: 0.00508
	loss_policy_2: 0.0167
	accuracy_policy_2: 0.80227
	loss_value_2: 0.03302
	loss_reward_2: 0.00504
	loss_policy_3: 0.01787
	accuracy_policy_3: 0.78898
	loss_value_3: 0.03321
	loss_reward_3: 0.00495
	loss_policy_4: 0.01918
	accuracy_policy_4: 0.77703
	loss_value_4: 0.03346
	loss_reward_4: 0.00506
	loss_policy_5: 0.02046
	accuracy_policy_5: 0.77082
	loss_value_5: 0.03385
	loss_reward_5: 0.00553
	loss_policy: 0.14684
	loss_value: 0.33023
	loss_reward: 0.02565
Optimization_Done 13800
[2024-05-04 12:36:15] [command] train weight_iter_13800.pkl 66 70
[2024-05-04 12:36:32] nn step 13850, lr: 0.06.
	loss_policy_0: 0.07769
	accuracy_policy_0: 0.80758
	loss_value_0: 0.16913
	loss_policy_1: 0.01968
	accuracy_policy_1: 0.7818
	loss_value_1: 0.03399
	loss_reward_1: 0.00562
	loss_policy_2: 0.02128
	accuracy_policy_2: 0.77219
	loss_value_2: 0.03428
	loss_reward_2: 0.00516
	loss_policy_3: 0.02305
	accuracy_policy_3: 0.76141
	loss_value_3: 0.03472
	loss_reward_3: 0.00536
	loss_policy_4: 0.02408
	accuracy_policy_4: 0.75684
	loss_value_4: 0.03513
	loss_reward_4: 0.0056
	loss_policy_5: 0.02536
	accuracy_policy_5: 0.74965
	loss_value_5: 0.03536
	loss_reward_5: 0.00611
	loss_policy: 0.19113
	loss_value: 0.34261
	loss_reward: 0.02785
[2024-05-04 12:36:49] nn step 13900, lr: 0.06.
	loss_policy_0: 0.07098
	accuracy_policy_0: 0.82145
	loss_value_0: 0.16813
	loss_policy_1: 0.01869
	accuracy_policy_1: 0.79059
	loss_value_1: 0.03405
	loss_reward_1: 0.00566
	loss_policy_2: 0.02029
	accuracy_policy_2: 0.78152
	loss_value_2: 0.03438
	loss_reward_2: 0.00531
	loss_policy_3: 0.02189
	accuracy_policy_3: 0.77078
	loss_value_3: 0.03466
	loss_reward_3: 0.00507
	loss_policy_4: 0.0233
	accuracy_policy_4: 0.75938
	loss_value_4: 0.03505
	loss_reward_4: 0.00561
	loss_policy_5: 0.02436
	accuracy_policy_5: 0.75953
	loss_value_5: 0.03555
	loss_reward_5: 0.00612
	loss_policy: 0.17951
	loss_value: 0.34182
	loss_reward: 0.02777
[2024-05-04 12:37:05] nn step 13950, lr: 0.06.
	loss_policy_0: 0.07005
	accuracy_policy_0: 0.82691
	loss_value_0: 0.1716
	loss_policy_1: 0.01818
	accuracy_policy_1: 0.79875
	loss_value_1: 0.03453
	loss_reward_1: 0.0058
	loss_policy_2: 0.02032
	accuracy_policy_2: 0.78395
	loss_value_2: 0.03486
	loss_reward_2: 0.00559
	loss_policy_3: 0.02213
	accuracy_policy_3: 0.775
	loss_value_3: 0.03528
	loss_reward_3: 0.00544
	loss_policy_4: 0.02337
	accuracy_policy_4: 0.76652
	loss_value_4: 0.03571
	loss_reward_4: 0.00582
	loss_policy_5: 0.02455
	accuracy_policy_5: 0.76016
	loss_value_5: 0.03615
	loss_reward_5: 0.0062
	loss_policy: 0.1786
	loss_value: 0.34813
	loss_reward: 0.02885
[2024-05-04 12:37:22] nn step 14000, lr: 0.06.
	loss_policy_0: 0.06619
	accuracy_policy_0: 0.82766
	loss_value_0: 0.16516
	loss_policy_1: 0.01759
	accuracy_policy_1: 0.79926
	loss_value_1: 0.03327
	loss_reward_1: 0.00558
	loss_policy_2: 0.01895
	accuracy_policy_2: 0.79113
	loss_value_2: 0.03356
	loss_reward_2: 0.00518
	loss_policy_3: 0.02064
	accuracy_policy_3: 0.78059
	loss_value_3: 0.034
	loss_reward_3: 0.00529
	loss_policy_4: 0.02214
	accuracy_policy_4: 0.76895
	loss_value_4: 0.03442
	loss_reward_4: 0.00559
	loss_policy_5: 0.02265
	accuracy_policy_5: 0.76531
	loss_value_5: 0.03493
	loss_reward_5: 0.00604
	loss_policy: 0.16816
	loss_value: 0.33534
	loss_reward: 0.02768
Optimization_Done 14000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 12:43:34] [command] train weight_iter_14000.pkl 67 71
[2024-05-04 12:44:06] nn step 14050, lr: 0.05.
	loss_policy_0: 0.0786
	accuracy_policy_0: 0.79586
	loss_value_0: 0.17179
	loss_policy_1: 0.02115
	accuracy_policy_1: 0.75121
	loss_value_1: 0.03439
	loss_reward_1: 0.00518
	loss_policy_2: 0.02322
	accuracy_policy_2: 0.73539
	loss_value_2: 0.03438
	loss_reward_2: 0.00482
	loss_policy_3: 0.02532
	accuracy_policy_3: 0.72055
	loss_value_3: 0.03467
	loss_reward_3: 0.00481
	loss_policy_4: 0.02761
	accuracy_policy_4: 0.71102
	loss_value_4: 0.0349
	loss_reward_4: 0.0051
	loss_policy_5: 0.02904
	accuracy_policy_5: 0.7052
	loss_value_5: 0.03498
	loss_reward_5: 0.00562
	loss_policy: 0.20494
	loss_value: 0.3451
	loss_reward: 0.02553
[2024-05-04 12:44:22] nn step 14100, lr: 0.05.
	loss_policy_0: 0.07136
	accuracy_policy_0: 0.80676
	loss_value_0: 0.16546
	loss_policy_1: 0.01972
	accuracy_policy_1: 0.7657
	loss_value_1: 0.0333
	loss_reward_1: 0.0051
	loss_policy_2: 0.02227
	accuracy_policy_2: 0.74594
	loss_value_2: 0.03362
	loss_reward_2: 0.00489
	loss_policy_3: 0.02436
	accuracy_policy_3: 0.73207
	loss_value_3: 0.03403
	loss_reward_3: 0.00492
	loss_policy_4: 0.0256
	accuracy_policy_4: 0.72688
	loss_value_4: 0.03416
	loss_reward_4: 0.00514
	loss_policy_5: 0.02724
	accuracy_policy_5: 0.71812
	loss_value_5: 0.03426
	loss_reward_5: 0.00547
	loss_policy: 0.19055
	loss_value: 0.33483
	loss_reward: 0.02552
[2024-05-04 12:44:38] nn step 14150, lr: 0.05.
	loss_policy_0: 0.06282
	accuracy_policy_0: 0.81016
	loss_value_0: 0.15034
	loss_policy_1: 0.01775
	accuracy_policy_1: 0.76898
	loss_value_1: 0.03029
	loss_reward_1: 0.00441
	loss_policy_2: 0.01946
	accuracy_policy_2: 0.75324
	loss_value_2: 0.0305
	loss_reward_2: 0.00435
	loss_policy_3: 0.02114
	accuracy_policy_3: 0.74441
	loss_value_3: 0.03073
	loss_reward_3: 0.00441
	loss_policy_4: 0.02306
	accuracy_policy_4: 0.72949
	loss_value_4: 0.03096
	loss_reward_4: 0.00461
	loss_policy_5: 0.02473
	accuracy_policy_5: 0.72082
	loss_value_5: 0.03111
	loss_reward_5: 0.005
	loss_policy: 0.16898
	loss_value: 0.30393
	loss_reward: 0.02278
[2024-05-04 12:44:54] nn step 14200, lr: 0.05.
	loss_policy_0: 0.06506
	accuracy_policy_0: 0.81344
	loss_value_0: 0.15951
	loss_policy_1: 0.01851
	accuracy_policy_1: 0.7677
	loss_value_1: 0.03204
	loss_reward_1: 0.00478
	loss_policy_2: 0.02031
	accuracy_policy_2: 0.75812
	loss_value_2: 0.03227
	loss_reward_2: 0.00454
	loss_policy_3: 0.0225
	accuracy_policy_3: 0.74574
	loss_value_3: 0.03261
	loss_reward_3: 0.00471
	loss_policy_4: 0.02367
	accuracy_policy_4: 0.73227
	loss_value_4: 0.03279
	loss_reward_4: 0.00486
	loss_policy_5: 0.02541
	accuracy_policy_5: 0.72227
	loss_value_5: 0.03308
	loss_reward_5: 0.00541
	loss_policy: 0.17546
	loss_value: 0.32229
	loss_reward: 0.0243
Optimization_Done 14200
[2024-05-04 12:47:02] [command] train weight_iter_14200.pkl 68 72
[2024-05-04 12:47:19] nn step 14250, lr: 0.05.
	loss_policy_0: 0.08979
	accuracy_policy_0: 0.76797
	loss_value_0: 0.16742
	loss_policy_1: 0.02291
	accuracy_policy_1: 0.73258
	loss_value_1: 0.03375
	loss_reward_1: 0.0051
	loss_policy_2: 0.0247
	accuracy_policy_2: 0.72355
	loss_value_2: 0.03396
	loss_reward_2: 0.00486
	loss_policy_3: 0.02679
	accuracy_policy_3: 0.71199
	loss_value_3: 0.03421
	loss_reward_3: 0.00476
	loss_policy_4: 0.02856
	accuracy_policy_4: 0.69418
	loss_value_4: 0.03449
	loss_reward_4: 0.00521
	loss_policy_5: 0.02984
	accuracy_policy_5: 0.69105
	loss_value_5: 0.03463
	loss_reward_5: 0.00552
	loss_policy: 0.2226
	loss_value: 0.33846
	loss_reward: 0.02545
[2024-05-04 12:47:35] nn step 14300, lr: 0.05.
	loss_policy_0: 0.07599
	accuracy_policy_0: 0.78781
	loss_value_0: 0.15865
	loss_policy_1: 0.01997
	accuracy_policy_1: 0.75164
	loss_value_1: 0.03186
	loss_reward_1: 0.00461
	loss_policy_2: 0.02242
	accuracy_policy_2: 0.7366
	loss_value_2: 0.03217
	loss_reward_2: 0.0044
	loss_policy_3: 0.02431
	accuracy_policy_3: 0.71922
	loss_value_3: 0.03239
	loss_reward_3: 0.00456
	loss_policy_4: 0.02606
	accuracy_policy_4: 0.7057
	loss_value_4: 0.03249
	loss_reward_4: 0.00486
	loss_policy_5: 0.02746
	accuracy_policy_5: 0.70289
	loss_value_5: 0.03261
	loss_reward_5: 0.00509
	loss_policy: 0.19622
	loss_value: 0.32017
	loss_reward: 0.02352
[2024-05-04 12:47:51] nn step 14350, lr: 0.05.
	loss_policy_0: 0.07973
	accuracy_policy_0: 0.7966
	loss_value_0: 0.17446
	loss_policy_1: 0.02141
	accuracy_policy_1: 0.75477
	loss_value_1: 0.03502
	loss_reward_1: 0.00508
	loss_policy_2: 0.02351
	accuracy_policy_2: 0.74215
	loss_value_2: 0.03526
	loss_reward_2: 0.0048
	loss_policy_3: 0.02586
	accuracy_policy_3: 0.72582
	loss_value_3: 0.03548
	loss_reward_3: 0.00498
	loss_policy_4: 0.02767
	accuracy_policy_4: 0.71535
	loss_value_4: 0.03569
	loss_reward_4: 0.00525
	loss_policy_5: 0.02973
	accuracy_policy_5: 0.70219
	loss_value_5: 0.03598
	loss_reward_5: 0.00567
	loss_policy: 0.2079
	loss_value: 0.35189
	loss_reward: 0.02577
[2024-05-04 12:48:08] nn step 14400, lr: 0.05.
	loss_policy_0: 0.07043
	accuracy_policy_0: 0.80105
	loss_value_0: 0.15836
	loss_policy_1: 0.01914
	accuracy_policy_1: 0.76219
	loss_value_1: 0.03176
	loss_reward_1: 0.00458
	loss_policy_2: 0.02099
	accuracy_policy_2: 0.74617
	loss_value_2: 0.03194
	loss_reward_2: 0.00433
	loss_policy_3: 0.02324
	accuracy_policy_3: 0.72672
	loss_value_3: 0.03219
	loss_reward_3: 0.00423
	loss_policy_4: 0.02498
	accuracy_policy_4: 0.72422
	loss_value_4: 0.03239
	loss_reward_4: 0.00477
	loss_policy_5: 0.02623
	accuracy_policy_5: 0.70957
	loss_value_5: 0.03273
	loss_reward_5: 0.00517
	loss_policy: 0.18501
	loss_value: 0.31937
	loss_reward: 0.02307
Optimization_Done 14400
[2024-05-04 12:50:12] [command] train weight_iter_14400.pkl 69 73
[2024-05-04 12:50:29] nn step 14450, lr: 0.05.
	loss_policy_0: 0.08747
	accuracy_policy_0: 0.76473
	loss_value_0: 0.15498
	loss_policy_1: 0.02225
	accuracy_policy_1: 0.72414
	loss_value_1: 0.03113
	loss_reward_1: 0.00489
	loss_policy_2: 0.02451
	accuracy_policy_2: 0.71547
	loss_value_2: 0.03143
	loss_reward_2: 0.00465
	loss_policy_3: 0.02654
	accuracy_policy_3: 0.70312
	loss_value_3: 0.03166
	loss_reward_3: 0.0046
	loss_policy_4: 0.02836
	accuracy_policy_4: 0.68773
	loss_value_4: 0.03201
	loss_reward_4: 0.00485
	loss_policy_5: 0.02944
	accuracy_policy_5: 0.67883
	loss_value_5: 0.03224
	loss_reward_5: 0.00544
	loss_policy: 0.21857
	loss_value: 0.31344
	loss_reward: 0.02443
[2024-05-04 12:50:46] nn step 14500, lr: 0.05.
	loss_policy_0: 0.08306
	accuracy_policy_0: 0.77066
	loss_value_0: 0.15515
	loss_policy_1: 0.02167
	accuracy_policy_1: 0.73418
	loss_value_1: 0.03132
	loss_reward_1: 0.005
	loss_policy_2: 0.02394
	accuracy_policy_2: 0.71625
	loss_value_2: 0.03167
	loss_reward_2: 0.00465
	loss_policy_3: 0.02596
	accuracy_policy_3: 0.70801
	loss_value_3: 0.03195
	loss_reward_3: 0.00458
	loss_policy_4: 0.02765
	accuracy_policy_4: 0.69438
	loss_value_4: 0.03221
	loss_reward_4: 0.00506
	loss_policy_5: 0.02863
	accuracy_policy_5: 0.69258
	loss_value_5: 0.03247
	loss_reward_5: 0.00532
	loss_policy: 0.21089
	loss_value: 0.31477
	loss_reward: 0.0246
[2024-05-04 12:51:02] nn step 14550, lr: 0.05.
	loss_policy_0: 0.08144
	accuracy_policy_0: 0.78688
	loss_value_0: 0.16547
	loss_policy_1: 0.02219
	accuracy_policy_1: 0.74297
	loss_value_1: 0.03328
	loss_reward_1: 0.00529
	loss_policy_2: 0.02465
	accuracy_policy_2: 0.72191
	loss_value_2: 0.03355
	loss_reward_2: 0.00479
	loss_policy_3: 0.0268
	accuracy_policy_3: 0.71793
	loss_value_3: 0.03385
	loss_reward_3: 0.00485
	loss_policy_4: 0.02871
	accuracy_policy_4: 0.70176
	loss_value_4: 0.03404
	loss_reward_4: 0.00545
	loss_policy_5: 0.02993
	accuracy_policy_5: 0.69473
	loss_value_5: 0.03454
	loss_reward_5: 0.00582
	loss_policy: 0.21372
	loss_value: 0.33473
	loss_reward: 0.0262
[2024-05-04 12:51:18] nn step 14600, lr: 0.05.
	loss_policy_0: 0.07643
	accuracy_policy_0: 0.78535
	loss_value_0: 0.15585
	loss_policy_1: 0.02023
	accuracy_policy_1: 0.74398
	loss_value_1: 0.0314
	loss_reward_1: 0.00502
	loss_policy_2: 0.02281
	accuracy_policy_2: 0.72648
	loss_value_2: 0.03164
	loss_reward_2: 0.00475
	loss_policy_3: 0.02475
	accuracy_policy_3: 0.71254
	loss_value_3: 0.03204
	loss_reward_3: 0.00477
	loss_policy_4: 0.02633
	accuracy_policy_4: 0.70172
	loss_value_4: 0.03242
	loss_reward_4: 0.00502
	loss_policy_5: 0.02789
	accuracy_policy_5: 0.69523
	loss_value_5: 0.03272
	loss_reward_5: 0.00549
	loss_policy: 0.19845
	loss_value: 0.31608
	loss_reward: 0.02506
Optimization_Done 14600
[2024-05-04 12:53:22] [command] train weight_iter_14600.pkl 70 74
[2024-05-04 12:53:40] nn step 14650, lr: 0.05.
	loss_policy_0: 0.08717
	accuracy_policy_0: 0.76008
	loss_value_0: 0.14908
	loss_policy_1: 0.02302
	accuracy_policy_1: 0.71875
	loss_value_1: 0.0302
	loss_reward_1: 0.00502
	loss_policy_2: 0.02557
	accuracy_policy_2: 0.69973
	loss_value_2: 0.03071
	loss_reward_2: 0.00455
	loss_policy_3: 0.02743
	accuracy_policy_3: 0.68398
	loss_value_3: 0.03115
	loss_reward_3: 0.00476
	loss_policy_4: 0.02923
	accuracy_policy_4: 0.67578
	loss_value_4: 0.0316
	loss_reward_4: 0.00532
	loss_policy_5: 0.03069
	accuracy_policy_5: 0.66887
	loss_value_5: 0.03191
	loss_reward_5: 0.00562
	loss_policy: 0.22311
	loss_value: 0.30465
	loss_reward: 0.02527
[2024-05-04 12:53:58] nn step 14700, lr: 0.05.
	loss_policy_0: 0.08271
	accuracy_policy_0: 0.77848
	loss_value_0: 0.15378
	loss_policy_1: 0.02299
	accuracy_policy_1: 0.72551
	loss_value_1: 0.03124
	loss_reward_1: 0.00526
	loss_policy_2: 0.02583
	accuracy_policy_2: 0.70914
	loss_value_2: 0.03168
	loss_reward_2: 0.00495
	loss_policy_3: 0.02781
	accuracy_policy_3: 0.69539
	loss_value_3: 0.03209
	loss_reward_3: 0.00494
	loss_policy_4: 0.02943
	accuracy_policy_4: 0.68184
	loss_value_4: 0.03251
	loss_reward_4: 0.00518
	loss_policy_5: 0.03054
	accuracy_policy_5: 0.68383
	loss_value_5: 0.03283
	loss_reward_5: 0.00603
	loss_policy: 0.21931
	loss_value: 0.31413
	loss_reward: 0.02635
[2024-05-04 12:54:14] nn step 14750, lr: 0.05.
	loss_policy_0: 0.08784
	accuracy_policy_0: 0.77742
	loss_value_0: 0.16286
	loss_policy_1: 0.0238
	accuracy_policy_1: 0.72719
	loss_value_1: 0.03296
	loss_reward_1: 0.00548
	loss_policy_2: 0.0271
	accuracy_policy_2: 0.70652
	loss_value_2: 0.03332
	loss_reward_2: 0.00514
	loss_policy_3: 0.02905
	accuracy_policy_3: 0.69633
	loss_value_3: 0.0336
	loss_reward_3: 0.00529
	loss_policy_4: 0.03025
	accuracy_policy_4: 0.69043
	loss_value_4: 0.03403
	loss_reward_4: 0.00556
	loss_policy_5: 0.03174
	accuracy_policy_5: 0.68293
	loss_value_5: 0.03458
	loss_reward_5: 0.00623
	loss_policy: 0.22979
	loss_value: 0.33135
	loss_reward: 0.02771
[2024-05-04 12:54:31] nn step 14800, lr: 0.05.
	loss_policy_0: 0.07931
	accuracy_policy_0: 0.78586
	loss_value_0: 0.15744
	loss_policy_1: 0.02235
	accuracy_policy_1: 0.73121
	loss_value_1: 0.03164
	loss_reward_1: 0.00528
	loss_policy_2: 0.02504
	accuracy_policy_2: 0.71562
	loss_value_2: 0.03212
	loss_reward_2: 0.00493
	loss_policy_3: 0.02687
	accuracy_policy_3: 0.70262
	loss_value_3: 0.03263
	loss_reward_3: 0.00497
	loss_policy_4: 0.02845
	accuracy_policy_4: 0.69883
	loss_value_4: 0.03322
	loss_reward_4: 0.00537
	loss_policy_5: 0.0296
	accuracy_policy_5: 0.68766
	loss_value_5: 0.03369
	loss_reward_5: 0.00609
	loss_policy: 0.21163
	loss_value: 0.32074
	loss_reward: 0.02664
Optimization_Done 14800
[2024-05-04 12:56:27] [command] train weight_iter_14800.pkl 71 75
[2024-05-04 12:56:45] nn step 14850, lr: 0.05.
	loss_policy_0: 0.09895
	accuracy_policy_0: 0.74383
	loss_value_0: 0.16088
	loss_policy_1: 0.02679
	accuracy_policy_1: 0.69254
	loss_value_1: 0.03259
	loss_reward_1: 0.00526
	loss_policy_2: 0.0298
	accuracy_policy_2: 0.6659
	loss_value_2: 0.03296
	loss_reward_2: 0.00476
	loss_policy_3: 0.03169
	accuracy_policy_3: 0.6573
	loss_value_3: 0.03335
	loss_reward_3: 0.00481
	loss_policy_4: 0.03312
	accuracy_policy_4: 0.64934
	loss_value_4: 0.03377
	loss_reward_4: 0.00511
	loss_policy_5: 0.03436
	accuracy_policy_5: 0.64172
	loss_value_5: 0.03415
	loss_reward_5: 0.00598
	loss_policy: 0.25472
	loss_value: 0.3277
	loss_reward: 0.02592
[2024-05-04 12:57:01] nn step 14900, lr: 0.05.
	loss_policy_0: 0.0952
	accuracy_policy_0: 0.75918
	loss_value_0: 0.16777
	loss_policy_1: 0.02627
	accuracy_policy_1: 0.70223
	loss_value_1: 0.03406
	loss_reward_1: 0.00525
	loss_policy_2: 0.02989
	accuracy_policy_2: 0.68207
	loss_value_2: 0.03451
	loss_reward_2: 0.00494
	loss_policy_3: 0.03159
	accuracy_policy_3: 0.66855
	loss_value_3: 0.03488
	loss_reward_3: 0.00509
	loss_policy_4: 0.03368
	accuracy_policy_4: 0.65816
	loss_value_4: 0.03522
	loss_reward_4: 0.00552
	loss_policy_5: 0.03542
	accuracy_policy_5: 0.64797
	loss_value_5: 0.0357
	loss_reward_5: 0.00607
	loss_policy: 0.25205
	loss_value: 0.34215
	loss_reward: 0.02687
[2024-05-04 12:57:18] nn step 14950, lr: 0.05.
	loss_policy_0: 0.08539
	accuracy_policy_0: 0.77016
	loss_value_0: 0.15631
	loss_policy_1: 0.02391
	accuracy_policy_1: 0.71723
	loss_value_1: 0.03162
	loss_reward_1: 0.0048
	loss_policy_2: 0.02664
	accuracy_policy_2: 0.69219
	loss_value_2: 0.03193
	loss_reward_2: 0.00463
	loss_policy_3: 0.02894
	accuracy_policy_3: 0.67555
	loss_value_3: 0.03227
	loss_reward_3: 0.00465
	loss_policy_4: 0.03064
	accuracy_policy_4: 0.66594
	loss_value_4: 0.03256
	loss_reward_4: 0.00499
	loss_policy_5: 0.03228
	accuracy_policy_5: 0.65449
	loss_value_5: 0.03302
	loss_reward_5: 0.00564
	loss_policy: 0.2278
	loss_value: 0.31771
	loss_reward: 0.02472
[2024-05-04 12:57:34] nn step 15000, lr: 0.05.
	loss_policy_0: 0.08631
	accuracy_policy_0: 0.77637
	loss_value_0: 0.16586
	loss_policy_1: 0.02499
	accuracy_policy_1: 0.71688
	loss_value_1: 0.03358
	loss_reward_1: 0.00529
	loss_policy_2: 0.02799
	accuracy_policy_2: 0.68922
	loss_value_2: 0.03403
	loss_reward_2: 0.00494
	loss_policy_3: 0.03008
	accuracy_policy_3: 0.68
	loss_value_3: 0.03439
	loss_reward_3: 0.00499
	loss_policy_4: 0.03247
	accuracy_policy_4: 0.66465
	loss_value_4: 0.03484
	loss_reward_4: 0.00546
	loss_policy_5: 0.03381
	accuracy_policy_5: 0.65879
	loss_value_5: 0.03526
	loss_reward_5: 0.00593
	loss_policy: 0.23565
	loss_value: 0.33795
	loss_reward: 0.02661
Optimization_Done 15000
[2024-05-04 12:59:38] [command] train weight_iter_15000.pkl 72 76
[2024-05-04 12:59:55] nn step 15050, lr: 0.05.
	loss_policy_0: 0.11478
	accuracy_policy_0: 0.73887
	loss_value_0: 0.17223
	loss_policy_1: 0.03019
	accuracy_policy_1: 0.68184
	loss_value_1: 0.03496
	loss_reward_1: 0.00523
	loss_policy_2: 0.03317
	accuracy_policy_2: 0.66223
	loss_value_2: 0.03546
	loss_reward_2: 0.00501
	loss_policy_3: 0.03518
	accuracy_policy_3: 0.64992
	loss_value_3: 0.03581
	loss_reward_3: 0.0052
	loss_policy_4: 0.0378
	accuracy_policy_4: 0.63535
	loss_value_4: 0.03628
	loss_reward_4: 0.00562
	loss_policy_5: 0.03882
	accuracy_policy_5: 0.63344
	loss_value_5: 0.03683
	loss_reward_5: 0.00634
	loss_policy: 0.28994
	loss_value: 0.35158
	loss_reward: 0.0274
[2024-05-04 13:00:12] nn step 15100, lr: 0.05.
	loss_policy_0: 0.10582
	accuracy_policy_0: 0.75934
	loss_value_0: 0.17209
	loss_policy_1: 0.02825
	accuracy_policy_1: 0.69863
	loss_value_1: 0.03496
	loss_reward_1: 0.00535
	loss_policy_2: 0.03109
	accuracy_policy_2: 0.68
	loss_value_2: 0.03534
	loss_reward_2: 0.00501
	loss_policy_3: 0.03351
	accuracy_policy_3: 0.66586
	loss_value_3: 0.03579
	loss_reward_3: 0.00511
	loss_policy_4: 0.03554
	accuracy_policy_4: 0.65164
	loss_value_4: 0.03617
	loss_reward_4: 0.00548
	loss_policy_5: 0.03743
	accuracy_policy_5: 0.64281
	loss_value_5: 0.03673
	loss_reward_5: 0.00616
	loss_policy: 0.27164
	loss_value: 0.35107
	loss_reward: 0.02711
[2024-05-04 13:00:28] nn step 15150, lr: 0.05.
	loss_policy_0: 0.09638
	accuracy_policy_0: 0.75781
	loss_value_0: 0.16095
	loss_policy_1: 0.02639
	accuracy_policy_1: 0.70188
	loss_value_1: 0.03273
	loss_reward_1: 0.00502
	loss_policy_2: 0.02913
	accuracy_policy_2: 0.67957
	loss_value_2: 0.03319
	loss_reward_2: 0.00484
	loss_policy_3: 0.03162
	accuracy_policy_3: 0.66703
	loss_value_3: 0.03355
	loss_reward_3: 0.00481
	loss_policy_4: 0.03326
	accuracy_policy_4: 0.65121
	loss_value_4: 0.03394
	loss_reward_4: 0.00521
	loss_policy_5: 0.03491
	accuracy_policy_5: 0.6409
	loss_value_5: 0.03435
	loss_reward_5: 0.00573
	loss_policy: 0.25169
	loss_value: 0.32871
	loss_reward: 0.02562
[2024-05-04 13:00:44] nn step 15200, lr: 0.05.
	loss_policy_0: 0.09926
	accuracy_policy_0: 0.76449
	loss_value_0: 0.16988
	loss_policy_1: 0.02718
	accuracy_policy_1: 0.70766
	loss_value_1: 0.0345
	loss_reward_1: 0.00539
	loss_policy_2: 0.03007
	accuracy_policy_2: 0.6893
	loss_value_2: 0.03497
	loss_reward_2: 0.00495
	loss_policy_3: 0.03247
	accuracy_policy_3: 0.67207
	loss_value_3: 0.03547
	loss_reward_3: 0.00499
	loss_policy_4: 0.0346
	accuracy_policy_4: 0.6575
	loss_value_4: 0.03608
	loss_reward_4: 0.00541
	loss_policy_5: 0.03596
	accuracy_policy_5: 0.65441
	loss_value_5: 0.03657
	loss_reward_5: 0.00622
	loss_policy: 0.25954
	loss_value: 0.34749
	loss_reward: 0.02696
Optimization_Done 15200
[2024-05-04 13:02:48] [command] train weight_iter_15200.pkl 73 77
[2024-05-04 13:03:05] nn step 15250, lr: 0.05.
	loss_policy_0: 0.12349
	accuracy_policy_0: 0.72043
	loss_value_0: 0.17092
	loss_policy_1: 0.0319
	accuracy_policy_1: 0.66492
	loss_value_1: 0.03474
	loss_reward_1: 0.00554
	loss_policy_2: 0.03455
	accuracy_policy_2: 0.65008
	loss_value_2: 0.03537
	loss_reward_2: 0.005
	loss_policy_3: 0.03663
	accuracy_policy_3: 0.63398
	loss_value_3: 0.0359
	loss_reward_3: 0.0051
	loss_policy_4: 0.03798
	accuracy_policy_4: 0.62543
	loss_value_4: 0.03648
	loss_reward_4: 0.00553
	loss_policy_5: 0.0401
	accuracy_policy_5: 0.61219
	loss_value_5: 0.03694
	loss_reward_5: 0.00641
	loss_policy: 0.30465
	loss_value: 0.35035
	loss_reward: 0.02757
[2024-05-04 13:03:22] nn step 15300, lr: 0.05.
	loss_policy_0: 0.10835
	accuracy_policy_0: 0.74062
	loss_value_0: 0.16322
	loss_policy_1: 0.02859
	accuracy_policy_1: 0.68172
	loss_value_1: 0.03324
	loss_reward_1: 0.00526
	loss_policy_2: 0.03143
	accuracy_policy_2: 0.66078
	loss_value_2: 0.03387
	loss_reward_2: 0.00491
	loss_policy_3: 0.03357
	accuracy_policy_3: 0.6493
	loss_value_3: 0.03442
	loss_reward_3: 0.00487
	loss_policy_4: 0.03545
	accuracy_policy_4: 0.63676
	loss_value_4: 0.03496
	loss_reward_4: 0.00558
	loss_policy_5: 0.03746
	accuracy_policy_5: 0.62
	loss_value_5: 0.03543
	loss_reward_5: 0.00602
	loss_policy: 0.27486
	loss_value: 0.33513
	loss_reward: 0.02664
[2024-05-04 13:03:38] nn step 15350, lr: 0.05.
	loss_policy_0: 0.10694
	accuracy_policy_0: 0.75469
	loss_value_0: 0.17
	loss_policy_1: 0.02932
	accuracy_policy_1: 0.68668
	loss_value_1: 0.03462
	loss_reward_1: 0.00539
	loss_policy_2: 0.03196
	accuracy_policy_2: 0.66918
	loss_value_2: 0.03512
	loss_reward_2: 0.00499
	loss_policy_3: 0.0342
	accuracy_policy_3: 0.6607
	loss_value_3: 0.03561
	loss_reward_3: 0.00528
	loss_policy_4: 0.03607
	accuracy_policy_4: 0.64184
	loss_value_4: 0.03624
	loss_reward_4: 0.00562
	loss_policy_5: 0.03827
	accuracy_policy_5: 0.63113
	loss_value_5: 0.03675
	loss_reward_5: 0.00629
	loss_policy: 0.27676
	loss_value: 0.34833
	loss_reward: 0.02757
[2024-05-04 13:03:54] nn step 15400, lr: 0.05.
	loss_policy_0: 0.10407
	accuracy_policy_0: 0.75699
	loss_value_0: 0.16824
	loss_policy_1: 0.02807
	accuracy_policy_1: 0.70289
	loss_value_1: 0.03412
	loss_reward_1: 0.00535
	loss_policy_2: 0.03171
	accuracy_policy_2: 0.6709
	loss_value_2: 0.03469
	loss_reward_2: 0.00493
	loss_policy_3: 0.03382
	accuracy_policy_3: 0.66391
	loss_value_3: 0.03532
	loss_reward_3: 0.00525
	loss_policy_4: 0.0359
	accuracy_policy_4: 0.64164
	loss_value_4: 0.03591
	loss_reward_4: 0.00567
	loss_policy_5: 0.03741
	accuracy_policy_5: 0.63293
	loss_value_5: 0.03647
	loss_reward_5: 0.00618
	loss_policy: 0.27098
	loss_value: 0.34475
	loss_reward: 0.02739
Optimization_Done 15400
[2024-05-04 13:05:58] [command] train weight_iter_15400.pkl 74 78
[2024-05-04 13:06:15] nn step 15450, lr: 0.05.
	loss_policy_0: 0.12837
	accuracy_policy_0: 0.71293
	loss_value_0: 0.17228
	loss_policy_1: 0.03253
	accuracy_policy_1: 0.66668
	loss_value_1: 0.0353
	loss_reward_1: 0.00549
	loss_policy_2: 0.03536
	accuracy_policy_2: 0.64297
	loss_value_2: 0.03603
	loss_reward_2: 0.00532
	loss_policy_3: 0.03765
	accuracy_policy_3: 0.62789
	loss_value_3: 0.03667
	loss_reward_3: 0.00535
	loss_policy_4: 0.03883
	accuracy_policy_4: 0.61887
	loss_value_4: 0.03729
	loss_reward_4: 0.00568
	loss_policy_5: 0.04074
	accuracy_policy_5: 0.60379
	loss_value_5: 0.03799
	loss_reward_5: 0.0064
	loss_policy: 0.31348
	loss_value: 0.35556
	loss_reward: 0.02823
[2024-05-04 13:06:31] nn step 15500, lr: 0.05.
	loss_policy_0: 0.12001
	accuracy_policy_0: 0.7293
	loss_value_0: 0.17116
	loss_policy_1: 0.03125
	accuracy_policy_1: 0.6791
	loss_value_1: 0.03513
	loss_reward_1: 0.00555
	loss_policy_2: 0.03392
	accuracy_policy_2: 0.65555
	loss_value_2: 0.03576
	loss_reward_2: 0.00519
	loss_policy_3: 0.0362
	accuracy_policy_3: 0.64027
	loss_value_3: 0.03653
	loss_reward_3: 0.00549
	loss_policy_4: 0.03784
	accuracy_policy_4: 0.6293
	loss_value_4: 0.0371
	loss_reward_4: 0.0058
	loss_policy_5: 0.03994
	accuracy_policy_5: 0.61543
	loss_value_5: 0.03776
	loss_reward_5: 0.00667
	loss_policy: 0.29916
	loss_value: 0.35344
	loss_reward: 0.02871
[2024-05-04 13:06:48] nn step 15550, lr: 0.05.
	loss_policy_0: 0.12381
	accuracy_policy_0: 0.73473
	loss_value_0: 0.17993
	loss_policy_1: 0.03248
	accuracy_policy_1: 0.67863
	loss_value_1: 0.03672
	loss_reward_1: 0.00591
	loss_policy_2: 0.03534
	accuracy_policy_2: 0.65797
	loss_value_2: 0.03738
	loss_reward_2: 0.00568
	loss_policy_3: 0.0377
	accuracy_policy_3: 0.63781
	loss_value_3: 0.03811
	loss_reward_3: 0.00547
	loss_policy_4: 0.0395
	accuracy_policy_4: 0.63152
	loss_value_4: 0.03885
	loss_reward_4: 0.00618
	loss_policy_5: 0.04138
	accuracy_policy_5: 0.61195
	loss_value_5: 0.03957
	loss_reward_5: 0.00696
	loss_policy: 0.31021
	loss_value: 0.37058
	loss_reward: 0.0302
[2024-05-04 13:07:04] nn step 15600, lr: 0.05.
	loss_policy_0: 0.12093
	accuracy_policy_0: 0.73781
	loss_value_0: 0.17744
	loss_policy_1: 0.03158
	accuracy_policy_1: 0.68199
	loss_value_1: 0.03633
	loss_reward_1: 0.00588
	loss_policy_2: 0.03444
	accuracy_policy_2: 0.66
	loss_value_2: 0.03709
	loss_reward_2: 0.00538
	loss_policy_3: 0.03678
	accuracy_policy_3: 0.64703
	loss_value_3: 0.03781
	loss_reward_3: 0.00557
	loss_policy_4: 0.03908
	accuracy_policy_4: 0.63125
	loss_value_4: 0.03859
	loss_reward_4: 0.00598
	loss_policy_5: 0.04105
	accuracy_policy_5: 0.62223
	loss_value_5: 0.03909
	loss_reward_5: 0.00706
	loss_policy: 0.30387
	loss_value: 0.36634
	loss_reward: 0.02987
Optimization_Done 15600
[2024-05-04 13:08:57] [command] train weight_iter_15600.pkl 75 79
[2024-05-04 13:09:15] nn step 15650, lr: 0.05.
	loss_policy_0: 0.1277
	accuracy_policy_0: 0.71137
	loss_value_0: 0.16649
	loss_policy_1: 0.03238
	accuracy_policy_1: 0.65488
	loss_value_1: 0.03401
	loss_reward_1: 0.00526
	loss_policy_2: 0.03497
	accuracy_policy_2: 0.6302
	loss_value_2: 0.03473
	loss_reward_2: 0.00501
	loss_policy_3: 0.03758
	accuracy_policy_3: 0.60762
	loss_value_3: 0.03545
	loss_reward_3: 0.00512
	loss_policy_4: 0.03884
	accuracy_policy_4: 0.5957
	loss_value_4: 0.03606
	loss_reward_4: 0.00574
	loss_policy_5: 0.041
	accuracy_policy_5: 0.5791
	loss_value_5: 0.03669
	loss_reward_5: 0.00644
	loss_policy: 0.31246
	loss_value: 0.34343
	loss_reward: 0.02757
[2024-05-04 13:09:31] nn step 15700, lr: 0.05.
	loss_policy_0: 0.13048
	accuracy_policy_0: 0.72508
	loss_value_0: 0.1791
	loss_policy_1: 0.03457
	accuracy_policy_1: 0.66215
	loss_value_1: 0.0367
	loss_reward_1: 0.00556
	loss_policy_2: 0.03716
	accuracy_policy_2: 0.64164
	loss_value_2: 0.03748
	loss_reward_2: 0.00539
	loss_policy_3: 0.03973
	accuracy_policy_3: 0.61867
	loss_value_3: 0.03819
	loss_reward_3: 0.0055
	loss_policy_4: 0.04188
	accuracy_policy_4: 0.6043
	loss_value_4: 0.039
	loss_reward_4: 0.00628
	loss_policy_5: 0.04356
	accuracy_policy_5: 0.58543
	loss_value_5: 0.0396
	loss_reward_5: 0.00691
	loss_policy: 0.32738
	loss_value: 0.37007
	loss_reward: 0.02963
[2024-05-04 13:09:47] nn step 15750, lr: 0.05.
	loss_policy_0: 0.12318
	accuracy_policy_0: 0.72609
	loss_value_0: 0.16991
	loss_policy_1: 0.03183
	accuracy_policy_1: 0.67297
	loss_value_1: 0.03488
	loss_reward_1: 0.0053
	loss_policy_2: 0.03507
	accuracy_policy_2: 0.6407
	loss_value_2: 0.03567
	loss_reward_2: 0.00495
	loss_policy_3: 0.03707
	accuracy_policy_3: 0.6227
	loss_value_3: 0.03635
	loss_reward_3: 0.0052
	loss_policy_4: 0.03899
	accuracy_policy_4: 0.61137
	loss_value_4: 0.03697
	loss_reward_4: 0.00579
	loss_policy_5: 0.04049
	accuracy_policy_5: 0.59824
	loss_value_5: 0.03767
	loss_reward_5: 0.00632
	loss_policy: 0.30663
	loss_value: 0.35145
	loss_reward: 0.02756
[2024-05-04 13:10:03] nn step 15800, lr: 0.05.
	loss_policy_0: 0.11682
	accuracy_policy_0: 0.73852
	loss_value_0: 0.16874
	loss_policy_1: 0.03104
	accuracy_policy_1: 0.67684
	loss_value_1: 0.03449
	loss_reward_1: 0.00554
	loss_policy_2: 0.03347
	accuracy_policy_2: 0.65094
	loss_value_2: 0.03524
	loss_reward_2: 0.00491
	loss_policy_3: 0.03613
	accuracy_policy_3: 0.63367
	loss_value_3: 0.03583
	loss_reward_3: 0.00515
	loss_policy_4: 0.03815
	accuracy_policy_4: 0.61496
	loss_value_4: 0.03648
	loss_reward_4: 0.00567
	loss_policy_5: 0.03999
	accuracy_policy_5: 0.60375
	loss_value_5: 0.03698
	loss_reward_5: 0.00644
	loss_policy: 0.29559
	loss_value: 0.34776
	loss_reward: 0.02771
Optimization_Done 15800
[2024-05-04 13:12:08] [command] train weight_iter_15800.pkl 76 80
[2024-05-04 13:12:26] nn step 15850, lr: 0.05.
	loss_policy_0: 0.14301
	accuracy_policy_0: 0.72457
	loss_value_0: 0.19425
	loss_policy_1: 0.03659
	accuracy_policy_1: 0.67289
	loss_value_1: 0.03974
	loss_reward_1: 0.00599
	loss_policy_2: 0.03986
	accuracy_policy_2: 0.64512
	loss_value_2: 0.04057
	loss_reward_2: 0.00569
	loss_policy_3: 0.0425
	accuracy_policy_3: 0.62707
	loss_value_3: 0.04137
	loss_reward_3: 0.00582
	loss_policy_4: 0.04439
	accuracy_policy_4: 0.61414
	loss_value_4: 0.04225
	loss_reward_4: 0.00656
	loss_policy_5: 0.04671
	accuracy_policy_5: 0.59637
	loss_value_5: 0.04317
	loss_reward_5: 0.00703
	loss_policy: 0.35306
	loss_value: 0.40135
	loss_reward: 0.03109
[2024-05-04 13:12:42] nn step 15900, lr: 0.05.
	loss_policy_0: 0.12333
	accuracy_policy_0: 0.73219
	loss_value_0: 0.1742
	loss_policy_1: 0.03199
	accuracy_policy_1: 0.68078
	loss_value_1: 0.03559
	loss_reward_1: 0.00531
	loss_policy_2: 0.03513
	accuracy_policy_2: 0.6507
	loss_value_2: 0.03635
	loss_reward_2: 0.00517
	loss_policy_3: 0.0372
	accuracy_policy_3: 0.63863
	loss_value_3: 0.03697
	loss_reward_3: 0.00536
	loss_policy_4: 0.03921
	accuracy_policy_4: 0.61957
	loss_value_4: 0.03766
	loss_reward_4: 0.0058
	loss_policy_5: 0.04105
	accuracy_policy_5: 0.60789
	loss_value_5: 0.03852
	loss_reward_5: 0.00635
	loss_policy: 0.30791
	loss_value: 0.35931
	loss_reward: 0.02801
[2024-05-04 13:12:58] nn step 15950, lr: 0.05.
	loss_policy_0: 0.1135
	accuracy_policy_0: 0.74102
	loss_value_0: 0.16388
	loss_policy_1: 0.02962
	accuracy_policy_1: 0.68445
	loss_value_1: 0.03359
	loss_reward_1: 0.00513
	loss_policy_2: 0.03175
	accuracy_policy_2: 0.66098
	loss_value_2: 0.03424
	loss_reward_2: 0.0048
	loss_policy_3: 0.03413
	accuracy_policy_3: 0.64406
	loss_value_3: 0.03499
	loss_reward_3: 0.00505
	loss_policy_4: 0.03611
	accuracy_policy_4: 0.6318
	loss_value_4: 0.03568
	loss_reward_4: 0.00557
	loss_policy_5: 0.03825
	accuracy_policy_5: 0.6123
	loss_value_5: 0.03636
	loss_reward_5: 0.0061
	loss_policy: 0.28336
	loss_value: 0.33874
	loss_reward: 0.02666
[2024-05-04 13:13:14] nn step 16000, lr: 0.05.
	loss_policy_0: 0.10864
	accuracy_policy_0: 0.74547
	loss_value_0: 0.16196
	loss_policy_1: 0.02867
	accuracy_policy_1: 0.68926
	loss_value_1: 0.03318
	loss_reward_1: 0.00495
	loss_policy_2: 0.03149
	accuracy_policy_2: 0.65738
	loss_value_2: 0.0341
	loss_reward_2: 0.00459
	loss_policy_3: 0.03376
	accuracy_policy_3: 0.64301
	loss_value_3: 0.03489
	loss_reward_3: 0.0049
	loss_policy_4: 0.03551
	accuracy_policy_4: 0.62684
	loss_value_4: 0.03552
	loss_reward_4: 0.00539
	loss_policy_5: 0.03703
	accuracy_policy_5: 0.61746
	loss_value_5: 0.03615
	loss_reward_5: 0.00579
	loss_policy: 0.27509
	loss_value: 0.33581
	loss_reward: 0.02563
Optimization_Done 16000
[2024-05-04 13:14:48] [command] train weight_iter_16000.pkl 77 81
[2024-05-04 13:15:05] nn step 16050, lr: 0.05.
	loss_policy_0: 0.13057
	accuracy_policy_0: 0.72164
	loss_value_0: 0.17279
	loss_policy_1: 0.03345
	accuracy_policy_1: 0.66219
	loss_value_1: 0.03546
	loss_reward_1: 0.00518
	loss_policy_2: 0.0361
	accuracy_policy_2: 0.63836
	loss_value_2: 0.03637
	loss_reward_2: 0.00484
	loss_policy_3: 0.03845
	accuracy_policy_3: 0.6216
	loss_value_3: 0.03707
	loss_reward_3: 0.00525
	loss_policy_4: 0.04004
	accuracy_policy_4: 0.60391
	loss_value_4: 0.03768
	loss_reward_4: 0.00565
	loss_policy_5: 0.04175
	accuracy_policy_5: 0.59332
	loss_value_5: 0.03852
	loss_reward_5: 0.00611
	loss_policy: 0.32036
	loss_value: 0.3579
	loss_reward: 0.02703
[2024-05-04 13:15:21] nn step 16100, lr: 0.05.
	loss_policy_0: 0.12415
	accuracy_policy_0: 0.7316
	loss_value_0: 0.17296
	loss_policy_1: 0.03165
	accuracy_policy_1: 0.67488
	loss_value_1: 0.03539
	loss_reward_1: 0.00543
	loss_policy_2: 0.0351
	accuracy_policy_2: 0.64582
	loss_value_2: 0.03622
	loss_reward_2: 0.0051
	loss_policy_3: 0.03724
	accuracy_policy_3: 0.62941
	loss_value_3: 0.03702
	loss_reward_3: 0.00507
	loss_policy_4: 0.03914
	accuracy_policy_4: 0.61203
	loss_value_4: 0.03781
	loss_reward_4: 0.00569
	loss_policy_5: 0.04101
	accuracy_policy_5: 0.6002
	loss_value_5: 0.03869
	loss_reward_5: 0.00632
	loss_policy: 0.3083
	loss_value: 0.3581
	loss_reward: 0.02761
[2024-05-04 13:15:37] nn step 16150, lr: 0.05.
	loss_policy_0: 0.11864
	accuracy_policy_0: 0.7352
	loss_value_0: 0.1675
	loss_policy_1: 0.0307
	accuracy_policy_1: 0.67422
	loss_value_1: 0.03427
	loss_reward_1: 0.00522
	loss_policy_2: 0.03331
	accuracy_policy_2: 0.65176
	loss_value_2: 0.03508
	loss_reward_2: 0.00472
	loss_policy_3: 0.03593
	accuracy_policy_3: 0.63688
	loss_value_3: 0.03593
	loss_reward_3: 0.00487
	loss_policy_4: 0.03779
	accuracy_policy_4: 0.61543
	loss_value_4: 0.03669
	loss_reward_4: 0.00546
	loss_policy_5: 0.03955
	accuracy_policy_5: 0.60465
	loss_value_5: 0.03751
	loss_reward_5: 0.00641
	loss_policy: 0.29591
	loss_value: 0.34698
	loss_reward: 0.02668
[2024-05-04 13:15:54] nn step 16200, lr: 0.05.
	loss_policy_0: 0.12007
	accuracy_policy_0: 0.7402
	loss_value_0: 0.17414
	loss_policy_1: 0.03106
	accuracy_policy_1: 0.68477
	loss_value_1: 0.03564
	loss_reward_1: 0.00538
	loss_policy_2: 0.03412
	accuracy_policy_2: 0.65617
	loss_value_2: 0.03655
	loss_reward_2: 0.00505
	loss_policy_3: 0.03638
	accuracy_policy_3: 0.63938
	loss_value_3: 0.0374
	loss_reward_3: 0.00522
	loss_policy_4: 0.03834
	accuracy_policy_4: 0.62699
	loss_value_4: 0.03825
	loss_reward_4: 0.00592
	loss_policy_5: 0.04004
	accuracy_policy_5: 0.60754
	loss_value_5: 0.03905
	loss_reward_5: 0.00656
	loss_policy: 0.30001
	loss_value: 0.36104
	loss_reward: 0.02813
Optimization_Done 16200
[2024-05-04 13:17:59] [command] train weight_iter_16200.pkl 78 82
[2024-05-04 13:18:16] nn step 16250, lr: 0.05.
	loss_policy_0: 0.14051
	accuracy_policy_0: 0.69605
	loss_value_0: 0.17082
	loss_policy_1: 0.03527
	accuracy_policy_1: 0.64793
	loss_value_1: 0.03526
	loss_reward_1: 0.00548
	loss_policy_2: 0.03785
	accuracy_policy_2: 0.62148
	loss_value_2: 0.03626
	loss_reward_2: 0.00528
	loss_policy_3: 0.0398
	accuracy_policy_3: 0.60789
	loss_value_3: 0.0373
	loss_reward_3: 0.00544
	loss_policy_4: 0.04174
	accuracy_policy_4: 0.5902
	loss_value_4: 0.03822
	loss_reward_4: 0.00606
	loss_policy_5: 0.04326
	accuracy_policy_5: 0.58359
	loss_value_5: 0.03907
	loss_reward_5: 0.00667
	loss_policy: 0.33843
	loss_value: 0.35693
	loss_reward: 0.02894
[2024-05-04 13:18:32] nn step 16300, lr: 0.05.
	loss_policy_0: 0.13507
	accuracy_policy_0: 0.71523
	loss_value_0: 0.17401
	loss_policy_1: 0.03467
	accuracy_policy_1: 0.66008
	loss_value_1: 0.03597
	loss_reward_1: 0.0057
	loss_policy_2: 0.0372
	accuracy_policy_2: 0.63992
	loss_value_2: 0.03699
	loss_reward_2: 0.00533
	loss_policy_3: 0.03959
	accuracy_policy_3: 0.62422
	loss_value_3: 0.03813
	loss_reward_3: 0.00553
	loss_policy_4: 0.04165
	accuracy_policy_4: 0.60645
	loss_value_4: 0.03903
	loss_reward_4: 0.00626
	loss_policy_5: 0.04301
	accuracy_policy_5: 0.5957
	loss_value_5: 0.03984
	loss_reward_5: 0.00703
	loss_policy: 0.33119
	loss_value: 0.36396
	loss_reward: 0.02985
[2024-05-04 13:18:48] nn step 16350, lr: 0.05.
	loss_policy_0: 0.13649
	accuracy_policy_0: 0.71816
	loss_value_0: 0.17786
	loss_policy_1: 0.0349
	accuracy_policy_1: 0.66336
	loss_value_1: 0.0367
	loss_reward_1: 0.00575
	loss_policy_2: 0.03766
	accuracy_policy_2: 0.64062
	loss_value_2: 0.03766
	loss_reward_2: 0.00552
	loss_policy_3: 0.04043
	accuracy_policy_3: 0.62516
	loss_value_3: 0.03872
	loss_reward_3: 0.00554
	loss_policy_4: 0.04227
	accuracy_policy_4: 0.61168
	loss_value_4: 0.03957
	loss_reward_4: 0.00612
	loss_policy_5: 0.04407
	accuracy_policy_5: 0.59801
	loss_value_5: 0.04068
	loss_reward_5: 0.007
	loss_policy: 0.33581
	loss_value: 0.37119
	loss_reward: 0.02993
[2024-05-04 13:19:05] nn step 16400, lr: 0.05.
	loss_policy_0: 0.13829
	accuracy_policy_0: 0.72426
	loss_value_0: 0.18257
	loss_policy_1: 0.03555
	accuracy_policy_1: 0.66512
	loss_value_1: 0.03757
	loss_reward_1: 0.00598
	loss_policy_2: 0.03839
	accuracy_policy_2: 0.64133
	loss_value_2: 0.0387
	loss_reward_2: 0.0057
	loss_policy_3: 0.0404
	accuracy_policy_3: 0.63008
	loss_value_3: 0.03957
	loss_reward_3: 0.00565
	loss_policy_4: 0.04252
	accuracy_policy_4: 0.61312
	loss_value_4: 0.04035
	loss_reward_4: 0.0063
	loss_policy_5: 0.04455
	accuracy_policy_5: 0.59996
	loss_value_5: 0.04124
	loss_reward_5: 0.0073
	loss_policy: 0.33971
	loss_value: 0.38
	loss_reward: 0.03094
Optimization_Done 16400
[2024-05-04 13:20:56] [command] train weight_iter_16400.pkl 79 83
[2024-05-04 13:21:14] nn step 16450, lr: 0.05.
	loss_policy_0: 0.14976
	accuracy_policy_0: 0.69293
	loss_value_0: 0.17714
	loss_policy_1: 0.037
	accuracy_policy_1: 0.63789
	loss_value_1: 0.03643
	loss_reward_1: 0.0057
	loss_policy_2: 0.0394
	accuracy_policy_2: 0.62141
	loss_value_2: 0.03739
	loss_reward_2: 0.00547
	loss_policy_3: 0.04154
	accuracy_policy_3: 0.60211
	loss_value_3: 0.03826
	loss_reward_3: 0.00558
	loss_policy_4: 0.04374
	accuracy_policy_4: 0.58426
	loss_value_4: 0.03904
	loss_reward_4: 0.00621
	loss_policy_5: 0.04593
	accuracy_policy_5: 0.56852
	loss_value_5: 0.03973
	loss_reward_5: 0.00703
	loss_policy: 0.35737
	loss_value: 0.368
	loss_reward: 0.02999
[2024-05-04 13:21:30] nn step 16500, lr: 0.05.
	loss_policy_0: 0.14508
	accuracy_policy_0: 0.70613
	loss_value_0: 0.1783
	loss_policy_1: 0.03574
	accuracy_policy_1: 0.65918
	loss_value_1: 0.03673
	loss_reward_1: 0.00564
	loss_policy_2: 0.03881
	accuracy_policy_2: 0.63391
	loss_value_2: 0.03768
	loss_reward_2: 0.00534
	loss_policy_3: 0.04135
	accuracy_policy_3: 0.61301
	loss_value_3: 0.03857
	loss_reward_3: 0.00548
	loss_policy_4: 0.04353
	accuracy_policy_4: 0.59352
	loss_value_4: 0.03941
	loss_reward_4: 0.00614
	loss_policy_5: 0.0456
	accuracy_policy_5: 0.57977
	loss_value_5: 0.04033
	loss_reward_5: 0.00705
	loss_policy: 0.35012
	loss_value: 0.37102
	loss_reward: 0.02965
[2024-05-04 13:21:46] nn step 16550, lr: 0.05.
	loss_policy_0: 0.13902
	accuracy_policy_0: 0.71293
	loss_value_0: 0.17668
	loss_policy_1: 0.03527
	accuracy_policy_1: 0.66262
	loss_value_1: 0.03634
	loss_reward_1: 0.00565
	loss_policy_2: 0.03763
	accuracy_policy_2: 0.64133
	loss_value_2: 0.03719
	loss_reward_2: 0.00527
	loss_policy_3: 0.04074
	accuracy_policy_3: 0.61277
	loss_value_3: 0.03794
	loss_reward_3: 0.00548
	loss_policy_4: 0.04207
	accuracy_policy_4: 0.60355
	loss_value_4: 0.03875
	loss_reward_4: 0.00584
	loss_policy_5: 0.04419
	accuracy_policy_5: 0.58754
	loss_value_5: 0.03948
	loss_reward_5: 0.00699
	loss_policy: 0.33892
	loss_value: 0.36639
	loss_reward: 0.02923
[2024-05-04 13:22:02] nn step 16600, lr: 0.05.
	loss_policy_0: 0.13975
	accuracy_policy_0: 0.72168
	loss_value_0: 0.18058
	loss_policy_1: 0.03572
	accuracy_policy_1: 0.66781
	loss_value_1: 0.03725
	loss_reward_1: 0.00596
	loss_policy_2: 0.03856
	accuracy_policy_2: 0.6373
	loss_value_2: 0.03827
	loss_reward_2: 0.00552
	loss_policy_3: 0.04082
	accuracy_policy_3: 0.62383
	loss_value_3: 0.03921
	loss_reward_3: 0.00551
	loss_policy_4: 0.04311
	accuracy_policy_4: 0.60676
	loss_value_4: 0.04008
	loss_reward_4: 0.00626
	loss_policy_5: 0.04503
	accuracy_policy_5: 0.59246
	loss_value_5: 0.04091
	loss_reward_5: 0.00716
	loss_policy: 0.343
	loss_value: 0.37628
	loss_reward: 0.03042
Optimization_Done 16600
[2024-05-04 13:24:08] [command] train weight_iter_16600.pkl 80 84
[2024-05-04 13:24:26] nn step 16650, lr: 0.05.
	loss_policy_0: 0.14593
	accuracy_policy_0: 0.69676
	loss_value_0: 0.17008
	loss_policy_1: 0.03694
	accuracy_policy_1: 0.6348
	loss_value_1: 0.03506
	loss_reward_1: 0.00522
	loss_policy_2: 0.03946
	accuracy_policy_2: 0.61078
	loss_value_2: 0.03618
	loss_reward_2: 0.00487
	loss_policy_3: 0.04219
	accuracy_policy_3: 0.5893
	loss_value_3: 0.0371
	loss_reward_3: 0.00528
	loss_policy_4: 0.04408
	accuracy_policy_4: 0.57988
	loss_value_4: 0.03781
	loss_reward_4: 0.00574
	loss_policy_5: 0.04587
	accuracy_policy_5: 0.56
	loss_value_5: 0.03852
	loss_reward_5: 0.00658
	loss_policy: 0.35448
	loss_value: 0.35475
	loss_reward: 0.02768
[2024-05-04 13:24:42] nn step 16700, lr: 0.05.
	loss_policy_0: 0.14323
	accuracy_policy_0: 0.70715
	loss_value_0: 0.17293
	loss_policy_1: 0.03654
	accuracy_policy_1: 0.64484
	loss_value_1: 0.03563
	loss_reward_1: 0.00548
	loss_policy_2: 0.03921
	accuracy_policy_2: 0.62539
	loss_value_2: 0.03674
	loss_reward_2: 0.00516
	loss_policy_3: 0.0412
	accuracy_policy_3: 0.60617
	loss_value_3: 0.03757
	loss_reward_3: 0.00523
	loss_policy_4: 0.04368
	accuracy_policy_4: 0.58637
	loss_value_4: 0.03853
	loss_reward_4: 0.00599
	loss_policy_5: 0.04567
	accuracy_policy_5: 0.57504
	loss_value_5: 0.03929
	loss_reward_5: 0.00657
	loss_policy: 0.34951
	loss_value: 0.36069
	loss_reward: 0.02844
[2024-05-04 13:24:59] nn step 16750, lr: 0.05.
	loss_policy_0: 0.13726
	accuracy_policy_0: 0.71953
	loss_value_0: 0.17477
	loss_policy_1: 0.03556
	accuracy_policy_1: 0.6552
	loss_value_1: 0.03594
	loss_reward_1: 0.0057
	loss_policy_2: 0.03855
	accuracy_policy_2: 0.62988
	loss_value_2: 0.03686
	loss_reward_2: 0.00506
	loss_policy_3: 0.04112
	accuracy_policy_3: 0.6098
	loss_value_3: 0.03786
	loss_reward_3: 0.00539
	loss_policy_4: 0.04344
	accuracy_policy_4: 0.59664
	loss_value_4: 0.03876
	loss_reward_4: 0.0061
	loss_policy_5: 0.0454
	accuracy_policy_5: 0.58285
	loss_value_5: 0.03966
	loss_reward_5: 0.0071
	loss_policy: 0.34134
	loss_value: 0.36385
	loss_reward: 0.02934
[2024-05-04 13:25:15] nn step 16800, lr: 0.05.
	loss_policy_0: 0.13323
	accuracy_policy_0: 0.72312
	loss_value_0: 0.17449
	loss_policy_1: 0.03486
	accuracy_policy_1: 0.6627
	loss_value_1: 0.03594
	loss_reward_1: 0.00529
	loss_policy_2: 0.03792
	accuracy_policy_2: 0.63512
	loss_value_2: 0.03691
	loss_reward_2: 0.00508
	loss_policy_3: 0.04044
	accuracy_policy_3: 0.61641
	loss_value_3: 0.03788
	loss_reward_3: 0.00544
	loss_policy_4: 0.04276
	accuracy_policy_4: 0.59812
	loss_value_4: 0.03872
	loss_reward_4: 0.00591
	loss_policy_5: 0.04433
	accuracy_policy_5: 0.58895
	loss_value_5: 0.03968
	loss_reward_5: 0.00697
	loss_policy: 0.33354
	loss_value: 0.36364
	loss_reward: 0.0287
Optimization_Done 16800
[2024-05-04 13:27:19] [command] train weight_iter_16800.pkl 81 85
[2024-05-04 13:27:37] nn step 16850, lr: 0.05.
	loss_policy_0: 0.14312
	accuracy_policy_0: 0.68414
	loss_value_0: 0.16763
	loss_policy_1: 0.03645
	accuracy_policy_1: 0.62383
	loss_value_1: 0.03443
	loss_reward_1: 0.00544
	loss_policy_2: 0.03911
	accuracy_policy_2: 0.59824
	loss_value_2: 0.03538
	loss_reward_2: 0.00506
	loss_policy_3: 0.04161
	accuracy_policy_3: 0.58035
	loss_value_3: 0.03644
	loss_reward_3: 0.00517
	loss_policy_4: 0.044
	accuracy_policy_4: 0.55895
	loss_value_4: 0.03739
	loss_reward_4: 0.00603
	loss_policy_5: 0.04578
	accuracy_policy_5: 0.54113
	loss_value_5: 0.03845
	loss_reward_5: 0.00695
	loss_policy: 0.35008
	loss_value: 0.34972
	loss_reward: 0.02864
[2024-05-04 13:27:53] nn step 16900, lr: 0.05.
	loss_policy_0: 0.13752
	accuracy_policy_0: 0.70383
	loss_value_0: 0.17201
	loss_policy_1: 0.03588
	accuracy_policy_1: 0.63777
	loss_value_1: 0.03555
	loss_reward_1: 0.00567
	loss_policy_2: 0.0388
	accuracy_policy_2: 0.61203
	loss_value_2: 0.03654
	loss_reward_2: 0.00506
	loss_policy_3: 0.04144
	accuracy_policy_3: 0.59496
	loss_value_3: 0.03767
	loss_reward_3: 0.00538
	loss_policy_4: 0.04442
	accuracy_policy_4: 0.56902
	loss_value_4: 0.03842
	loss_reward_4: 0.00593
	loss_policy_5: 0.04639
	accuracy_policy_5: 0.54984
	loss_value_5: 0.03929
	loss_reward_5: 0.00709
	loss_policy: 0.34445
	loss_value: 0.35948
	loss_reward: 0.02914
[2024-05-04 13:28:09] nn step 16950, lr: 0.05.
	loss_policy_0: 0.13676
	accuracy_policy_0: 0.70895
	loss_value_0: 0.17172
	loss_policy_1: 0.03601
	accuracy_policy_1: 0.64066
	loss_value_1: 0.03537
	loss_reward_1: 0.00573
	loss_policy_2: 0.03864
	accuracy_policy_2: 0.61777
	loss_value_2: 0.03654
	loss_reward_2: 0.00525
	loss_policy_3: 0.04157
	accuracy_policy_3: 0.59766
	loss_value_3: 0.03749
	loss_reward_3: 0.00542
	loss_policy_4: 0.04394
	accuracy_policy_4: 0.57367
	loss_value_4: 0.03843
	loss_reward_4: 0.00625
	loss_policy_5: 0.0461
	accuracy_policy_5: 0.55566
	loss_value_5: 0.03927
	loss_reward_5: 0.00723
	loss_policy: 0.34303
	loss_value: 0.35881
	loss_reward: 0.02987
[2024-05-04 13:28:26] nn step 17000, lr: 0.05.
	loss_policy_0: 0.13641
	accuracy_policy_0: 0.72117
	loss_value_0: 0.17747
	loss_policy_1: 0.0362
	accuracy_policy_1: 0.65086
	loss_value_1: 0.03669
	loss_reward_1: 0.0059
	loss_policy_2: 0.03929
	accuracy_policy_2: 0.62348
	loss_value_2: 0.03799
	loss_reward_2: 0.00538
	loss_policy_3: 0.04199
	accuracy_policy_3: 0.60719
	loss_value_3: 0.03889
	loss_reward_3: 0.00559
	loss_policy_4: 0.04441
	accuracy_policy_4: 0.5891
	loss_value_4: 0.03988
	loss_reward_4: 0.0065
	loss_policy_5: 0.04708
	accuracy_policy_5: 0.56746
	loss_value_5: 0.0409
	loss_reward_5: 0.0075
	loss_policy: 0.3454
	loss_value: 0.37181
	loss_reward: 0.03087
Optimization_Done 17000
[2024-05-04 13:30:29] [command] train weight_iter_17000.pkl 82 86
[2024-05-04 13:30:46] nn step 17050, lr: 0.05.
	loss_policy_0: 0.16537
	accuracy_policy_0: 0.6666
	loss_value_0: 0.17142
	loss_policy_1: 0.04036
	accuracy_policy_1: 0.60633
	loss_value_1: 0.0356
	loss_reward_1: 0.00584
	loss_policy_2: 0.04303
	accuracy_policy_2: 0.5832
	loss_value_2: 0.03681
	loss_reward_2: 0.00525
	loss_policy_3: 0.04565
	accuracy_policy_3: 0.56203
	loss_value_3: 0.03786
	loss_reward_3: 0.00538
	loss_policy_4: 0.04759
	accuracy_policy_4: 0.54859
	loss_value_4: 0.03882
	loss_reward_4: 0.0063
	loss_policy_5: 0.05013
	accuracy_policy_5: 0.53062
	loss_value_5: 0.03981
	loss_reward_5: 0.00731
	loss_policy: 0.39213
	loss_value: 0.36031
	loss_reward: 0.03008
[2024-05-04 13:31:03] nn step 17100, lr: 0.05.
	loss_policy_0: 0.15678
	accuracy_policy_0: 0.6902
	loss_value_0: 0.17727
	loss_policy_1: 0.03981
	accuracy_policy_1: 0.62578
	loss_value_1: 0.03655
	loss_reward_1: 0.0061
	loss_policy_2: 0.04257
	accuracy_policy_2: 0.60152
	loss_value_2: 0.0377
	loss_reward_2: 0.00554
	loss_policy_3: 0.04624
	accuracy_policy_3: 0.57883
	loss_value_3: 0.03881
	loss_reward_3: 0.00574
	loss_policy_4: 0.04847
	accuracy_policy_4: 0.56141
	loss_value_4: 0.03982
	loss_reward_4: 0.0065
	loss_policy_5: 0.05043
	accuracy_policy_5: 0.54723
	loss_value_5: 0.04074
	loss_reward_5: 0.00765
	loss_policy: 0.3843
	loss_value: 0.37088
	loss_reward: 0.03152
[2024-05-04 13:31:19] nn step 17150, lr: 0.05.
	loss_policy_0: 0.14367
	accuracy_policy_0: 0.6952
	loss_value_0: 0.1675
	loss_policy_1: 0.03695
	accuracy_policy_1: 0.6332
	loss_value_1: 0.03467
	loss_reward_1: 0.00555
	loss_policy_2: 0.03957
	accuracy_policy_2: 0.60641
	loss_value_2: 0.03557
	loss_reward_2: 0.00517
	loss_policy_3: 0.04232
	accuracy_policy_3: 0.59035
	loss_value_3: 0.03658
	loss_reward_3: 0.00548
	loss_policy_4: 0.04482
	accuracy_policy_4: 0.56773
	loss_value_4: 0.03764
	loss_reward_4: 0.00624
	loss_policy_5: 0.04657
	accuracy_policy_5: 0.5527
	loss_value_5: 0.03867
	loss_reward_5: 0.00721
	loss_policy: 0.3539
	loss_value: 0.35063
	loss_reward: 0.02966
[2024-05-04 13:31:35] nn step 17200, lr: 0.05.
	loss_policy_0: 0.14431
	accuracy_policy_0: 0.70016
	loss_value_0: 0.17103
	loss_policy_1: 0.0375
	accuracy_policy_1: 0.63082
	loss_value_1: 0.03574
	loss_reward_1: 0.0056
	loss_policy_2: 0.0403
	accuracy_policy_2: 0.60871
	loss_value_2: 0.0368
	loss_reward_2: 0.00542
	loss_policy_3: 0.04304
	accuracy_policy_3: 0.59328
	loss_value_3: 0.03779
	loss_reward_3: 0.00567
	loss_policy_4: 0.046
	accuracy_policy_4: 0.57059
	loss_value_4: 0.03894
	loss_reward_4: 0.00605
	loss_policy_5: 0.04803
	accuracy_policy_5: 0.55031
	loss_value_5: 0.04003
	loss_reward_5: 0.00741
	loss_policy: 0.35917
	loss_value: 0.36033
	loss_reward: 0.03015
Optimization_Done 17200
[2024-05-04 13:33:34] [command] train weight_iter_17200.pkl 83 87
[2024-05-04 13:33:51] nn step 17250, lr: 0.05.
	loss_policy_0: 0.18085
	accuracy_policy_0: 0.65191
	loss_value_0: 0.18062
	loss_policy_1: 0.04415
	accuracy_policy_1: 0.58641
	loss_value_1: 0.03741
	loss_reward_1: 0.00613
	loss_policy_2: 0.04713
	accuracy_policy_2: 0.5659
	loss_value_2: 0.03862
	loss_reward_2: 0.00567
	loss_policy_3: 0.04936
	accuracy_policy_3: 0.55016
	loss_value_3: 0.03974
	loss_reward_3: 0.00575
	loss_policy_4: 0.05213
	accuracy_policy_4: 0.52789
	loss_value_4: 0.04062
	loss_reward_4: 0.00672
	loss_policy_5: 0.05399
	accuracy_policy_5: 0.51496
	loss_value_5: 0.04162
	loss_reward_5: 0.00751
	loss_policy: 0.42761
	loss_value: 0.37862
	loss_reward: 0.03178
[2024-05-04 13:34:07] nn step 17300, lr: 0.05.
	loss_policy_0: 0.16175
	accuracy_policy_0: 0.66824
	loss_value_0: 0.17343
	loss_policy_1: 0.04085
	accuracy_policy_1: 0.60793
	loss_value_1: 0.03613
	loss_reward_1: 0.00628
	loss_policy_2: 0.04317
	accuracy_policy_2: 0.58996
	loss_value_2: 0.03737
	loss_reward_2: 0.00566
	loss_policy_3: 0.04644
	accuracy_policy_3: 0.56332
	loss_value_3: 0.03831
	loss_reward_3: 0.00576
	loss_policy_4: 0.04844
	accuracy_policy_4: 0.55125
	loss_value_4: 0.03912
	loss_reward_4: 0.00649
	loss_policy_5: 0.05104
	accuracy_policy_5: 0.5266
	loss_value_5: 0.0402
	loss_reward_5: 0.00728
	loss_policy: 0.39168
	loss_value: 0.36456
	loss_reward: 0.03147
[2024-05-04 13:34:23] nn step 17350, lr: 0.05.
	loss_policy_0: 0.15811
	accuracy_policy_0: 0.68152
	loss_value_0: 0.17374
	loss_policy_1: 0.04007
	accuracy_policy_1: 0.61418
	loss_value_1: 0.03601
	loss_reward_1: 0.00594
	loss_policy_2: 0.04287
	accuracy_policy_2: 0.58855
	loss_value_2: 0.03704
	loss_reward_2: 0.0053
	loss_policy_3: 0.04545
	accuracy_policy_3: 0.57105
	loss_value_3: 0.03806
	loss_reward_3: 0.00571
	loss_policy_4: 0.04813
	accuracy_policy_4: 0.54719
	loss_value_4: 0.03909
	loss_reward_4: 0.00647
	loss_policy_5: 0.05003
	accuracy_policy_5: 0.5357
	loss_value_5: 0.03994
	loss_reward_5: 0.00743
	loss_policy: 0.38466
	loss_value: 0.36389
	loss_reward: 0.03085
[2024-05-04 13:34:40] nn step 17400, lr: 0.05.
	loss_policy_0: 0.14852
	accuracy_policy_0: 0.68953
	loss_value_0: 0.17053
	loss_policy_1: 0.03869
	accuracy_policy_1: 0.61582
	loss_value_1: 0.03549
	loss_reward_1: 0.0058
	loss_policy_2: 0.04156
	accuracy_policy_2: 0.59906
	loss_value_2: 0.03662
	loss_reward_2: 0.00542
	loss_policy_3: 0.04385
	accuracy_policy_3: 0.57906
	loss_value_3: 0.03766
	loss_reward_3: 0.0055
	loss_policy_4: 0.04624
	accuracy_policy_4: 0.56281
	loss_value_4: 0.0388
	loss_reward_4: 0.00634
	loss_policy_5: 0.04834
	accuracy_policy_5: 0.54082
	loss_value_5: 0.03969
	loss_reward_5: 0.00743
	loss_policy: 0.3672
	loss_value: 0.35878
	loss_reward: 0.03049
Optimization_Done 17400
[2024-05-04 13:36:31] [command] train weight_iter_17400.pkl 84 88
[2024-05-04 13:36:48] nn step 17450, lr: 0.05.
	loss_policy_0: 0.1625
	accuracy_policy_0: 0.66801
	loss_value_0: 0.17689
	loss_policy_1: 0.04103
	accuracy_policy_1: 0.60199
	loss_value_1: 0.03677
	loss_reward_1: 0.00601
	loss_policy_2: 0.04387
	accuracy_policy_2: 0.57941
	loss_value_2: 0.03816
	loss_reward_2: 0.00547
	loss_policy_3: 0.04746
	accuracy_policy_3: 0.55785
	loss_value_3: 0.03935
	loss_reward_3: 0.00592
	loss_policy_4: 0.04961
	accuracy_policy_4: 0.54082
	loss_value_4: 0.04044
	loss_reward_4: 0.00623
	loss_policy_5: 0.05194
	accuracy_policy_5: 0.5218
	loss_value_5: 0.04135
	loss_reward_5: 0.00723
	loss_policy: 0.39641
	loss_value: 0.37295
	loss_reward: 0.03087
[2024-05-04 13:37:05] nn step 17500, lr: 0.05.
	loss_policy_0: 0.1573
	accuracy_policy_0: 0.67207
	loss_value_0: 0.17599
	loss_policy_1: 0.04025
	accuracy_policy_1: 0.61004
	loss_value_1: 0.03654
	loss_reward_1: 0.00594
	loss_policy_2: 0.04294
	accuracy_policy_2: 0.59074
	loss_value_2: 0.03771
	loss_reward_2: 0.0054
	loss_policy_3: 0.04698
	accuracy_policy_3: 0.55934
	loss_value_3: 0.03885
	loss_reward_3: 0.00585
	loss_policy_4: 0.04943
	accuracy_policy_4: 0.54434
	loss_value_4: 0.03987
	loss_reward_4: 0.0063
	loss_policy_5: 0.05223
	accuracy_policy_5: 0.52406
	loss_value_5: 0.04094
	loss_reward_5: 0.00722
	loss_policy: 0.38913
	loss_value: 0.3699
	loss_reward: 0.0307
[2024-05-04 13:37:21] nn step 17550, lr: 0.05.
	loss_policy_0: 0.15726
	accuracy_policy_0: 0.69305
	loss_value_0: 0.18266
	loss_policy_1: 0.04091
	accuracy_policy_1: 0.62336
	loss_value_1: 0.03821
	loss_reward_1: 0.00605
	loss_policy_2: 0.04432
	accuracy_policy_2: 0.59238
	loss_value_2: 0.03929
	loss_reward_2: 0.0056
	loss_policy_3: 0.04728
	accuracy_policy_3: 0.57676
	loss_value_3: 0.04037
	loss_reward_3: 0.00587
	loss_policy_4: 0.04988
	accuracy_policy_4: 0.55301
	loss_value_4: 0.04135
	loss_reward_4: 0.00643
	loss_policy_5: 0.05261
	accuracy_policy_5: 0.53516
	loss_value_5: 0.04235
	loss_reward_5: 0.0076
	loss_policy: 0.39227
	loss_value: 0.38422
	loss_reward: 0.03156
[2024-05-04 13:37:37] nn step 17600, lr: 0.05.
	loss_policy_0: 0.14297
	accuracy_policy_0: 0.69016
	loss_value_0: 0.16643
	loss_policy_1: 0.03714
	accuracy_policy_1: 0.61828
	loss_value_1: 0.03484
	loss_reward_1: 0.00555
	loss_policy_2: 0.04007
	accuracy_policy_2: 0.59598
	loss_value_2: 0.03598
	loss_reward_2: 0.00518
	loss_policy_3: 0.04303
	accuracy_policy_3: 0.57484
	loss_value_3: 0.03703
	loss_reward_3: 0.00528
	loss_policy_4: 0.04537
	accuracy_policy_4: 0.56109
	loss_value_4: 0.0381
	loss_reward_4: 0.0059
	loss_policy_5: 0.04747
	accuracy_policy_5: 0.5418
	loss_value_5: 0.03903
	loss_reward_5: 0.00705
	loss_policy: 0.35605
	loss_value: 0.35141
	loss_reward: 0.02896
Optimization_Done 17600
[2024-05-04 13:39:37] [command] train weight_iter_17600.pkl 85 89
[2024-05-04 13:39:55] nn step 17650, lr: 0.05.
	loss_policy_0: 0.17161
	accuracy_policy_0: 0.66863
	loss_value_0: 0.18653
	loss_policy_1: 0.04276
	accuracy_policy_1: 0.60629
	loss_value_1: 0.03877
	loss_reward_1: 0.00643
	loss_policy_2: 0.04577
	accuracy_policy_2: 0.58559
	loss_value_2: 0.04008
	loss_reward_2: 0.00603
	loss_policy_3: 0.04891
	accuracy_policy_3: 0.55902
	loss_value_3: 0.04132
	loss_reward_3: 0.00622
	loss_policy_4: 0.0516
	accuracy_policy_4: 0.54098
	loss_value_4: 0.0424
	loss_reward_4: 0.00676
	loss_policy_5: 0.05374
	accuracy_policy_5: 0.5266
	loss_value_5: 0.04352
	loss_reward_5: 0.00789
	loss_policy: 0.41438
	loss_value: 0.39262
	loss_reward: 0.03333
[2024-05-04 13:40:11] nn step 17700, lr: 0.05.
	loss_policy_0: 0.15446
	accuracy_policy_0: 0.68652
	loss_value_0: 0.17752
	loss_policy_1: 0.03938
	accuracy_policy_1: 0.61988
	loss_value_1: 0.03687
	loss_reward_1: 0.00604
	loss_policy_2: 0.04259
	accuracy_policy_2: 0.59777
	loss_value_2: 0.03813
	loss_reward_2: 0.00575
	loss_policy_3: 0.04562
	accuracy_policy_3: 0.57441
	loss_value_3: 0.03924
	loss_reward_3: 0.00594
	loss_policy_4: 0.04859
	accuracy_policy_4: 0.5548
	loss_value_4: 0.04034
	loss_reward_4: 0.00653
	loss_policy_5: 0.05137
	accuracy_policy_5: 0.52746
	loss_value_5: 0.04147
	loss_reward_5: 0.00747
	loss_policy: 0.38201
	loss_value: 0.37357
	loss_reward: 0.03173
[2024-05-04 13:40:27] nn step 17750, lr: 0.05.
	loss_policy_0: 0.15229
	accuracy_policy_0: 0.6891
	loss_value_0: 0.17827
	loss_policy_1: 0.03894
	accuracy_policy_1: 0.62352
	loss_value_1: 0.03691
	loss_reward_1: 0.00634
	loss_policy_2: 0.04222
	accuracy_policy_2: 0.59809
	loss_value_2: 0.03809
	loss_reward_2: 0.00554
	loss_policy_3: 0.04531
	accuracy_policy_3: 0.57688
	loss_value_3: 0.03922
	loss_reward_3: 0.00604
	loss_policy_4: 0.04818
	accuracy_policy_4: 0.56246
	loss_value_4: 0.04022
	loss_reward_4: 0.00648
	loss_policy_5: 0.05071
	accuracy_policy_5: 0.54129
	loss_value_5: 0.04126
	loss_reward_5: 0.00775
	loss_policy: 0.37764
	loss_value: 0.37395
	loss_reward: 0.03214
[2024-05-04 13:40:43] nn step 17800, lr: 0.05.
	loss_policy_0: 0.14447
	accuracy_policy_0: 0.6977
	loss_value_0: 0.17337
	loss_policy_1: 0.03759
	accuracy_policy_1: 0.62727
	loss_value_1: 0.03602
	loss_reward_1: 0.00613
	loss_policy_2: 0.04087
	accuracy_policy_2: 0.60348
	loss_value_2: 0.03707
	loss_reward_2: 0.0057
	loss_policy_3: 0.04373
	accuracy_policy_3: 0.58582
	loss_value_3: 0.03819
	loss_reward_3: 0.00592
	loss_policy_4: 0.04644
	accuracy_policy_4: 0.5648
	loss_value_4: 0.0392
	loss_reward_4: 0.00627
	loss_policy_5: 0.04879
	accuracy_policy_5: 0.54105
	loss_value_5: 0.04023
	loss_reward_5: 0.00738
	loss_policy: 0.36188
	loss_value: 0.36408
	loss_reward: 0.03138
Optimization_Done 17800
[2024-05-04 13:42:45] [command] train weight_iter_17800.pkl 86 90
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 13:53:27] [command] train weight_iter_17800.pkl 86 90
[2024-05-04 13:54:06] nn step 17850, lr: 0.04.
	loss_policy_0: 0.16501
	accuracy_policy_0: 0.67445
	loss_value_0: 0.19334
	loss_policy_1: 0.0427
	accuracy_policy_1: 0.59887
	loss_value_1: 0.04023
	loss_reward_1: 0.00621
	loss_policy_2: 0.04617
	accuracy_policy_2: 0.57238
	loss_value_2: 0.04146
	loss_reward_2: 0.00587
	loss_policy_3: 0.05035
	accuracy_policy_3: 0.54645
	loss_value_3: 0.04251
	loss_reward_3: 0.00614
	loss_policy_4: 0.05363
	accuracy_policy_4: 0.52699
	loss_value_4: 0.04348
	loss_reward_4: 0.00669
	loss_policy_5: 0.05588
	accuracy_policy_5: 0.50707
	loss_value_5: 0.04436
	loss_reward_5: 0.00784
	loss_policy: 0.41374
	loss_value: 0.40539
	loss_reward: 0.03275
[2024-05-04 13:54:22] nn step 17900, lr: 0.04.
	loss_policy_0: 0.14824
	accuracy_policy_0: 0.68793
	loss_value_0: 0.17794
	loss_policy_1: 0.03945
	accuracy_policy_1: 0.61141
	loss_value_1: 0.03708
	loss_reward_1: 0.00593
	loss_policy_2: 0.04286
	accuracy_policy_2: 0.58684
	loss_value_2: 0.03853
	loss_reward_2: 0.00564
	loss_policy_3: 0.04634
	accuracy_policy_3: 0.55953
	loss_value_3: 0.03956
	loss_reward_3: 0.00585
	loss_policy_4: 0.0492
	accuracy_policy_4: 0.54109
	loss_value_4: 0.04065
	loss_reward_4: 0.00645
	loss_policy_5: 0.05196
	accuracy_policy_5: 0.51828
	loss_value_5: 0.04165
	loss_reward_5: 0.00718
	loss_policy: 0.37804
	loss_value: 0.37541
	loss_reward: 0.03105
[2024-05-04 13:54:38] nn step 17950, lr: 0.04.
	loss_policy_0: 0.15233
	accuracy_policy_0: 0.69211
	loss_value_0: 0.18731
	loss_policy_1: 0.0408
	accuracy_policy_1: 0.62012
	loss_value_1: 0.03909
	loss_reward_1: 0.00617
	loss_policy_2: 0.0444
	accuracy_policy_2: 0.58883
	loss_value_2: 0.04032
	loss_reward_2: 0.00576
	loss_policy_3: 0.04793
	accuracy_policy_3: 0.57129
	loss_value_3: 0.04143
	loss_reward_3: 0.00631
	loss_policy_4: 0.05109
	accuracy_policy_4: 0.55004
	loss_value_4: 0.04259
	loss_reward_4: 0.00669
	loss_policy_5: 0.0535
	accuracy_policy_5: 0.53195
	loss_value_5: 0.04367
	loss_reward_5: 0.00774
	loss_policy: 0.39005
	loss_value: 0.39442
	loss_reward: 0.03266
[2024-05-04 13:54:54] nn step 18000, lr: 0.04.
	loss_policy_0: 0.15654
	accuracy_policy_0: 0.70066
	loss_value_0: 0.19141
	loss_policy_1: 0.04113
	accuracy_policy_1: 0.62676
	loss_value_1: 0.03969
	loss_reward_1: 0.00624
	loss_policy_2: 0.04448
	accuracy_policy_2: 0.60137
	loss_value_2: 0.04109
	loss_reward_2: 0.00602
	loss_policy_3: 0.0483
	accuracy_policy_3: 0.57926
	loss_value_3: 0.04255
	loss_reward_3: 0.00644
	loss_policy_4: 0.05152
	accuracy_policy_4: 0.55703
	loss_value_4: 0.04385
	loss_reward_4: 0.00702
	loss_policy_5: 0.0545
	accuracy_policy_5: 0.53598
	loss_value_5: 0.04489
	loss_reward_5: 0.00776
	loss_policy: 0.39647
	loss_value: 0.40348
	loss_reward: 0.03349
Optimization_Done 18000
[2024-05-04 13:57:14] [command] train weight_iter_18000.pkl 87 91
[2024-05-04 13:57:31] nn step 18050, lr: 0.04.
	loss_policy_0: 0.16601
	accuracy_policy_0: 0.65656
	loss_value_0: 0.18838
	loss_policy_1: 0.04192
	accuracy_policy_1: 0.59207
	loss_value_1: 0.03914
	loss_reward_1: 0.00616
	loss_policy_2: 0.04577
	accuracy_policy_2: 0.56445
	loss_value_2: 0.04034
	loss_reward_2: 0.00582
	loss_policy_3: 0.04883
	accuracy_policy_3: 0.54648
	loss_value_3: 0.04149
	loss_reward_3: 0.00614
	loss_policy_4: 0.0511
	accuracy_policy_4: 0.52902
	loss_value_4: 0.04258
	loss_reward_4: 0.00673
	loss_policy_5: 0.05349
	accuracy_policy_5: 0.50855
	loss_value_5: 0.04362
	loss_reward_5: 0.00765
	loss_policy: 0.40712
	loss_value: 0.39556
	loss_reward: 0.03251
[2024-05-04 13:57:47] nn step 18100, lr: 0.04.
	loss_policy_0: 0.16165
	accuracy_policy_0: 0.67957
	loss_value_0: 0.19535
	loss_policy_1: 0.04227
	accuracy_policy_1: 0.60672
	loss_value_1: 0.04063
	loss_reward_1: 0.00652
	loss_policy_2: 0.04543
	accuracy_policy_2: 0.585
	loss_value_2: 0.04206
	loss_reward_2: 0.00592
	loss_policy_3: 0.04921
	accuracy_policy_3: 0.55977
	loss_value_3: 0.04326
	loss_reward_3: 0.00639
	loss_policy_4: 0.05237
	accuracy_policy_4: 0.53539
	loss_value_4: 0.04442
	loss_reward_4: 0.00701
	loss_policy_5: 0.05499
	accuracy_policy_5: 0.51707
	loss_value_5: 0.04551
	loss_reward_5: 0.00783
	loss_policy: 0.40591
	loss_value: 0.41123
	loss_reward: 0.03368
[2024-05-04 13:58:04] nn step 18150, lr: 0.04.
	loss_policy_0: 0.14849
	accuracy_policy_0: 0.68418
	loss_value_0: 0.18293
	loss_policy_1: 0.03876
	accuracy_policy_1: 0.61324
	loss_value_1: 0.038
	loss_reward_1: 0.00624
	loss_policy_2: 0.04189
	accuracy_policy_2: 0.59461
	loss_value_2: 0.03936
	loss_reward_2: 0.00551
	loss_policy_3: 0.04567
	accuracy_policy_3: 0.56266
	loss_value_3: 0.04029
	loss_reward_3: 0.00596
	loss_policy_4: 0.04818
	accuracy_policy_4: 0.54926
	loss_value_4: 0.0414
	loss_reward_4: 0.00658
	loss_policy_5: 0.05057
	accuracy_policy_5: 0.52051
	loss_value_5: 0.04257
	loss_reward_5: 0.00763
	loss_policy: 0.37358
	loss_value: 0.38455
	loss_reward: 0.03191
[2024-05-04 13:58:20] nn step 18200, lr: 0.04.
	loss_policy_0: 0.14376
	accuracy_policy_0: 0.6918
	loss_value_0: 0.18304
	loss_policy_1: 0.03822
	accuracy_policy_1: 0.62012
	loss_value_1: 0.03817
	loss_reward_1: 0.00609
	loss_policy_2: 0.04173
	accuracy_policy_2: 0.59473
	loss_value_2: 0.03922
	loss_reward_2: 0.00563
	loss_policy_3: 0.04447
	accuracy_policy_3: 0.57328
	loss_value_3: 0.04036
	loss_reward_3: 0.00606
	loss_policy_4: 0.04758
	accuracy_policy_4: 0.55379
	loss_value_4: 0.04127
	loss_reward_4: 0.00655
	loss_policy_5: 0.05009
	accuracy_policy_5: 0.53492
	loss_value_5: 0.04227
	loss_reward_5: 0.00778
	loss_policy: 0.36586
	loss_value: 0.38434
	loss_reward: 0.03211
Optimization_Done 18200
[2024-05-04 14:00:28] [command] train weight_iter_18200.pkl 88 92
[2024-05-04 14:00:47] nn step 18250, lr: 0.04.
	loss_policy_0: 0.15043
	accuracy_policy_0: 0.66961
	loss_value_0: 0.17145
	loss_policy_1: 0.03844
	accuracy_policy_1: 0.60344
	loss_value_1: 0.03568
	loss_reward_1: 0.00568
	loss_policy_2: 0.04145
	accuracy_policy_2: 0.57641
	loss_value_2: 0.03701
	loss_reward_2: 0.00517
	loss_policy_3: 0.04453
	accuracy_policy_3: 0.55379
	loss_value_3: 0.03799
	loss_reward_3: 0.00558
	loss_policy_4: 0.04717
	accuracy_policy_4: 0.53375
	loss_value_4: 0.0392
	loss_reward_4: 0.00614
	loss_policy_5: 0.04963
	accuracy_policy_5: 0.51348
	loss_value_5: 0.04022
	loss_reward_5: 0.00724
	loss_policy: 0.37165
	loss_value: 0.36155
	loss_reward: 0.0298
[2024-05-04 14:01:03] nn step 18300, lr: 0.04.
	loss_policy_0: 0.14485
	accuracy_policy_0: 0.68086
	loss_value_0: 0.17282
	loss_policy_1: 0.03725
	accuracy_policy_1: 0.61469
	loss_value_1: 0.03616
	loss_reward_1: 0.00566
	loss_policy_2: 0.04037
	accuracy_policy_2: 0.5927
	loss_value_2: 0.03739
	loss_reward_2: 0.00541
	loss_policy_3: 0.04331
	accuracy_policy_3: 0.5673
	loss_value_3: 0.03858
	loss_reward_3: 0.00574
	loss_policy_4: 0.04604
	accuracy_policy_4: 0.54785
	loss_value_4: 0.03964
	loss_reward_4: 0.00648
	loss_policy_5: 0.04858
	accuracy_policy_5: 0.52934
	loss_value_5: 0.04074
	loss_reward_5: 0.00748
	loss_policy: 0.3604
	loss_value: 0.36532
	loss_reward: 0.03077
[2024-05-04 14:01:19] nn step 18350, lr: 0.04.
	loss_policy_0: 0.14904
	accuracy_policy_0: 0.69043
	loss_value_0: 0.18289
	loss_policy_1: 0.03854
	accuracy_policy_1: 0.62023
	loss_value_1: 0.03802
	loss_reward_1: 0.00596
	loss_policy_2: 0.04243
	accuracy_policy_2: 0.59258
	loss_value_2: 0.03916
	loss_reward_2: 0.0056
	loss_policy_3: 0.04551
	accuracy_policy_3: 0.57641
	loss_value_3: 0.0404
	loss_reward_3: 0.00602
	loss_policy_4: 0.04894
	accuracy_policy_4: 0.55074
	loss_value_4: 0.04146
	loss_reward_4: 0.00693
	loss_policy_5: 0.0514
	accuracy_policy_5: 0.53109
	loss_value_5: 0.04272
	loss_reward_5: 0.00791
	loss_policy: 0.37585
	loss_value: 0.38465
	loss_reward: 0.03242
[2024-05-04 14:01:35] nn step 18400, lr: 0.04.
	loss_policy_0: 0.1467
	accuracy_policy_0: 0.69234
	loss_value_0: 0.18417
	loss_policy_1: 0.03872
	accuracy_policy_1: 0.61594
	loss_value_1: 0.03845
	loss_reward_1: 0.00599
	loss_policy_2: 0.04208
	accuracy_policy_2: 0.60305
	loss_value_2: 0.0397
	loss_reward_2: 0.00566
	loss_policy_3: 0.04606
	accuracy_policy_3: 0.57227
	loss_value_3: 0.04076
	loss_reward_3: 0.00611
	loss_policy_4: 0.04851
	accuracy_policy_4: 0.555
	loss_value_4: 0.04188
	loss_reward_4: 0.0068
	loss_policy_5: 0.05119
	accuracy_policy_5: 0.53473
	loss_value_5: 0.04287
	loss_reward_5: 0.00811
	loss_policy: 0.37326
	loss_value: 0.38782
	loss_reward: 0.03269
Optimization_Done 18400
[2024-05-04 14:03:11] [command] train weight_iter_18400.pkl 89 93
[2024-05-04 14:03:29] nn step 18450, lr: 0.04.
	loss_policy_0: 0.15254
	accuracy_policy_0: 0.68332
	loss_value_0: 0.17709
	loss_policy_1: 0.03979
	accuracy_policy_1: 0.61258
	loss_value_1: 0.0371
	loss_reward_1: 0.00581
	loss_policy_2: 0.04321
	accuracy_policy_2: 0.58969
	loss_value_2: 0.03858
	loss_reward_2: 0.00534
	loss_policy_3: 0.04655
	accuracy_policy_3: 0.56293
	loss_value_3: 0.03977
	loss_reward_3: 0.00577
	loss_policy_4: 0.04936
	accuracy_policy_4: 0.54512
	loss_value_4: 0.04094
	loss_reward_4: 0.00627
	loss_policy_5: 0.05244
	accuracy_policy_5: 0.51871
	loss_value_5: 0.04213
	loss_reward_5: 0.00754
	loss_policy: 0.38389
	loss_value: 0.3756
	loss_reward: 0.03072
[2024-05-04 14:03:45] nn step 18500, lr: 0.04.
	loss_policy_0: 0.14768
	accuracy_policy_0: 0.69676
	loss_value_0: 0.17935
	loss_policy_1: 0.03918
	accuracy_policy_1: 0.62785
	loss_value_1: 0.03763
	loss_reward_1: 0.00599
	loss_policy_2: 0.04312
	accuracy_policy_2: 0.5973
	loss_value_2: 0.03901
	loss_reward_2: 0.00557
	loss_policy_3: 0.04681
	accuracy_policy_3: 0.57098
	loss_value_3: 0.04032
	loss_reward_3: 0.00577
	loss_policy_4: 0.0491
	accuracy_policy_4: 0.5607
	loss_value_4: 0.04136
	loss_reward_4: 0.00632
	loss_policy_5: 0.05192
	accuracy_policy_5: 0.53961
	loss_value_5: 0.04251
	loss_reward_5: 0.00756
	loss_policy: 0.3778
	loss_value: 0.38018
	loss_reward: 0.03122
[2024-05-04 14:04:02] nn step 18550, lr: 0.04.
	loss_policy_0: 0.13756
	accuracy_policy_0: 0.70668
	loss_value_0: 0.17266
	loss_policy_1: 0.03737
	accuracy_policy_1: 0.62742
	loss_value_1: 0.03634
	loss_reward_1: 0.0059
	loss_policy_2: 0.04146
	accuracy_policy_2: 0.59984
	loss_value_2: 0.03746
	loss_reward_2: 0.00513
	loss_policy_3: 0.0448
	accuracy_policy_3: 0.57371
	loss_value_3: 0.03865
	loss_reward_3: 0.00555
	loss_policy_4: 0.04729
	accuracy_policy_4: 0.55363
	loss_value_4: 0.03972
	loss_reward_4: 0.00631
	loss_policy_5: 0.04994
	accuracy_policy_5: 0.54043
	loss_value_5: 0.04095
	loss_reward_5: 0.0074
	loss_policy: 0.35843
	loss_value: 0.36579
	loss_reward: 0.03029
[2024-05-04 14:04:18] nn step 18600, lr: 0.04.
	loss_policy_0: 0.12788
	accuracy_policy_0: 0.71895
	loss_value_0: 0.16632
	loss_policy_1: 0.03543
	accuracy_policy_1: 0.63543
	loss_value_1: 0.0349
	loss_reward_1: 0.00543
	loss_policy_2: 0.03929
	accuracy_policy_2: 0.60645
	loss_value_2: 0.0361
	loss_reward_2: 0.00496
	loss_policy_3: 0.04242
	accuracy_policy_3: 0.5873
	loss_value_3: 0.0372
	loss_reward_3: 0.00561
	loss_policy_4: 0.04518
	accuracy_policy_4: 0.56383
	loss_value_4: 0.03836
	loss_reward_4: 0.00612
	loss_policy_5: 0.04746
	accuracy_policy_5: 0.54473
	loss_value_5: 0.03946
	loss_reward_5: 0.00725
	loss_policy: 0.33766
	loss_value: 0.35234
	loss_reward: 0.02936
Optimization_Done 18600
[2024-05-04 14:06:26] [command] train weight_iter_18600.pkl 90 94
[2024-05-04 14:06:44] nn step 18650, lr: 0.04.
	loss_policy_0: 0.16959
	accuracy_policy_0: 0.66398
	loss_value_0: 0.17985
	loss_policy_1: 0.0435
	accuracy_policy_1: 0.59602
	loss_value_1: 0.03757
	loss_reward_1: 0.0056
	loss_policy_2: 0.0476
	accuracy_policy_2: 0.56227
	loss_value_2: 0.03897
	loss_reward_2: 0.00478
	loss_policy_3: 0.05129
	accuracy_policy_3: 0.54152
	loss_value_3: 0.04014
	loss_reward_3: 0.00521
	loss_policy_4: 0.05378
	accuracy_policy_4: 0.52426
	loss_value_4: 0.04126
	loss_reward_4: 0.00602
	loss_policy_5: 0.0569
	accuracy_policy_5: 0.4991
	loss_value_5: 0.04253
	loss_reward_5: 0.00698
	loss_policy: 0.42265
	loss_value: 0.38032
	loss_reward: 0.02858
[2024-05-04 14:07:00] nn step 18700, lr: 0.04.
	loss_policy_0: 0.15016
	accuracy_policy_0: 0.69152
	loss_value_0: 0.17367
	loss_policy_1: 0.04101
	accuracy_policy_1: 0.61008
	loss_value_1: 0.03643
	loss_reward_1: 0.00544
	loss_policy_2: 0.04496
	accuracy_policy_2: 0.58461
	loss_value_2: 0.03802
	loss_reward_2: 0.00497
	loss_policy_3: 0.04862
	accuracy_policy_3: 0.55488
	loss_value_3: 0.03937
	loss_reward_3: 0.00531
	loss_policy_4: 0.05191
	accuracy_policy_4: 0.53426
	loss_value_4: 0.04048
	loss_reward_4: 0.00607
	loss_policy_5: 0.0543
	accuracy_policy_5: 0.51699
	loss_value_5: 0.04169
	loss_reward_5: 0.00709
	loss_policy: 0.39095
	loss_value: 0.36965
	loss_reward: 0.02889
[2024-05-04 14:07:16] nn step 18750, lr: 0.04.
	loss_policy_0: 0.13853
	accuracy_policy_0: 0.70887
	loss_value_0: 0.16529
	loss_policy_1: 0.0376
	accuracy_policy_1: 0.62293
	loss_value_1: 0.03483
	loss_reward_1: 0.00542
	loss_policy_2: 0.04191
	accuracy_policy_2: 0.59469
	loss_value_2: 0.03631
	loss_reward_2: 0.00484
	loss_policy_3: 0.04523
	accuracy_policy_3: 0.575
	loss_value_3: 0.03771
	loss_reward_3: 0.00517
	loss_policy_4: 0.04807
	accuracy_policy_4: 0.55629
	loss_value_4: 0.03872
	loss_reward_4: 0.0057
	loss_policy_5: 0.05074
	accuracy_policy_5: 0.53469
	loss_value_5: 0.03988
	loss_reward_5: 0.00667
	loss_policy: 0.36208
	loss_value: 0.35275
	loss_reward: 0.02781
[2024-05-04 14:07:32] nn step 18800, lr: 0.04.
	loss_policy_0: 0.14318
	accuracy_policy_0: 0.70719
	loss_value_0: 0.17365
	loss_policy_1: 0.03973
	accuracy_policy_1: 0.62687
	loss_value_1: 0.03644
	loss_reward_1: 0.00566
	loss_policy_2: 0.04344
	accuracy_policy_2: 0.59926
	loss_value_2: 0.03791
	loss_reward_2: 0.00512
	loss_policy_3: 0.04735
	accuracy_policy_3: 0.57363
	loss_value_3: 0.03919
	loss_reward_3: 0.00544
	loss_policy_4: 0.05113
	accuracy_policy_4: 0.55164
	loss_value_4: 0.04022
	loss_reward_4: 0.00598
	loss_policy_5: 0.05331
	accuracy_policy_5: 0.53422
	loss_value_5: 0.04145
	loss_reward_5: 0.00725
	loss_policy: 0.37813
	loss_value: 0.36886
	loss_reward: 0.02944
Optimization_Done 18800
[2024-05-04 14:09:40] [command] train weight_iter_18800.pkl 91 95
[2024-05-04 14:09:58] nn step 18850, lr: 0.04.
	loss_policy_0: 0.1604
	accuracy_policy_0: 0.66402
	loss_value_0: 0.15844
	loss_policy_1: 0.04011
	accuracy_policy_1: 0.59602
	loss_value_1: 0.03324
	loss_reward_1: 0.00573
	loss_policy_2: 0.04411
	accuracy_policy_2: 0.56875
	loss_value_2: 0.03468
	loss_reward_2: 0.00498
	loss_policy_3: 0.04713
	accuracy_policy_3: 0.54512
	loss_value_3: 0.03605
	loss_reward_3: 0.00538
	loss_policy_4: 0.04933
	accuracy_policy_4: 0.53125
	loss_value_4: 0.03711
	loss_reward_4: 0.00594
	loss_policy_5: 0.05145
	accuracy_policy_5: 0.51313
	loss_value_5: 0.0382
	loss_reward_5: 0.00676
	loss_policy: 0.39253
	loss_value: 0.33773
	loss_reward: 0.0288
[2024-05-04 14:10:14] nn step 18900, lr: 0.04.
	loss_policy_0: 0.15249
	accuracy_policy_0: 0.69734
	loss_value_0: 0.16744
	loss_policy_1: 0.04086
	accuracy_policy_1: 0.61738
	loss_value_1: 0.0353
	loss_reward_1: 0.00583
	loss_policy_2: 0.04503
	accuracy_policy_2: 0.58578
	loss_value_2: 0.03673
	loss_reward_2: 0.00524
	loss_policy_3: 0.04814
	accuracy_policy_3: 0.56324
	loss_value_3: 0.03788
	loss_reward_3: 0.00554
	loss_policy_4: 0.0511
	accuracy_policy_4: 0.54676
	loss_value_4: 0.03922
	loss_reward_4: 0.00619
	loss_policy_5: 0.05343
	accuracy_policy_5: 0.53668
	loss_value_5: 0.04046
	loss_reward_5: 0.00739
	loss_policy: 0.39105
	loss_value: 0.35703
	loss_reward: 0.03019
[2024-05-04 14:10:30] nn step 18950, lr: 0.04.
	loss_policy_0: 0.1437
	accuracy_policy_0: 0.69676
	loss_value_0: 0.15904
	loss_policy_1: 0.03846
	accuracy_policy_1: 0.61777
	loss_value_1: 0.03355
	loss_reward_1: 0.00563
	loss_policy_2: 0.04272
	accuracy_policy_2: 0.58883
	loss_value_2: 0.03498
	loss_reward_2: 0.0049
	loss_policy_3: 0.0457
	accuracy_policy_3: 0.57074
	loss_value_3: 0.03612
	loss_reward_3: 0.00538
	loss_policy_4: 0.04819
	accuracy_policy_4: 0.55547
	loss_value_4: 0.03724
	loss_reward_4: 0.00594
	loss_policy_5: 0.05039
	accuracy_policy_5: 0.53434
	loss_value_5: 0.03838
	loss_reward_5: 0.00696
	loss_policy: 0.36915
	loss_value: 0.33931
	loss_reward: 0.02881
[2024-05-04 14:10:47] nn step 19000, lr: 0.04.
	loss_policy_0: 0.14779
	accuracy_policy_0: 0.70012
	loss_value_0: 0.16541
	loss_policy_1: 0.03957
	accuracy_policy_1: 0.62406
	loss_value_1: 0.0347
	loss_reward_1: 0.00598
	loss_policy_2: 0.04316
	accuracy_policy_2: 0.59785
	loss_value_2: 0.03604
	loss_reward_2: 0.00498
	loss_policy_3: 0.04679
	accuracy_policy_3: 0.57586
	loss_value_3: 0.03715
	loss_reward_3: 0.00537
	loss_policy_4: 0.04988
	accuracy_policy_4: 0.55418
	loss_value_4: 0.03833
	loss_reward_4: 0.00599
	loss_policy_5: 0.05177
	accuracy_policy_5: 0.54176
	loss_value_5: 0.03955
	loss_reward_5: 0.00735
	loss_policy: 0.37897
	loss_value: 0.35117
	loss_reward: 0.02967
Optimization_Done 19000
[2024-05-04 14:12:55] [command] train weight_iter_19000.pkl 92 96
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 14:16:53] [command] train weight_iter_19000.pkl 93 96
[2024-05-04 14:17:23] nn step 19050, lr: 0.04.
	loss_policy_0: 0.17227
	accuracy_policy_0: 0.65742
	loss_value_0: 0.17664
	loss_policy_1: 0.04409
	accuracy_policy_1: 0.5852
	loss_value_1: 0.03674
	loss_reward_1: 0.00473
	loss_policy_2: 0.04854
	accuracy_policy_2: 0.54922
	loss_value_2: 0.03832
	loss_reward_2: 0.00454
	loss_policy_3: 0.05187
	accuracy_policy_3: 0.52672
	loss_value_3: 0.03932
	loss_reward_3: 0.0049
	loss_policy_4: 0.05485
	accuracy_policy_4: 0.50617
	loss_value_4: 0.04038
	loss_reward_4: 0.00536
	loss_policy_5: 0.05726
	accuracy_policy_5: 0.48984
	loss_value_5: 0.04147
	loss_reward_5: 0.00624
	loss_policy: 0.42888
	loss_value: 0.37287
	loss_reward: 0.02578
[2024-05-04 14:17:40] nn step 19100, lr: 0.04.
	loss_policy_0: 0.16497
	accuracy_policy_0: 0.67426
	loss_value_0: 0.17341
	loss_policy_1: 0.04397
	accuracy_policy_1: 0.59438
	loss_value_1: 0.03654
	loss_reward_1: 0.00504
	loss_policy_2: 0.04777
	accuracy_policy_2: 0.57309
	loss_value_2: 0.03785
	loss_reward_2: 0.00484
	loss_policy_3: 0.05179
	accuracy_policy_3: 0.54605
	loss_value_3: 0.03893
	loss_reward_3: 0.00518
	loss_policy_4: 0.05533
	accuracy_policy_4: 0.52488
	loss_value_4: 0.04024
	loss_reward_4: 0.00559
	loss_policy_5: 0.05825
	accuracy_policy_5: 0.50086
	loss_value_5: 0.0414
	loss_reward_5: 0.00649
	loss_policy: 0.42209
	loss_value: 0.36837
	loss_reward: 0.02714
[2024-05-04 14:17:56] nn step 19150, lr: 0.04.
	loss_policy_0: 0.15426
	accuracy_policy_0: 0.68855
	loss_value_0: 0.16358
	loss_policy_1: 0.04184
	accuracy_policy_1: 0.60652
	loss_value_1: 0.03456
	loss_reward_1: 0.00495
	loss_policy_2: 0.04603
	accuracy_policy_2: 0.57273
	loss_value_2: 0.03598
	loss_reward_2: 0.00472
	loss_policy_3: 0.04923
	accuracy_policy_3: 0.5523
	loss_value_3: 0.03726
	loss_reward_3: 0.00484
	loss_policy_4: 0.05338
	accuracy_policy_4: 0.52777
	loss_value_4: 0.03846
	loss_reward_4: 0.00543
	loss_policy_5: 0.05519
	accuracy_policy_5: 0.50992
	loss_value_5: 0.03952
	loss_reward_5: 0.00655
	loss_policy: 0.39992
	loss_value: 0.34936
	loss_reward: 0.02649
[2024-05-04 14:18:12] nn step 19200, lr: 0.04.
	loss_policy_0: 0.14875
	accuracy_policy_0: 0.69762
	loss_value_0: 0.15965
	loss_policy_1: 0.04088
	accuracy_policy_1: 0.6075
	loss_value_1: 0.03354
	loss_reward_1: 0.00488
	loss_policy_2: 0.04476
	accuracy_policy_2: 0.58207
	loss_value_2: 0.03501
	loss_reward_2: 0.00455
	loss_policy_3: 0.04829
	accuracy_policy_3: 0.55891
	loss_value_3: 0.03626
	loss_reward_3: 0.00484
	loss_policy_4: 0.05198
	accuracy_policy_4: 0.53293
	loss_value_4: 0.0374
	loss_reward_4: 0.00536
	loss_policy_5: 0.05446
	accuracy_policy_5: 0.51699
	loss_value_5: 0.03845
	loss_reward_5: 0.00627
	loss_policy: 0.38911
	loss_value: 0.3403
	loss_reward: 0.02589
Optimization_Done 19200
[2024-05-04 14:20:17] [command] train weight_iter_19200.pkl 94 97
[2024-05-04 14:20:35] nn step 19250, lr: 0.04.
	loss_policy_0: 0.17298
	accuracy_policy_0: 0.65758
	loss_value_0: 0.17005
	loss_policy_1: 0.04362
	accuracy_policy_1: 0.58879
	loss_value_1: 0.03574
	loss_reward_1: 0.0052
	loss_policy_2: 0.04739
	accuracy_policy_2: 0.56094
	loss_value_2: 0.03732
	loss_reward_2: 0.00512
	loss_policy_3: 0.05123
	accuracy_policy_3: 0.54043
	loss_value_3: 0.03859
	loss_reward_3: 0.0055
	loss_policy_4: 0.05422
	accuracy_policy_4: 0.5159
	loss_value_4: 0.0397
	loss_reward_4: 0.00594
	loss_policy_5: 0.05769
	accuracy_policy_5: 0.49578
	loss_value_5: 0.04104
	loss_reward_5: 0.00696
	loss_policy: 0.42712
	loss_value: 0.36244
	loss_reward: 0.02871
[2024-05-04 14:20:51] nn step 19300, lr: 0.04.
	loss_policy_0: 0.14758
	accuracy_policy_0: 0.6898
	loss_value_0: 0.16417
	loss_policy_1: 0.03957
	accuracy_policy_1: 0.6134
	loss_value_1: 0.03439
	loss_reward_1: 0.00511
	loss_policy_2: 0.04387
	accuracy_policy_2: 0.58246
	loss_value_2: 0.0358
	loss_reward_2: 0.00471
	loss_policy_3: 0.04798
	accuracy_policy_3: 0.5532
	loss_value_3: 0.03704
	loss_reward_3: 0.00519
	loss_policy_4: 0.05049
	accuracy_policy_4: 0.54023
	loss_value_4: 0.03813
	loss_reward_4: 0.00581
	loss_policy_5: 0.05316
	accuracy_policy_5: 0.52059
	loss_value_5: 0.03915
	loss_reward_5: 0.00664
	loss_policy: 0.38265
	loss_value: 0.34869
	loss_reward: 0.02745
[2024-05-04 14:21:08] nn step 19350, lr: 0.04.
	loss_policy_0: 0.13591
	accuracy_policy_0: 0.70266
	loss_value_0: 0.15366
	loss_policy_1: 0.03687
	accuracy_policy_1: 0.61785
	loss_value_1: 0.03235
	loss_reward_1: 0.00504
	loss_policy_2: 0.04054
	accuracy_policy_2: 0.59227
	loss_value_2: 0.03361
	loss_reward_2: 0.00459
	loss_policy_3: 0.04414
	accuracy_policy_3: 0.56621
	loss_value_3: 0.03468
	loss_reward_3: 0.00504
	loss_policy_4: 0.04729
	accuracy_policy_4: 0.54188
	loss_value_4: 0.03583
	loss_reward_4: 0.00548
	loss_policy_5: 0.04975
	accuracy_policy_5: 0.52547
	loss_value_5: 0.03709
	loss_reward_5: 0.00636
	loss_policy: 0.35451
	loss_value: 0.32722
	loss_reward: 0.02651
[2024-05-04 14:21:24] nn step 19400, lr: 0.04.
	loss_policy_0: 0.14224
	accuracy_policy_0: 0.71246
	loss_value_0: 0.16571
	loss_policy_1: 0.03949
	accuracy_policy_1: 0.62387
	loss_value_1: 0.03467
	loss_reward_1: 0.00524
	loss_policy_2: 0.04385
	accuracy_policy_2: 0.60086
	loss_value_2: 0.03629
	loss_reward_2: 0.00482
	loss_policy_3: 0.04738
	accuracy_policy_3: 0.5741
	loss_value_3: 0.03761
	loss_reward_3: 0.00532
	loss_policy_4: 0.051
	accuracy_policy_4: 0.55059
	loss_value_4: 0.0389
	loss_reward_4: 0.00605
	loss_policy_5: 0.05418
	accuracy_policy_5: 0.53023
	loss_value_5: 0.04007
	loss_reward_5: 0.0071
	loss_policy: 0.37814
	loss_value: 0.35325
	loss_reward: 0.02852
Optimization_Done 19400
[2024-05-04 14:23:27] [command] train weight_iter_19400.pkl 95 98
[2024-05-04 14:23:45] nn step 19450, lr: 0.04.
	loss_policy_0: 0.15322
	accuracy_policy_0: 0.66141
	loss_value_0: 0.1534
	loss_policy_1: 0.03912
	accuracy_policy_1: 0.59324
	loss_value_1: 0.03233
	loss_reward_1: 0.00574
	loss_policy_2: 0.04272
	accuracy_policy_2: 0.57551
	loss_value_2: 0.03371
	loss_reward_2: 0.00514
	loss_policy_3: 0.04614
	accuracy_policy_3: 0.55559
	loss_value_3: 0.03476
	loss_reward_3: 0.00582
	loss_policy_4: 0.04911
	accuracy_policy_4: 0.53328
	loss_value_4: 0.03603
	loss_reward_4: 0.00635
	loss_policy_5: 0.05168
	accuracy_policy_5: 0.51445
	loss_value_5: 0.03719
	loss_reward_5: 0.0073
	loss_policy: 0.38198
	loss_value: 0.32742
	loss_reward: 0.03035
[2024-05-04 14:24:01] nn step 19500, lr: 0.04.
	loss_policy_0: 0.12769
	accuracy_policy_0: 0.70066
	loss_value_0: 0.1417
	loss_policy_1: 0.03459
	accuracy_policy_1: 0.62277
	loss_value_1: 0.0297
	loss_reward_1: 0.0055
	loss_policy_2: 0.03816
	accuracy_policy_2: 0.59395
	loss_value_2: 0.03108
	loss_reward_2: 0.0049
	loss_policy_3: 0.04142
	accuracy_policy_3: 0.57449
	loss_value_3: 0.03216
	loss_reward_3: 0.00542
	loss_policy_4: 0.04487
	accuracy_policy_4: 0.54699
	loss_value_4: 0.03343
	loss_reward_4: 0.00591
	loss_policy_5: 0.04736
	accuracy_policy_5: 0.53266
	loss_value_5: 0.03467
	loss_reward_5: 0.00679
	loss_policy: 0.33408
	loss_value: 0.30274
	loss_reward: 0.02852
[2024-05-04 14:24:17] nn step 19550, lr: 0.04.
	loss_policy_0: 0.14192
	accuracy_policy_0: 0.71094
	loss_value_0: 0.16335
	loss_policy_1: 0.03919
	accuracy_policy_1: 0.62949
	loss_value_1: 0.03436
	loss_reward_1: 0.00615
	loss_policy_2: 0.04347
	accuracy_policy_2: 0.59977
	loss_value_2: 0.03606
	loss_reward_2: 0.00576
	loss_policy_3: 0.047
	accuracy_policy_3: 0.57523
	loss_value_3: 0.03731
	loss_reward_3: 0.00618
	loss_policy_4: 0.05105
	accuracy_policy_4: 0.55176
	loss_value_4: 0.03855
	loss_reward_4: 0.0068
	loss_policy_5: 0.05389
	accuracy_policy_5: 0.53184
	loss_value_5: 0.03976
	loss_reward_5: 0.00819
	loss_policy: 0.37652
	loss_value: 0.3494
	loss_reward: 0.03308
[2024-05-04 14:24:33] nn step 19600, lr: 0.04.
	loss_policy_0: 0.14247
	accuracy_policy_0: 0.70602
	loss_value_0: 0.16292
	loss_policy_1: 0.03904
	accuracy_policy_1: 0.62457
	loss_value_1: 0.03431
	loss_reward_1: 0.0062
	loss_policy_2: 0.04282
	accuracy_policy_2: 0.60348
	loss_value_2: 0.03577
	loss_reward_2: 0.00576
	loss_policy_3: 0.04629
	accuracy_policy_3: 0.58395
	loss_value_3: 0.037
	loss_reward_3: 0.0062
	loss_policy_4: 0.0502
	accuracy_policy_4: 0.55742
	loss_value_4: 0.03838
	loss_reward_4: 0.00701
	loss_policy_5: 0.05377
	accuracy_policy_5: 0.52895
	loss_value_5: 0.03972
	loss_reward_5: 0.00809
	loss_policy: 0.37458
	loss_value: 0.34809
	loss_reward: 0.03325
Optimization_Done 19600
[2024-05-04 14:26:36] [command] train weight_iter_19600.pkl 96 99
[2024-05-04 14:26:54] nn step 19650, lr: 0.04.
	loss_policy_0: 0.14476
	accuracy_policy_0: 0.6775
	loss_value_0: 0.14572
	loss_policy_1: 0.03784
	accuracy_policy_1: 0.60008
	loss_value_1: 0.0305
	loss_reward_1: 0.00502
	loss_policy_2: 0.04152
	accuracy_policy_2: 0.57746
	loss_value_2: 0.03176
	loss_reward_2: 0.00453
	loss_policy_3: 0.04435
	accuracy_policy_3: 0.55305
	loss_value_3: 0.03311
	loss_reward_3: 0.00512
	loss_policy_4: 0.04782
	accuracy_policy_4: 0.5298
	loss_value_4: 0.03433
	loss_reward_4: 0.00549
	loss_policy_5: 0.0501
	accuracy_policy_5: 0.51234
	loss_value_5: 0.03553
	loss_reward_5: 0.00636
	loss_policy: 0.36638
	loss_value: 0.31096
	loss_reward: 0.02651
[2024-05-04 14:27:10] nn step 19700, lr: 0.04.
	loss_policy_0: 0.13569
	accuracy_policy_0: 0.70945
	loss_value_0: 0.14947
	loss_policy_1: 0.0366
	accuracy_policy_1: 0.63023
	loss_value_1: 0.03147
	loss_reward_1: 0.00518
	loss_policy_2: 0.04087
	accuracy_policy_2: 0.59598
	loss_value_2: 0.03276
	loss_reward_2: 0.00461
	loss_policy_3: 0.04441
	accuracy_policy_3: 0.57512
	loss_value_3: 0.03402
	loss_reward_3: 0.00506
	loss_policy_4: 0.04746
	accuracy_policy_4: 0.55387
	loss_value_4: 0.03529
	loss_reward_4: 0.00585
	loss_policy_5: 0.04998
	accuracy_policy_5: 0.53766
	loss_value_5: 0.03644
	loss_reward_5: 0.00697
	loss_policy: 0.35502
	loss_value: 0.31945
	loss_reward: 0.02767
[2024-05-04 14:27:28] nn step 19750, lr: 0.04.
	loss_policy_0: 0.12807
	accuracy_policy_0: 0.71074
	loss_value_0: 0.14321
	loss_policy_1: 0.03505
	accuracy_policy_1: 0.62914
	loss_value_1: 0.0302
	loss_reward_1: 0.00506
	loss_policy_2: 0.03853
	accuracy_policy_2: 0.6041
	loss_value_2: 0.03145
	loss_reward_2: 0.00454
	loss_policy_3: 0.04256
	accuracy_policy_3: 0.57207
	loss_value_3: 0.03265
	loss_reward_3: 0.00486
	loss_policy_4: 0.04483
	accuracy_policy_4: 0.56063
	loss_value_4: 0.03382
	loss_reward_4: 0.00566
	loss_policy_5: 0.04822
	accuracy_policy_5: 0.53098
	loss_value_5: 0.03497
	loss_reward_5: 0.00673
	loss_policy: 0.33726
	loss_value: 0.3063
	loss_reward: 0.02684
[2024-05-04 14:27:44] nn step 19800, lr: 0.04.
	loss_policy_0: 0.12505
	accuracy_policy_0: 0.71918
	loss_value_0: 0.14393
	loss_policy_1: 0.0347
	accuracy_policy_1: 0.6375
	loss_value_1: 0.0304
	loss_reward_1: 0.00499
	loss_policy_2: 0.03877
	accuracy_policy_2: 0.59973
	loss_value_2: 0.03169
	loss_reward_2: 0.00469
	loss_policy_3: 0.04219
	accuracy_policy_3: 0.57332
	loss_value_3: 0.033
	loss_reward_3: 0.00504
	loss_policy_4: 0.04473
	accuracy_policy_4: 0.56129
	loss_value_4: 0.03394
	loss_reward_4: 0.00547
	loss_policy_5: 0.04726
	accuracy_policy_5: 0.53629
	loss_value_5: 0.0351
	loss_reward_5: 0.00675
	loss_policy: 0.3327
	loss_value: 0.30806
	loss_reward: 0.02693
Optimization_Done 19800
[2024-05-04 14:29:51] [command] train weight_iter_19800.pkl 97 100
[2024-05-04 14:30:09] nn step 19850, lr: 0.04.
	loss_policy_0: 0.14345
	accuracy_policy_0: 0.67137
	loss_value_0: 0.13914
	loss_policy_1: 0.03749
	accuracy_policy_1: 0.59578
	loss_value_1: 0.02922
	loss_reward_1: 0.00496
	loss_policy_2: 0.04136
	accuracy_policy_2: 0.56742
	loss_value_2: 0.03054
	loss_reward_2: 0.00468
	loss_policy_3: 0.04403
	accuracy_policy_3: 0.54848
	loss_value_3: 0.03177
	loss_reward_3: 0.00511
	loss_policy_4: 0.04667
	accuracy_policy_4: 0.52707
	loss_value_4: 0.03293
	loss_reward_4: 0.00554
	loss_policy_5: 0.04928
	accuracy_policy_5: 0.50672
	loss_value_5: 0.03401
	loss_reward_5: 0.00671
	loss_policy: 0.36228
	loss_value: 0.2976
	loss_reward: 0.02698
[2024-05-04 14:30:25] nn step 19900, lr: 0.04.
	loss_policy_0: 0.13086
	accuracy_policy_0: 0.69668
	loss_value_0: 0.13575
	loss_policy_1: 0.03544
	accuracy_policy_1: 0.61906
	loss_value_1: 0.02857
	loss_reward_1: 0.00509
	loss_policy_2: 0.03923
	accuracy_policy_2: 0.58277
	loss_value_2: 0.0301
	loss_reward_2: 0.00459
	loss_policy_3: 0.04242
	accuracy_policy_3: 0.56008
	loss_value_3: 0.03138
	loss_reward_3: 0.00493
	loss_policy_4: 0.04482
	accuracy_policy_4: 0.54254
	loss_value_4: 0.03257
	loss_reward_4: 0.00544
	loss_policy_5: 0.048
	accuracy_policy_5: 0.51863
	loss_value_5: 0.03373
	loss_reward_5: 0.00647
	loss_policy: 0.34076
	loss_value: 0.29209
	loss_reward: 0.02652
[2024-05-04 14:30:42] nn step 19950, lr: 0.04.
	loss_policy_0: 0.13651
	accuracy_policy_0: 0.70992
	loss_value_0: 0.14858
	loss_policy_1: 0.03789
	accuracy_policy_1: 0.62734
	loss_value_1: 0.03146
	loss_reward_1: 0.00551
	loss_policy_2: 0.04229
	accuracy_policy_2: 0.59477
	loss_value_2: 0.03298
	loss_reward_2: 0.0052
	loss_policy_3: 0.04568
	accuracy_policy_3: 0.5748
	loss_value_3: 0.03428
	loss_reward_3: 0.00542
	loss_policy_4: 0.04896
	accuracy_policy_4: 0.55402
	loss_value_4: 0.03547
	loss_reward_4: 0.00599
	loss_policy_5: 0.05139
	accuracy_policy_5: 0.53145
	loss_value_5: 0.03681
	loss_reward_5: 0.00719
	loss_policy: 0.36273
	loss_value: 0.31958
	loss_reward: 0.02932
[2024-05-04 14:30:59] nn step 20000, lr: 0.04.
	loss_policy_0: 0.13504
	accuracy_policy_0: 0.71062
	loss_value_0: 0.14974
	loss_policy_1: 0.03728
	accuracy_policy_1: 0.6291
	loss_value_1: 0.03139
	loss_reward_1: 0.00552
	loss_policy_2: 0.04168
	accuracy_policy_2: 0.60137
	loss_value_2: 0.03284
	loss_reward_2: 0.00506
	loss_policy_3: 0.04522
	accuracy_policy_3: 0.57211
	loss_value_3: 0.0344
	loss_reward_3: 0.00555
	loss_policy_4: 0.04839
	accuracy_policy_4: 0.55387
	loss_value_4: 0.0355
	loss_reward_4: 0.00594
	loss_policy_5: 0.05078
	accuracy_policy_5: 0.53508
	loss_value_5: 0.03669
	loss_reward_5: 0.00723
	loss_policy: 0.35839
	loss_value: 0.32057
	loss_reward: 0.0293
Optimization_Done 20000
[2024-05-04 14:33:07] [command] train weight_iter_20000.pkl 98 101
[2024-05-04 14:33:25] nn step 20050, lr: 0.04.
	loss_policy_0: 0.16346
	accuracy_policy_0: 0.63594
	loss_value_0: 0.14169
	loss_policy_1: 0.0417
	accuracy_policy_1: 0.56996
	loss_value_1: 0.03011
	loss_reward_1: 0.00555
	loss_policy_2: 0.04509
	accuracy_policy_2: 0.5432
	loss_value_2: 0.03159
	loss_reward_2: 0.00519
	loss_policy_3: 0.04902
	accuracy_policy_3: 0.51469
	loss_value_3: 0.03278
	loss_reward_3: 0.00541
	loss_policy_4: 0.05134
	accuracy_policy_4: 0.50043
	loss_value_4: 0.03395
	loss_reward_4: 0.00589
	loss_policy_5: 0.05421
	accuracy_policy_5: 0.47375
	loss_value_5: 0.03519
	loss_reward_5: 0.00715
	loss_policy: 0.40482
	loss_value: 0.30532
	loss_reward: 0.02919
[2024-05-04 14:33:42] nn step 20100, lr: 0.04.
	loss_policy_0: 0.13977
	accuracy_policy_0: 0.67777
	loss_value_0: 0.13333
	loss_policy_1: 0.03845
	accuracy_policy_1: 0.58465
	loss_value_1: 0.02808
	loss_reward_1: 0.00517
	loss_policy_2: 0.04212
	accuracy_policy_2: 0.55879
	loss_value_2: 0.02952
	loss_reward_2: 0.00483
	loss_policy_3: 0.04509
	accuracy_policy_3: 0.53527
	loss_value_3: 0.03087
	loss_reward_3: 0.00533
	loss_policy_4: 0.04751
	accuracy_policy_4: 0.51477
	loss_value_4: 0.03204
	loss_reward_4: 0.00572
	loss_policy_5: 0.05057
	accuracy_policy_5: 0.48852
	loss_value_5: 0.03318
	loss_reward_5: 0.00666
	loss_policy: 0.36352
	loss_value: 0.28702
	loss_reward: 0.02772
[2024-05-04 14:33:58] nn step 20150, lr: 0.04.
	loss_policy_0: 0.14241
	accuracy_policy_0: 0.68098
	loss_value_0: 0.13757
	loss_policy_1: 0.03842
	accuracy_policy_1: 0.60215
	loss_value_1: 0.02922
	loss_reward_1: 0.00546
	loss_policy_2: 0.04274
	accuracy_policy_2: 0.56633
	loss_value_2: 0.03062
	loss_reward_2: 0.00498
	loss_policy_3: 0.04608
	accuracy_policy_3: 0.5466
	loss_value_3: 0.03189
	loss_reward_3: 0.00528
	loss_policy_4: 0.04886
	accuracy_policy_4: 0.52434
	loss_value_4: 0.0331
	loss_reward_4: 0.00589
	loss_policy_5: 0.05143
	accuracy_policy_5: 0.50152
	loss_value_5: 0.03415
	loss_reward_5: 0.00689
	loss_policy: 0.36993
	loss_value: 0.29655
	loss_reward: 0.0285
[2024-05-04 14:34:15] nn step 20200, lr: 0.04.
	loss_policy_0: 0.13472
	accuracy_policy_0: 0.68801
	loss_value_0: 0.1342
	loss_policy_1: 0.03719
	accuracy_policy_1: 0.60336
	loss_value_1: 0.02844
	loss_reward_1: 0.00537
	loss_policy_2: 0.04116
	accuracy_policy_2: 0.57035
	loss_value_2: 0.02988
	loss_reward_2: 0.00477
	loss_policy_3: 0.04428
	accuracy_policy_3: 0.54777
	loss_value_3: 0.03116
	loss_reward_3: 0.00527
	loss_policy_4: 0.04716
	accuracy_policy_4: 0.52934
	loss_value_4: 0.03225
	loss_reward_4: 0.00571
	loss_policy_5: 0.04979
	accuracy_policy_5: 0.50426
	loss_value_5: 0.03334
	loss_reward_5: 0.00674
	loss_policy: 0.3543
	loss_value: 0.28926
	loss_reward: 0.02785
Optimization_Done 20200
[2024-05-04 14:36:24] [command] train weight_iter_20200.pkl 99 102
[2024-05-04 14:36:43] nn step 20250, lr: 0.04.
	loss_policy_0: 0.16003
	accuracy_policy_0: 0.64953
	loss_value_0: 0.13893
	loss_policy_1: 0.04148
	accuracy_policy_1: 0.57543
	loss_value_1: 0.02969
	loss_reward_1: 0.00573
	loss_policy_2: 0.04522
	accuracy_policy_2: 0.54121
	loss_value_2: 0.03093
	loss_reward_2: 0.00516
	loss_policy_3: 0.04815
	accuracy_policy_3: 0.52578
	loss_value_3: 0.03221
	loss_reward_3: 0.00552
	loss_policy_4: 0.05092
	accuracy_policy_4: 0.50215
	loss_value_4: 0.03352
	loss_reward_4: 0.00607
	loss_policy_5: 0.05341
	accuracy_policy_5: 0.48605
	loss_value_5: 0.03452
	loss_reward_5: 0.00701
	loss_policy: 0.39921
	loss_value: 0.2998
	loss_reward: 0.02949
[2024-05-04 14:37:00] nn step 20300, lr: 0.04.
	loss_policy_0: 0.16245
	accuracy_policy_0: 0.6707
	loss_value_0: 0.14885
	loss_policy_1: 0.04299
	accuracy_policy_1: 0.58938
	loss_value_1: 0.03153
	loss_reward_1: 0.00597
	loss_policy_2: 0.04692
	accuracy_policy_2: 0.56191
	loss_value_2: 0.0331
	loss_reward_2: 0.00552
	loss_policy_3: 0.05109
	accuracy_policy_3: 0.53273
	loss_value_3: 0.03457
	loss_reward_3: 0.00603
	loss_policy_4: 0.05458
	accuracy_policy_4: 0.51512
	loss_value_4: 0.03599
	loss_reward_4: 0.0066
	loss_policy_5: 0.05732
	accuracy_policy_5: 0.49234
	loss_value_5: 0.03714
	loss_reward_5: 0.00793
	loss_policy: 0.41536
	loss_value: 0.32118
	loss_reward: 0.03204
[2024-05-04 14:37:17] nn step 20350, lr: 0.04.
	loss_policy_0: 0.14109
	accuracy_policy_0: 0.68605
	loss_value_0: 0.13767
	loss_policy_1: 0.03892
	accuracy_policy_1: 0.60316
	loss_value_1: 0.0293
	loss_reward_1: 0.00564
	loss_policy_2: 0.04275
	accuracy_policy_2: 0.56727
	loss_value_2: 0.03063
	loss_reward_2: 0.00531
	loss_policy_3: 0.0464
	accuracy_policy_3: 0.5402
	loss_value_3: 0.03197
	loss_reward_3: 0.00543
	loss_policy_4: 0.04914
	accuracy_policy_4: 0.52188
	loss_value_4: 0.03308
	loss_reward_4: 0.00603
	loss_policy_5: 0.05171
	accuracy_policy_5: 0.50227
	loss_value_5: 0.03427
	loss_reward_5: 0.00708
	loss_policy: 0.37001
	loss_value: 0.29694
	loss_reward: 0.0295
[2024-05-04 14:37:34] nn step 20400, lr: 0.04.
	loss_policy_0: 0.14411
	accuracy_policy_0: 0.69125
	loss_value_0: 0.14496
	loss_policy_1: 0.0396
	accuracy_policy_1: 0.6032
	loss_value_1: 0.03038
	loss_reward_1: 0.00579
	loss_policy_2: 0.04391
	accuracy_policy_2: 0.56945
	loss_value_2: 0.03185
	loss_reward_2: 0.00524
	loss_policy_3: 0.04751
	accuracy_policy_3: 0.54535
	loss_value_3: 0.03318
	loss_reward_3: 0.00586
	loss_policy_4: 0.05051
	accuracy_policy_4: 0.52363
	loss_value_4: 0.03458
	loss_reward_4: 0.00632
	loss_policy_5: 0.0535
	accuracy_policy_5: 0.50133
	loss_value_5: 0.03575
	loss_reward_5: 0.00743
	loss_policy: 0.37914
	loss_value: 0.31071
	loss_reward: 0.03063
Optimization_Done 20400
[2024-05-04 14:39:41] [command] train weight_iter_20400.pkl 100 103
[2024-05-04 14:40:16] nn step 20450, lr: 0.04.
	loss_policy_0: 0.16565
	accuracy_policy_0: 0.63633
	loss_value_0: 0.15527
	loss_policy_1: 0.04429
	accuracy_policy_1: 0.55109
	loss_value_1: 0.03269
	loss_reward_1: 0.00629
	loss_policy_2: 0.04911
	accuracy_policy_2: 0.51566
	loss_value_2: 0.03418
	loss_reward_2: 0.00579
	loss_policy_3: 0.05253
	accuracy_policy_3: 0.48445
	loss_value_3: 0.03553
	loss_reward_3: 0.00604
	loss_policy_4: 0.05572
	accuracy_policy_4: 0.46379
	loss_value_4: 0.03661
	loss_reward_4: 0.00661
	loss_policy_5: 0.0584
	accuracy_policy_5: 0.44355
	loss_value_5: 0.03788
	loss_reward_5: 0.00797
	loss_policy: 0.42571
	loss_value: 0.33216
	loss_reward: 0.0327
[2024-05-04 14:40:39] nn step 20500, lr: 0.04.
	loss_policy_0: 0.14816
	accuracy_policy_0: 0.66859
	loss_value_0: 0.14882
	loss_policy_1: 0.04099
	accuracy_policy_1: 0.5709
	loss_value_1: 0.03132
	loss_reward_1: 0.00591
	loss_policy_2: 0.0455
	accuracy_policy_2: 0.53512
	loss_value_2: 0.0327
	loss_reward_2: 0.00553
	loss_policy_3: 0.04927
	accuracy_policy_3: 0.50187
	loss_value_3: 0.03418
	loss_reward_3: 0.00572
	loss_policy_4: 0.05212
	accuracy_policy_4: 0.48543
	loss_value_4: 0.03548
	loss_reward_4: 0.00644
	loss_policy_5: 0.05495
	accuracy_policy_5: 0.45879
	loss_value_5: 0.03643
	loss_reward_5: 0.00769
	loss_policy: 0.39098
	loss_value: 0.31893
	loss_reward: 0.03129
[2024-05-04 14:41:11] nn step 20550, lr: 0.04.
	loss_policy_0: 0.13921
	accuracy_policy_0: 0.68047
	loss_value_0: 0.14599
	loss_policy_1: 0.039
	accuracy_policy_1: 0.5859
	loss_value_1: 0.03101
	loss_reward_1: 0.00587
	loss_policy_2: 0.04347
	accuracy_policy_2: 0.54621
	loss_value_2: 0.03239
	loss_reward_2: 0.00527
	loss_policy_3: 0.04679
	accuracy_policy_3: 0.51961
	loss_value_3: 0.03353
	loss_reward_3: 0.00586
	loss_policy_4: 0.05011
	accuracy_policy_4: 0.49648
	loss_value_4: 0.03484
	loss_reward_4: 0.00629
	loss_policy_5: 0.05288
	accuracy_policy_5: 0.47012
	loss_value_5: 0.03588
	loss_reward_5: 0.00753
	loss_policy: 0.37144
	loss_value: 0.31364
	loss_reward: 0.03082
[2024-05-04 14:41:40] nn step 20600, lr: 0.04.
	loss_policy_0: 0.14641
	accuracy_policy_0: 0.68555
	loss_value_0: 0.15602
	loss_policy_1: 0.04149
	accuracy_policy_1: 0.58766
	loss_value_1: 0.03297
	loss_reward_1: 0.00634
	loss_policy_2: 0.04597
	accuracy_policy_2: 0.55074
	loss_value_2: 0.03454
	loss_reward_2: 0.00566
	loss_policy_3: 0.04989
	accuracy_policy_3: 0.52625
	loss_value_3: 0.03593
	loss_reward_3: 0.00607
	loss_policy_4: 0.05335
	accuracy_policy_4: 0.49953
	loss_value_4: 0.0371
	loss_reward_4: 0.00669
	loss_policy_5: 0.05617
	accuracy_policy_5: 0.47992
	loss_value_5: 0.0383
	loss_reward_5: 0.00822
	loss_policy: 0.39328
	loss_value: 0.33486
	loss_reward: 0.03298
Optimization_Done 20600
[2024-05-04 14:44:10] [command] train weight_iter_20600.pkl 101 104
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-04 21:49:26] [command] train weight_iter_20600.pkl 102 104
[2024-05-04 21:49:52] nn step 20650, lr: 0.04.
	loss_policy_0: 0.19204
	accuracy_policy_0: 0.64414
	loss_value_0: 0.1855
	loss_policy_1: 0.05142
	accuracy_policy_1: 0.55652
	loss_value_1: 0.03894
	loss_reward_1: 0.00705
	loss_policy_2: 0.05742
	accuracy_policy_2: 0.51746
	loss_value_2: 0.04087
	loss_reward_2: 0.00662
	loss_policy_3: 0.06184
	accuracy_policy_3: 0.49312
	loss_value_3: 0.04229
	loss_reward_3: 0.00729
	loss_policy_4: 0.06579
	accuracy_policy_4: 0.46473
	loss_value_4: 0.04369
	loss_reward_4: 0.00782
	loss_policy_5: 0.06923
	accuracy_policy_5: 0.44324
	loss_value_5: 0.0451
	loss_reward_5: 0.00895
	loss_policy: 0.49775
	loss_value: 0.39638
	loss_reward: 0.03774
[2024-05-04 21:50:09] nn step 20700, lr: 0.04.
	loss_policy_0: 0.15113
	accuracy_policy_0: 0.68234
	loss_value_0: 0.15942
	loss_policy_1: 0.04376
	accuracy_policy_1: 0.58617
	loss_value_1: 0.03354
	loss_reward_1: 0.00641
	loss_policy_2: 0.0489
	accuracy_policy_2: 0.54945
	loss_value_2: 0.03541
	loss_reward_2: 0.00601
	loss_policy_3: 0.05328
	accuracy_policy_3: 0.51301
	loss_value_3: 0.03695
	loss_reward_3: 0.00646
	loss_policy_4: 0.05712
	accuracy_policy_4: 0.4882
	loss_value_4: 0.03833
	loss_reward_4: 0.00688
	loss_policy_5: 0.06063
	accuracy_policy_5: 0.46406
	loss_value_5: 0.0394
	loss_reward_5: 0.00813
	loss_policy: 0.41482
	loss_value: 0.34304
	loss_reward: 0.03388
[2024-05-04 21:50:25] nn step 20750, lr: 0.04.
	loss_policy_0: 0.1516
	accuracy_policy_0: 0.68703
	loss_value_0: 0.1647
	loss_policy_1: 0.04375
	accuracy_policy_1: 0.59371
	loss_value_1: 0.03507
	loss_reward_1: 0.00645
	loss_policy_2: 0.04893
	accuracy_policy_2: 0.55277
	loss_value_2: 0.03689
	loss_reward_2: 0.0061
	loss_policy_3: 0.05374
	accuracy_policy_3: 0.52598
	loss_value_3: 0.03836
	loss_reward_3: 0.00663
	loss_policy_4: 0.0577
	accuracy_policy_4: 0.49672
	loss_value_4: 0.03969
	loss_reward_4: 0.00716
	loss_policy_5: 0.06056
	accuracy_policy_5: 0.47645
	loss_value_5: 0.04065
	loss_reward_5: 0.00846
	loss_policy: 0.41628
	loss_value: 0.35536
	loss_reward: 0.0348
[2024-05-04 21:50:41] nn step 20800, lr: 0.04.
	loss_policy_0: 0.13319
	accuracy_policy_0: 0.69621
	loss_value_0: 0.1518
	loss_policy_1: 0.03944
	accuracy_policy_1: 0.59387
	loss_value_1: 0.03212
	loss_reward_1: 0.00584
	loss_policy_2: 0.04464
	accuracy_policy_2: 0.55656
	loss_value_2: 0.03375
	loss_reward_2: 0.00538
	loss_policy_3: 0.04791
	accuracy_policy_3: 0.53176
	loss_value_3: 0.03496
	loss_reward_3: 0.00595
	loss_policy_4: 0.05149
	accuracy_policy_4: 0.50762
	loss_value_4: 0.03619
	loss_reward_4: 0.00659
	loss_policy_5: 0.05451
	accuracy_policy_5: 0.47875
	loss_value_5: 0.03723
	loss_reward_5: 0.00781
	loss_policy: 0.37118
	loss_value: 0.32605
	loss_reward: 0.03156
Optimization_Done 20800
[2024-05-04 21:52:46] [command] train weight_iter_20800.pkl 103 105
[2024-05-04 21:53:03] nn step 20850, lr: 0.04.
	loss_policy_0: 0.17849
	accuracy_policy_0: 0.66105
	loss_value_0: 0.17188
	loss_policy_1: 0.04617
	accuracy_policy_1: 0.58363
	loss_value_1: 0.0362
	loss_reward_1: 0.00673
	loss_policy_2: 0.05109
	accuracy_policy_2: 0.5502
	loss_value_2: 0.03793
	loss_reward_2: 0.00618
	loss_policy_3: 0.05544
	accuracy_policy_3: 0.52543
	loss_value_3: 0.03943
	loss_reward_3: 0.00671
	loss_policy_4: 0.05842
	accuracy_policy_4: 0.50309
	loss_value_4: 0.04047
	loss_reward_4: 0.00702
	loss_policy_5: 0.06126
	accuracy_policy_5: 0.48543
	loss_value_5: 0.04165
	loss_reward_5: 0.00814
	loss_policy: 0.45087
	loss_value: 0.36756
	loss_reward: 0.03478
[2024-05-04 21:53:20] nn step 20900, lr: 0.04.
	loss_policy_0: 0.14204
	accuracy_policy_0: 0.69516
	loss_value_0: 0.15469
	loss_policy_1: 0.04012
	accuracy_policy_1: 0.61074
	loss_value_1: 0.03283
	loss_reward_1: 0.00581
	loss_policy_2: 0.04465
	accuracy_policy_2: 0.57746
	loss_value_2: 0.03441
	loss_reward_2: 0.00554
	loss_policy_3: 0.04893
	accuracy_policy_3: 0.54773
	loss_value_3: 0.03571
	loss_reward_3: 0.00622
	loss_policy_4: 0.05238
	accuracy_policy_4: 0.5177
	loss_value_4: 0.03691
	loss_reward_4: 0.00668
	loss_policy_5: 0.05498
	accuracy_policy_5: 0.49707
	loss_value_5: 0.03794
	loss_reward_5: 0.00762
	loss_policy: 0.38309
	loss_value: 0.3325
	loss_reward: 0.03187
[2024-05-04 21:53:36] nn step 20950, lr: 0.04.
	loss_policy_0: 0.13536
	accuracy_policy_0: 0.7073
	loss_value_0: 0.154
	loss_policy_1: 0.03879
	accuracy_policy_1: 0.62352
	loss_value_1: 0.03279
	loss_reward_1: 0.00598
	loss_policy_2: 0.04374
	accuracy_policy_2: 0.58305
	loss_value_2: 0.03432
	loss_reward_2: 0.00552
	loss_policy_3: 0.04823
	accuracy_policy_3: 0.54922
	loss_value_3: 0.03579
	loss_reward_3: 0.00592
	loss_policy_4: 0.05122
	accuracy_policy_4: 0.52691
	loss_value_4: 0.03685
	loss_reward_4: 0.00666
	loss_policy_5: 0.05413
	accuracy_policy_5: 0.50535
	loss_value_5: 0.03799
	loss_reward_5: 0.00778
	loss_policy: 0.37147
	loss_value: 0.33175
	loss_reward: 0.03185
[2024-05-04 21:53:53] nn step 21000, lr: 0.04.
	loss_policy_0: 0.12984
	accuracy_policy_0: 0.71344
	loss_value_0: 0.15385
	loss_policy_1: 0.03783
	accuracy_policy_1: 0.61773
	loss_value_1: 0.03244
	loss_reward_1: 0.00602
	loss_policy_2: 0.04315
	accuracy_policy_2: 0.58152
	loss_value_2: 0.03382
	loss_reward_2: 0.00558
	loss_policy_3: 0.04685
	accuracy_policy_3: 0.55395
	loss_value_3: 0.03517
	loss_reward_3: 0.00601
	loss_policy_4: 0.05092
	accuracy_policy_4: 0.52422
	loss_value_4: 0.03638
	loss_reward_4: 0.00651
	loss_policy_5: 0.05323
	accuracy_policy_5: 0.50824
	loss_value_5: 0.03757
	loss_reward_5: 0.0077
	loss_policy: 0.36183
	loss_value: 0.32923
	loss_reward: 0.03184
Optimization_Done 21000
[2024-05-04 21:55:57] [command] train weight_iter_21000.pkl 104 106
[2024-05-04 21:56:16] nn step 21050, lr: 0.04.
	loss_policy_0: 0.14638
	accuracy_policy_0: 0.67344
	loss_value_0: 0.16109
	loss_policy_1: 0.03906
	accuracy_policy_1: 0.59531
	loss_value_1: 0.03394
	loss_reward_1: 0.00615
	loss_policy_2: 0.0428
	accuracy_policy_2: 0.56328
	loss_value_2: 0.03538
	loss_reward_2: 0.00577
	loss_policy_3: 0.04616
	accuracy_policy_3: 0.53676
	loss_value_3: 0.03691
	loss_reward_3: 0.00642
	loss_policy_4: 0.04904
	accuracy_policy_4: 0.51344
	loss_value_4: 0.03807
	loss_reward_4: 0.00687
	loss_policy_5: 0.05175
	accuracy_policy_5: 0.49281
	loss_value_5: 0.03921
	loss_reward_5: 0.00827
	loss_policy: 0.37519
	loss_value: 0.34459
	loss_reward: 0.03348
[2024-05-04 21:56:32] nn step 21100, lr: 0.04.
	loss_policy_0: 0.13251
	accuracy_policy_0: 0.70484
	loss_value_0: 0.16424
	loss_policy_1: 0.03806
	accuracy_policy_1: 0.6152
	loss_value_1: 0.03503
	loss_reward_1: 0.00642
	loss_policy_2: 0.04206
	accuracy_policy_2: 0.58199
	loss_value_2: 0.03644
	loss_reward_2: 0.00588
	loss_policy_3: 0.04587
	accuracy_policy_3: 0.55777
	loss_value_3: 0.03779
	loss_reward_3: 0.00641
	loss_policy_4: 0.04901
	accuracy_policy_4: 0.53145
	loss_value_4: 0.03891
	loss_reward_4: 0.00712
	loss_policy_5: 0.05162
	accuracy_policy_5: 0.51422
	loss_value_5: 0.03982
	loss_reward_5: 0.00844
	loss_policy: 0.35913
	loss_value: 0.35223
	loss_reward: 0.03427
[2024-05-04 21:56:49] nn step 21150, lr: 0.04.
	loss_policy_0: 0.12638
	accuracy_policy_0: 0.71312
	loss_value_0: 0.16353
	loss_policy_1: 0.03644
	accuracy_policy_1: 0.6193
	loss_value_1: 0.03477
	loss_reward_1: 0.00636
	loss_policy_2: 0.04065
	accuracy_policy_2: 0.58871
	loss_value_2: 0.03611
	loss_reward_2: 0.00597
	loss_policy_3: 0.04469
	accuracy_policy_3: 0.56
	loss_value_3: 0.03755
	loss_reward_3: 0.00642
	loss_policy_4: 0.04804
	accuracy_policy_4: 0.53031
	loss_value_4: 0.03861
	loss_reward_4: 0.007
	loss_policy_5: 0.05051
	accuracy_policy_5: 0.51395
	loss_value_5: 0.03956
	loss_reward_5: 0.00834
	loss_policy: 0.3467
	loss_value: 0.35013
	loss_reward: 0.03409
[2024-05-04 21:57:05] nn step 21200, lr: 0.04.
	loss_policy_0: 0.12219
	accuracy_policy_0: 0.72078
	loss_value_0: 0.16203
	loss_policy_1: 0.03606
	accuracy_policy_1: 0.62488
	loss_value_1: 0.03438
	loss_reward_1: 0.00611
	loss_policy_2: 0.04004
	accuracy_policy_2: 0.59473
	loss_value_2: 0.03583
	loss_reward_2: 0.00564
	loss_policy_3: 0.04443
	accuracy_policy_3: 0.56906
	loss_value_3: 0.03701
	loss_reward_3: 0.0063
	loss_policy_4: 0.04729
	accuracy_policy_4: 0.53664
	loss_value_4: 0.03826
	loss_reward_4: 0.00696
	loss_policy_5: 0.05042
	accuracy_policy_5: 0.51727
	loss_value_5: 0.03936
	loss_reward_5: 0.00806
	loss_policy: 0.34044
	loss_value: 0.34687
	loss_reward: 0.03307
Optimization_Done 21200
[2024-05-04 21:59:08] [command] train weight_iter_21200.pkl 105 107
[2024-05-04 21:59:25] nn step 21250, lr: 0.04.
	loss_policy_0: 0.13122
	accuracy_policy_0: 0.70004
	loss_value_0: 0.15296
	loss_policy_1: 0.03567
	accuracy_policy_1: 0.61473
	loss_value_1: 0.03252
	loss_reward_1: 0.00539
	loss_policy_2: 0.03866
	accuracy_policy_2: 0.58883
	loss_value_2: 0.03387
	loss_reward_2: 0.0048
	loss_policy_3: 0.0422
	accuracy_policy_3: 0.55742
	loss_value_3: 0.03502
	loss_reward_3: 0.00533
	loss_policy_4: 0.04451
	accuracy_policy_4: 0.5341
	loss_value_4: 0.03596
	loss_reward_4: 0.00589
	loss_policy_5: 0.0464
	accuracy_policy_5: 0.52355
	loss_value_5: 0.03682
	loss_reward_5: 0.00686
	loss_policy: 0.33866
	loss_value: 0.32715
	loss_reward: 0.02827
[2024-05-04 21:59:42] nn step 21300, lr: 0.04.
	loss_policy_0: 0.11244
	accuracy_policy_0: 0.72129
	loss_value_0: 0.14593
	loss_policy_1: 0.03207
	accuracy_policy_1: 0.63504
	loss_value_1: 0.03132
	loss_reward_1: 0.0051
	loss_policy_2: 0.03565
	accuracy_policy_2: 0.60262
	loss_value_2: 0.03255
	loss_reward_2: 0.00471
	loss_policy_3: 0.03837
	accuracy_policy_3: 0.57883
	loss_value_3: 0.03363
	loss_reward_3: 0.00492
	loss_policy_4: 0.04124
	accuracy_policy_4: 0.55926
	loss_value_4: 0.0346
	loss_reward_4: 0.0056
	loss_policy_5: 0.04322
	accuracy_policy_5: 0.54363
	loss_value_5: 0.03538
	loss_reward_5: 0.00655
	loss_policy: 0.30299
	loss_value: 0.31339
	loss_reward: 0.02687
[2024-05-04 21:59:58] nn step 21350, lr: 0.04.
	loss_policy_0: 0.11369
	accuracy_policy_0: 0.73598
	loss_value_0: 0.15308
	loss_policy_1: 0.0331
	accuracy_policy_1: 0.64055
	loss_value_1: 0.03261
	loss_reward_1: 0.00542
	loss_policy_2: 0.03686
	accuracy_policy_2: 0.61348
	loss_value_2: 0.03396
	loss_reward_2: 0.00496
	loss_policy_3: 0.04064
	accuracy_policy_3: 0.58129
	loss_value_3: 0.03541
	loss_reward_3: 0.00525
	loss_policy_4: 0.04305
	accuracy_policy_4: 0.55773
	loss_value_4: 0.03639
	loss_reward_4: 0.00579
	loss_policy_5: 0.04513
	accuracy_policy_5: 0.54262
	loss_value_5: 0.03751
	loss_reward_5: 0.00704
	loss_policy: 0.31245
	loss_value: 0.32895
	loss_reward: 0.02847
[2024-05-04 22:00:14] nn step 21400, lr: 0.04.
	loss_policy_0: 0.10837
	accuracy_policy_0: 0.74379
	loss_value_0: 0.14887
	loss_policy_1: 0.0318
	accuracy_policy_1: 0.64941
	loss_value_1: 0.03169
	loss_reward_1: 0.00516
	loss_policy_2: 0.03594
	accuracy_policy_2: 0.61383
	loss_value_2: 0.03321
	loss_reward_2: 0.00466
	loss_policy_3: 0.03902
	accuracy_policy_3: 0.59145
	loss_value_3: 0.03451
	loss_reward_3: 0.0052
	loss_policy_4: 0.04106
	accuracy_policy_4: 0.56824
	loss_value_4: 0.03534
	loss_reward_4: 0.00573
	loss_policy_5: 0.04407
	accuracy_policy_5: 0.5466
	loss_value_5: 0.03624
	loss_reward_5: 0.00678
	loss_policy: 0.30026
	loss_value: 0.31987
	loss_reward: 0.02754
Optimization_Done 21400
[2024-05-04 22:02:20] [command] train weight_iter_21400.pkl 106 108
[2024-05-04 22:02:38] nn step 21450, lr: 0.04.
	loss_policy_0: 0.14047
	accuracy_policy_0: 0.64105
	loss_value_0: 0.13842
	loss_policy_1: 0.03588
	accuracy_policy_1: 0.56656
	loss_value_1: 0.0295
	loss_reward_1: 0.00476
	loss_policy_2: 0.03859
	accuracy_policy_2: 0.54016
	loss_value_2: 0.03076
	loss_reward_2: 0.00433
	loss_policy_3: 0.04094
	accuracy_policy_3: 0.51461
	loss_value_3: 0.0319
	loss_reward_3: 0.00476
	loss_policy_4: 0.0433
	accuracy_policy_4: 0.50098
	loss_value_4: 0.03283
	loss_reward_4: 0.005
	loss_policy_5: 0.04524
	accuracy_policy_5: 0.48148
	loss_value_5: 0.03368
	loss_reward_5: 0.00576
	loss_policy: 0.34443
	loss_value: 0.29709
	loss_reward: 0.02461
[2024-05-04 22:02:54] nn step 21500, lr: 0.04.
	loss_policy_0: 0.11498
	accuracy_policy_0: 0.69938
	loss_value_0: 0.1365
	loss_policy_1: 0.03207
	accuracy_policy_1: 0.60953
	loss_value_1: 0.02903
	loss_reward_1: 0.00472
	loss_policy_2: 0.03536
	accuracy_policy_2: 0.58109
	loss_value_2: 0.03034
	loss_reward_2: 0.00427
	loss_policy_3: 0.03839
	accuracy_policy_3: 0.54449
	loss_value_3: 0.03152
	loss_reward_3: 0.00469
	loss_policy_4: 0.04002
	accuracy_policy_4: 0.53637
	loss_value_4: 0.03239
	loss_reward_4: 0.00499
	loss_policy_5: 0.04225
	accuracy_policy_5: 0.51512
	loss_value_5: 0.03334
	loss_reward_5: 0.00569
	loss_policy: 0.30307
	loss_value: 0.29311
	loss_reward: 0.02437
[2024-05-04 22:03:10] nn step 21550, lr: 0.04.
	loss_policy_0: 0.12232
	accuracy_policy_0: 0.71211
	loss_value_0: 0.1502
	loss_policy_1: 0.03432
	accuracy_policy_1: 0.62176
	loss_value_1: 0.03196
	loss_reward_1: 0.00507
	loss_policy_2: 0.03775
	accuracy_policy_2: 0.595
	loss_value_2: 0.03318
	loss_reward_2: 0.00467
	loss_policy_3: 0.04067
	accuracy_policy_3: 0.56613
	loss_value_3: 0.03452
	loss_reward_3: 0.00492
	loss_policy_4: 0.0432
	accuracy_policy_4: 0.54211
	loss_value_4: 0.03554
	loss_reward_4: 0.00537
	loss_policy_5: 0.04559
	accuracy_policy_5: 0.52605
	loss_value_5: 0.03659
	loss_reward_5: 0.00633
	loss_policy: 0.32385
	loss_value: 0.322
	loss_reward: 0.02634
[2024-05-04 22:03:27] nn step 21600, lr: 0.04.
	loss_policy_0: 0.10554
	accuracy_policy_0: 0.7316
	loss_value_0: 0.13959
	loss_policy_1: 0.0314
	accuracy_policy_1: 0.62625
	loss_value_1: 0.02971
	loss_reward_1: 0.00481
	loss_policy_2: 0.03435
	accuracy_policy_2: 0.59297
	loss_value_2: 0.03081
	loss_reward_2: 0.00422
	loss_policy_3: 0.03727
	accuracy_policy_3: 0.56695
	loss_value_3: 0.03199
	loss_reward_3: 0.00469
	loss_policy_4: 0.03938
	accuracy_policy_4: 0.54785
	loss_value_4: 0.03293
	loss_reward_4: 0.00515
	loss_policy_5: 0.04119
	accuracy_policy_5: 0.53754
	loss_value_5: 0.03392
	loss_reward_5: 0.00617
	loss_policy: 0.28912
	loss_value: 0.29894
	loss_reward: 0.02505
Optimization_Done 21600
[2024-05-04 22:05:30] [command] train weight_iter_21600.pkl 107 109
[2024-05-04 22:05:48] nn step 21650, lr: 0.04.
	loss_policy_0: 0.17352
	accuracy_policy_0: 0.61805
	loss_value_0: 0.1432
	loss_policy_1: 0.04186
	accuracy_policy_1: 0.55836
	loss_value_1: 0.03054
	loss_reward_1: 0.00476
	loss_policy_2: 0.04504
	accuracy_policy_2: 0.53836
	loss_value_2: 0.03191
	loss_reward_2: 0.00438
	loss_policy_3: 0.04732
	accuracy_policy_3: 0.51648
	loss_value_3: 0.03322
	loss_reward_3: 0.00451
	loss_policy_4: 0.04943
	accuracy_policy_4: 0.50184
	loss_value_4: 0.03437
	loss_reward_4: 0.00489
	loss_policy_5: 0.05236
	accuracy_policy_5: 0.48207
	loss_value_5: 0.03538
	loss_reward_5: 0.00558
	loss_policy: 0.40953
	loss_value: 0.30863
	loss_reward: 0.02412
[2024-05-04 22:06:04] nn step 21700, lr: 0.04.
	loss_policy_0: 0.15104
	accuracy_policy_0: 0.66945
	loss_value_0: 0.14069
	loss_policy_1: 0.03833
	accuracy_policy_1: 0.60309
	loss_value_1: 0.02997
	loss_reward_1: 0.00468
	loss_policy_2: 0.04196
	accuracy_policy_2: 0.57063
	loss_value_2: 0.03144
	loss_reward_2: 0.00426
	loss_policy_3: 0.04493
	accuracy_policy_3: 0.55312
	loss_value_3: 0.03265
	loss_reward_3: 0.00457
	loss_policy_4: 0.04735
	accuracy_policy_4: 0.53211
	loss_value_4: 0.03377
	loss_reward_4: 0.00482
	loss_policy_5: 0.0496
	accuracy_policy_5: 0.5152
	loss_value_5: 0.03497
	loss_reward_5: 0.00556
	loss_policy: 0.3732
	loss_value: 0.30349
	loss_reward: 0.0239
[2024-05-04 22:06:20] nn step 21750, lr: 0.04.
	loss_policy_0: 0.13994
	accuracy_policy_0: 0.68367
	loss_value_0: 0.13935
	loss_policy_1: 0.03714
	accuracy_policy_1: 0.60922
	loss_value_1: 0.02984
	loss_reward_1: 0.00467
	loss_policy_2: 0.04018
	accuracy_policy_2: 0.58637
	loss_value_2: 0.03129
	loss_reward_2: 0.00427
	loss_policy_3: 0.04295
	accuracy_policy_3: 0.56168
	loss_value_3: 0.03274
	loss_reward_3: 0.00441
	loss_policy_4: 0.04514
	accuracy_policy_4: 0.55164
	loss_value_4: 0.03385
	loss_reward_4: 0.0049
	loss_policy_5: 0.04766
	accuracy_policy_5: 0.53512
	loss_value_5: 0.03489
	loss_reward_5: 0.0056
	loss_policy: 0.353
	loss_value: 0.30196
	loss_reward: 0.02384
[2024-05-04 22:06:37] nn step 21800, lr: 0.04.
	loss_policy_0: 0.12915
	accuracy_policy_0: 0.69652
	loss_value_0: 0.13416
	loss_policy_1: 0.03493
	accuracy_policy_1: 0.61516
	loss_value_1: 0.02873
	loss_reward_1: 0.00446
	loss_policy_2: 0.03815
	accuracy_policy_2: 0.59258
	loss_value_2: 0.03014
	loss_reward_2: 0.00397
	loss_policy_3: 0.04061
	accuracy_policy_3: 0.57574
	loss_value_3: 0.03146
	loss_reward_3: 0.00438
	loss_policy_4: 0.04323
	accuracy_policy_4: 0.55457
	loss_value_4: 0.03248
	loss_reward_4: 0.00478
	loss_policy_5: 0.0451
	accuracy_policy_5: 0.53566
	loss_value_5: 0.03372
	loss_reward_5: 0.00554
	loss_policy: 0.33117
	loss_value: 0.29069
	loss_reward: 0.02313
Optimization_Done 21800
[2024-05-04 22:08:42] [command] train weight_iter_21800.pkl 108 110
[2024-05-04 22:09:00] nn step 21850, lr: 0.04.
	loss_policy_0: 0.15293
	accuracy_policy_0: 0.6577
	loss_value_0: 0.15056
	loss_policy_1: 0.03954
	accuracy_policy_1: 0.58457
	loss_value_1: 0.03165
	loss_reward_1: 0.00583
	loss_policy_2: 0.04337
	accuracy_policy_2: 0.56016
	loss_value_2: 0.03322
	loss_reward_2: 0.00526
	loss_policy_3: 0.04634
	accuracy_policy_3: 0.54016
	loss_value_3: 0.0344
	loss_reward_3: 0.00572
	loss_policy_4: 0.04863
	accuracy_policy_4: 0.5184
	loss_value_4: 0.0354
	loss_reward_4: 0.00608
	loss_policy_5: 0.05074
	accuracy_policy_5: 0.50422
	loss_value_5: 0.03643
	loss_reward_5: 0.00679
	loss_policy: 0.38155
	loss_value: 0.32165
	loss_reward: 0.02968
[2024-05-04 22:09:16] nn step 21900, lr: 0.04.
	loss_policy_0: 0.14697
	accuracy_policy_0: 0.69469
	loss_value_0: 0.16434
	loss_policy_1: 0.04047
	accuracy_policy_1: 0.61129
	loss_value_1: 0.03487
	loss_reward_1: 0.00611
	loss_policy_2: 0.04477
	accuracy_policy_2: 0.57875
	loss_value_2: 0.03627
	loss_reward_2: 0.00575
	loss_policy_3: 0.04749
	accuracy_policy_3: 0.5641
	loss_value_3: 0.0376
	loss_reward_3: 0.0059
	loss_policy_4: 0.05057
	accuracy_policy_4: 0.53941
	loss_value_4: 0.03881
	loss_reward_4: 0.00652
	loss_policy_5: 0.0532
	accuracy_policy_5: 0.52305
	loss_value_5: 0.03978
	loss_reward_5: 0.00768
	loss_policy: 0.38346
	loss_value: 0.35168
	loss_reward: 0.03196
[2024-05-04 22:09:33] nn step 21950, lr: 0.04.
	loss_policy_0: 0.13335
	accuracy_policy_0: 0.7077
	loss_value_0: 0.15647
	loss_policy_1: 0.03736
	accuracy_policy_1: 0.61418
	loss_value_1: 0.03294
	loss_reward_1: 0.00579
	loss_policy_2: 0.04135
	accuracy_policy_2: 0.59121
	loss_value_2: 0.03449
	loss_reward_2: 0.00553
	loss_policy_3: 0.04464
	accuracy_policy_3: 0.56504
	loss_value_3: 0.03564
	loss_reward_3: 0.00579
	loss_policy_4: 0.0476
	accuracy_policy_4: 0.54312
	loss_value_4: 0.03673
	loss_reward_4: 0.00614
	loss_policy_5: 0.04958
	accuracy_policy_5: 0.52934
	loss_value_5: 0.03759
	loss_reward_5: 0.00726
	loss_policy: 0.35388
	loss_value: 0.33385
	loss_reward: 0.03051
[2024-05-04 22:09:50] nn step 22000, lr: 0.04.
	loss_policy_0: 0.12282
	accuracy_policy_0: 0.7193
	loss_value_0: 0.15239
	loss_policy_1: 0.03538
	accuracy_policy_1: 0.6252
	loss_value_1: 0.03215
	loss_reward_1: 0.0056
	loss_policy_2: 0.03934
	accuracy_policy_2: 0.59797
	loss_value_2: 0.03359
	loss_reward_2: 0.00497
	loss_policy_3: 0.04241
	accuracy_policy_3: 0.57387
	loss_value_3: 0.03486
	loss_reward_3: 0.0055
	loss_policy_4: 0.04562
	accuracy_policy_4: 0.55012
	loss_value_4: 0.03601
	loss_reward_4: 0.00609
	loss_policy_5: 0.04818
	accuracy_policy_5: 0.53066
	loss_value_5: 0.03709
	loss_reward_5: 0.0069
	loss_policy: 0.33375
	loss_value: 0.32608
	loss_reward: 0.02907
Optimization_Done 22000
[2024-05-04 22:11:48] [command] train weight_iter_22000.pkl 109 111
[2024-05-04 22:12:06] nn step 22050, lr: 0.04.
	loss_policy_0: 0.16351
	accuracy_policy_0: 0.67652
	loss_value_0: 0.17882
	loss_policy_1: 0.04389
	accuracy_policy_1: 0.59062
	loss_value_1: 0.03771
	loss_reward_1: 0.00677
	loss_policy_2: 0.04883
	accuracy_policy_2: 0.56328
	loss_value_2: 0.03926
	loss_reward_2: 0.00623
	loss_policy_3: 0.05245
	accuracy_policy_3: 0.53398
	loss_value_3: 0.04076
	loss_reward_3: 0.00688
	loss_policy_4: 0.05547
	accuracy_policy_4: 0.51441
	loss_value_4: 0.04214
	loss_reward_4: 0.00712
	loss_policy_5: 0.05792
	accuracy_policy_5: 0.50004
	loss_value_5: 0.04305
	loss_reward_5: 0.00818
	loss_policy: 0.42207
	loss_value: 0.38174
	loss_reward: 0.03517
[2024-05-04 22:12:22] nn step 22100, lr: 0.04.
	loss_policy_0: 0.13294
	accuracy_policy_0: 0.70879
	loss_value_0: 0.16391
	loss_policy_1: 0.03818
	accuracy_policy_1: 0.61848
	loss_value_1: 0.03449
	loss_reward_1: 0.00609
	loss_policy_2: 0.04298
	accuracy_policy_2: 0.58219
	loss_value_2: 0.03587
	loss_reward_2: 0.00577
	loss_policy_3: 0.0467
	accuracy_policy_3: 0.56023
	loss_value_3: 0.0372
	loss_reward_3: 0.006
	loss_policy_4: 0.04989
	accuracy_policy_4: 0.53438
	loss_value_4: 0.03852
	loss_reward_4: 0.0067
	loss_policy_5: 0.05199
	accuracy_policy_5: 0.51883
	loss_value_5: 0.03955
	loss_reward_5: 0.00764
	loss_policy: 0.36268
	loss_value: 0.34954
	loss_reward: 0.0322
[2024-05-04 22:12:39] nn step 22150, lr: 0.04.
	loss_policy_0: 0.14203
	accuracy_policy_0: 0.72492
	loss_value_0: 0.18605
	loss_policy_1: 0.0424
	accuracy_policy_1: 0.62426
	loss_value_1: 0.03887
	loss_reward_1: 0.00708
	loss_policy_2: 0.04756
	accuracy_policy_2: 0.58973
	loss_value_2: 0.04037
	loss_reward_2: 0.00626
	loss_policy_3: 0.05159
	accuracy_policy_3: 0.5666
	loss_value_3: 0.04177
	loss_reward_3: 0.00675
	loss_policy_4: 0.05447
	accuracy_policy_4: 0.54734
	loss_value_4: 0.04305
	loss_reward_4: 0.0072
	loss_policy_5: 0.05694
	accuracy_policy_5: 0.53719
	loss_value_5: 0.04437
	loss_reward_5: 0.00864
	loss_policy: 0.39499
	loss_value: 0.39449
	loss_reward: 0.03593
[2024-05-04 22:12:55] nn step 22200, lr: 0.04.
	loss_policy_0: 0.13024
	accuracy_policy_0: 0.7257
	loss_value_0: 0.17231
	loss_policy_1: 0.03847
	accuracy_policy_1: 0.62754
	loss_value_1: 0.03619
	loss_reward_1: 0.0067
	loss_policy_2: 0.04332
	accuracy_policy_2: 0.59363
	loss_value_2: 0.03761
	loss_reward_2: 0.00601
	loss_policy_3: 0.04698
	accuracy_policy_3: 0.56906
	loss_value_3: 0.03898
	loss_reward_3: 0.0061
	loss_policy_4: 0.04966
	accuracy_policy_4: 0.55086
	loss_value_4: 0.0401
	loss_reward_4: 0.00687
	loss_policy_5: 0.05238
	accuracy_policy_5: 0.53375
	loss_value_5: 0.04113
	loss_reward_5: 0.00814
	loss_policy: 0.36105
	loss_value: 0.36632
	loss_reward: 0.03381
Optimization_Done 22200
[2024-05-04 22:14:36] [command] train weight_iter_22200.pkl 110 112
[2024-05-04 22:14:53] nn step 22250, lr: 0.04.
	loss_policy_0: 0.15281
	accuracy_policy_0: 0.67027
	loss_value_0: 0.18509
	loss_policy_1: 0.04151
	accuracy_policy_1: 0.58668
	loss_value_1: 0.03856
	loss_reward_1: 0.00627
	loss_policy_2: 0.04527
	accuracy_policy_2: 0.55324
	loss_value_2: 0.03966
	loss_reward_2: 0.00571
	loss_policy_3: 0.04943
	accuracy_policy_3: 0.5223
	loss_value_3: 0.04076
	loss_reward_3: 0.00628
	loss_policy_4: 0.05187
	accuracy_policy_4: 0.50008
	loss_value_4: 0.04158
	loss_reward_4: 0.00681
	loss_policy_5: 0.05374
	accuracy_policy_5: 0.49137
	loss_value_5: 0.04236
	loss_reward_5: 0.00759
	loss_policy: 0.39463
	loss_value: 0.388
	loss_reward: 0.03268
[2024-05-04 22:15:10] nn step 22300, lr: 0.04.
	loss_policy_0: 0.13864
	accuracy_policy_0: 0.70824
	loss_value_0: 0.18449
	loss_policy_1: 0.04014
	accuracy_policy_1: 0.60789
	loss_value_1: 0.03863
	loss_reward_1: 0.00646
	loss_policy_2: 0.0448
	accuracy_policy_2: 0.57309
	loss_value_2: 0.03982
	loss_reward_2: 0.00584
	loss_policy_3: 0.04843
	accuracy_policy_3: 0.53859
	loss_value_3: 0.04074
	loss_reward_3: 0.00621
	loss_policy_4: 0.05183
	accuracy_policy_4: 0.5157
	loss_value_4: 0.04147
	loss_reward_4: 0.00663
	loss_policy_5: 0.0535
	accuracy_policy_5: 0.50398
	loss_value_5: 0.04232
	loss_reward_5: 0.00807
	loss_policy: 0.37734
	loss_value: 0.38746
	loss_reward: 0.03322
[2024-05-04 22:15:26] nn step 22350, lr: 0.04.
	loss_policy_0: 0.1317
	accuracy_policy_0: 0.7198
	loss_value_0: 0.18236
	loss_policy_1: 0.03993
	accuracy_policy_1: 0.61316
	loss_value_1: 0.03829
	loss_reward_1: 0.00656
	loss_policy_2: 0.04481
	accuracy_policy_2: 0.57648
	loss_value_2: 0.03961
	loss_reward_2: 0.00593
	loss_policy_3: 0.04827
	accuracy_policy_3: 0.54695
	loss_value_3: 0.04047
	loss_reward_3: 0.00629
	loss_policy_4: 0.0509
	accuracy_policy_4: 0.52391
	loss_value_4: 0.04157
	loss_reward_4: 0.00712
	loss_policy_5: 0.05369
	accuracy_policy_5: 0.50598
	loss_value_5: 0.04234
	loss_reward_5: 0.00837
	loss_policy: 0.3693
	loss_value: 0.38463
	loss_reward: 0.03427
[2024-05-04 22:15:43] nn step 22400, lr: 0.04.
	loss_policy_0: 0.12249
	accuracy_policy_0: 0.7268
	loss_value_0: 0.17395
	loss_policy_1: 0.03778
	accuracy_policy_1: 0.61781
	loss_value_1: 0.03667
	loss_reward_1: 0.00637
	loss_policy_2: 0.04202
	accuracy_policy_2: 0.58148
	loss_value_2: 0.03768
	loss_reward_2: 0.00561
	loss_policy_3: 0.04501
	accuracy_policy_3: 0.55859
	loss_value_3: 0.03879
	loss_reward_3: 0.00609
	loss_policy_4: 0.04845
	accuracy_policy_4: 0.53254
	loss_value_4: 0.03965
	loss_reward_4: 0.00652
	loss_policy_5: 0.05063
	accuracy_policy_5: 0.51855
	loss_value_5: 0.04051
	loss_reward_5: 0.00776
	loss_policy: 0.34638
	loss_value: 0.36725
	loss_reward: 0.03234
Optimization_Done 22400
[2024-05-04 22:17:42] [command] train weight_iter_22400.pkl 111 113
[2024-05-04 22:18:00] nn step 22450, lr: 0.04.
	loss_policy_0: 0.15438
	accuracy_policy_0: 0.6509
	loss_value_0: 0.16805
	loss_policy_1: 0.04059
	accuracy_policy_1: 0.57477
	loss_value_1: 0.03524
	loss_reward_1: 0.00558
	loss_policy_2: 0.0445
	accuracy_policy_2: 0.53984
	loss_value_2: 0.03666
	loss_reward_2: 0.00536
	loss_policy_3: 0.04775
	accuracy_policy_3: 0.50785
	loss_value_3: 0.03767
	loss_reward_3: 0.00577
	loss_policy_4: 0.05007
	accuracy_policy_4: 0.49199
	loss_value_4: 0.03862
	loss_reward_4: 0.00644
	loss_policy_5: 0.05293
	accuracy_policy_5: 0.47285
	loss_value_5: 0.03944
	loss_reward_5: 0.00721
	loss_policy: 0.39021
	loss_value: 0.35569
	loss_reward: 0.03036
[2024-05-04 22:18:17] nn step 22500, lr: 0.04.
	loss_policy_0: 0.14995
	accuracy_policy_0: 0.68691
	loss_value_0: 0.17937
	loss_policy_1: 0.04185
	accuracy_policy_1: 0.6009
	loss_value_1: 0.03777
	loss_reward_1: 0.0061
	loss_policy_2: 0.04565
	accuracy_policy_2: 0.56809
	loss_value_2: 0.03897
	loss_reward_2: 0.00575
	loss_policy_3: 0.04923
	accuracy_policy_3: 0.54102
	loss_value_3: 0.04023
	loss_reward_3: 0.00617
	loss_policy_4: 0.05242
	accuracy_policy_4: 0.51523
	loss_value_4: 0.04132
	loss_reward_4: 0.007
	loss_policy_5: 0.05482
	accuracy_policy_5: 0.50121
	loss_value_5: 0.04211
	loss_reward_5: 0.00801
	loss_policy: 0.39391
	loss_value: 0.37978
	loss_reward: 0.03303
[2024-05-04 22:18:33] nn step 22550, lr: 0.04.
	loss_policy_0: 0.13991
	accuracy_policy_0: 0.70039
	loss_value_0: 0.17626
	loss_policy_1: 0.03985
	accuracy_policy_1: 0.60391
	loss_value_1: 0.03689
	loss_reward_1: 0.00615
	loss_policy_2: 0.04364
	accuracy_policy_2: 0.57609
	loss_value_2: 0.03809
	loss_reward_2: 0.00567
	loss_policy_3: 0.04737
	accuracy_policy_3: 0.54891
	loss_value_3: 0.03933
	loss_reward_3: 0.00599
	loss_policy_4: 0.05054
	accuracy_policy_4: 0.5291
	loss_value_4: 0.04028
	loss_reward_4: 0.00679
	loss_policy_5: 0.05281
	accuracy_policy_5: 0.50793
	loss_value_5: 0.04118
	loss_reward_5: 0.00791
	loss_policy: 0.37412
	loss_value: 0.37203
	loss_reward: 0.03251
[2024-05-04 22:18:50] nn step 22600, lr: 0.04.
	loss_policy_0: 0.13689
	accuracy_policy_0: 0.70473
	loss_value_0: 0.17667
	loss_policy_1: 0.03884
	accuracy_policy_1: 0.61656
	loss_value_1: 0.03699
	loss_reward_1: 0.00621
	loss_policy_2: 0.04279
	accuracy_policy_2: 0.58695
	loss_value_2: 0.03829
	loss_reward_2: 0.00553
	loss_policy_3: 0.0466
	accuracy_policy_3: 0.55445
	loss_value_3: 0.03953
	loss_reward_3: 0.00625
	loss_policy_4: 0.04954
	accuracy_policy_4: 0.53656
	loss_value_4: 0.04035
	loss_reward_4: 0.0068
	loss_policy_5: 0.05198
	accuracy_policy_5: 0.51809
	loss_value_5: 0.04135
	loss_reward_5: 0.00796
	loss_policy: 0.36664
	loss_value: 0.37318
	loss_reward: 0.03274
Optimization_Done 22600
[2024-05-04 22:20:48] [command] train weight_iter_22600.pkl 112 114
[2024-05-04 22:21:05] nn step 22650, lr: 0.04.
	loss_policy_0: 0.19689
	accuracy_policy_0: 0.62289
	loss_value_0: 0.16701
	loss_policy_1: 0.04684
	accuracy_policy_1: 0.56605
	loss_value_1: 0.03506
	loss_reward_1: 0.00512
	loss_policy_2: 0.05014
	accuracy_policy_2: 0.5323
	loss_value_2: 0.03651
	loss_reward_2: 0.00495
	loss_policy_3: 0.05335
	accuracy_policy_3: 0.50797
	loss_value_3: 0.03784
	loss_reward_3: 0.00538
	loss_policy_4: 0.05589
	accuracy_policy_4: 0.48652
	loss_value_4: 0.03904
	loss_reward_4: 0.00605
	loss_policy_5: 0.05846
	accuracy_policy_5: 0.4632
	loss_value_5: 0.04006
	loss_reward_5: 0.00684
	loss_policy: 0.46158
	loss_value: 0.35551
	loss_reward: 0.02834
[2024-05-04 22:21:22] nn step 22700, lr: 0.04.
	loss_policy_0: 0.16231
	accuracy_policy_0: 0.68051
	loss_value_0: 0.16137
	loss_policy_1: 0.04202
	accuracy_policy_1: 0.59871
	loss_value_1: 0.0342
	loss_reward_1: 0.00508
	loss_policy_2: 0.04526
	accuracy_policy_2: 0.57145
	loss_value_2: 0.0357
	loss_reward_2: 0.00513
	loss_policy_3: 0.0484
	accuracy_policy_3: 0.54562
	loss_value_3: 0.03707
	loss_reward_3: 0.00533
	loss_policy_4: 0.05185
	accuracy_policy_4: 0.51871
	loss_value_4: 0.03822
	loss_reward_4: 0.00593
	loss_policy_5: 0.05449
	accuracy_policy_5: 0.49449
	loss_value_5: 0.03935
	loss_reward_5: 0.00676
	loss_policy: 0.40431
	loss_value: 0.34591
	loss_reward: 0.02823
[2024-05-04 22:21:39] nn step 22750, lr: 0.04.
	loss_policy_0: 0.14053
	accuracy_policy_0: 0.69004
	loss_value_0: 0.14587
	loss_policy_1: 0.03714
	accuracy_policy_1: 0.60289
	loss_value_1: 0.03087
	loss_reward_1: 0.00467
	loss_policy_2: 0.03974
	accuracy_policy_2: 0.57977
	loss_value_2: 0.03225
	loss_reward_2: 0.00457
	loss_policy_3: 0.04253
	accuracy_policy_3: 0.56117
	loss_value_3: 0.03329
	loss_reward_3: 0.00476
	loss_policy_4: 0.04522
	accuracy_policy_4: 0.53715
	loss_value_4: 0.03415
	loss_reward_4: 0.00537
	loss_policy_5: 0.04815
	accuracy_policy_5: 0.51008
	loss_value_5: 0.03526
	loss_reward_5: 0.00627
	loss_policy: 0.35331
	loss_value: 0.31167
	loss_reward: 0.02564
[2024-05-04 22:21:55] nn step 22800, lr: 0.04.
	loss_policy_0: 0.15181
	accuracy_policy_0: 0.69359
	loss_value_0: 0.16191
	loss_policy_1: 0.0404
	accuracy_policy_1: 0.61238
	loss_value_1: 0.03418
	loss_reward_1: 0.00559
	loss_policy_2: 0.04412
	accuracy_policy_2: 0.58535
	loss_value_2: 0.0359
	loss_reward_2: 0.00501
	loss_policy_3: 0.04723
	accuracy_policy_3: 0.56223
	loss_value_3: 0.03704
	loss_reward_3: 0.0055
	loss_policy_4: 0.05036
	accuracy_policy_4: 0.53184
	loss_value_4: 0.03808
	loss_reward_4: 0.00591
	loss_policy_5: 0.05292
	accuracy_policy_5: 0.51684
	loss_value_5: 0.03913
	loss_reward_5: 0.00699
	loss_policy: 0.38684
	loss_value: 0.34624
	loss_reward: 0.02901
Optimization_Done 22800
[2024-05-04 22:23:59] [command] train weight_iter_22800.pkl 113 115
[2024-05-04 22:24:16] nn step 22850, lr: 0.04.
	loss_policy_0: 0.16436
	accuracy_policy_0: 0.6682
	loss_value_0: 0.16144
	loss_policy_1: 0.04195
	accuracy_policy_1: 0.59246
	loss_value_1: 0.03406
	loss_reward_1: 0.00612
	loss_policy_2: 0.04517
	accuracy_policy_2: 0.56883
	loss_value_2: 0.03558
	loss_reward_2: 0.00564
	loss_policy_3: 0.04814
	accuracy_policy_3: 0.54359
	loss_value_3: 0.03691
	loss_reward_3: 0.00617
	loss_policy_4: 0.05069
	accuracy_policy_4: 0.52148
	loss_value_4: 0.03804
	loss_reward_4: 0.0067
	loss_policy_5: 0.05286
	accuracy_policy_5: 0.50656
	loss_value_5: 0.03893
	loss_reward_5: 0.00784
	loss_policy: 0.40317
	loss_value: 0.34495
	loss_reward: 0.03248
[2024-05-04 22:24:33] nn step 22900, lr: 0.04.
	loss_policy_0: 0.14441
	accuracy_policy_0: 0.69883
	loss_value_0: 0.15613
	loss_policy_1: 0.03827
	accuracy_policy_1: 0.62219
	loss_value_1: 0.03295
	loss_reward_1: 0.00582
	loss_policy_2: 0.04203
	accuracy_policy_2: 0.58898
	loss_value_2: 0.03441
	loss_reward_2: 0.00551
	loss_policy_3: 0.0452
	accuracy_policy_3: 0.56051
	loss_value_3: 0.03586
	loss_reward_3: 0.00603
	loss_policy_4: 0.04764
	accuracy_policy_4: 0.54152
	loss_value_4: 0.03699
	loss_reward_4: 0.00659
	loss_policy_5: 0.0495
	accuracy_policy_5: 0.53012
	loss_value_5: 0.03796
	loss_reward_5: 0.00755
	loss_policy: 0.36706
	loss_value: 0.3343
	loss_reward: 0.0315
[2024-05-04 22:24:49] nn step 22950, lr: 0.04.
	loss_policy_0: 0.15219
	accuracy_policy_0: 0.71004
	loss_value_0: 0.17128
	loss_policy_1: 0.04134
	accuracy_policy_1: 0.62684
	loss_value_1: 0.03629
	loss_reward_1: 0.00643
	loss_policy_2: 0.04465
	accuracy_policy_2: 0.59684
	loss_value_2: 0.03784
	loss_reward_2: 0.00608
	loss_policy_3: 0.04861
	accuracy_policy_3: 0.56645
	loss_value_3: 0.03909
	loss_reward_3: 0.0065
	loss_policy_4: 0.05129
	accuracy_policy_4: 0.55426
	loss_value_4: 0.04034
	loss_reward_4: 0.00692
	loss_policy_5: 0.05431
	accuracy_policy_5: 0.5316
	loss_value_5: 0.04167
	loss_reward_5: 0.00833
	loss_policy: 0.39239
	loss_value: 0.36651
	loss_reward: 0.03426
[2024-05-04 22:25:06] nn step 23000, lr: 0.04.
	loss_policy_0: 0.13237
	accuracy_policy_0: 0.7173
	loss_value_0: 0.1552
	loss_policy_1: 0.0367
	accuracy_policy_1: 0.6298
	loss_value_1: 0.03276
	loss_reward_1: 0.00584
	loss_policy_2: 0.04063
	accuracy_policy_2: 0.59727
	loss_value_2: 0.03416
	loss_reward_2: 0.00532
	loss_policy_3: 0.04342
	accuracy_policy_3: 0.56969
	loss_value_3: 0.03551
	loss_reward_3: 0.00579
	loss_policy_4: 0.04635
	accuracy_policy_4: 0.5527
	loss_value_4: 0.03659
	loss_reward_4: 0.00639
	loss_policy_5: 0.04821
	accuracy_policy_5: 0.53586
	loss_value_5: 0.03741
	loss_reward_5: 0.00751
	loss_policy: 0.34768
	loss_value: 0.33163
	loss_reward: 0.03084
Optimization_Done 23000
[2024-05-04 22:27:07] [command] train weight_iter_23000.pkl 114 116
[2024-05-04 22:27:24] nn step 23050, lr: 0.04.
	loss_policy_0: 0.15679
	accuracy_policy_0: 0.69711
	loss_value_0: 0.16735
	loss_policy_1: 0.04057
	accuracy_policy_1: 0.61668
	loss_value_1: 0.03531
	loss_reward_1: 0.00597
	loss_policy_2: 0.04392
	accuracy_policy_2: 0.58922
	loss_value_2: 0.0367
	loss_reward_2: 0.00544
	loss_policy_3: 0.047
	accuracy_policy_3: 0.56648
	loss_value_3: 0.03783
	loss_reward_3: 0.00592
	loss_policy_4: 0.04913
	accuracy_policy_4: 0.54859
	loss_value_4: 0.03905
	loss_reward_4: 0.00633
	loss_policy_5: 0.05116
	accuracy_policy_5: 0.53484
	loss_value_5: 0.03992
	loss_reward_5: 0.00707
	loss_policy: 0.38857
	loss_value: 0.35616
	loss_reward: 0.03072
[2024-05-04 22:27:41] nn step 23100, lr: 0.04.
	loss_policy_0: 0.135
	accuracy_policy_0: 0.71918
	loss_value_0: 0.15934
	loss_policy_1: 0.03701
	accuracy_policy_1: 0.63664
	loss_value_1: 0.03373
	loss_reward_1: 0.0055
	loss_policy_2: 0.04083
	accuracy_policy_2: 0.60301
	loss_value_2: 0.03502
	loss_reward_2: 0.00515
	loss_policy_3: 0.04377
	accuracy_policy_3: 0.58051
	loss_value_3: 0.03587
	loss_reward_3: 0.0057
	loss_policy_4: 0.04568
	accuracy_policy_4: 0.56617
	loss_value_4: 0.03686
	loss_reward_4: 0.0059
	loss_policy_5: 0.04716
	accuracy_policy_5: 0.5525
	loss_value_5: 0.03782
	loss_reward_5: 0.00705
	loss_policy: 0.34944
	loss_value: 0.33863
	loss_reward: 0.0293
[2024-05-04 22:27:58] nn step 23150, lr: 0.04.
	loss_policy_0: 0.13319
	accuracy_policy_0: 0.72539
	loss_value_0: 0.15992
	loss_policy_1: 0.03681
	accuracy_policy_1: 0.6391
	loss_value_1: 0.03378
	loss_reward_1: 0.0056
	loss_policy_2: 0.03997
	accuracy_policy_2: 0.6077
	loss_value_2: 0.03497
	loss_reward_2: 0.00516
	loss_policy_3: 0.04312
	accuracy_policy_3: 0.57988
	loss_value_3: 0.03618
	loss_reward_3: 0.00551
	loss_policy_4: 0.04543
	accuracy_policy_4: 0.56656
	loss_value_4: 0.03739
	loss_reward_4: 0.00619
	loss_policy_5: 0.04712
	accuracy_policy_5: 0.55758
	loss_value_5: 0.03815
	loss_reward_5: 0.00713
	loss_policy: 0.34563
	loss_value: 0.34038
	loss_reward: 0.02959
[2024-05-04 22:28:14] nn step 23200, lr: 0.04.
	loss_policy_0: 0.13305
	accuracy_policy_0: 0.73242
	loss_value_0: 0.16232
	loss_policy_1: 0.03703
	accuracy_policy_1: 0.64219
	loss_value_1: 0.03398
	loss_reward_1: 0.00586
	loss_policy_2: 0.0403
	accuracy_policy_2: 0.61609
	loss_value_2: 0.03536
	loss_reward_2: 0.00521
	loss_policy_3: 0.04363
	accuracy_policy_3: 0.58734
	loss_value_3: 0.03665
	loss_reward_3: 0.00575
	loss_policy_4: 0.0459
	accuracy_policy_4: 0.57156
	loss_value_4: 0.03784
	loss_reward_4: 0.00605
	loss_policy_5: 0.04768
	accuracy_policy_5: 0.55867
	loss_value_5: 0.03891
	loss_reward_5: 0.00722
	loss_policy: 0.34759
	loss_value: 0.34505
	loss_reward: 0.03009
Optimization_Done 23200
[2024-05-04 22:29:59] [command] train weight_iter_23200.pkl 115 117
[2024-05-04 22:30:16] nn step 23250, lr: 0.04.
	loss_policy_0: 0.14636
	accuracy_policy_0: 0.66016
	loss_value_0: 0.1573
	loss_policy_1: 0.03778
	accuracy_policy_1: 0.5818
	loss_value_1: 0.03306
	loss_reward_1: 0.00583
	loss_policy_2: 0.04119
	accuracy_policy_2: 0.5502
	loss_value_2: 0.03427
	loss_reward_2: 0.00538
	loss_policy_3: 0.04407
	accuracy_policy_3: 0.52379
	loss_value_3: 0.03532
	loss_reward_3: 0.00605
	loss_policy_4: 0.04589
	accuracy_policy_4: 0.50883
	loss_value_4: 0.03631
	loss_reward_4: 0.00669
	loss_policy_5: 0.04758
	accuracy_policy_5: 0.49473
	loss_value_5: 0.03723
	loss_reward_5: 0.00767
	loss_policy: 0.36287
	loss_value: 0.33348
	loss_reward: 0.03161
[2024-05-04 22:30:33] nn step 23300, lr: 0.04.
	loss_policy_0: 0.13161
	accuracy_policy_0: 0.69934
	loss_value_0: 0.16223
	loss_policy_1: 0.03614
	accuracy_policy_1: 0.61891
	loss_value_1: 0.03421
	loss_reward_1: 0.00601
	loss_policy_2: 0.03942
	accuracy_policy_2: 0.59078
	loss_value_2: 0.03549
	loss_reward_2: 0.00564
	loss_policy_3: 0.04249
	accuracy_policy_3: 0.55395
	loss_value_3: 0.03672
	loss_reward_3: 0.00624
	loss_policy_4: 0.04522
	accuracy_policy_4: 0.54266
	loss_value_4: 0.03747
	loss_reward_4: 0.00683
	loss_policy_5: 0.04676
	accuracy_policy_5: 0.52406
	loss_value_5: 0.03833
	loss_reward_5: 0.00802
	loss_policy: 0.34165
	loss_value: 0.34446
	loss_reward: 0.03274
[2024-05-04 22:30:49] nn step 23350, lr: 0.04.
	loss_policy_0: 0.13101
	accuracy_policy_0: 0.71238
	loss_value_0: 0.16727
	loss_policy_1: 0.03639
	accuracy_policy_1: 0.61875
	loss_value_1: 0.03508
	loss_reward_1: 0.0062
	loss_policy_2: 0.04036
	accuracy_policy_2: 0.58504
	loss_value_2: 0.03635
	loss_reward_2: 0.00565
	loss_policy_3: 0.04319
	accuracy_policy_3: 0.56395
	loss_value_3: 0.03745
	loss_reward_3: 0.00643
	loss_policy_4: 0.04579
	accuracy_policy_4: 0.54258
	loss_value_4: 0.03855
	loss_reward_4: 0.00688
	loss_policy_5: 0.04734
	accuracy_policy_5: 0.53027
	loss_value_5: 0.03941
	loss_reward_5: 0.00824
	loss_policy: 0.34408
	loss_value: 0.35411
	loss_reward: 0.0334
[2024-05-04 22:31:05] nn step 23400, lr: 0.04.
	loss_policy_0: 0.12112
	accuracy_policy_0: 0.72773
	loss_value_0: 0.16532
	loss_policy_1: 0.03487
	accuracy_policy_1: 0.6291
	loss_value_1: 0.03478
	loss_reward_1: 0.00603
	loss_policy_2: 0.03917
	accuracy_policy_2: 0.59332
	loss_value_2: 0.036
	loss_reward_2: 0.0056
	loss_policy_3: 0.04178
	accuracy_policy_3: 0.57641
	loss_value_3: 0.03721
	loss_reward_3: 0.00615
	loss_policy_4: 0.04445
	accuracy_policy_4: 0.55
	loss_value_4: 0.03815
	loss_reward_4: 0.00659
	loss_policy_5: 0.04644
	accuracy_policy_5: 0.53594
	loss_value_5: 0.0392
	loss_reward_5: 0.00804
	loss_policy: 0.32783
	loss_value: 0.35065
	loss_reward: 0.03241
Optimization_Done 23400
[2024-05-04 22:33:10] [command] train weight_iter_23400.pkl 116 118
[2024-05-04 22:33:28] nn step 23450, lr: 0.04.
	loss_policy_0: 0.17866
	accuracy_policy_0: 0.61547
	loss_value_0: 0.16529
	loss_policy_1: 0.0443
	accuracy_policy_1: 0.5448
	loss_value_1: 0.03472
	loss_reward_1: 0.00556
	loss_policy_2: 0.04653
	accuracy_policy_2: 0.52426
	loss_value_2: 0.03624
	loss_reward_2: 0.00508
	loss_policy_3: 0.05006
	accuracy_policy_3: 0.50289
	loss_value_3: 0.03746
	loss_reward_3: 0.00556
	loss_policy_4: 0.05234
	accuracy_policy_4: 0.48672
	loss_value_4: 0.03866
	loss_reward_4: 0.00607
	loss_policy_5: 0.054
	accuracy_policy_5: 0.46777
	loss_value_5: 0.0396
	loss_reward_5: 0.00701
	loss_policy: 0.42589
	loss_value: 0.35197
	loss_reward: 0.02928
[2024-05-04 22:33:45] nn step 23500, lr: 0.04.
	loss_policy_0: 0.15422
	accuracy_policy_0: 0.66492
	loss_value_0: 0.16107
	loss_policy_1: 0.04083
	accuracy_policy_1: 0.57855
	loss_value_1: 0.03403
	loss_reward_1: 0.00547
	loss_policy_2: 0.04436
	accuracy_policy_2: 0.54766
	loss_value_2: 0.03558
	loss_reward_2: 0.00508
	loss_policy_3: 0.04702
	accuracy_policy_3: 0.53148
	loss_value_3: 0.03688
	loss_reward_3: 0.00546
	loss_policy_4: 0.04984
	accuracy_policy_4: 0.51297
	loss_value_4: 0.03798
	loss_reward_4: 0.0062
	loss_policy_5: 0.0513
	accuracy_policy_5: 0.49781
	loss_value_5: 0.03901
	loss_reward_5: 0.00694
	loss_policy: 0.38755
	loss_value: 0.34454
	loss_reward: 0.02916
[2024-05-04 22:34:01] nn step 23550, lr: 0.04.
	loss_policy_0: 0.14417
	accuracy_policy_0: 0.67562
	loss_value_0: 0.15549
	loss_policy_1: 0.03849
	accuracy_policy_1: 0.58508
	loss_value_1: 0.03278
	loss_reward_1: 0.00545
	loss_policy_2: 0.04188
	accuracy_policy_2: 0.55992
	loss_value_2: 0.03409
	loss_reward_2: 0.00504
	loss_policy_3: 0.04491
	accuracy_policy_3: 0.53582
	loss_value_3: 0.03534
	loss_reward_3: 0.00535
	loss_policy_4: 0.0473
	accuracy_policy_4: 0.51734
	loss_value_4: 0.03643
	loss_reward_4: 0.00586
	loss_policy_5: 0.04944
	accuracy_policy_5: 0.5023
	loss_value_5: 0.03744
	loss_reward_5: 0.00704
	loss_policy: 0.3662
	loss_value: 0.33157
	loss_reward: 0.02874
[2024-05-04 22:34:17] nn step 23600, lr: 0.04.
	loss_policy_0: 0.14063
	accuracy_policy_0: 0.69238
	loss_value_0: 0.15792
	loss_policy_1: 0.03821
	accuracy_policy_1: 0.5957
	loss_value_1: 0.03323
	loss_reward_1: 0.00536
	loss_policy_2: 0.04173
	accuracy_policy_2: 0.56715
	loss_value_2: 0.0346
	loss_reward_2: 0.00487
	loss_policy_3: 0.04478
	accuracy_policy_3: 0.54727
	loss_value_3: 0.036
	loss_reward_3: 0.00541
	loss_policy_4: 0.04719
	accuracy_policy_4: 0.52719
	loss_value_4: 0.03713
	loss_reward_4: 0.00595
	loss_policy_5: 0.04914
	accuracy_policy_5: 0.5134
	loss_value_5: 0.03808
	loss_reward_5: 0.00702
	loss_policy: 0.36167
	loss_value: 0.33695
	loss_reward: 0.0286
Optimization_Done 23600
[2024-05-04 22:36:20] [command] train weight_iter_23600.pkl 117 119
[2024-05-04 22:36:37] nn step 23650, lr: 0.04.
	loss_policy_0: 0.15093
	accuracy_policy_0: 0.68562
	loss_value_0: 0.17238
	loss_policy_1: 0.04014
	accuracy_policy_1: 0.60605
	loss_value_1: 0.03614
	loss_reward_1: 0.00684
	loss_policy_2: 0.04407
	accuracy_policy_2: 0.57594
	loss_value_2: 0.03774
	loss_reward_2: 0.00643
	loss_policy_3: 0.04754
	accuracy_policy_3: 0.54906
	loss_value_3: 0.03893
	loss_reward_3: 0.00683
	loss_policy_4: 0.04961
	accuracy_policy_4: 0.53246
	loss_value_4: 0.04
	loss_reward_4: 0.00765
	loss_policy_5: 0.05164
	accuracy_policy_5: 0.52277
	loss_value_5: 0.04087
	loss_reward_5: 0.00866
	loss_policy: 0.38392
	loss_value: 0.36606
	loss_reward: 0.03642
[2024-05-04 22:36:54] nn step 23700, lr: 0.04.
	loss_policy_0: 0.14229
	accuracy_policy_0: 0.71262
	loss_value_0: 0.17382
	loss_policy_1: 0.03949
	accuracy_policy_1: 0.62273
	loss_value_1: 0.03655
	loss_reward_1: 0.00691
	loss_policy_2: 0.04389
	accuracy_policy_2: 0.59023
	loss_value_2: 0.03811
	loss_reward_2: 0.00644
	loss_policy_3: 0.04682
	accuracy_policy_3: 0.56676
	loss_value_3: 0.03948
	loss_reward_3: 0.00707
	loss_policy_4: 0.04877
	accuracy_policy_4: 0.55945
	loss_value_4: 0.04061
	loss_reward_4: 0.00757
	loss_policy_5: 0.05158
	accuracy_policy_5: 0.53684
	loss_value_5: 0.04156
	loss_reward_5: 0.00891
	loss_policy: 0.37285
	loss_value: 0.37013
	loss_reward: 0.0369
[2024-05-04 22:37:10] nn step 23750, lr: 0.04.
	loss_policy_0: 0.14463
	accuracy_policy_0: 0.71227
	loss_value_0: 0.17961
	loss_policy_1: 0.04023
	accuracy_policy_1: 0.62125
	loss_value_1: 0.03791
	loss_reward_1: 0.00718
	loss_policy_2: 0.04414
	accuracy_policy_2: 0.59012
	loss_value_2: 0.03934
	loss_reward_2: 0.00645
	loss_policy_3: 0.04801
	accuracy_policy_3: 0.56277
	loss_value_3: 0.04097
	loss_reward_3: 0.00702
	loss_policy_4: 0.05028
	accuracy_policy_4: 0.54922
	loss_value_4: 0.04214
	loss_reward_4: 0.0078
	loss_policy_5: 0.05252
	accuracy_policy_5: 0.53793
	loss_value_5: 0.04301
	loss_reward_5: 0.00905
	loss_policy: 0.37981
	loss_value: 0.38297
	loss_reward: 0.03749
[2024-05-04 22:37:27] nn step 23800, lr: 0.04.
	loss_policy_0: 0.13213
	accuracy_policy_0: 0.71117
	loss_value_0: 0.16807
	loss_policy_1: 0.03722
	accuracy_policy_1: 0.62703
	loss_value_1: 0.03534
	loss_reward_1: 0.00662
	loss_policy_2: 0.04112
	accuracy_policy_2: 0.59316
	loss_value_2: 0.03652
	loss_reward_2: 0.00621
	loss_policy_3: 0.04482
	accuracy_policy_3: 0.56605
	loss_value_3: 0.03759
	loss_reward_3: 0.00665
	loss_policy_4: 0.04677
	accuracy_policy_4: 0.5525
	loss_value_4: 0.0386
	loss_reward_4: 0.00716
	loss_policy_5: 0.04869
	accuracy_policy_5: 0.53906
	loss_value_5: 0.0397
	loss_reward_5: 0.00845
	loss_policy: 0.35074
	loss_value: 0.35581
	loss_reward: 0.03509
Optimization_Done 23800
[2024-05-04 22:39:29] [command] train weight_iter_23800.pkl 118 120
[2024-05-04 22:39:46] nn step 23850, lr: 0.04.
	loss_policy_0: 0.14985
	accuracy_policy_0: 0.67605
	loss_value_0: 0.1639
	loss_policy_1: 0.03962
	accuracy_policy_1: 0.59746
	loss_value_1: 0.03457
	loss_reward_1: 0.00644
	loss_policy_2: 0.04311
	accuracy_policy_2: 0.56383
	loss_value_2: 0.03611
	loss_reward_2: 0.00586
	loss_policy_3: 0.04579
	accuracy_policy_3: 0.54754
	loss_value_3: 0.0375
	loss_reward_3: 0.00651
	loss_policy_4: 0.0483
	accuracy_policy_4: 0.5325
	loss_value_4: 0.03868
	loss_reward_4: 0.00697
	loss_policy_5: 0.04996
	accuracy_policy_5: 0.51246
	loss_value_5: 0.03965
	loss_reward_5: 0.00803
	loss_policy: 0.37664
	loss_value: 0.35041
	loss_reward: 0.03381
[2024-05-04 22:40:03] nn step 23900, lr: 0.04.
	loss_policy_0: 0.13413
	accuracy_policy_0: 0.70875
	loss_value_0: 0.16366
	loss_policy_1: 0.03743
	accuracy_policy_1: 0.61539
	loss_value_1: 0.03475
	loss_reward_1: 0.00638
	loss_policy_2: 0.04081
	accuracy_policy_2: 0.58945
	loss_value_2: 0.03614
	loss_reward_2: 0.00576
	loss_policy_3: 0.04398
	accuracy_policy_3: 0.56617
	loss_value_3: 0.03729
	loss_reward_3: 0.00635
	loss_policy_4: 0.04681
	accuracy_policy_4: 0.54738
	loss_value_4: 0.03851
	loss_reward_4: 0.00685
	loss_policy_5: 0.04854
	accuracy_policy_5: 0.54051
	loss_value_5: 0.03965
	loss_reward_5: 0.00786
	loss_policy: 0.35171
	loss_value: 0.35
	loss_reward: 0.0332
[2024-05-04 22:40:19] nn step 23950, lr: 0.04.
	loss_policy_0: 0.13637
	accuracy_policy_0: 0.71363
	loss_value_0: 0.17402
	loss_policy_1: 0.03814
	accuracy_policy_1: 0.63188
	loss_value_1: 0.03688
	loss_reward_1: 0.00692
	loss_policy_2: 0.0429
	accuracy_policy_2: 0.59066
	loss_value_2: 0.03833
	loss_reward_2: 0.00623
	loss_policy_3: 0.0463
	accuracy_policy_3: 0.56797
	loss_value_3: 0.03942
	loss_reward_3: 0.0065
	loss_policy_4: 0.04878
	accuracy_policy_4: 0.55152
	loss_value_4: 0.04045
	loss_reward_4: 0.00726
	loss_policy_5: 0.05126
	accuracy_policy_5: 0.53801
	loss_value_5: 0.04155
	loss_reward_5: 0.00844
	loss_policy: 0.36374
	loss_value: 0.37065
	loss_reward: 0.03535
[2024-05-04 22:40:36] nn step 24000, lr: 0.04.
	loss_policy_0: 0.12379
	accuracy_policy_0: 0.72457
	loss_value_0: 0.16377
	loss_policy_1: 0.03555
	accuracy_policy_1: 0.63938
	loss_value_1: 0.0343
	loss_reward_1: 0.00636
	loss_policy_2: 0.03964
	accuracy_policy_2: 0.6027
	loss_value_2: 0.03566
	loss_reward_2: 0.0057
	loss_policy_3: 0.04318
	accuracy_policy_3: 0.57008
	loss_value_3: 0.03678
	loss_reward_3: 0.00615
	loss_policy_4: 0.04544
	accuracy_policy_4: 0.55875
	loss_value_4: 0.03781
	loss_reward_4: 0.00669
	loss_policy_5: 0.04723
	accuracy_policy_5: 0.54496
	loss_value_5: 0.03903
	loss_reward_5: 0.00807
	loss_policy: 0.33483
	loss_value: 0.34735
	loss_reward: 0.03298
Optimization_Done 24000
[2024-05-04 22:42:27] [command] train weight_iter_24000.pkl 119 121
[2024-05-04 22:42:44] nn step 24050, lr: 0.04.
	loss_policy_0: 0.15063
	accuracy_policy_0: 0.67859
	loss_value_0: 0.17176
	loss_policy_1: 0.03873
	accuracy_policy_1: 0.60984
	loss_value_1: 0.03609
	loss_reward_1: 0.00622
	loss_policy_2: 0.04271
	accuracy_policy_2: 0.57207
	loss_value_2: 0.03731
	loss_reward_2: 0.00573
	loss_policy_3: 0.04556
	accuracy_policy_3: 0.55434
	loss_value_3: 0.03839
	loss_reward_3: 0.0064
	loss_policy_4: 0.04745
	accuracy_policy_4: 0.53895
	loss_value_4: 0.03925
	loss_reward_4: 0.00683
	loss_policy_5: 0.04989
	accuracy_policy_5: 0.5193
	loss_value_5: 0.04044
	loss_reward_5: 0.00803
	loss_policy: 0.37497
	loss_value: 0.36323
	loss_reward: 0.03321
[2024-05-04 22:43:01] nn step 24100, lr: 0.04.
	loss_policy_0: 0.1359
	accuracy_policy_0: 0.71527
	loss_value_0: 0.17633
	loss_policy_1: 0.03691
	accuracy_policy_1: 0.63668
	loss_value_1: 0.03738
	loss_reward_1: 0.00666
	loss_policy_2: 0.04141
	accuracy_policy_2: 0.60117
	loss_value_2: 0.03859
	loss_reward_2: 0.006
	loss_policy_3: 0.04472
	accuracy_policy_3: 0.57238
	loss_value_3: 0.0396
	loss_reward_3: 0.00637
	loss_policy_4: 0.04752
	accuracy_policy_4: 0.55406
	loss_value_4: 0.04069
	loss_reward_4: 0.00715
	loss_policy_5: 0.04944
	accuracy_policy_5: 0.5373
	loss_value_5: 0.04182
	loss_reward_5: 0.00838
	loss_policy: 0.35591
	loss_value: 0.37441
	loss_reward: 0.03456
[2024-05-04 22:43:17] nn step 24150, lr: 0.04.
	loss_policy_0: 0.12623
	accuracy_policy_0: 0.7316
	loss_value_0: 0.17437
	loss_policy_1: 0.03599
	accuracy_policy_1: 0.64703
	loss_value_1: 0.0369
	loss_reward_1: 0.00641
	loss_policy_2: 0.03996
	accuracy_policy_2: 0.60711
	loss_value_2: 0.03813
	loss_reward_2: 0.00586
	loss_policy_3: 0.04313
	accuracy_policy_3: 0.58387
	loss_value_3: 0.03936
	loss_reward_3: 0.00647
	loss_policy_4: 0.04632
	accuracy_policy_4: 0.5593
	loss_value_4: 0.04048
	loss_reward_4: 0.00696
	loss_policy_5: 0.04841
	accuracy_policy_5: 0.54582
	loss_value_5: 0.04157
	loss_reward_5: 0.00822
	loss_policy: 0.34005
	loss_value: 0.37081
	loss_reward: 0.03392
[2024-05-04 22:43:34] nn step 24200, lr: 0.04.
	loss_policy_0: 0.12058
	accuracy_policy_0: 0.73879
	loss_value_0: 0.17167
	loss_policy_1: 0.03436
	accuracy_policy_1: 0.65473
	loss_value_1: 0.03632
	loss_reward_1: 0.00642
	loss_policy_2: 0.03817
	accuracy_policy_2: 0.62043
	loss_value_2: 0.03751
	loss_reward_2: 0.00577
	loss_policy_3: 0.0421
	accuracy_policy_3: 0.5873
	loss_value_3: 0.03832
	loss_reward_3: 0.00633
	loss_policy_4: 0.04448
	accuracy_policy_4: 0.5684
	loss_value_4: 0.0394
	loss_reward_4: 0.00715
	loss_policy_5: 0.04694
	accuracy_policy_5: 0.55434
	loss_value_5: 0.0405
	loss_reward_5: 0.00818
	loss_policy: 0.32663
	loss_value: 0.36372
	loss_reward: 0.03385
Optimization_Done 24200
[2024-05-04 22:45:25] [command] train weight_iter_24200.pkl 120 122
[2024-05-04 22:45:43] nn step 24250, lr: 0.04.
	loss_policy_0: 0.15663
	accuracy_policy_0: 0.67039
	loss_value_0: 0.16107
	loss_policy_1: 0.03846
	accuracy_policy_1: 0.61133
	loss_value_1: 0.03386
	loss_reward_1: 0.00557
	loss_policy_2: 0.04083
	accuracy_policy_2: 0.59152
	loss_value_2: 0.0353
	loss_reward_2: 0.00508
	loss_policy_3: 0.04382
	accuracy_policy_3: 0.56563
	loss_value_3: 0.03642
	loss_reward_3: 0.00543
	loss_policy_4: 0.04624
	accuracy_policy_4: 0.54336
	loss_value_4: 0.03765
	loss_reward_4: 0.00581
	loss_policy_5: 0.04872
	accuracy_policy_5: 0.51844
	loss_value_5: 0.03859
	loss_reward_5: 0.00674
	loss_policy: 0.37472
	loss_value: 0.3429
	loss_reward: 0.02862
[2024-05-04 22:45:59] nn step 24300, lr: 0.04.
	loss_policy_0: 0.13903
	accuracy_policy_0: 0.70629
	loss_value_0: 0.16159
	loss_policy_1: 0.03621
	accuracy_policy_1: 0.63941
	loss_value_1: 0.03423
	loss_reward_1: 0.00585
	loss_policy_2: 0.03972
	accuracy_policy_2: 0.61102
	loss_value_2: 0.03571
	loss_reward_2: 0.00533
	loss_policy_3: 0.04299
	accuracy_policy_3: 0.58906
	loss_value_3: 0.03691
	loss_reward_3: 0.00577
	loss_policy_4: 0.04576
	accuracy_policy_4: 0.5698
	loss_value_4: 0.03803
	loss_reward_4: 0.00618
	loss_policy_5: 0.04792
	accuracy_policy_5: 0.54895
	loss_value_5: 0.03909
	loss_reward_5: 0.00719
	loss_policy: 0.35163
	loss_value: 0.34556
	loss_reward: 0.03032
[2024-05-04 22:46:15] nn step 24350, lr: 0.04.
	loss_policy_0: 0.13697
	accuracy_policy_0: 0.72125
	loss_value_0: 0.16792
	loss_policy_1: 0.03642
	accuracy_policy_1: 0.64965
	loss_value_1: 0.03557
	loss_reward_1: 0.006
	loss_policy_2: 0.04043
	accuracy_policy_2: 0.62258
	loss_value_2: 0.03714
	loss_reward_2: 0.00548
	loss_policy_3: 0.04343
	accuracy_policy_3: 0.6032
	loss_value_3: 0.03842
	loss_reward_3: 0.00584
	loss_policy_4: 0.04642
	accuracy_policy_4: 0.57824
	loss_value_4: 0.03966
	loss_reward_4: 0.0066
	loss_policy_5: 0.04899
	accuracy_policy_5: 0.56117
	loss_value_5: 0.04074
	loss_reward_5: 0.00732
	loss_policy: 0.35266
	loss_value: 0.35945
	loss_reward: 0.03123
[2024-05-04 22:46:32] nn step 24400, lr: 0.04.
	loss_policy_0: 0.13613
	accuracy_policy_0: 0.7268
	loss_value_0: 0.17193
	loss_policy_1: 0.03666
	accuracy_policy_1: 0.65816
	loss_value_1: 0.0366
	loss_reward_1: 0.00626
	loss_policy_2: 0.04056
	accuracy_policy_2: 0.62824
	loss_value_2: 0.03796
	loss_reward_2: 0.00571
	loss_policy_3: 0.04395
	accuracy_policy_3: 0.60207
	loss_value_3: 0.03916
	loss_reward_3: 0.00604
	loss_policy_4: 0.04681
	accuracy_policy_4: 0.57984
	loss_value_4: 0.04026
	loss_reward_4: 0.00661
	loss_policy_5: 0.04929
	accuracy_policy_5: 0.56273
	loss_value_5: 0.04139
	loss_reward_5: 0.00749
	loss_policy: 0.3534
	loss_value: 0.3673
	loss_reward: 0.03212
Optimization_Done 24400
[2024-05-04 22:48:34] [command] train weight_iter_24400.pkl 121 123
[2024-05-04 22:48:52] nn step 24450, lr: 0.04.
	loss_policy_0: 0.1702
	accuracy_policy_0: 0.6727
	loss_value_0: 0.17418
	loss_policy_1: 0.0417
	accuracy_policy_1: 0.61211
	loss_value_1: 0.03678
	loss_reward_1: 0.00612
	loss_policy_2: 0.04512
	accuracy_policy_2: 0.58328
	loss_value_2: 0.03832
	loss_reward_2: 0.00596
	loss_policy_3: 0.04895
	accuracy_policy_3: 0.55762
	loss_value_3: 0.03972
	loss_reward_3: 0.00626
	loss_policy_4: 0.05195
	accuracy_policy_4: 0.53594
	loss_value_4: 0.04096
	loss_reward_4: 0.00674
	loss_policy_5: 0.05458
	accuracy_policy_5: 0.51246
	loss_value_5: 0.04206
	loss_reward_5: 0.00776
	loss_policy: 0.4125
	loss_value: 0.37203
	loss_reward: 0.03283
[2024-05-04 22:49:08] nn step 24500, lr: 0.04.
	loss_policy_0: 0.15089
	accuracy_policy_0: 0.71043
	loss_value_0: 0.1801
	loss_policy_1: 0.04056
	accuracy_policy_1: 0.63254
	loss_value_1: 0.03808
	loss_reward_1: 0.00618
	loss_policy_2: 0.04407
	accuracy_policy_2: 0.60566
	loss_value_2: 0.03979
	loss_reward_2: 0.0061
	loss_policy_3: 0.04811
	accuracy_policy_3: 0.57852
	loss_value_3: 0.0411
	loss_reward_3: 0.00665
	loss_policy_4: 0.05087
	accuracy_policy_4: 0.56012
	loss_value_4: 0.0424
	loss_reward_4: 0.00723
	loss_policy_5: 0.05412
	accuracy_policy_5: 0.53656
	loss_value_5: 0.0435
	loss_reward_5: 0.00815
	loss_policy: 0.38861
	loss_value: 0.38497
	loss_reward: 0.03431
[2024-05-04 22:49:25] nn step 24550, lr: 0.04.
	loss_policy_0: 0.13883
	accuracy_policy_0: 0.72039
	loss_value_0: 0.1723
	loss_policy_1: 0.0376
	accuracy_policy_1: 0.6402
	loss_value_1: 0.0363
	loss_reward_1: 0.00609
	loss_policy_2: 0.04105
	accuracy_policy_2: 0.61453
	loss_value_2: 0.03771
	loss_reward_2: 0.00576
	loss_policy_3: 0.045
	accuracy_policy_3: 0.58703
	loss_value_3: 0.03895
	loss_reward_3: 0.00634
	loss_policy_4: 0.04778
	accuracy_policy_4: 0.56645
	loss_value_4: 0.03995
	loss_reward_4: 0.00641
	loss_policy_5: 0.05041
	accuracy_policy_5: 0.54488
	loss_value_5: 0.04108
	loss_reward_5: 0.00757
	loss_policy: 0.36068
	loss_value: 0.36629
	loss_reward: 0.03218
[2024-05-04 22:49:42] nn step 24600, lr: 0.04.
	loss_policy_0: 0.1381
	accuracy_policy_0: 0.72836
	loss_value_0: 0.176
	loss_policy_1: 0.03763
	accuracy_policy_1: 0.64543
	loss_value_1: 0.0372
	loss_reward_1: 0.00626
	loss_policy_2: 0.04142
	accuracy_policy_2: 0.62023
	loss_value_2: 0.03894
	loss_reward_2: 0.00579
	loss_policy_3: 0.04486
	accuracy_policy_3: 0.58941
	loss_value_3: 0.04019
	loss_reward_3: 0.00645
	loss_policy_4: 0.04838
	accuracy_policy_4: 0.56609
	loss_value_4: 0.04137
	loss_reward_4: 0.00684
	loss_policy_5: 0.05107
	accuracy_policy_5: 0.54695
	loss_value_5: 0.04244
	loss_reward_5: 0.00798
	loss_policy: 0.36146
	loss_value: 0.37613
	loss_reward: 0.03332
Optimization_Done 24600
[2024-05-04 22:51:42] [command] train weight_iter_24600.pkl 122 124
[2024-05-04 22:52:00] nn step 24650, lr: 0.04.
	loss_policy_0: 0.1495
	accuracy_policy_0: 0.68656
	loss_value_0: 0.17137
	loss_policy_1: 0.03952
	accuracy_policy_1: 0.60648
	loss_value_1: 0.03612
	loss_reward_1: 0.00642
	loss_policy_2: 0.04312
	accuracy_policy_2: 0.58434
	loss_value_2: 0.03788
	loss_reward_2: 0.0058
	loss_policy_3: 0.04731
	accuracy_policy_3: 0.54918
	loss_value_3: 0.0394
	loss_reward_3: 0.00638
	loss_policy_4: 0.0496
	accuracy_policy_4: 0.5316
	loss_value_4: 0.04054
	loss_reward_4: 0.00679
	loss_policy_5: 0.05226
	accuracy_policy_5: 0.51355
	loss_value_5: 0.04158
	loss_reward_5: 0.00809
	loss_policy: 0.38132
	loss_value: 0.36689
	loss_reward: 0.03349
[2024-05-04 22:52:17] nn step 24700, lr: 0.04.
	loss_policy_0: 0.13671
	accuracy_policy_0: 0.71941
	loss_value_0: 0.17014
	loss_policy_1: 0.03688
	accuracy_policy_1: 0.63641
	loss_value_1: 0.03588
	loss_reward_1: 0.00632
	loss_policy_2: 0.04093
	accuracy_policy_2: 0.6043
	loss_value_2: 0.03729
	loss_reward_2: 0.0057
	loss_policy_3: 0.04416
	accuracy_policy_3: 0.58035
	loss_value_3: 0.03862
	loss_reward_3: 0.00628
	loss_policy_4: 0.04744
	accuracy_policy_4: 0.55531
	loss_value_4: 0.0398
	loss_reward_4: 0.007
	loss_policy_5: 0.04988
	accuracy_policy_5: 0.53512
	loss_value_5: 0.04087
	loss_reward_5: 0.00787
	loss_policy: 0.356
	loss_value: 0.3626
	loss_reward: 0.03318
[2024-05-04 22:52:33] nn step 24750, lr: 0.04.
	loss_policy_0: 0.13549
	accuracy_policy_0: 0.72512
	loss_value_0: 0.17688
	loss_policy_1: 0.03746
	accuracy_policy_1: 0.64398
	loss_value_1: 0.03748
	loss_reward_1: 0.00658
	loss_policy_2: 0.04188
	accuracy_policy_2: 0.61504
	loss_value_2: 0.03909
	loss_reward_2: 0.00597
	loss_policy_3: 0.04577
	accuracy_policy_3: 0.58523
	loss_value_3: 0.04032
	loss_reward_3: 0.00664
	loss_policy_4: 0.04872
	accuracy_policy_4: 0.56551
	loss_value_4: 0.04128
	loss_reward_4: 0.007
	loss_policy_5: 0.05138
	accuracy_policy_5: 0.54801
	loss_value_5: 0.04244
	loss_reward_5: 0.00817
	loss_policy: 0.36069
	loss_value: 0.37749
	loss_reward: 0.03436
[2024-05-04 22:52:50] nn step 24800, lr: 0.04.
	loss_policy_0: 0.13032
	accuracy_policy_0: 0.73441
	loss_value_0: 0.17564
	loss_policy_1: 0.03669
	accuracy_policy_1: 0.65227
	loss_value_1: 0.03728
	loss_reward_1: 0.00653
	loss_policy_2: 0.04122
	accuracy_policy_2: 0.61176
	loss_value_2: 0.03909
	loss_reward_2: 0.00625
	loss_policy_3: 0.04462
	accuracy_policy_3: 0.59156
	loss_value_3: 0.04046
	loss_reward_3: 0.00643
	loss_policy_4: 0.0477
	accuracy_policy_4: 0.5673
	loss_value_4: 0.04168
	loss_reward_4: 0.00696
	loss_policy_5: 0.05044
	accuracy_policy_5: 0.55793
	loss_value_5: 0.04266
	loss_reward_5: 0.0084
	loss_policy: 0.35099
	loss_value: 0.3768
	loss_reward: 0.03457
Optimization_Done 24800
[2024-05-04 22:54:53] [command] train weight_iter_24800.pkl 123 125
[2024-05-04 22:55:11] nn step 24850, lr: 0.04.
	loss_policy_0: 0.1458
	accuracy_policy_0: 0.68598
	loss_value_0: 0.16335
	loss_policy_1: 0.03754
	accuracy_policy_1: 0.61418
	loss_value_1: 0.03433
	loss_reward_1: 0.00547
	loss_policy_2: 0.04064
	accuracy_policy_2: 0.58496
	loss_value_2: 0.03566
	loss_reward_2: 0.00478
	loss_policy_3: 0.04371
	accuracy_policy_3: 0.56125
	loss_value_3: 0.03683
	loss_reward_3: 0.00535
	loss_policy_4: 0.04583
	accuracy_policy_4: 0.53531
	loss_value_4: 0.03784
	loss_reward_4: 0.00582
	loss_policy_5: 0.04786
	accuracy_policy_5: 0.52383
	loss_value_5: 0.03886
	loss_reward_5: 0.00702
	loss_policy: 0.36138
	loss_value: 0.34686
	loss_reward: 0.02844
[2024-05-04 22:55:27] nn step 24900, lr: 0.04.
	loss_policy_0: 0.12217
	accuracy_policy_0: 0.72621
	loss_value_0: 0.1617
	loss_policy_1: 0.0345
	accuracy_policy_1: 0.63504
	loss_value_1: 0.03393
	loss_reward_1: 0.00548
	loss_policy_2: 0.0377
	accuracy_policy_2: 0.60516
	loss_value_2: 0.03511
	loss_reward_2: 0.00496
	loss_policy_3: 0.041
	accuracy_policy_3: 0.57746
	loss_value_3: 0.03613
	loss_reward_3: 0.00532
	loss_policy_4: 0.04379
	accuracy_policy_4: 0.55367
	loss_value_4: 0.03711
	loss_reward_4: 0.00585
	loss_policy_5: 0.04584
	accuracy_policy_5: 0.54246
	loss_value_5: 0.03803
	loss_reward_5: 0.00683
	loss_policy: 0.325
	loss_value: 0.34201
	loss_reward: 0.02845
[2024-05-04 22:55:44] nn step 24950, lr: 0.04.
	loss_policy_0: 0.12317
	accuracy_policy_0: 0.72863
	loss_value_0: 0.16643
	loss_policy_1: 0.03463
	accuracy_policy_1: 0.64457
	loss_value_1: 0.0351
	loss_reward_1: 0.00571
	loss_policy_2: 0.03843
	accuracy_policy_2: 0.60922
	loss_value_2: 0.0364
	loss_reward_2: 0.00527
	loss_policy_3: 0.04157
	accuracy_policy_3: 0.58457
	loss_value_3: 0.03774
	loss_reward_3: 0.00575
	loss_policy_4: 0.04428
	accuracy_policy_4: 0.56457
	loss_value_4: 0.03874
	loss_reward_4: 0.00616
	loss_policy_5: 0.04699
	accuracy_policy_5: 0.5457
	loss_value_5: 0.03989
	loss_reward_5: 0.0074
	loss_policy: 0.32908
	loss_value: 0.3543
	loss_reward: 0.03029
[2024-05-04 22:56:00] nn step 25000, lr: 0.04.
	loss_policy_0: 0.111
	accuracy_policy_0: 0.73801
	loss_value_0: 0.15441
	loss_policy_1: 0.03202
	accuracy_policy_1: 0.64656
	loss_value_1: 0.03262
	loss_reward_1: 0.00519
	loss_policy_2: 0.03553
	accuracy_policy_2: 0.61898
	loss_value_2: 0.03385
	loss_reward_2: 0.00484
	loss_policy_3: 0.03886
	accuracy_policy_3: 0.5893
	loss_value_3: 0.03486
	loss_reward_3: 0.00506
	loss_policy_4: 0.04158
	accuracy_policy_4: 0.56543
	loss_value_4: 0.03601
	loss_reward_4: 0.00578
	loss_policy_5: 0.04365
	accuracy_policy_5: 0.5473
	loss_value_5: 0.03705
	loss_reward_5: 0.00676
	loss_policy: 0.30264
	loss_value: 0.3288
	loss_reward: 0.02763
Optimization_Done 25000A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-05 06:02:47] [command] train weight_iter_25000.pkl 123 126
[2024-05-05 06:03:25] nn step 25050, lr: 0.02.
	loss_policy_0: 0.13783
	accuracy_policy_0: 0.70777
	loss_value_0: 0.16294
	loss_policy_1: 0.03663
	accuracy_policy_1: 0.62594
	loss_value_1: 0.03416
	loss_reward_1: 0.00468
	loss_policy_2: 0.04018
	accuracy_policy_2: 0.59246
	loss_value_2: 0.0357
	loss_reward_2: 0.00475
	loss_policy_3: 0.04326
	accuracy_policy_3: 0.56395
	loss_value_3: 0.03666
	loss_reward_3: 0.00502
	loss_policy_4: 0.04631
	accuracy_policy_4: 0.53574
	loss_value_4: 0.03799
	loss_reward_4: 0.00548
	loss_policy_5: 0.04853
	accuracy_policy_5: 0.51941
	loss_value_5: 0.03895
	loss_reward_5: 0.00623
	loss_policy: 0.35274
	loss_value: 0.3464
	loss_reward: 0.02616
[2024-05-05 06:03:42] nn step 25100, lr: 0.02.
	loss_policy_0: 0.12944
	accuracy_policy_0: 0.72895
	loss_value_0: 0.16504
	loss_policy_1: 0.03592
	accuracy_policy_1: 0.6434
	loss_value_1: 0.03475
	loss_reward_1: 0.00516
	loss_policy_2: 0.03973
	accuracy_policy_2: 0.61062
	loss_value_2: 0.0364
	loss_reward_2: 0.00493
	loss_policy_3: 0.0426
	accuracy_policy_3: 0.58832
	loss_value_3: 0.03763
	loss_reward_3: 0.00529
	loss_policy_4: 0.04625
	accuracy_policy_4: 0.55887
	loss_value_4: 0.03883
	loss_reward_4: 0.00559
	loss_policy_5: 0.04839
	accuracy_policy_5: 0.5466
	loss_value_5: 0.03978
	loss_reward_5: 0.00656
	loss_policy: 0.34233
	loss_value: 0.35243
	loss_reward: 0.02752
[2024-05-05 06:03:58] nn step 25150, lr: 0.02.
	loss_policy_0: 0.119
	accuracy_policy_0: 0.73125
	loss_value_0: 0.15674
	loss_policy_1: 0.03359
	accuracy_policy_1: 0.6532
	loss_value_1: 0.03324
	loss_reward_1: 0.00497
	loss_policy_2: 0.03701
	accuracy_policy_2: 0.62105
	loss_value_2: 0.03462
	loss_reward_2: 0.00455
	loss_policy_3: 0.04043
	accuracy_policy_3: 0.59039
	loss_value_3: 0.03587
	loss_reward_3: 0.00507
	loss_policy_4: 0.04322
	accuracy_policy_4: 0.57281
	loss_value_4: 0.03695
	loss_reward_4: 0.00543
	loss_policy_5: 0.04559
	accuracy_policy_5: 0.55473
	loss_value_5: 0.03809
	loss_reward_5: 0.00626
	loss_policy: 0.31884
	loss_value: 0.33552
	loss_reward: 0.02629
[2024-05-05 06:04:15] nn step 25200, lr: 0.02.
	loss_policy_0: 0.12061
	accuracy_policy_0: 0.73867
	loss_value_0: 0.16269
	loss_policy_1: 0.03454
	accuracy_policy_1: 0.65012
	loss_value_1: 0.03424
	loss_reward_1: 0.00504
	loss_policy_2: 0.03826
	accuracy_policy_2: 0.6202
	loss_value_2: 0.03572
	loss_reward_2: 0.00469
	loss_policy_3: 0.0416
	accuracy_policy_3: 0.59359
	loss_value_3: 0.03722
	loss_reward_3: 0.00547
	loss_policy_4: 0.04435
	accuracy_policy_4: 0.57191
	loss_value_4: 0.03837
	loss_reward_4: 0.00571
	loss_policy_5: 0.04713
	accuracy_policy_5: 0.54809
	loss_value_5: 0.0393
	loss_reward_5: 0.0067
	loss_policy: 0.32649
	loss_value: 0.34754
	loss_reward: 0.02761
Optimization_Done 25200
[2024-05-05 06:06:32] [command] train weight_iter_25200.pkl 124 127
[2024-05-05 06:06:50] nn step 25250, lr: 0.02.
	loss_policy_0: 0.16458
	accuracy_policy_0: 0.6709
	loss_value_0: 0.16307
	loss_policy_1: 0.041
	accuracy_policy_1: 0.6077
	loss_value_1: 0.03448
	loss_reward_1: 0.00475
	loss_policy_2: 0.04358
	accuracy_policy_2: 0.58988
	loss_value_2: 0.03606
	loss_reward_2: 0.00444
	loss_policy_3: 0.04687
	accuracy_policy_3: 0.56266
	loss_value_3: 0.03754
	loss_reward_3: 0.00487
	loss_policy_4: 0.04949
	accuracy_policy_4: 0.54238
	loss_value_4: 0.03872
	loss_reward_4: 0.00517
	loss_policy_5: 0.05194
	accuracy_policy_5: 0.52512
	loss_value_5: 0.03991
	loss_reward_5: 0.00579
	loss_policy: 0.39745
	loss_value: 0.34978
	loss_reward: 0.02503
[2024-05-05 06:07:07] nn step 25300, lr: 0.02.
	loss_policy_0: 0.13463
	accuracy_policy_0: 0.71004
	loss_value_0: 0.15507
	loss_policy_1: 0.0364
	accuracy_policy_1: 0.6316
	loss_value_1: 0.03283
	loss_reward_1: 0.00474
	loss_policy_2: 0.03965
	accuracy_policy_2: 0.60449
	loss_value_2: 0.03431
	loss_reward_2: 0.00437
	loss_policy_3: 0.0424
	accuracy_policy_3: 0.58801
	loss_value_3: 0.03576
	loss_reward_3: 0.00475
	loss_policy_4: 0.04519
	accuracy_policy_4: 0.56688
	loss_value_4: 0.03689
	loss_reward_4: 0.00508
	loss_policy_5: 0.04746
	accuracy_policy_5: 0.54754
	loss_value_5: 0.03783
	loss_reward_5: 0.00568
	loss_policy: 0.34573
	loss_value: 0.3327
	loss_reward: 0.02462
[2024-05-05 06:07:23] nn step 25350, lr: 0.02.
	loss_policy_0: 0.14065
	accuracy_policy_0: 0.71918
	loss_value_0: 0.16628
	loss_policy_1: 0.0385
	accuracy_policy_1: 0.63801
	loss_value_1: 0.03528
	loss_reward_1: 0.00483
	loss_policy_2: 0.04221
	accuracy_policy_2: 0.61406
	loss_value_2: 0.0371
	loss_reward_2: 0.00462
	loss_policy_3: 0.04533
	accuracy_policy_3: 0.58957
	loss_value_3: 0.03847
	loss_reward_3: 0.00509
	loss_policy_4: 0.04871
	accuracy_policy_4: 0.56578
	loss_value_4: 0.03973
	loss_reward_4: 0.00539
	loss_policy_5: 0.05139
	accuracy_policy_5: 0.55027
	loss_value_5: 0.04106
	loss_reward_5: 0.00623
	loss_policy: 0.36678
	loss_value: 0.35792
	loss_reward: 0.02617
[2024-05-05 06:07:40] nn step 25400, lr: 0.02.
	loss_policy_0: 0.12221
	accuracy_policy_0: 0.73266
	loss_value_0: 0.14939
	loss_policy_1: 0.03387
	accuracy_policy_1: 0.64578
	loss_value_1: 0.03141
	loss_reward_1: 0.00438
	loss_policy_2: 0.03684
	accuracy_policy_2: 0.62039
	loss_value_2: 0.03294
	loss_reward_2: 0.00433
	loss_policy_3: 0.0405
	accuracy_policy_3: 0.5918
	loss_value_3: 0.03438
	loss_reward_3: 0.00466
	loss_policy_4: 0.04326
	accuracy_policy_4: 0.57676
	loss_value_4: 0.03546
	loss_reward_4: 0.00482
	loss_policy_5: 0.04557
	accuracy_policy_5: 0.56293
	loss_value_5: 0.03658
	loss_reward_5: 0.0056
	loss_policy: 0.32225
	loss_value: 0.32016
	loss_reward: 0.0238
Optimization_Done 25400
[2024-05-05 06:09:46] [command] train weight_iter_25400.pkl 125 128
[2024-05-05 06:10:06] nn step 25450, lr: 0.02.
	loss_policy_0: 0.153
	accuracy_policy_0: 0.67078
	loss_value_0: 0.14894
	loss_policy_1: 0.03786
	accuracy_policy_1: 0.61016
	loss_value_1: 0.03151
	loss_reward_1: 0.00477
	loss_policy_2: 0.04099
	accuracy_policy_2: 0.58504
	loss_value_2: 0.03288
	loss_reward_2: 0.00463
	loss_policy_3: 0.04369
	accuracy_policy_3: 0.56273
	loss_value_3: 0.03421
	loss_reward_3: 0.00496
	loss_policy_4: 0.04608
	accuracy_policy_4: 0.54457
	loss_value_4: 0.03547
	loss_reward_4: 0.00532
	loss_policy_5: 0.04811
	accuracy_policy_5: 0.52844
	loss_value_5: 0.03657
	loss_reward_5: 0.00615
	loss_policy: 0.36973
	loss_value: 0.31958
	loss_reward: 0.02582
[2024-05-05 06:10:22] nn step 25500, lr: 0.02.
	loss_policy_0: 0.15136
	accuracy_policy_0: 0.70809
	loss_value_0: 0.17045
	loss_policy_1: 0.04099
	accuracy_policy_1: 0.63281
	loss_value_1: 0.03617
	loss_reward_1: 0.00566
	loss_policy_2: 0.04418
	accuracy_policy_2: 0.60359
	loss_value_2: 0.03776
	loss_reward_2: 0.00537
	loss_policy_3: 0.04798
	accuracy_policy_3: 0.5852
	loss_value_3: 0.03918
	loss_reward_3: 0.00565
	loss_policy_4: 0.05068
	accuracy_policy_4: 0.56785
	loss_value_4: 0.04041
	loss_reward_4: 0.00592
	loss_policy_5: 0.05324
	accuracy_policy_5: 0.55145
	loss_value_5: 0.0415
	loss_reward_5: 0.00706
	loss_policy: 0.38842
	loss_value: 0.36546
	loss_reward: 0.02965
[2024-05-05 06:10:39] nn step 25550, lr: 0.02.
	loss_policy_0: 0.12644
	accuracy_policy_0: 0.71441
	loss_value_0: 0.14596
	loss_policy_1: 0.03424
	accuracy_policy_1: 0.6425
	loss_value_1: 0.03113
	loss_reward_1: 0.00471
	loss_policy_2: 0.03751
	accuracy_policy_2: 0.61715
	loss_value_2: 0.03258
	loss_reward_2: 0.0044
	loss_policy_3: 0.04009
	accuracy_policy_3: 0.59453
	loss_value_3: 0.0337
	loss_reward_3: 0.00481
	loss_policy_4: 0.04244
	accuracy_policy_4: 0.57312
	loss_value_4: 0.03471
	loss_reward_4: 0.0051
	loss_policy_5: 0.0451
	accuracy_policy_5: 0.55555
	loss_value_5: 0.03593
	loss_reward_5: 0.00606
	loss_policy: 0.32583
	loss_value: 0.31402
	loss_reward: 0.02508
[2024-05-05 06:10:56] nn step 25600, lr: 0.02.
	loss_policy_0: 0.14236
	accuracy_policy_0: 0.72449
	loss_value_0: 0.16843
	loss_policy_1: 0.03888
	accuracy_policy_1: 0.64324
	loss_value_1: 0.03575
	loss_reward_1: 0.00527
	loss_policy_2: 0.04293
	accuracy_policy_2: 0.61445
	loss_value_2: 0.0374
	loss_reward_2: 0.00505
	loss_policy_3: 0.04634
	accuracy_policy_3: 0.59266
	loss_value_3: 0.03886
	loss_reward_3: 0.00554
	loss_policy_4: 0.04907
	accuracy_policy_4: 0.57363
	loss_value_4: 0.04029
	loss_reward_4: 0.0058
	loss_policy_5: 0.05228
	accuracy_policy_5: 0.55254
	loss_value_5: 0.0414
	loss_reward_5: 0.00697
	loss_policy: 0.37186
	loss_value: 0.36213
	loss_reward: 0.02863
Optimization_Done 25600
[2024-05-05 06:13:02] [command] train weight_iter_25600.pkl 126 129
[2024-05-05 06:13:20] nn step 25650, lr: 0.02.
	loss_policy_0: 0.13919
	accuracy_policy_0: 0.69785
	loss_value_0: 0.14994
	loss_policy_1: 0.03744
	accuracy_policy_1: 0.61473
	loss_value_1: 0.03207
	loss_reward_1: 0.00504
	loss_policy_2: 0.04033
	accuracy_policy_2: 0.5959
	loss_value_2: 0.03356
	loss_reward_2: 0.00475
	loss_policy_3: 0.04284
	accuracy_policy_3: 0.57438
	loss_value_3: 0.03479
	loss_reward_3: 0.00504
	loss_policy_4: 0.04554
	accuracy_policy_4: 0.55691
	loss_value_4: 0.0359
	loss_reward_4: 0.00527
	loss_policy_5: 0.04797
	accuracy_policy_5: 0.54141
	loss_value_5: 0.03698
	loss_reward_5: 0.0062
	loss_policy: 0.35332
	loss_value: 0.32325
	loss_reward: 0.02628
[2024-05-05 06:13:37] nn step 25700, lr: 0.02.
	loss_policy_0: 0.12917
	accuracy_policy_0: 0.71996
	loss_value_0: 0.15207
	loss_policy_1: 0.03587
	accuracy_policy_1: 0.63379
	loss_value_1: 0.03252
	loss_reward_1: 0.00503
	loss_policy_2: 0.03891
	accuracy_policy_2: 0.6143
	loss_value_2: 0.0339
	loss_reward_2: 0.00459
	loss_policy_3: 0.04205
	accuracy_policy_3: 0.58777
	loss_value_3: 0.03519
	loss_reward_3: 0.0051
	loss_policy_4: 0.0446
	accuracy_policy_4: 0.57133
	loss_value_4: 0.03623
	loss_reward_4: 0.00525
	loss_policy_5: 0.04764
	accuracy_policy_5: 0.55211
	loss_value_5: 0.0374
	loss_reward_5: 0.00616
	loss_policy: 0.33825
	loss_value: 0.32731
	loss_reward: 0.02613
[2024-05-05 06:13:53] nn step 25750, lr: 0.02.
	loss_policy_0: 0.11953
	accuracy_policy_0: 0.73004
	loss_value_0: 0.14762
	loss_policy_1: 0.03388
	accuracy_policy_1: 0.64359
	loss_value_1: 0.03143
	loss_reward_1: 0.00499
	loss_policy_2: 0.03742
	accuracy_policy_2: 0.61883
	loss_value_2: 0.03288
	loss_reward_2: 0.00453
	loss_policy_3: 0.04035
	accuracy_policy_3: 0.5941
	loss_value_3: 0.03405
	loss_reward_3: 0.00491
	loss_policy_4: 0.0429
	accuracy_policy_4: 0.57613
	loss_value_4: 0.0353
	loss_reward_4: 0.00508
	loss_policy_5: 0.04528
	accuracy_policy_5: 0.55473
	loss_value_5: 0.03645
	loss_reward_5: 0.00601
	loss_policy: 0.31937
	loss_value: 0.31774
	loss_reward: 0.02552
[2024-05-05 06:14:10] nn step 25800, lr: 0.02.
	loss_policy_0: 0.1191
	accuracy_policy_0: 0.74141
	loss_value_0: 0.15198
	loss_policy_1: 0.03485
	accuracy_policy_1: 0.64141
	loss_value_1: 0.03234
	loss_reward_1: 0.00514
	loss_policy_2: 0.03848
	accuracy_policy_2: 0.61996
	loss_value_2: 0.03371
	loss_reward_2: 0.00476
	loss_policy_3: 0.04176
	accuracy_policy_3: 0.59289
	loss_value_3: 0.03517
	loss_reward_3: 0.00515
	loss_policy_4: 0.04411
	accuracy_policy_4: 0.57727
	loss_value_4: 0.03646
	loss_reward_4: 0.00531
	loss_policy_5: 0.04674
	accuracy_policy_5: 0.55355
	loss_value_5: 0.03758
	loss_reward_5: 0.00629
	loss_policy: 0.32503
	loss_value: 0.32725
	loss_reward: 0.02665
Optimization_Done 25800
[2024-05-05 06:16:18] [command] train weight_iter_25800.pkl 127 130
[2024-05-05 06:16:36] nn step 25850, lr: 0.02.
	loss_policy_0: 0.13506
	accuracy_policy_0: 0.69613
	loss_value_0: 0.15253
	loss_policy_1: 0.03695
	accuracy_policy_1: 0.61223
	loss_value_1: 0.03232
	loss_reward_1: 0.00511
	loss_policy_2: 0.04003
	accuracy_policy_2: 0.58652
	loss_value_2: 0.03408
	loss_reward_2: 0.00487
	loss_policy_3: 0.04322
	accuracy_policy_3: 0.55945
	loss_value_3: 0.03534
	loss_reward_3: 0.00529
	loss_policy_4: 0.04581
	accuracy_policy_4: 0.53957
	loss_value_4: 0.03652
	loss_reward_4: 0.00581
	loss_policy_5: 0.04831
	accuracy_policy_5: 0.51504
	loss_value_5: 0.03779
	loss_reward_5: 0.00669
	loss_policy: 0.34938
	loss_value: 0.32858
	loss_reward: 0.02778
[2024-05-05 06:16:53] nn step 25900, lr: 0.02.
	loss_policy_0: 0.12224
	accuracy_policy_0: 0.72207
	loss_value_0: 0.15433
	loss_policy_1: 0.03547
	accuracy_policy_1: 0.63355
	loss_value_1: 0.0328
	loss_reward_1: 0.00534
	loss_policy_2: 0.03923
	accuracy_policy_2: 0.60027
	loss_value_2: 0.03427
	loss_reward_2: 0.00491
	loss_policy_3: 0.04212
	accuracy_policy_3: 0.57645
	loss_value_3: 0.03555
	loss_reward_3: 0.00553
	loss_policy_4: 0.04479
	accuracy_policy_4: 0.55738
	loss_value_4: 0.03681
	loss_reward_4: 0.00576
	loss_policy_5: 0.04725
	accuracy_policy_5: 0.5434
	loss_value_5: 0.03806
	loss_reward_5: 0.00695
	loss_policy: 0.3311
	loss_value: 0.33183
	loss_reward: 0.0285
[2024-05-05 06:17:09] nn step 25950, lr: 0.02.
	loss_policy_0: 0.11995
	accuracy_policy_0: 0.73371
	loss_value_0: 0.1558
	loss_policy_1: 0.03493
	accuracy_policy_1: 0.64223
	loss_value_1: 0.03331
	loss_reward_1: 0.00531
	loss_policy_2: 0.03848
	accuracy_policy_2: 0.61102
	loss_value_2: 0.03445
	loss_reward_2: 0.00492
	loss_policy_3: 0.04184
	accuracy_policy_3: 0.58695
	loss_value_3: 0.03564
	loss_reward_3: 0.00549
	loss_policy_4: 0.04466
	accuracy_policy_4: 0.56961
	loss_value_4: 0.03686
	loss_reward_4: 0.00585
	loss_policy_5: 0.04744
	accuracy_policy_5: 0.5448
	loss_value_5: 0.03808
	loss_reward_5: 0.00693
	loss_policy: 0.3273
	loss_value: 0.33415
	loss_reward: 0.02851
[2024-05-05 06:17:26] nn step 26000, lr: 0.02.
	loss_policy_0: 0.10798
	accuracy_policy_0: 0.74246
	loss_value_0: 0.14496
	loss_policy_1: 0.03196
	accuracy_policy_1: 0.64676
	loss_value_1: 0.03088
	loss_reward_1: 0.00503
	loss_policy_2: 0.03531
	accuracy_policy_2: 0.61633
	loss_value_2: 0.03247
	loss_reward_2: 0.00452
	loss_policy_3: 0.03835
	accuracy_policy_3: 0.59449
	loss_value_3: 0.0337
	loss_reward_3: 0.00506
	loss_policy_4: 0.04069
	accuracy_policy_4: 0.57586
	loss_value_4: 0.03489
	loss_reward_4: 0.00527
	loss_policy_5: 0.04325
	accuracy_policy_5: 0.55277
	loss_value_5: 0.03604
	loss_reward_5: 0.00621
	loss_policy: 0.29754
	loss_value: 0.31293
	loss_reward: 0.0261
Optimization_Done 26000
[2024-05-05 06:19:31] [command] train weight_iter_26000.pkl 128 131
[2024-05-05 06:19:49] nn step 26050, lr: 0.02.
	loss_policy_0: 0.13277
	accuracy_policy_0: 0.69582
	loss_value_0: 0.15261
	loss_policy_1: 0.0357
	accuracy_policy_1: 0.61844
	loss_value_1: 0.03233
	loss_reward_1: 0.00547
	loss_policy_2: 0.0384
	accuracy_policy_2: 0.59496
	loss_value_2: 0.0338
	loss_reward_2: 0.00517
	loss_policy_3: 0.04128
	accuracy_policy_3: 0.57137
	loss_value_3: 0.03516
	loss_reward_3: 0.00552
	loss_policy_4: 0.04374
	accuracy_policy_4: 0.54766
	loss_value_4: 0.03633
	loss_reward_4: 0.00564
	loss_policy_5: 0.04589
	accuracy_policy_5: 0.53234
	loss_value_5: 0.03742
	loss_reward_5: 0.00666
	loss_policy: 0.33778
	loss_value: 0.32765
	loss_reward: 0.02846
[2024-05-05 06:20:06] nn step 26100, lr: 0.02.
	loss_policy_0: 0.11667
	accuracy_policy_0: 0.72238
	loss_value_0: 0.15065
	loss_policy_1: 0.03349
	accuracy_policy_1: 0.64328
	loss_value_1: 0.03206
	loss_reward_1: 0.00553
	loss_policy_2: 0.03619
	accuracy_policy_2: 0.62027
	loss_value_2: 0.03328
	loss_reward_2: 0.00512
	loss_policy_3: 0.0388
	accuracy_policy_3: 0.59613
	loss_value_3: 0.03458
	loss_reward_3: 0.00541
	loss_policy_4: 0.04135
	accuracy_policy_4: 0.57785
	loss_value_4: 0.03587
	loss_reward_4: 0.00562
	loss_policy_5: 0.04366
	accuracy_policy_5: 0.55914
	loss_value_5: 0.03698
	loss_reward_5: 0.0066
	loss_policy: 0.31016
	loss_value: 0.32342
	loss_reward: 0.02829
[2024-05-05 06:20:23] nn step 26150, lr: 0.02.
	loss_policy_0: 0.11614
	accuracy_policy_0: 0.73504
	loss_value_0: 0.15523
	loss_policy_1: 0.0339
	accuracy_policy_1: 0.63961
	loss_value_1: 0.03282
	loss_reward_1: 0.00551
	loss_policy_2: 0.03668
	accuracy_policy_2: 0.6232
	loss_value_2: 0.03425
	loss_reward_2: 0.00533
	loss_policy_3: 0.03971
	accuracy_policy_3: 0.59715
	loss_value_3: 0.03551
	loss_reward_3: 0.00569
	loss_policy_4: 0.0422
	accuracy_policy_4: 0.57828
	loss_value_4: 0.03666
	loss_reward_4: 0.00604
	loss_policy_5: 0.04481
	accuracy_policy_5: 0.56008
	loss_value_5: 0.03788
	loss_reward_5: 0.00693
	loss_policy: 0.31344
	loss_value: 0.33236
	loss_reward: 0.02949
[2024-05-05 06:20:40] nn step 26200, lr: 0.02.
	loss_policy_0: 0.10871
	accuracy_policy_0: 0.73898
	loss_value_0: 0.15014
	loss_policy_1: 0.03246
	accuracy_policy_1: 0.6427
	loss_value_1: 0.03195
	loss_reward_1: 0.00523
	loss_policy_2: 0.03506
	accuracy_policy_2: 0.6248
	loss_value_2: 0.0334
	loss_reward_2: 0.00497
	loss_policy_3: 0.03804
	accuracy_policy_3: 0.5973
	loss_value_3: 0.03452
	loss_reward_3: 0.00541
	loss_policy_4: 0.04005
	accuracy_policy_4: 0.58148
	loss_value_4: 0.03565
	loss_reward_4: 0.00575
	loss_policy_5: 0.043
	accuracy_policy_5: 0.55691
	loss_value_5: 0.03699
	loss_reward_5: 0.0069
	loss_policy: 0.29732
	loss_value: 0.32265
	loss_reward: 0.02827
Optimization_Done 26200
[2024-05-05 06:22:48] [command] train weight_iter_26200.pkl 129 132
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-05 06:40:22] [command] train weight_iter_26200.pkl 130 132
[2024-05-05 06:40:48] nn step 26250, lr: 0.015.
	loss_policy_0: 0.14125
	accuracy_policy_0: 0.69809
	loss_value_0: 0.16785
	loss_policy_1: 0.03768
	accuracy_policy_1: 0.61824
	loss_value_1: 0.03522
	loss_reward_1: 0.00506
	loss_policy_2: 0.04142
	accuracy_policy_2: 0.5875
	loss_value_2: 0.0369
	loss_reward_2: 0.00495
	loss_policy_3: 0.04446
	accuracy_policy_3: 0.56246
	loss_value_3: 0.03838
	loss_reward_3: 0.00523
	loss_policy_4: 0.04756
	accuracy_policy_4: 0.54043
	loss_value_4: 0.03975
	loss_reward_4: 0.00572
	loss_policy_5: 0.05016
	accuracy_policy_5: 0.51711
	loss_value_5: 0.04103
	loss_reward_5: 0.00641
	loss_policy: 0.36253
	loss_value: 0.35912
	loss_reward: 0.02737
[2024-05-05 06:41:04] nn step 26300, lr: 0.015.
	loss_policy_0: 0.13079
	accuracy_policy_0: 0.72176
	loss_value_0: 0.17315
	loss_policy_1: 0.03792
	accuracy_policy_1: 0.6327
	loss_value_1: 0.03679
	loss_reward_1: 0.00518
	loss_policy_2: 0.04164
	accuracy_policy_2: 0.60531
	loss_value_2: 0.03823
	loss_reward_2: 0.00515
	loss_policy_3: 0.04479
	accuracy_policy_3: 0.58238
	loss_value_3: 0.03975
	loss_reward_3: 0.0055
	loss_policy_4: 0.04758
	accuracy_policy_4: 0.56527
	loss_value_4: 0.04103
	loss_reward_4: 0.00599
	loss_policy_5: 0.05058
	accuracy_policy_5: 0.54266
	loss_value_5: 0.04218
	loss_reward_5: 0.00671
	loss_policy: 0.35331
	loss_value: 0.37111
	loss_reward: 0.02854
[2024-05-05 06:41:20] nn step 26350, lr: 0.015.
	loss_policy_0: 0.12197
	accuracy_policy_0: 0.73223
	loss_value_0: 0.16957
	loss_policy_1: 0.03487
	accuracy_policy_1: 0.64871
	loss_value_1: 0.03589
	loss_reward_1: 0.00519
	loss_policy_2: 0.0386
	accuracy_policy_2: 0.62344
	loss_value_2: 0.03739
	loss_reward_2: 0.00504
	loss_policy_3: 0.04253
	accuracy_policy_3: 0.5943
	loss_value_3: 0.03889
	loss_reward_3: 0.00553
	loss_policy_4: 0.04541
	accuracy_policy_4: 0.57355
	loss_value_4: 0.04021
	loss_reward_4: 0.0058
	loss_policy_5: 0.04845
	accuracy_policy_5: 0.55141
	loss_value_5: 0.04125
	loss_reward_5: 0.00672
	loss_policy: 0.33183
	loss_value: 0.3632
	loss_reward: 0.02829
[2024-05-05 06:41:36] nn step 26400, lr: 0.015.
	loss_policy_0: 0.11748
	accuracy_policy_0: 0.7402
	loss_value_0: 0.17229
	loss_policy_1: 0.03472
	accuracy_policy_1: 0.64945
	loss_value_1: 0.03631
	loss_reward_1: 0.00542
	loss_policy_2: 0.03859
	accuracy_policy_2: 0.62813
	loss_value_2: 0.03809
	loss_reward_2: 0.00526
	loss_policy_3: 0.0418
	accuracy_policy_3: 0.5998
	loss_value_3: 0.03956
	loss_reward_3: 0.00555
	loss_policy_4: 0.04538
	accuracy_policy_4: 0.57977
	loss_value_4: 0.04077
	loss_reward_4: 0.00592
	loss_policy_5: 0.04814
	accuracy_policy_5: 0.55949
	loss_value_5: 0.04203
	loss_reward_5: 0.00668
	loss_policy: 0.32611
	loss_value: 0.36904
	loss_reward: 0.02883
Optimization_Done 26400
[2024-05-05 06:43:41] [command] train weight_iter_26400.pkl 131 133
[2024-05-05 06:43:58] nn step 26450, lr: 0.015.
	loss_policy_0: 0.16304
	accuracy_policy_0: 0.66449
	loss_value_0: 0.17262
	loss_policy_1: 0.04081
	accuracy_policy_1: 0.60098
	loss_value_1: 0.03612
	loss_reward_1: 0.00507
	loss_policy_2: 0.04391
	accuracy_policy_2: 0.58395
	loss_value_2: 0.03785
	loss_reward_2: 0.00501
	loss_policy_3: 0.04681
	accuracy_policy_3: 0.55902
	loss_value_3: 0.03927
	loss_reward_3: 0.0053
	loss_policy_4: 0.04961
	accuracy_policy_4: 0.53969
	loss_value_4: 0.04038
	loss_reward_4: 0.00575
	loss_policy_5: 0.05244
	accuracy_policy_5: 0.52156
	loss_value_5: 0.04173
	loss_reward_5: 0.00626
	loss_policy: 0.39662
	loss_value: 0.36796
	loss_reward: 0.02739
[2024-05-05 06:44:14] nn step 26500, lr: 0.015.
	loss_policy_0: 0.12971
	accuracy_policy_0: 0.72164
	loss_value_0: 0.16584
	loss_policy_1: 0.03652
	accuracy_policy_1: 0.64039
	loss_value_1: 0.03514
	loss_reward_1: 0.005
	loss_policy_2: 0.03926
	accuracy_policy_2: 0.62227
	loss_value_2: 0.0368
	loss_reward_2: 0.00493
	loss_policy_3: 0.04322
	accuracy_policy_3: 0.59008
	loss_value_3: 0.03809
	loss_reward_3: 0.00525
	loss_policy_4: 0.04657
	accuracy_policy_4: 0.5691
	loss_value_4: 0.03923
	loss_reward_4: 0.00565
	loss_policy_5: 0.04838
	accuracy_policy_5: 0.5525
	loss_value_5: 0.04042
	loss_reward_5: 0.00615
	loss_policy: 0.34366
	loss_value: 0.35552
	loss_reward: 0.02698
[2024-05-05 06:44:30] nn step 26550, lr: 0.015.
	loss_policy_0: 0.13232
	accuracy_policy_0: 0.72863
	loss_value_0: 0.17729
	loss_policy_1: 0.03788
	accuracy_policy_1: 0.64684
	loss_value_1: 0.03751
	loss_reward_1: 0.00527
	loss_policy_2: 0.0409
	accuracy_policy_2: 0.62824
	loss_value_2: 0.03924
	loss_reward_2: 0.00518
	loss_policy_3: 0.04482
	accuracy_policy_3: 0.5973
	loss_value_3: 0.0409
	loss_reward_3: 0.00563
	loss_policy_4: 0.04789
	accuracy_policy_4: 0.57855
	loss_value_4: 0.04219
	loss_reward_4: 0.00604
	loss_policy_5: 0.05104
	accuracy_policy_5: 0.56246
	loss_value_5: 0.04357
	loss_reward_5: 0.00692
	loss_policy: 0.35486
	loss_value: 0.38069
	loss_reward: 0.02904
[2024-05-05 06:44:46] nn step 26600, lr: 0.015.
	loss_policy_0: 0.12096
	accuracy_policy_0: 0.74266
	loss_value_0: 0.1715
	loss_policy_1: 0.03533
	accuracy_policy_1: 0.65684
	loss_value_1: 0.03646
	loss_reward_1: 0.00516
	loss_policy_2: 0.03909
	accuracy_policy_2: 0.63402
	loss_value_2: 0.038
	loss_reward_2: 0.00532
	loss_policy_3: 0.04265
	accuracy_policy_3: 0.60637
	loss_value_3: 0.03937
	loss_reward_3: 0.00558
	loss_policy_4: 0.0454
	accuracy_policy_4: 0.5823
	loss_value_4: 0.04077
	loss_reward_4: 0.0058
	loss_policy_5: 0.04842
	accuracy_policy_5: 0.56473
	loss_value_5: 0.04194
	loss_reward_5: 0.0065
	loss_policy: 0.33185
	loss_value: 0.36803
	loss_reward: 0.02836
Optimization_Done 26600
[2024-05-05 06:46:49] [command] train weight_iter_26600.pkl 132 134
[2024-05-05 06:47:07] nn step 26650, lr: 0.015.
	loss_policy_0: 0.13547
	accuracy_policy_0: 0.71715
	loss_value_0: 0.17002
	loss_policy_1: 0.03625
	accuracy_policy_1: 0.64672
	loss_value_1: 0.03587
	loss_reward_1: 0.00555
	loss_policy_2: 0.04
	accuracy_policy_2: 0.61836
	loss_value_2: 0.03738
	loss_reward_2: 0.00526
	loss_policy_3: 0.04363
	accuracy_policy_3: 0.59199
	loss_value_3: 0.03862
	loss_reward_3: 0.00573
	loss_policy_4: 0.04694
	accuracy_policy_4: 0.56625
	loss_value_4: 0.04001
	loss_reward_4: 0.00606
	loss_policy_5: 0.04922
	accuracy_policy_5: 0.54883
	loss_value_5: 0.04116
	loss_reward_5: 0.00695
	loss_policy: 0.35151
	loss_value: 0.36306
	loss_reward: 0.02954
[2024-05-05 06:47:23] nn step 26700, lr: 0.015.
	loss_policy_0: 0.11021
	accuracy_policy_0: 0.75594
	loss_value_0: 0.16633
	loss_policy_1: 0.03354
	accuracy_policy_1: 0.66266
	loss_value_1: 0.03542
	loss_reward_1: 0.0055
	loss_policy_2: 0.03789
	accuracy_policy_2: 0.63211
	loss_value_2: 0.03689
	loss_reward_2: 0.00507
	loss_policy_3: 0.04074
	accuracy_policy_3: 0.60746
	loss_value_3: 0.03816
	loss_reward_3: 0.00554
	loss_policy_4: 0.04397
	accuracy_policy_4: 0.57996
	loss_value_4: 0.03908
	loss_reward_4: 0.00589
	loss_policy_5: 0.04635
	accuracy_policy_5: 0.56227
	loss_value_5: 0.04017
	loss_reward_5: 0.00684
	loss_policy: 0.3127
	loss_value: 0.35606
	loss_reward: 0.02884
[2024-05-05 06:47:39] nn step 26750, lr: 0.015.
	loss_policy_0: 0.10463
	accuracy_policy_0: 0.7657
	loss_value_0: 0.16487
	loss_policy_1: 0.03225
	accuracy_policy_1: 0.66863
	loss_value_1: 0.03479
	loss_reward_1: 0.00525
	loss_policy_2: 0.0361
	accuracy_policy_2: 0.64078
	loss_value_2: 0.0362
	loss_reward_2: 0.00511
	loss_policy_3: 0.03923
	accuracy_policy_3: 0.61438
	loss_value_3: 0.03744
	loss_reward_3: 0.00561
	loss_policy_4: 0.04235
	accuracy_policy_4: 0.59445
	loss_value_4: 0.03869
	loss_reward_4: 0.00598
	loss_policy_5: 0.0447
	accuracy_policy_5: 0.57184
	loss_value_5: 0.03985
	loss_reward_5: 0.00686
	loss_policy: 0.29927
	loss_value: 0.35184
	loss_reward: 0.02881
[2024-05-05 06:47:55] nn step 26800, lr: 0.015.
	loss_policy_0: 0.10131
	accuracy_policy_0: 0.77164
	loss_value_0: 0.16655
	loss_policy_1: 0.0316
	accuracy_policy_1: 0.68082
	loss_value_1: 0.03497
	loss_reward_1: 0.00538
	loss_policy_2: 0.03577
	accuracy_policy_2: 0.64773
	loss_value_2: 0.03623
	loss_reward_2: 0.00492
	loss_policy_3: 0.03939
	accuracy_policy_3: 0.61812
	loss_value_3: 0.03761
	loss_reward_3: 0.00546
	loss_policy_4: 0.04207
	accuracy_policy_4: 0.59602
	loss_value_4: 0.03881
	loss_reward_4: 0.00577
	loss_policy_5: 0.04467
	accuracy_policy_5: 0.57734
	loss_value_5: 0.03987
	loss_reward_5: 0.00672
	loss_policy: 0.2948
	loss_value: 0.35404
	loss_reward: 0.02825
Optimization_Done 26800
[2024-05-05 06:49:58] [command] train weight_iter_26800.pkl 133 135
[2024-05-05 06:50:15] nn step 26850, lr: 0.015.
	loss_policy_0: 0.10535
	accuracy_policy_0: 0.76082
	loss_value_0: 0.1535
	loss_policy_1: 0.03084
	accuracy_policy_1: 0.67285
	loss_value_1: 0.03245
	loss_reward_1: 0.00535
	loss_policy_2: 0.03456
	accuracy_policy_2: 0.64055
	loss_value_2: 0.03396
	loss_reward_2: 0.00494
	loss_policy_3: 0.03763
	accuracy_policy_3: 0.61344
	loss_value_3: 0.03528
	loss_reward_3: 0.00531
	loss_policy_4: 0.04051
	accuracy_policy_4: 0.5943
	loss_value_4: 0.03632
	loss_reward_4: 0.00566
	loss_policy_5: 0.04257
	accuracy_policy_5: 0.57441
	loss_value_5: 0.03752
	loss_reward_5: 0.00673
	loss_policy: 0.29144
	loss_value: 0.32902
	loss_reward: 0.02798
[2024-05-05 06:50:31] nn step 26900, lr: 0.015.
	loss_policy_0: 0.0944
	accuracy_policy_0: 0.78137
	loss_value_0: 0.15149
	loss_policy_1: 0.02975
	accuracy_policy_1: 0.6834
	loss_value_1: 0.03221
	loss_reward_1: 0.00527
	loss_policy_2: 0.03355
	accuracy_policy_2: 0.64961
	loss_value_2: 0.03355
	loss_reward_2: 0.0048
	loss_policy_3: 0.03634
	accuracy_policy_3: 0.62777
	loss_value_3: 0.03475
	loss_reward_3: 0.0055
	loss_policy_4: 0.0391
	accuracy_policy_4: 0.60551
	loss_value_4: 0.03607
	loss_reward_4: 0.00574
	loss_policy_5: 0.04074
	accuracy_policy_5: 0.58969
	loss_value_5: 0.03719
	loss_reward_5: 0.00645
	loss_policy: 0.27388
	loss_value: 0.32526
	loss_reward: 0.02776
[2024-05-05 06:50:48] nn step 26950, lr: 0.015.
	loss_policy_0: 0.0873
	accuracy_policy_0: 0.78695
	loss_value_0: 0.14551
	loss_policy_1: 0.02782
	accuracy_policy_1: 0.68871
	loss_value_1: 0.031
	loss_reward_1: 0.00517
	loss_policy_2: 0.03152
	accuracy_policy_2: 0.65633
	loss_value_2: 0.0321
	loss_reward_2: 0.00491
	loss_policy_3: 0.03456
	accuracy_policy_3: 0.63465
	loss_value_3: 0.03327
	loss_reward_3: 0.00515
	loss_policy_4: 0.03671
	accuracy_policy_4: 0.61199
	loss_value_4: 0.03435
	loss_reward_4: 0.0055
	loss_policy_5: 0.03866
	accuracy_policy_5: 0.59617
	loss_value_5: 0.03544
	loss_reward_5: 0.00646
	loss_policy: 0.25656
	loss_value: 0.31167
	loss_reward: 0.0272
[2024-05-05 06:51:04] nn step 27000, lr: 0.015.
	loss_policy_0: 0.0834
	accuracy_policy_0: 0.79316
	loss_value_0: 0.13942
	loss_policy_1: 0.02673
	accuracy_policy_1: 0.6898
	loss_value_1: 0.02946
	loss_reward_1: 0.00488
	loss_policy_2: 0.03026
	accuracy_policy_2: 0.65578
	loss_value_2: 0.0309
	loss_reward_2: 0.00462
	loss_policy_3: 0.03314
	accuracy_policy_3: 0.62945
	loss_value_3: 0.03206
	loss_reward_3: 0.00498
	loss_policy_4: 0.03533
	accuracy_policy_4: 0.61344
	loss_value_4: 0.03313
	loss_reward_4: 0.00525
	loss_policy_5: 0.03723
	accuracy_policy_5: 0.59539
	loss_value_5: 0.03405
	loss_reward_5: 0.00616
	loss_policy: 0.24609
	loss_value: 0.29903
	loss_reward: 0.02588
Optimization_Done 27000
[2024-05-05 06:53:11] [command] train weight_iter_27000.pkl 134 136
[2024-05-05 06:53:28] nn step 27050, lr: 0.015.
	loss_policy_0: 0.11115
	accuracy_policy_0: 0.73984
	loss_value_0: 0.15112
	loss_policy_1: 0.03119
	accuracy_policy_1: 0.65625
	loss_value_1: 0.0323
	loss_reward_1: 0.00539
	loss_policy_2: 0.03446
	accuracy_policy_2: 0.62156
	loss_value_2: 0.03381
	loss_reward_2: 0.00509
	loss_policy_3: 0.037
	accuracy_policy_3: 0.5991
	loss_value_3: 0.03509
	loss_reward_3: 0.00565
	loss_policy_4: 0.03867
	accuracy_policy_4: 0.59031
	loss_value_4: 0.03635
	loss_reward_4: 0.00587
	loss_policy_5: 0.04081
	accuracy_policy_5: 0.56348
	loss_value_5: 0.03741
	loss_reward_5: 0.00692
	loss_policy: 0.29328
	loss_value: 0.32608
	loss_reward: 0.02891
[2024-05-05 06:53:44] nn step 27100, lr: 0.015.
	loss_policy_0: 0.08825
	accuracy_policy_0: 0.78355
	loss_value_0: 0.1478
	loss_policy_1: 0.02836
	accuracy_policy_1: 0.68867
	loss_value_1: 0.03156
	loss_reward_1: 0.00523
	loss_policy_2: 0.03187
	accuracy_policy_2: 0.65023
	loss_value_2: 0.03295
	loss_reward_2: 0.00513
	loss_policy_3: 0.03481
	accuracy_policy_3: 0.62586
	loss_value_3: 0.03387
	loss_reward_3: 0.00555
	loss_policy_4: 0.03706
	accuracy_policy_4: 0.61129
	loss_value_4: 0.03517
	loss_reward_4: 0.00596
	loss_policy_5: 0.03872
	accuracy_policy_5: 0.59168
	loss_value_5: 0.03641
	loss_reward_5: 0.0071
	loss_policy: 0.25906
	loss_value: 0.31776
	loss_reward: 0.02897
[2024-05-05 06:54:00] nn step 27150, lr: 0.015.
	loss_policy_0: 0.087
	accuracy_policy_0: 0.78793
	loss_value_0: 0.14847
	loss_policy_1: 0.02853
	accuracy_policy_1: 0.68582
	loss_value_1: 0.03159
	loss_reward_1: 0.00543
	loss_policy_2: 0.03225
	accuracy_policy_2: 0.64641
	loss_value_2: 0.03295
	loss_reward_2: 0.00518
	loss_policy_3: 0.03418
	accuracy_policy_3: 0.6325
	loss_value_3: 0.03428
	loss_reward_3: 0.00572
	loss_policy_4: 0.03627
	accuracy_policy_4: 0.62195
	loss_value_4: 0.03546
	loss_reward_4: 0.00602
	loss_policy_5: 0.03893
	accuracy_policy_5: 0.59504
	loss_value_5: 0.03668
	loss_reward_5: 0.00708
	loss_policy: 0.25716
	loss_value: 0.31944
	loss_reward: 0.02943
[2024-05-05 06:54:16] nn step 27200, lr: 0.015.
	loss_policy_0: 0.07875
	accuracy_policy_0: 0.80094
	loss_value_0: 0.14368
	loss_policy_1: 0.02672
	accuracy_policy_1: 0.69496
	loss_value_1: 0.03047
	loss_reward_1: 0.00531
	loss_policy_2: 0.03048
	accuracy_policy_2: 0.66109
	loss_value_2: 0.03176
	loss_reward_2: 0.00495
	loss_policy_3: 0.03333
	accuracy_policy_3: 0.63633
	loss_value_3: 0.03285
	loss_reward_3: 0.00545
	loss_policy_4: 0.03514
	accuracy_policy_4: 0.61938
	loss_value_4: 0.03411
	loss_reward_4: 0.00587
	loss_policy_5: 0.03725
	accuracy_policy_5: 0.60035
	loss_value_5: 0.03524
	loss_reward_5: 0.00688
	loss_policy: 0.24167
	loss_value: 0.30811
	loss_reward: 0.02846
Optimization_Done 27200
[2024-05-05 06:56:19] [command] train weight_iter_27200.pkl 135 137
[2024-05-05 06:56:37] nn step 27250, lr: 0.015.
	loss_policy_0: 0.12991
	accuracy_policy_0: 0.70039
	loss_value_0: 0.14698
	loss_policy_1: 0.03297
	accuracy_policy_1: 0.63883
	loss_value_1: 0.03141
	loss_reward_1: 0.00518
	loss_policy_2: 0.03553
	accuracy_policy_2: 0.6157
	loss_value_2: 0.03311
	loss_reward_2: 0.00498
	loss_policy_3: 0.03832
	accuracy_policy_3: 0.59867
	loss_value_3: 0.03441
	loss_reward_3: 0.00533
	loss_policy_4: 0.0401
	accuracy_policy_4: 0.57543
	loss_value_4: 0.03559
	loss_reward_4: 0.0055
	loss_policy_5: 0.0426
	accuracy_policy_5: 0.55488
	loss_value_5: 0.03681
	loss_reward_5: 0.00625
	loss_policy: 0.31943
	loss_value: 0.3183
	loss_reward: 0.02724
[2024-05-05 06:56:53] nn step 27300, lr: 0.015.
	loss_policy_0: 0.10484
	accuracy_policy_0: 0.75109
	loss_value_0: 0.14
	loss_policy_1: 0.0301
	accuracy_policy_1: 0.66902
	loss_value_1: 0.03003
	loss_reward_1: 0.00517
	loss_policy_2: 0.03353
	accuracy_policy_2: 0.63762
	loss_value_2: 0.03173
	loss_reward_2: 0.00486
	loss_policy_3: 0.03542
	accuracy_policy_3: 0.62246
	loss_value_3: 0.03322
	loss_reward_3: 0.00528
	loss_policy_4: 0.03777
	accuracy_policy_4: 0.60617
	loss_value_4: 0.03464
	loss_reward_4: 0.00555
	loss_policy_5: 0.03977
	accuracy_policy_5: 0.58941
	loss_value_5: 0.03563
	loss_reward_5: 0.00638
	loss_policy: 0.28143
	loss_value: 0.30525
	loss_reward: 0.02725
[2024-05-05 06:57:09] nn step 27350, lr: 0.015.
	loss_policy_0: 0.0932
	accuracy_policy_0: 0.76449
	loss_value_0: 0.13383
	loss_policy_1: 0.02835
	accuracy_policy_1: 0.67668
	loss_value_1: 0.02858
	loss_reward_1: 0.00477
	loss_policy_2: 0.03052
	accuracy_policy_2: 0.65262
	loss_value_2: 0.03006
	loss_reward_2: 0.00472
	loss_policy_3: 0.03344
	accuracy_policy_3: 0.62691
	loss_value_3: 0.03141
	loss_reward_3: 0.00494
	loss_policy_4: 0.0348
	accuracy_policy_4: 0.6152
	loss_value_4: 0.03259
	loss_reward_4: 0.00529
	loss_policy_5: 0.03701
	accuracy_policy_5: 0.59562
	loss_value_5: 0.03375
	loss_reward_5: 0.00601
	loss_policy: 0.25731
	loss_value: 0.29022
	loss_reward: 0.02572
[2024-05-05 06:57:26] nn step 27400, lr: 0.015.
	loss_policy_0: 0.09403
	accuracy_policy_0: 0.77137
	loss_value_0: 0.14042
	loss_policy_1: 0.02846
	accuracy_policy_1: 0.6823
	loss_value_1: 0.03005
	loss_reward_1: 0.00516
	loss_policy_2: 0.03193
	accuracy_policy_2: 0.65457
	loss_value_2: 0.03161
	loss_reward_2: 0.00488
	loss_policy_3: 0.0344
	accuracy_policy_3: 0.63219
	loss_value_3: 0.03279
	loss_reward_3: 0.00525
	loss_policy_4: 0.03674
	accuracy_policy_4: 0.61199
	loss_value_4: 0.0341
	loss_reward_4: 0.00536
	loss_policy_5: 0.03878
	accuracy_policy_5: 0.60117
	loss_value_5: 0.0353
	loss_reward_5: 0.00631
	loss_policy: 0.26435
	loss_value: 0.30426
	loss_reward: 0.02696
Optimization_Done 27400
[2024-05-05 06:59:30] [command] train weight_iter_27400.pkl 136 138
[2024-05-05 06:59:48] nn step 27450, lr: 0.015.
	loss_policy_0: 0.13124
	accuracy_policy_0: 0.73359
	loss_value_0: 0.16914
	loss_policy_1: 0.03724
	accuracy_policy_1: 0.64594
	loss_value_1: 0.03602
	loss_reward_1: 0.00682
	loss_policy_2: 0.04093
	accuracy_policy_2: 0.61598
	loss_value_2: 0.03776
	loss_reward_2: 0.00677
	loss_policy_3: 0.04442
	accuracy_policy_3: 0.59062
	loss_value_3: 0.03895
	loss_reward_3: 0.00723
	loss_policy_4: 0.04702
	accuracy_policy_4: 0.5709
	loss_value_4: 0.04055
	loss_reward_4: 0.00788
	loss_policy_5: 0.04929
	accuracy_policy_5: 0.55133
	loss_value_5: 0.04198
	loss_reward_5: 0.009
	loss_policy: 0.35013
	loss_value: 0.3644
	loss_reward: 0.03771
[2024-05-05 07:00:05] nn step 27500, lr: 0.015.
	loss_policy_0: 0.1108
	accuracy_policy_0: 0.76203
	loss_value_0: 0.15972
	loss_policy_1: 0.03348
	accuracy_policy_1: 0.66629
	loss_value_1: 0.03419
	loss_reward_1: 0.00658
	loss_policy_2: 0.03699
	accuracy_policy_2: 0.64082
	loss_value_2: 0.03586
	loss_reward_2: 0.00643
	loss_policy_3: 0.04023
	accuracy_policy_3: 0.61676
	loss_value_3: 0.03723
	loss_reward_3: 0.00706
	loss_policy_4: 0.04326
	accuracy_policy_4: 0.59246
	loss_value_4: 0.03836
	loss_reward_4: 0.00744
	loss_policy_5: 0.04567
	accuracy_policy_5: 0.57535
	loss_value_5: 0.03971
	loss_reward_5: 0.00839
	loss_policy: 0.31043
	loss_value: 0.34508
	loss_reward: 0.03591
[2024-05-05 07:00:21] nn step 27550, lr: 0.015.
	loss_policy_0: 0.1025
	accuracy_policy_0: 0.77172
	loss_value_0: 0.15544
	loss_policy_1: 0.03184
	accuracy_policy_1: 0.67059
	loss_value_1: 0.03318
	loss_reward_1: 0.0066
	loss_policy_2: 0.03547
	accuracy_policy_2: 0.64102
	loss_value_2: 0.03455
	loss_reward_2: 0.00616
	loss_policy_3: 0.03853
	accuracy_policy_3: 0.62277
	loss_value_3: 0.036
	loss_reward_3: 0.00657
	loss_policy_4: 0.04094
	accuracy_policy_4: 0.60328
	loss_value_4: 0.03735
	loss_reward_4: 0.00724
	loss_policy_5: 0.044
	accuracy_policy_5: 0.57445
	loss_value_5: 0.03871
	loss_reward_5: 0.00837
	loss_policy: 0.29327
	loss_value: 0.33524
	loss_reward: 0.03495
[2024-05-05 07:00:38] nn step 27600, lr: 0.015.
	loss_policy_0: 0.10447
	accuracy_policy_0: 0.7723
	loss_value_0: 0.16028
	loss_policy_1: 0.03298
	accuracy_policy_1: 0.67402
	loss_value_1: 0.03412
	loss_reward_1: 0.00656
	loss_policy_2: 0.0365
	accuracy_policy_2: 0.64512
	loss_value_2: 0.03561
	loss_reward_2: 0.0064
	loss_policy_3: 0.03939
	accuracy_policy_3: 0.62039
	loss_value_3: 0.03711
	loss_reward_3: 0.00695
	loss_policy_4: 0.0421
	accuracy_policy_4: 0.60199
	loss_value_4: 0.03831
	loss_reward_4: 0.00733
	loss_policy_5: 0.04527
	accuracy_policy_5: 0.5798
	loss_value_5: 0.03978
	loss_reward_5: 0.00865
	loss_policy: 0.30071
	loss_value: 0.34521
	loss_reward: 0.0359
Optimization_Done 27600
[2024-05-05 07:02:39] [command] train weight_iter_27600.pkl 137 139
[2024-05-05 07:02:58] nn step 27650, lr: 0.015.
	loss_policy_0: 0.13467
	accuracy_policy_0: 0.72176
	loss_value_0: 0.15748
	loss_policy_1: 0.03661
	accuracy_policy_1: 0.63832
	loss_value_1: 0.03345
	loss_reward_1: 0.00679
	loss_policy_2: 0.03994
	accuracy_policy_2: 0.61387
	loss_value_2: 0.03492
	loss_reward_2: 0.00636
	loss_policy_3: 0.04309
	accuracy_policy_3: 0.58746
	loss_value_3: 0.03617
	loss_reward_3: 0.0068
	loss_policy_4: 0.04609
	accuracy_policy_4: 0.5643
	loss_value_4: 0.03746
	loss_reward_4: 0.00711
	loss_policy_5: 0.04844
	accuracy_policy_5: 0.54867
	loss_value_5: 0.03853
	loss_reward_5: 0.00831
	loss_policy: 0.34883
	loss_value: 0.33801
	loss_reward: 0.03538
[2024-05-05 07:03:14] nn step 27700, lr: 0.015.
	loss_policy_0: 0.1165
	accuracy_policy_0: 0.76207
	loss_value_0: 0.16611
	loss_policy_1: 0.03667
	accuracy_policy_1: 0.66234
	loss_value_1: 0.03533
	loss_reward_1: 0.00718
	loss_policy_2: 0.03999
	accuracy_policy_2: 0.63828
	loss_value_2: 0.03674
	loss_reward_2: 0.00677
	loss_policy_3: 0.04383
	accuracy_policy_3: 0.61187
	loss_value_3: 0.03804
	loss_reward_3: 0.00731
	loss_policy_4: 0.04654
	accuracy_policy_4: 0.59516
	loss_value_4: 0.03943
	loss_reward_4: 0.00808
	loss_policy_5: 0.049
	accuracy_policy_5: 0.57641
	loss_value_5: 0.04073
	loss_reward_5: 0.0089
	loss_policy: 0.33253
	loss_value: 0.35638
	loss_reward: 0.03824
[2024-05-05 07:03:30] nn step 27750, lr: 0.015.
	loss_policy_0: 0.11383
	accuracy_policy_0: 0.76734
	loss_value_0: 0.16541
	loss_policy_1: 0.03566
	accuracy_policy_1: 0.6707
	loss_value_1: 0.03487
	loss_reward_1: 0.00721
	loss_policy_2: 0.03983
	accuracy_policy_2: 0.63797
	loss_value_2: 0.03655
	loss_reward_2: 0.00689
	loss_policy_3: 0.04341
	accuracy_policy_3: 0.60789
	loss_value_3: 0.03816
	loss_reward_3: 0.00716
	loss_policy_4: 0.04657
	accuracy_policy_4: 0.58746
	loss_value_4: 0.0393
	loss_reward_4: 0.00759
	loss_policy_5: 0.04899
	accuracy_policy_5: 0.5752
	loss_value_5: 0.04039
	loss_reward_5: 0.00927
	loss_policy: 0.32829
	loss_value: 0.35467
	loss_reward: 0.03812
[2024-05-05 07:03:47] nn step 27800, lr: 0.015.
	loss_policy_0: 0.11001
	accuracy_policy_0: 0.77652
	loss_value_0: 0.16493
	loss_policy_1: 0.03527
	accuracy_policy_1: 0.67129
	loss_value_1: 0.03494
	loss_reward_1: 0.00718
	loss_policy_2: 0.03939
	accuracy_policy_2: 0.64531
	loss_value_2: 0.0362
	loss_reward_2: 0.00695
	loss_policy_3: 0.04276
	accuracy_policy_3: 0.61988
	loss_value_3: 0.03762
	loss_reward_3: 0.00726
	loss_policy_4: 0.04557
	accuracy_policy_4: 0.60039
	loss_value_4: 0.03896
	loss_reward_4: 0.00777
	loss_policy_5: 0.0485
	accuracy_policy_5: 0.5843
	loss_value_5: 0.04019
	loss_reward_5: 0.00921
	loss_policy: 0.32151
	loss_value: 0.35285
	loss_reward: 0.03838
Optimization_Done 27800
[2024-05-05 07:05:30] [command] train weight_iter_27800.pkl 138 140
[2024-05-05 07:05:48] nn step 27850, lr: 0.015.
	loss_policy_0: 0.12311
	accuracy_policy_0: 0.7327
	loss_value_0: 0.15307
	loss_policy_1: 0.03474
	accuracy_policy_1: 0.64684
	loss_value_1: 0.03214
	loss_reward_1: 0.00633
	loss_policy_2: 0.03822
	accuracy_policy_2: 0.60977
	loss_value_2: 0.03349
	loss_reward_2: 0.00589
	loss_policy_3: 0.0415
	accuracy_policy_3: 0.58473
	loss_value_3: 0.0347
	loss_reward_3: 0.00641
	loss_policy_4: 0.04366
	accuracy_policy_4: 0.56449
	loss_value_4: 0.03579
	loss_reward_4: 0.00658
	loss_policy_5: 0.04556
	accuracy_policy_5: 0.5475
	loss_value_5: 0.03678
	loss_reward_5: 0.00805
	loss_policy: 0.32679
	loss_value: 0.32596
	loss_reward: 0.03326
[2024-05-05 07:06:04] nn step 27900, lr: 0.015.
	loss_policy_0: 0.11524
	accuracy_policy_0: 0.76711
	loss_value_0: 0.16437
	loss_policy_1: 0.03538
	accuracy_policy_1: 0.6641
	loss_value_1: 0.0347
	loss_reward_1: 0.00687
	loss_policy_2: 0.03947
	accuracy_policy_2: 0.63352
	loss_value_2: 0.036
	loss_reward_2: 0.00632
	loss_policy_3: 0.0435
	accuracy_policy_3: 0.60047
	loss_value_3: 0.03713
	loss_reward_3: 0.0068
	loss_policy_4: 0.04583
	accuracy_policy_4: 0.5852
	loss_value_4: 0.03825
	loss_reward_4: 0.00738
	loss_policy_5: 0.0487
	accuracy_policy_5: 0.5632
	loss_value_5: 0.03948
	loss_reward_5: 0.00871
	loss_policy: 0.32812
	loss_value: 0.34993
	loss_reward: 0.03607
[2024-05-05 07:06:21] nn step 27950, lr: 0.015.
	loss_policy_0: 0.10401
	accuracy_policy_0: 0.76957
	loss_value_0: 0.15396
	loss_policy_1: 0.03289
	accuracy_policy_1: 0.67043
	loss_value_1: 0.03257
	loss_reward_1: 0.00653
	loss_policy_2: 0.03653
	accuracy_policy_2: 0.63918
	loss_value_2: 0.03402
	loss_reward_2: 0.00591
	loss_policy_3: 0.04011
	accuracy_policy_3: 0.60746
	loss_value_3: 0.03532
	loss_reward_3: 0.00625
	loss_policy_4: 0.0423
	accuracy_policy_4: 0.59051
	loss_value_4: 0.03653
	loss_reward_4: 0.00695
	loss_policy_5: 0.04488
	accuracy_policy_5: 0.56793
	loss_value_5: 0.03768
	loss_reward_5: 0.00832
	loss_policy: 0.30071
	loss_value: 0.33008
	loss_reward: 0.03396
[2024-05-05 07:06:37] nn step 28000, lr: 0.015.
	loss_policy_0: 0.10065
	accuracy_policy_0: 0.78508
	loss_value_0: 0.15871
	loss_policy_1: 0.03374
	accuracy_policy_1: 0.67285
	loss_value_1: 0.03353
	loss_reward_1: 0.00666
	loss_policy_2: 0.03722
	accuracy_policy_2: 0.64488
	loss_value_2: 0.03495
	loss_reward_2: 0.00621
	loss_policy_3: 0.04112
	accuracy_policy_3: 0.61176
	loss_value_3: 0.03606
	loss_reward_3: 0.00667
	loss_policy_4: 0.04396
	accuracy_policy_4: 0.59164
	loss_value_4: 0.03721
	loss_reward_4: 0.00729
	loss_policy_5: 0.04601
	accuracy_policy_5: 0.57355
	loss_value_5: 0.0383
	loss_reward_5: 0.00856
	loss_policy: 0.30271
	loss_value: 0.33876
	loss_reward: 0.0354
Optimization_Done 28000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-05 07:53:22] [command] train weight_iter_28000.pkl 139 141
[2024-05-05 07:53:50] nn step 28050, lr: 0.015.
	loss_policy_0: 0.12589
	accuracy_policy_0: 0.7368
	loss_value_0: 0.16253
	loss_policy_1: 0.03527
	accuracy_policy_1: 0.6509
	loss_value_1: 0.03434
	loss_reward_1: 0.00498
	loss_policy_2: 0.03851
	accuracy_policy_2: 0.6182
	loss_value_2: 0.03572
	loss_reward_2: 0.00479
	loss_policy_3: 0.04165
	accuracy_policy_3: 0.59004
	loss_value_3: 0.03691
	loss_reward_3: 0.00511
	loss_policy_4: 0.04447
	accuracy_policy_4: 0.56633
	loss_value_4: 0.03784
	loss_reward_4: 0.00552
	loss_policy_5: 0.04712
	accuracy_policy_5: 0.54305
	loss_value_5: 0.03852
	loss_reward_5: 0.0063
	loss_policy: 0.33292
	loss_value: 0.34586
	loss_reward: 0.02671
[2024-05-05 07:54:07] nn step 28100, lr: 0.015.
	loss_policy_0: 0.11709
	accuracy_policy_0: 0.75539
	loss_value_0: 0.16451
	loss_policy_1: 0.03463
	accuracy_policy_1: 0.6682
	loss_value_1: 0.03484
	loss_reward_1: 0.00524
	loss_policy_2: 0.03828
	accuracy_policy_2: 0.6373
	loss_value_2: 0.03624
	loss_reward_2: 0.00496
	loss_policy_3: 0.04219
	accuracy_policy_3: 0.60676
	loss_value_3: 0.03749
	loss_reward_3: 0.00534
	loss_policy_4: 0.04431
	accuracy_policy_4: 0.58445
	loss_value_4: 0.03849
	loss_reward_4: 0.00592
	loss_policy_5: 0.04665
	accuracy_policy_5: 0.5732
	loss_value_5: 0.03933
	loss_reward_5: 0.00647
	loss_policy: 0.32315
	loss_value: 0.35089
	loss_reward: 0.02792
[2024-05-05 07:54:23] nn step 28150, lr: 0.015.
	loss_policy_0: 0.106
	accuracy_policy_0: 0.76148
	loss_value_0: 0.15712
	loss_policy_1: 0.03233
	accuracy_policy_1: 0.67215
	loss_value_1: 0.03328
	loss_reward_1: 0.0051
	loss_policy_2: 0.03581
	accuracy_policy_2: 0.64234
	loss_value_2: 0.03446
	loss_reward_2: 0.00481
	loss_policy_3: 0.03903
	accuracy_policy_3: 0.61508
	loss_value_3: 0.0357
	loss_reward_3: 0.00523
	loss_policy_4: 0.04122
	accuracy_policy_4: 0.5968
	loss_value_4: 0.03674
	loss_reward_4: 0.00566
	loss_policy_5: 0.04352
	accuracy_policy_5: 0.57898
	loss_value_5: 0.0379
	loss_reward_5: 0.00621
	loss_policy: 0.29791
	loss_value: 0.33521
	loss_reward: 0.02701
[2024-05-05 07:54:39] nn step 28200, lr: 0.015.
	loss_policy_0: 0.11043
	accuracy_policy_0: 0.76867
	loss_value_0: 0.16854
	loss_policy_1: 0.03433
	accuracy_policy_1: 0.67398
	loss_value_1: 0.03609
	loss_reward_1: 0.00548
	loss_policy_2: 0.03806
	accuracy_policy_2: 0.645
	loss_value_2: 0.03736
	loss_reward_2: 0.00514
	loss_policy_3: 0.04151
	accuracy_policy_3: 0.61469
	loss_value_3: 0.03856
	loss_reward_3: 0.00528
	loss_policy_4: 0.04431
	accuracy_policy_4: 0.59629
	loss_value_4: 0.03969
	loss_reward_4: 0.00585
	loss_policy_5: 0.04628
	accuracy_policy_5: 0.58387
	loss_value_5: 0.04072
	loss_reward_5: 0.00689
	loss_policy: 0.31492
	loss_value: 0.36096
	loss_reward: 0.02864
Optimization_Done 28200
[2024-05-05 07:56:51] [command] train weight_iter_28200.pkl 140 142
[2024-05-05 07:57:08] nn step 28250, lr: 0.015.
	loss_policy_0: 0.13434
	accuracy_policy_0: 0.70105
	loss_value_0: 0.15094
	loss_policy_1: 0.03442
	accuracy_policy_1: 0.62984
	loss_value_1: 0.03181
	loss_reward_1: 0.00439
	loss_policy_2: 0.03744
	accuracy_policy_2: 0.6059
	loss_value_2: 0.03314
	loss_reward_2: 0.0042
	loss_policy_3: 0.04011
	accuracy_policy_3: 0.58277
	loss_value_3: 0.03427
	loss_reward_3: 0.00461
	loss_policy_4: 0.04294
	accuracy_policy_4: 0.55395
	loss_value_4: 0.03536
	loss_reward_4: 0.00521
	loss_policy_5: 0.04519
	accuracy_policy_5: 0.53695
	loss_value_5: 0.03645
	loss_reward_5: 0.0057
	loss_policy: 0.33445
	loss_value: 0.32196
	loss_reward: 0.02411
[2024-05-05 07:57:25] nn step 28300, lr: 0.015.
	loss_policy_0: 0.11899
	accuracy_policy_0: 0.75105
	loss_value_0: 0.16089
	loss_policy_1: 0.03414
	accuracy_policy_1: 0.6648
	loss_value_1: 0.03422
	loss_reward_1: 0.00472
	loss_policy_2: 0.03761
	accuracy_policy_2: 0.64055
	loss_value_2: 0.03567
	loss_reward_2: 0.00472
	loss_policy_3: 0.04116
	accuracy_policy_3: 0.61035
	loss_value_3: 0.03711
	loss_reward_3: 0.00481
	loss_policy_4: 0.0439
	accuracy_policy_4: 0.58539
	loss_value_4: 0.03845
	loss_reward_4: 0.00541
	loss_policy_5: 0.04643
	accuracy_policy_5: 0.56762
	loss_value_5: 0.03956
	loss_reward_5: 0.0061
	loss_policy: 0.32223
	loss_value: 0.3459
	loss_reward: 0.02576
[2024-05-05 07:57:41] nn step 28350, lr: 0.015.
	loss_policy_0: 0.11635
	accuracy_policy_0: 0.7577
	loss_value_0: 0.16641
	loss_policy_1: 0.03413
	accuracy_policy_1: 0.67188
	loss_value_1: 0.03534
	loss_reward_1: 0.00496
	loss_policy_2: 0.03789
	accuracy_policy_2: 0.6441
	loss_value_2: 0.03693
	loss_reward_2: 0.00466
	loss_policy_3: 0.04141
	accuracy_policy_3: 0.61746
	loss_value_3: 0.03838
	loss_reward_3: 0.00517
	loss_policy_4: 0.04459
	accuracy_policy_4: 0.59742
	loss_value_4: 0.03972
	loss_reward_4: 0.00541
	loss_policy_5: 0.04696
	accuracy_policy_5: 0.5775
	loss_value_5: 0.04069
	loss_reward_5: 0.00636
	loss_policy: 0.32133
	loss_value: 0.35746
	loss_reward: 0.02656
[2024-05-05 07:57:57] nn step 28400, lr: 0.015.
	loss_policy_0: 0.11545
	accuracy_policy_0: 0.76309
	loss_value_0: 0.16981
	loss_policy_1: 0.03441
	accuracy_policy_1: 0.66973
	loss_value_1: 0.03603
	loss_reward_1: 0.00486
	loss_policy_2: 0.03844
	accuracy_policy_2: 0.64535
	loss_value_2: 0.03734
	loss_reward_2: 0.00475
	loss_policy_3: 0.042
	accuracy_policy_3: 0.61961
	loss_value_3: 0.03867
	loss_reward_3: 0.00532
	loss_policy_4: 0.04447
	accuracy_policy_4: 0.60113
	loss_value_4: 0.03984
	loss_reward_4: 0.00561
	loss_policy_5: 0.04723
	accuracy_policy_5: 0.5784
	loss_value_5: 0.04088
	loss_reward_5: 0.00642
	loss_policy: 0.322
	loss_value: 0.36257
	loss_reward: 0.02697
Optimization_Done 28400
[2024-05-05 07:59:58] [command] train weight_iter_28400.pkl 141 143
[2024-05-05 08:00:17] nn step 28450, lr: 0.015.
	loss_policy_0: 0.12882
	accuracy_policy_0: 0.71871
	loss_value_0: 0.16438
	loss_policy_1: 0.03557
	accuracy_policy_1: 0.63961
	loss_value_1: 0.03493
	loss_reward_1: 0.00544
	loss_policy_2: 0.03922
	accuracy_policy_2: 0.60852
	loss_value_2: 0.0365
	loss_reward_2: 0.00504
	loss_policy_3: 0.04309
	accuracy_policy_3: 0.58152
	loss_value_3: 0.03787
	loss_reward_3: 0.00556
	loss_policy_4: 0.04618
	accuracy_policy_4: 0.5582
	loss_value_4: 0.03907
	loss_reward_4: 0.00583
	loss_policy_5: 0.04904
	accuracy_policy_5: 0.52965
	loss_value_5: 0.04015
	loss_reward_5: 0.00695
	loss_policy: 0.34193
	loss_value: 0.35289
	loss_reward: 0.02882
[2024-05-05 08:00:33] nn step 28500, lr: 0.015.
	loss_policy_0: 0.11776
	accuracy_policy_0: 0.75383
	loss_value_0: 0.17163
	loss_policy_1: 0.03485
	accuracy_policy_1: 0.65566
	loss_value_1: 0.03626
	loss_reward_1: 0.00583
	loss_policy_2: 0.03811
	accuracy_policy_2: 0.63813
	loss_value_2: 0.03762
	loss_reward_2: 0.00542
	loss_policy_3: 0.04236
	accuracy_policy_3: 0.60727
	loss_value_3: 0.03933
	loss_reward_3: 0.00594
	loss_policy_4: 0.04536
	accuracy_policy_4: 0.58855
	loss_value_4: 0.04047
	loss_reward_4: 0.00614
	loss_policy_5: 0.04819
	accuracy_policy_5: 0.56523
	loss_value_5: 0.04173
	loss_reward_5: 0.00719
	loss_policy: 0.32662
	loss_value: 0.36704
	loss_reward: 0.03053
[2024-05-05 08:00:49] nn step 28550, lr: 0.015.
	loss_policy_0: 0.1035
	accuracy_policy_0: 0.76152
	loss_value_0: 0.16102
	loss_policy_1: 0.03163
	accuracy_policy_1: 0.66848
	loss_value_1: 0.03401
	loss_reward_1: 0.00542
	loss_policy_2: 0.03537
	accuracy_policy_2: 0.63652
	loss_value_2: 0.03548
	loss_reward_2: 0.00513
	loss_policy_3: 0.03878
	accuracy_policy_3: 0.61199
	loss_value_3: 0.03673
	loss_reward_3: 0.00548
	loss_policy_4: 0.04188
	accuracy_policy_4: 0.59367
	loss_value_4: 0.03805
	loss_reward_4: 0.00562
	loss_policy_5: 0.0446
	accuracy_policy_5: 0.5734
	loss_value_5: 0.03909
	loss_reward_5: 0.00685
	loss_policy: 0.29576
	loss_value: 0.34438
	loss_reward: 0.02849
[2024-05-05 08:01:05] nn step 28600, lr: 0.015.
	loss_policy_0: 0.10197
	accuracy_policy_0: 0.76844
	loss_value_0: 0.16199
	loss_policy_1: 0.03126
	accuracy_policy_1: 0.67703
	loss_value_1: 0.03413
	loss_reward_1: 0.00539
	loss_policy_2: 0.03498
	accuracy_policy_2: 0.64824
	loss_value_2: 0.03539
	loss_reward_2: 0.00493
	loss_policy_3: 0.03865
	accuracy_policy_3: 0.61863
	loss_value_3: 0.03659
	loss_reward_3: 0.00551
	loss_policy_4: 0.0417
	accuracy_policy_4: 0.59473
	loss_value_4: 0.03783
	loss_reward_4: 0.00589
	loss_policy_5: 0.04437
	accuracy_policy_5: 0.58008
	loss_value_5: 0.03892
	loss_reward_5: 0.00689
	loss_policy: 0.29294
	loss_value: 0.34486
	loss_reward: 0.02861
Optimization_Done 28600
[2024-05-05 08:03:05] [command] train weight_iter_28600.pkl 142 144
[2024-05-05 08:03:22] nn step 28650, lr: 0.015.
	loss_policy_0: 0.10616
	accuracy_policy_0: 0.73934
	loss_value_0: 0.14487
	loss_policy_1: 0.03121
	accuracy_policy_1: 0.64484
	loss_value_1: 0.03088
	loss_reward_1: 0.00536
	loss_policy_2: 0.0343
	accuracy_policy_2: 0.61781
	loss_value_2: 0.03211
	loss_reward_2: 0.00501
	loss_policy_3: 0.03738
	accuracy_policy_3: 0.59465
	loss_value_3: 0.03342
	loss_reward_3: 0.00539
	loss_policy_4: 0.04026
	accuracy_policy_4: 0.56836
	loss_value_4: 0.03453
	loss_reward_4: 0.00544
	loss_policy_5: 0.04298
	accuracy_policy_5: 0.54715
	loss_value_5: 0.03571
	loss_reward_5: 0.00674
	loss_policy: 0.29229
	loss_value: 0.31152
	loss_reward: 0.02795
[2024-05-05 08:03:39] nn step 28700, lr: 0.015.
	loss_policy_0: 0.09121
	accuracy_policy_0: 0.76422
	loss_value_0: 0.1377
	loss_policy_1: 0.02815
	accuracy_policy_1: 0.66895
	loss_value_1: 0.0292
	loss_reward_1: 0.00506
	loss_policy_2: 0.03115
	accuracy_policy_2: 0.63637
	loss_value_2: 0.03059
	loss_reward_2: 0.00469
	loss_policy_3: 0.03395
	accuracy_policy_3: 0.60902
	loss_value_3: 0.03168
	loss_reward_3: 0.00501
	loss_policy_4: 0.03717
	accuracy_policy_4: 0.58211
	loss_value_4: 0.0326
	loss_reward_4: 0.00535
	loss_policy_5: 0.03924
	accuracy_policy_5: 0.5668
	loss_value_5: 0.03355
	loss_reward_5: 0.00638
	loss_policy: 0.26087
	loss_value: 0.29533
	loss_reward: 0.02649
[2024-05-05 08:03:55] nn step 28750, lr: 0.015.
	loss_policy_0: 0.08876
	accuracy_policy_0: 0.77898
	loss_value_0: 0.14352
	loss_policy_1: 0.02862
	accuracy_policy_1: 0.67633
	loss_value_1: 0.03023
	loss_reward_1: 0.00534
	loss_policy_2: 0.03222
	accuracy_policy_2: 0.64711
	loss_value_2: 0.03168
	loss_reward_2: 0.00486
	loss_policy_3: 0.0351
	accuracy_policy_3: 0.6152
	loss_value_3: 0.03309
	loss_reward_3: 0.00544
	loss_policy_4: 0.03754
	accuracy_policy_4: 0.60383
	loss_value_4: 0.03441
	loss_reward_4: 0.00544
	loss_policy_5: 0.04061
	accuracy_policy_5: 0.57855
	loss_value_5: 0.03573
	loss_reward_5: 0.00654
	loss_policy: 0.26286
	loss_value: 0.30867
	loss_reward: 0.02761
[2024-05-05 08:04:11] nn step 28800, lr: 0.015.
	loss_policy_0: 0.08786
	accuracy_policy_0: 0.78488
	loss_value_0: 0.14326
	loss_policy_1: 0.02842
	accuracy_policy_1: 0.68242
	loss_value_1: 0.03044
	loss_reward_1: 0.00525
	loss_policy_2: 0.03171
	accuracy_policy_2: 0.65145
	loss_value_2: 0.03174
	loss_reward_2: 0.00472
	loss_policy_3: 0.03504
	accuracy_policy_3: 0.62223
	loss_value_3: 0.03295
	loss_reward_3: 0.0054
	loss_policy_4: 0.03798
	accuracy_policy_4: 0.60137
	loss_value_4: 0.03422
	loss_reward_4: 0.00547
	loss_policy_5: 0.04083
	accuracy_policy_5: 0.57703
	loss_value_5: 0.03537
	loss_reward_5: 0.00654
	loss_policy: 0.26184
	loss_value: 0.30799
	loss_reward: 0.02737
Optimization_Done 28800
[2024-05-05 08:06:14] [command] train weight_iter_28800.pkl 143 145
[2024-05-05 08:06:32] nn step 28850, lr: 0.015.
	loss_policy_0: 0.11048
	accuracy_policy_0: 0.725
	loss_value_0: 0.13641
	loss_policy_1: 0.03013
	accuracy_policy_1: 0.64168
	loss_value_1: 0.02913
	loss_reward_1: 0.00512
	loss_policy_2: 0.0328
	accuracy_policy_2: 0.61848
	loss_value_2: 0.03063
	loss_reward_2: 0.0048
	loss_policy_3: 0.03571
	accuracy_policy_3: 0.59598
	loss_value_3: 0.03194
	loss_reward_3: 0.00541
	loss_policy_4: 0.0379
	accuracy_policy_4: 0.57348
	loss_value_4: 0.03301
	loss_reward_4: 0.00585
	loss_policy_5: 0.03968
	accuracy_policy_5: 0.55891
	loss_value_5: 0.03422
	loss_reward_5: 0.00648
	loss_policy: 0.2867
	loss_value: 0.29534
	loss_reward: 0.02766
[2024-05-05 08:06:49] nn step 28900, lr: 0.015.
	loss_policy_0: 0.09184
	accuracy_policy_0: 0.76531
	loss_value_0: 0.14055
	loss_policy_1: 0.02767
	accuracy_policy_1: 0.67699
	loss_value_1: 0.02983
	loss_reward_1: 0.00521
	loss_policy_2: 0.03062
	accuracy_policy_2: 0.64809
	loss_value_2: 0.03131
	loss_reward_2: 0.00502
	loss_policy_3: 0.0333
	accuracy_policy_3: 0.6268
	loss_value_3: 0.03241
	loss_reward_3: 0.00549
	loss_policy_4: 0.03572
	accuracy_policy_4: 0.60684
	loss_value_4: 0.03359
	loss_reward_4: 0.00568
	loss_policy_5: 0.03807
	accuracy_policy_5: 0.58621
	loss_value_5: 0.03481
	loss_reward_5: 0.00673
	loss_policy: 0.2572
	loss_value: 0.3025
	loss_reward: 0.02814
[2024-05-05 08:07:05] nn step 28950, lr: 0.015.
	loss_policy_0: 0.08605
	accuracy_policy_0: 0.77715
	loss_value_0: 0.14224
	loss_policy_1: 0.02693
	accuracy_policy_1: 0.67812
	loss_value_1: 0.02992
	loss_reward_1: 0.00522
	loss_policy_2: 0.03029
	accuracy_policy_2: 0.65219
	loss_value_2: 0.03142
	loss_reward_2: 0.00502
	loss_policy_3: 0.03292
	accuracy_policy_3: 0.63559
	loss_value_3: 0.03257
	loss_reward_3: 0.0054
	loss_policy_4: 0.0354
	accuracy_policy_4: 0.61234
	loss_value_4: 0.0338
	loss_reward_4: 0.00576
	loss_policy_5: 0.03738
	accuracy_policy_5: 0.59652
	loss_value_5: 0.03483
	loss_reward_5: 0.00673
	loss_policy: 0.24897
	loss_value: 0.30477
	loss_reward: 0.02813
[2024-05-05 08:07:21] nn step 29000, lr: 0.015.
	loss_policy_0: 0.07485
	accuracy_policy_0: 0.78688
	loss_value_0: 0.12793
	loss_policy_1: 0.02414
	accuracy_policy_1: 0.68316
	loss_value_1: 0.02678
	loss_reward_1: 0.00479
	loss_policy_2: 0.02687
	accuracy_policy_2: 0.65836
	loss_value_2: 0.0279
	loss_reward_2: 0.00441
	loss_policy_3: 0.02916
	accuracy_policy_3: 0.63551
	loss_value_3: 0.02905
	loss_reward_3: 0.00492
	loss_policy_4: 0.03118
	accuracy_policy_4: 0.61746
	loss_value_4: 0.0301
	loss_reward_4: 0.00528
	loss_policy_5: 0.03342
	accuracy_policy_5: 0.59316
	loss_value_5: 0.03115
	loss_reward_5: 0.00614
	loss_policy: 0.21961
	loss_value: 0.27291
	loss_reward: 0.02554
Optimization_Done 29000
[2024-05-05 08:09:23] [command] train weight_iter_29000.pkl 144 146
[2024-05-05 08:09:41] nn step 29050, lr: 0.015.
	loss_policy_0: 0.12802
	accuracy_policy_0: 0.69379
	loss_value_0: 0.13739
	loss_policy_1: 0.03269
	accuracy_policy_1: 0.63094
	loss_value_1: 0.02909
	loss_reward_1: 0.00479
	loss_policy_2: 0.03478
	accuracy_policy_2: 0.60379
	loss_value_2: 0.03038
	loss_reward_2: 0.00436
	loss_policy_3: 0.03706
	accuracy_policy_3: 0.59082
	loss_value_3: 0.03148
	loss_reward_3: 0.0048
	loss_policy_4: 0.03902
	accuracy_policy_4: 0.57219
	loss_value_4: 0.03262
	loss_reward_4: 0.00501
	loss_policy_5: 0.04082
	accuracy_policy_5: 0.55426
	loss_value_5: 0.03385
	loss_reward_5: 0.00581
	loss_policy: 0.31239
	loss_value: 0.29482
	loss_reward: 0.02477
[2024-05-05 08:09:57] nn step 29100, lr: 0.015.
	loss_policy_0: 0.11022
	accuracy_policy_0: 0.73875
	loss_value_0: 0.1453
	loss_policy_1: 0.03109
	accuracy_policy_1: 0.66148
	loss_value_1: 0.03046
	loss_reward_1: 0.00487
	loss_policy_2: 0.03418
	accuracy_policy_2: 0.63152
	loss_value_2: 0.03146
	loss_reward_2: 0.0047
	loss_policy_3: 0.03615
	accuracy_policy_3: 0.62184
	loss_value_3: 0.03268
	loss_reward_3: 0.00496
	loss_policy_4: 0.03814
	accuracy_policy_4: 0.60609
	loss_value_4: 0.03391
	loss_reward_4: 0.00545
	loss_policy_5: 0.04057
	accuracy_policy_5: 0.58637
	loss_value_5: 0.03501
	loss_reward_5: 0.00618
	loss_policy: 0.29035
	loss_value: 0.30881
	loss_reward: 0.02616
[2024-05-05 08:10:13] nn step 29150, lr: 0.015.
	loss_policy_0: 0.10462
	accuracy_policy_0: 0.75484
	loss_value_0: 0.14884
	loss_policy_1: 0.03071
	accuracy_policy_1: 0.67082
	loss_value_1: 0.03115
	loss_reward_1: 0.0051
	loss_policy_2: 0.03337
	accuracy_policy_2: 0.64652
	loss_value_2: 0.03237
	loss_reward_2: 0.00483
	loss_policy_3: 0.03613
	accuracy_policy_3: 0.62715
	loss_value_3: 0.03354
	loss_reward_3: 0.00519
	loss_policy_4: 0.03854
	accuracy_policy_4: 0.61309
	loss_value_4: 0.03493
	loss_reward_4: 0.00566
	loss_policy_5: 0.04077
	accuracy_policy_5: 0.59172
	loss_value_5: 0.03613
	loss_reward_5: 0.00653
	loss_policy: 0.28412
	loss_value: 0.31696
	loss_reward: 0.02732
[2024-05-05 08:10:30] nn step 29200, lr: 0.015.
	loss_policy_0: 0.09657
	accuracy_policy_0: 0.76895
	loss_value_0: 0.1446
	loss_policy_1: 0.02898
	accuracy_policy_1: 0.6743
	loss_value_1: 0.03035
	loss_reward_1: 0.00505
	loss_policy_2: 0.03163
	accuracy_policy_2: 0.65223
	loss_value_2: 0.03158
	loss_reward_2: 0.00455
	loss_policy_3: 0.03399
	accuracy_policy_3: 0.63488
	loss_value_3: 0.03255
	loss_reward_3: 0.00499
	loss_policy_4: 0.03621
	accuracy_policy_4: 0.61648
	loss_value_4: 0.03379
	loss_reward_4: 0.00527
	loss_policy_5: 0.03837
	accuracy_policy_5: 0.60273
	loss_value_5: 0.03498
	loss_reward_5: 0.00617
	loss_policy: 0.26575
	loss_value: 0.30786
	loss_reward: 0.02602
Optimization_Done 29200
[2024-05-05 08:12:34] [command] train weight_iter_29200.pkl 145 147
[2024-05-05 08:12:53] nn step 29250, lr: 0.015.
	loss_policy_0: 0.13022
	accuracy_policy_0: 0.69012
	loss_value_0: 0.15087
	loss_policy_1: 0.03522
	accuracy_policy_1: 0.60531
	loss_value_1: 0.03175
	loss_reward_1: 0.00569
	loss_policy_2: 0.03783
	accuracy_policy_2: 0.58387
	loss_value_2: 0.03333
	loss_reward_2: 0.00544
	loss_policy_3: 0.04082
	accuracy_policy_3: 0.56437
	loss_value_3: 0.03457
	loss_reward_3: 0.00571
	loss_policy_4: 0.04296
	accuracy_policy_4: 0.54648
	loss_value_4: 0.03565
	loss_reward_4: 0.00636
	loss_policy_5: 0.04504
	accuracy_policy_5: 0.52879
	loss_value_5: 0.03662
	loss_reward_5: 0.0073
	loss_policy: 0.3321
	loss_value: 0.32278
	loss_reward: 0.0305
[2024-05-05 08:13:09] nn step 29300, lr: 0.015.
	loss_policy_0: 0.11664
	accuracy_policy_0: 0.73293
	loss_value_0: 0.15421
	loss_policy_1: 0.03373
	accuracy_policy_1: 0.64332
	loss_value_1: 0.03267
	loss_reward_1: 0.00581
	loss_policy_2: 0.03695
	accuracy_policy_2: 0.61297
	loss_value_2: 0.03398
	loss_reward_2: 0.00552
	loss_policy_3: 0.03973
	accuracy_policy_3: 0.59055
	loss_value_3: 0.03523
	loss_reward_3: 0.00589
	loss_policy_4: 0.0426
	accuracy_policy_4: 0.57047
	loss_value_4: 0.03634
	loss_reward_4: 0.00627
	loss_policy_5: 0.04515
	accuracy_policy_5: 0.55406
	loss_value_5: 0.0374
	loss_reward_5: 0.00756
	loss_policy: 0.3148
	loss_value: 0.32983
	loss_reward: 0.03105
[2024-05-05 08:13:26] nn step 29350, lr: 0.015.
	loss_policy_0: 0.10901
	accuracy_policy_0: 0.74844
	loss_value_0: 0.15306
	loss_policy_1: 0.03294
	accuracy_policy_1: 0.64875
	loss_value_1: 0.0323
	loss_reward_1: 0.00587
	loss_policy_2: 0.03565
	accuracy_policy_2: 0.62727
	loss_value_2: 0.03371
	loss_reward_2: 0.00546
	loss_policy_3: 0.03848
	accuracy_policy_3: 0.60398
	loss_value_3: 0.03494
	loss_reward_3: 0.00576
	loss_policy_4: 0.0408
	accuracy_policy_4: 0.5891
	loss_value_4: 0.03602
	loss_reward_4: 0.00642
	loss_policy_5: 0.04388
	accuracy_policy_5: 0.56273
	loss_value_5: 0.03706
	loss_reward_5: 0.00731
	loss_policy: 0.30074
	loss_value: 0.32709
	loss_reward: 0.03082
[2024-05-05 08:13:43] nn step 29400, lr: 0.015.
	loss_policy_0: 0.1139
	accuracy_policy_0: 0.75
	loss_value_0: 0.16081
	loss_policy_1: 0.03427
	accuracy_policy_1: 0.65105
	loss_value_1: 0.03416
	loss_reward_1: 0.0062
	loss_policy_2: 0.03769
	accuracy_policy_2: 0.62734
	loss_value_2: 0.03547
	loss_reward_2: 0.0058
	loss_policy_3: 0.04064
	accuracy_policy_3: 0.61047
	loss_value_3: 0.0365
	loss_reward_3: 0.00628
	loss_policy_4: 0.04343
	accuracy_policy_4: 0.58719
	loss_value_4: 0.03763
	loss_reward_4: 0.00678
	loss_policy_5: 0.04591
	accuracy_policy_5: 0.56848
	loss_value_5: 0.03904
	loss_reward_5: 0.00809
	loss_policy: 0.31584
	loss_value: 0.34362
	loss_reward: 0.03315
Optimization_Done 29400
[2024-05-05 08:15:47] [command] train weight_iter_29400.pkl 146 148
[2024-05-05 08:16:05] nn step 29450, lr: 0.015.
	loss_policy_0: 0.14828
	accuracy_policy_0: 0.68434
	loss_value_0: 0.16506
	loss_policy_1: 0.04157
	accuracy_policy_1: 0.58555
	loss_value_1: 0.03481
	loss_reward_1: 0.00653
	loss_policy_2: 0.04447
	accuracy_policy_2: 0.55984
	loss_value_2: 0.0363
	loss_reward_2: 0.00598
	loss_policy_3: 0.04799
	accuracy_policy_3: 0.53805
	loss_value_3: 0.03766
	loss_reward_3: 0.0066
	loss_policy_4: 0.05046
	accuracy_policy_4: 0.51441
	loss_value_4: 0.03883
	loss_reward_4: 0.00711
	loss_policy_5: 0.0533
	accuracy_policy_5: 0.5023
	loss_value_5: 0.03984
	loss_reward_5: 0.00813
	loss_policy: 0.38607
	loss_value: 0.35251
	loss_reward: 0.03435
[2024-05-05 08:16:22] nn step 29500, lr: 0.015.
	loss_policy_0: 0.13379
	accuracy_policy_0: 0.72156
	loss_value_0: 0.16961
	loss_policy_1: 0.04034
	accuracy_policy_1: 0.61652
	loss_value_1: 0.03601
	loss_reward_1: 0.00664
	loss_policy_2: 0.04403
	accuracy_policy_2: 0.59102
	loss_value_2: 0.03749
	loss_reward_2: 0.00641
	loss_policy_3: 0.04754
	accuracy_policy_3: 0.56789
	loss_value_3: 0.03881
	loss_reward_3: 0.0068
	loss_policy_4: 0.05046
	accuracy_policy_4: 0.54246
	loss_value_4: 0.04022
	loss_reward_4: 0.00729
	loss_policy_5: 0.053
	accuracy_policy_5: 0.52613
	loss_value_5: 0.04144
	loss_reward_5: 0.00846
	loss_policy: 0.36916
	loss_value: 0.36357
	loss_reward: 0.03561
[2024-05-05 08:16:39] nn step 29550, lr: 0.015.
	loss_policy_0: 0.13425
	accuracy_policy_0: 0.73008
	loss_value_0: 0.17469
	loss_policy_1: 0.04088
	accuracy_policy_1: 0.62676
	loss_value_1: 0.03678
	loss_reward_1: 0.00708
	loss_policy_2: 0.04471
	accuracy_policy_2: 0.60145
	loss_value_2: 0.0385
	loss_reward_2: 0.00666
	loss_policy_3: 0.0478
	accuracy_policy_3: 0.57535
	loss_value_3: 0.03989
	loss_reward_3: 0.00701
	loss_policy_4: 0.05101
	accuracy_policy_4: 0.56008
	loss_value_4: 0.04115
	loss_reward_4: 0.00736
	loss_policy_5: 0.05415
	accuracy_policy_5: 0.53367
	loss_value_5: 0.0424
	loss_reward_5: 0.00874
	loss_policy: 0.37281
	loss_value: 0.37341
	loss_reward: 0.03685
[2024-05-05 08:16:56] nn step 29600, lr: 0.015.
	loss_policy_0: 0.12645
	accuracy_policy_0: 0.74406
	loss_value_0: 0.16946
	loss_policy_1: 0.03901
	accuracy_policy_1: 0.63199
	loss_value_1: 0.03577
	loss_reward_1: 0.00701
	loss_policy_2: 0.0427
	accuracy_policy_2: 0.60957
	loss_value_2: 0.03739
	loss_reward_2: 0.00642
	loss_policy_3: 0.04637
	accuracy_policy_3: 0.58168
	loss_value_3: 0.03859
	loss_reward_3: 0.0067
	loss_policy_4: 0.04976
	accuracy_policy_4: 0.55348
	loss_value_4: 0.04004
	loss_reward_4: 0.00761
	loss_policy_5: 0.05248
	accuracy_policy_5: 0.53836
	loss_value_5: 0.04131
	loss_reward_5: 0.00861
	loss_policy: 0.35676
	loss_value: 0.36256
	loss_reward: 0.03634
Optimization_Done 29600
[2024-05-05 08:18:39] [command] train weight_iter_29600.pkl 147 149
[2024-05-05 08:18:58] nn step 29650, lr: 0.015.
	loss_policy_0: 0.16287
	accuracy_policy_0: 0.68066
	loss_value_0: 0.17904
	loss_policy_1: 0.04435
	accuracy_policy_1: 0.58414
	loss_value_1: 0.03746
	loss_reward_1: 0.00639
	loss_policy_2: 0.04709
	accuracy_policy_2: 0.56434
	loss_value_2: 0.03907
	loss_reward_2: 0.00589
	loss_policy_3: 0.05079
	accuracy_policy_3: 0.52906
	loss_value_3: 0.04027
	loss_reward_3: 0.00615
	loss_policy_4: 0.05287
	accuracy_policy_4: 0.51414
	loss_value_4: 0.04122
	loss_reward_4: 0.00675
	loss_policy_5: 0.05597
	accuracy_policy_5: 0.48871
	loss_value_5: 0.04238
	loss_reward_5: 0.00776
	loss_policy: 0.41394
	loss_value: 0.37943
	loss_reward: 0.03294
[2024-05-05 08:19:15] nn step 29700, lr: 0.015.
	loss_policy_0: 0.13145
	accuracy_policy_0: 0.71973
	loss_value_0: 0.16339
	loss_policy_1: 0.03879
	accuracy_policy_1: 0.61457
	loss_value_1: 0.03459
	loss_reward_1: 0.00599
	loss_policy_2: 0.04167
	accuracy_policy_2: 0.5866
	loss_value_2: 0.03598
	loss_reward_2: 0.00556
	loss_policy_3: 0.04557
	accuracy_policy_3: 0.55582
	loss_value_3: 0.03714
	loss_reward_3: 0.0061
	loss_policy_4: 0.04771
	accuracy_policy_4: 0.54016
	loss_value_4: 0.0383
	loss_reward_4: 0.00644
	loss_policy_5: 0.04989
	accuracy_policy_5: 0.52016
	loss_value_5: 0.03919
	loss_reward_5: 0.00734
	loss_policy: 0.35507
	loss_value: 0.3486
	loss_reward: 0.03143
[2024-05-05 08:19:32] nn step 29750, lr: 0.015.
	loss_policy_0: 0.13438
	accuracy_policy_0: 0.73855
	loss_value_0: 0.17519
	loss_policy_1: 0.04109
	accuracy_policy_1: 0.62523
	loss_value_1: 0.03697
	loss_reward_1: 0.00648
	loss_policy_2: 0.04548
	accuracy_policy_2: 0.5909
	loss_value_2: 0.03871
	loss_reward_2: 0.00608
	loss_policy_3: 0.04885
	accuracy_policy_3: 0.56031
	loss_value_3: 0.03978
	loss_reward_3: 0.00649
	loss_policy_4: 0.05154
	accuracy_policy_4: 0.54379
	loss_value_4: 0.04096
	loss_reward_4: 0.00702
	loss_policy_5: 0.05368
	accuracy_policy_5: 0.52484
	loss_value_5: 0.04203
	loss_reward_5: 0.00827
	loss_policy: 0.37503
	loss_value: 0.37365
	loss_reward: 0.03434
[2024-05-05 08:19:49] nn step 29800, lr: 0.015.
	loss_policy_0: 0.13502
	accuracy_policy_0: 0.74371
	loss_value_0: 0.17923
	loss_policy_1: 0.04188
	accuracy_policy_1: 0.62867
	loss_value_1: 0.03764
	loss_reward_1: 0.00681
	loss_policy_2: 0.04586
	accuracy_policy_2: 0.5998
	loss_value_2: 0.03919
	loss_reward_2: 0.00635
	loss_policy_3: 0.04994
	accuracy_policy_3: 0.56922
	loss_value_3: 0.04047
	loss_reward_3: 0.00659
	loss_policy_4: 0.0524
	accuracy_policy_4: 0.54434
	loss_value_4: 0.0415
	loss_reward_4: 0.00701
	loss_policy_5: 0.05501
	accuracy_policy_5: 0.52598
	loss_value_5: 0.04252
	loss_reward_5: 0.00842
	loss_policy: 0.38011
	loss_value: 0.38055
	loss_reward: 0.03517
Optimization_Done 29800
[2024-05-05 08:21:52] [command] train weight_iter_29800.pkl 148 150
[2024-05-05 08:22:11] nn step 29850, lr: 0.015.
	loss_policy_0: 0.15521
	accuracy_policy_0: 0.67375
	loss_value_0: 0.16588
	loss_policy_1: 0.04027
	accuracy_policy_1: 0.59656
	loss_value_1: 0.03461
	loss_reward_1: 0.00529
	loss_policy_2: 0.04375
	accuracy_policy_2: 0.56555
	loss_value_2: 0.03608
	loss_reward_2: 0.00475
	loss_policy_3: 0.04723
	accuracy_policy_3: 0.53516
	loss_value_3: 0.03717
	loss_reward_3: 0.00514
	loss_policy_4: 0.04915
	accuracy_policy_4: 0.51434
	loss_value_4: 0.03823
	loss_reward_4: 0.00588
	loss_policy_5: 0.05174
	accuracy_policy_5: 0.49527
	loss_value_5: 0.03942
	loss_reward_5: 0.0064
	loss_policy: 0.38735
	loss_value: 0.35139
	loss_reward: 0.02747
[2024-05-05 08:22:28] nn step 29900, lr: 0.015.
	loss_policy_0: 0.1384
	accuracy_policy_0: 0.71391
	loss_value_0: 0.17406
	loss_policy_1: 0.0395
	accuracy_policy_1: 0.62246
	loss_value_1: 0.03674
	loss_reward_1: 0.00565
	loss_policy_2: 0.04335
	accuracy_policy_2: 0.59828
	loss_value_2: 0.03819
	loss_reward_2: 0.00513
	loss_policy_3: 0.04673
	accuracy_policy_3: 0.56676
	loss_value_3: 0.03943
	loss_reward_3: 0.00551
	loss_policy_4: 0.04981
	accuracy_policy_4: 0.54516
	loss_value_4: 0.04051
	loss_reward_4: 0.00602
	loss_policy_5: 0.05276
	accuracy_policy_5: 0.52387
	loss_value_5: 0.04171
	loss_reward_5: 0.00703
	loss_policy: 0.37056
	loss_value: 0.37064
	loss_reward: 0.02934
[2024-05-05 08:22:45] nn step 29950, lr: 0.015.
	loss_policy_0: 0.13779
	accuracy_policy_0: 0.72715
	loss_value_0: 0.17796
	loss_policy_1: 0.04028
	accuracy_policy_1: 0.62949
	loss_value_1: 0.03771
	loss_reward_1: 0.0056
	loss_policy_2: 0.04386
	accuracy_policy_2: 0.60324
	loss_value_2: 0.03913
	loss_reward_2: 0.00528
	loss_policy_3: 0.04745
	accuracy_policy_3: 0.57215
	loss_value_3: 0.04031
	loss_reward_3: 0.00574
	loss_policy_4: 0.05022
	accuracy_policy_4: 0.55527
	loss_value_4: 0.0414
	loss_reward_4: 0.00646
	loss_policy_5: 0.0527
	accuracy_policy_5: 0.53434
	loss_value_5: 0.04251
	loss_reward_5: 0.00718
	loss_policy: 0.37229
	loss_value: 0.37902
	loss_reward: 0.03025
[2024-05-05 08:23:03] nn step 30000, lr: 0.015.
	loss_policy_0: 0.12556
	accuracy_policy_0: 0.73965
	loss_value_0: 0.17213
	loss_policy_1: 0.03787
	accuracy_policy_1: 0.63297
	loss_value_1: 0.03641
	loss_reward_1: 0.00558
	loss_policy_2: 0.04171
	accuracy_policy_2: 0.59949
	loss_value_2: 0.03786
	loss_reward_2: 0.00527
	loss_policy_3: 0.04502
	accuracy_policy_3: 0.57746
	loss_value_3: 0.03891
	loss_reward_3: 0.0057
	loss_policy_4: 0.04749
	accuracy_policy_4: 0.56098
	loss_value_4: 0.03996
	loss_reward_4: 0.00607
	loss_policy_5: 0.05044
	accuracy_policy_5: 0.54062
	loss_value_5: 0.04102
	loss_reward_5: 0.0069
	loss_policy: 0.34808
	loss_value: 0.36629
	loss_reward: 0.02953
Optimization_Done 30000
[2024-05-05 08:24:58] [command] train weight_iter_30000.pkl 149 151
[2024-05-05 08:25:17] nn step 30050, lr: 0.015.
	loss_policy_0: 0.15464
	accuracy_policy_0: 0.68527
	loss_value_0: 0.17596
	loss_policy_1: 0.04149
	accuracy_policy_1: 0.60465
	loss_value_1: 0.03692
	loss_reward_1: 0.00523
	loss_policy_2: 0.04499
	accuracy_policy_2: 0.57422
	loss_value_2: 0.03839
	loss_reward_2: 0.005
	loss_policy_3: 0.04839
	accuracy_policy_3: 0.54836
	loss_value_3: 0.03985
	loss_reward_3: 0.00566
	loss_policy_4: 0.05134
	accuracy_policy_4: 0.52664
	loss_value_4: 0.04127
	loss_reward_4: 0.00601
	loss_policy_5: 0.05366
	accuracy_policy_5: 0.50695
	loss_value_5: 0.04262
	loss_reward_5: 0.00667
	loss_policy: 0.3945
	loss_value: 0.37501
	loss_reward: 0.02857
[2024-05-05 08:25:34] nn step 30100, lr: 0.015.
	loss_policy_0: 0.14131
	accuracy_policy_0: 0.71328
	loss_value_0: 0.17726
	loss_policy_1: 0.04015
	accuracy_policy_1: 0.62535
	loss_value_1: 0.03743
	loss_reward_1: 0.00515
	loss_policy_2: 0.04353
	accuracy_policy_2: 0.59855
	loss_value_2: 0.03879
	loss_reward_2: 0.00507
	loss_policy_3: 0.04696
	accuracy_policy_3: 0.56832
	loss_value_3: 0.04003
	loss_reward_3: 0.00558
	loss_policy_4: 0.05039
	accuracy_policy_4: 0.54457
	loss_value_4: 0.04131
	loss_reward_4: 0.00601
	loss_policy_5: 0.05331
	accuracy_policy_5: 0.52344
	loss_value_5: 0.04259
	loss_reward_5: 0.00679
	loss_policy: 0.37565
	loss_value: 0.37741
	loss_reward: 0.02859
[2024-05-05 08:25:52] nn step 30150, lr: 0.015.
	loss_policy_0: 0.13454
	accuracy_policy_0: 0.72836
	loss_value_0: 0.17739
	loss_policy_1: 0.03891
	accuracy_policy_1: 0.63113
	loss_value_1: 0.03747
	loss_reward_1: 0.00521
	loss_policy_2: 0.04243
	accuracy_policy_2: 0.60562
	loss_value_2: 0.03905
	loss_reward_2: 0.00514
	loss_policy_3: 0.04569
	accuracy_policy_3: 0.57969
	loss_value_3: 0.04029
	loss_reward_3: 0.0055
	loss_policy_4: 0.04926
	accuracy_policy_4: 0.55078
	loss_value_4: 0.04163
	loss_reward_4: 0.00583
	loss_policy_5: 0.05219
	accuracy_policy_5: 0.53168
	loss_value_5: 0.0427
	loss_reward_5: 0.00674
	loss_policy: 0.36302
	loss_value: 0.37854
	loss_reward: 0.02843
[2024-05-05 08:26:09] nn step 30200, lr: 0.015.
	loss_policy_0: 0.12459
	accuracy_policy_0: 0.73512
	loss_value_0: 0.16982
	loss_policy_1: 0.03624
	accuracy_policy_1: 0.63969
	loss_value_1: 0.03579
	loss_reward_1: 0.00512
	loss_policy_2: 0.0403
	accuracy_policy_2: 0.60922
	loss_value_2: 0.03712
	loss_reward_2: 0.00491
	loss_policy_3: 0.04372
	accuracy_policy_3: 0.58055
	loss_value_3: 0.03827
	loss_reward_3: 0.00541
	loss_policy_4: 0.04679
	accuracy_policy_4: 0.55676
	loss_value_4: 0.03958
	loss_reward_4: 0.00568
	loss_policy_5: 0.04933
	accuracy_policy_5: 0.53766
	loss_value_5: 0.04087
	loss_reward_5: 0.00656
	loss_policy: 0.34097
	loss_value: 0.36146
	loss_reward: 0.02768
Optimization_Done 30200
[2024-05-05 08:27:49] [command] train weight_iter_30200.pkl 150 152
[2024-05-05 08:28:07] nn step 30250, lr: 0.015.
	loss_policy_0: 0.1392
	accuracy_policy_0: 0.67504
	loss_value_0: 0.15522
	loss_policy_1: 0.03769
	accuracy_policy_1: 0.59102
	loss_value_1: 0.03291
	loss_reward_1: 0.00571
	loss_policy_2: 0.04144
	accuracy_policy_2: 0.55852
	loss_value_2: 0.03454
	loss_reward_2: 0.00528
	loss_policy_3: 0.04463
	accuracy_policy_3: 0.53773
	loss_value_3: 0.03573
	loss_reward_3: 0.00577
	loss_policy_4: 0.04713
	accuracy_policy_4: 0.51414
	loss_value_4: 0.03688
	loss_reward_4: 0.00623
	loss_policy_5: 0.04983
	accuracy_policy_5: 0.4927
	loss_value_5: 0.03822
	loss_reward_5: 0.00726
	loss_policy: 0.35992
	loss_value: 0.33351
	loss_reward: 0.03026
[2024-05-05 08:28:24] nn step 30300, lr: 0.015.
	loss_policy_0: 0.12389
	accuracy_policy_0: 0.71047
	loss_value_0: 0.15265
	loss_policy_1: 0.03591
	accuracy_policy_1: 0.61414
	loss_value_1: 0.03228
	loss_reward_1: 0.00535
	loss_policy_2: 0.03927
	accuracy_policy_2: 0.57969
	loss_value_2: 0.03358
	loss_reward_2: 0.005
	loss_policy_3: 0.04221
	accuracy_policy_3: 0.55738
	loss_value_3: 0.03493
	loss_reward_3: 0.00584
	loss_policy_4: 0.04479
	accuracy_policy_4: 0.53473
	loss_value_4: 0.0362
	loss_reward_4: 0.00609
	loss_policy_5: 0.0478
	accuracy_policy_5: 0.51145
	loss_value_5: 0.03739
	loss_reward_5: 0.00715
	loss_policy: 0.33387
	loss_value: 0.32702
	loss_reward: 0.02944
[2024-05-05 08:28:41] nn step 30350, lr: 0.015.
	loss_policy_0: 0.13142
	accuracy_policy_0: 0.71793
	loss_value_0: 0.16334
	loss_policy_1: 0.0373
	accuracy_policy_1: 0.6191
	loss_value_1: 0.0348
	loss_reward_1: 0.00589
	loss_policy_2: 0.04178
	accuracy_policy_2: 0.58645
	loss_value_2: 0.036
	loss_reward_2: 0.00542
	loss_policy_3: 0.0452
	accuracy_policy_3: 0.56035
	loss_value_3: 0.03725
	loss_reward_3: 0.00619
	loss_policy_4: 0.04808
	accuracy_policy_4: 0.54043
	loss_value_4: 0.03857
	loss_reward_4: 0.00663
	loss_policy_5: 0.05137
	accuracy_policy_5: 0.51129
	loss_value_5: 0.0399
	loss_reward_5: 0.00755
	loss_policy: 0.35515
	loss_value: 0.34985
	loss_reward: 0.03169
[2024-05-05 08:28:58] nn step 30400, lr: 0.015.
	loss_policy_0: 0.12284
	accuracy_policy_0: 0.72898
	loss_value_0: 0.15776
	loss_policy_1: 0.03641
	accuracy_policy_1: 0.62586
	loss_value_1: 0.03348
	loss_reward_1: 0.00578
	loss_policy_2: 0.03968
	accuracy_policy_2: 0.59387
	loss_value_2: 0.03492
	loss_reward_2: 0.00547
	loss_policy_3: 0.04292
	accuracy_policy_3: 0.57031
	loss_value_3: 0.03639
	loss_reward_3: 0.00589
	loss_policy_4: 0.04608
	accuracy_policy_4: 0.54289
	loss_value_4: 0.03767
	loss_reward_4: 0.00611
	loss_policy_5: 0.04969
	accuracy_policy_5: 0.51645
	loss_value_5: 0.03895
	loss_reward_5: 0.00747
	loss_policy: 0.33763
	loss_value: 0.33917
	loss_reward: 0.03071
Optimization_Done 30400
[2024-05-05 08:30:59] [command] train weight_iter_30400.pkl 151 153
[2024-05-05 08:31:17] nn step 30450, lr: 0.015.
	loss_policy_0: 0.11242
	accuracy_policy_0: 0.70543
	loss_value_0: 0.12994
	loss_policy_1: 0.03271
	accuracy_policy_1: 0.60625
	loss_value_1: 0.02768
	loss_reward_1: 0.00489
	loss_policy_2: 0.03506
	accuracy_policy_2: 0.58379
	loss_value_2: 0.02879
	loss_reward_2: 0.00438
	loss_policy_3: 0.03766
	accuracy_policy_3: 0.55016
	loss_value_3: 0.03013
	loss_reward_3: 0.00481
	loss_policy_4: 0.03995
	accuracy_policy_4: 0.53113
	loss_value_4: 0.03137
	loss_reward_4: 0.00521
	loss_policy_5: 0.04238
	accuracy_policy_5: 0.50879
	loss_value_5: 0.03243
	loss_reward_5: 0.00634
	loss_policy: 0.30017
	loss_value: 0.28034
	loss_reward: 0.02563
[2024-05-05 08:31:34] nn step 30500, lr: 0.015.
	loss_policy_0: 0.10485
	accuracy_policy_0: 0.73242
	loss_value_0: 0.13233
	loss_policy_1: 0.03209
	accuracy_policy_1: 0.61781
	loss_value_1: 0.02833
	loss_reward_1: 0.005
	loss_policy_2: 0.03495
	accuracy_policy_2: 0.59508
	loss_value_2: 0.02961
	loss_reward_2: 0.00453
	loss_policy_3: 0.03798
	accuracy_policy_3: 0.5707
	loss_value_3: 0.03094
	loss_reward_3: 0.00518
	loss_policy_4: 0.04046
	accuracy_policy_4: 0.54449
	loss_value_4: 0.03199
	loss_reward_4: 0.00544
	loss_policy_5: 0.04247
	accuracy_policy_5: 0.5257
	loss_value_5: 0.03329
	loss_reward_5: 0.00666
	loss_policy: 0.29279
	loss_value: 0.28649
	loss_reward: 0.02682
[2024-05-05 08:31:52] nn step 30550, lr: 0.015.
	loss_policy_0: 0.09686
	accuracy_policy_0: 0.74266
	loss_value_0: 0.12639
	loss_policy_1: 0.03005
	accuracy_policy_1: 0.62949
	loss_value_1: 0.02699
	loss_reward_1: 0.00487
	loss_policy_2: 0.03292
	accuracy_policy_2: 0.59785
	loss_value_2: 0.02829
	loss_reward_2: 0.00446
	loss_policy_3: 0.03551
	accuracy_policy_3: 0.57422
	loss_value_3: 0.02939
	loss_reward_3: 0.00482
	loss_policy_4: 0.03766
	accuracy_policy_4: 0.5557
	loss_value_4: 0.03044
	loss_reward_4: 0.00515
	loss_policy_5: 0.03995
	accuracy_policy_5: 0.53195
	loss_value_5: 0.03172
	loss_reward_5: 0.00636
	loss_policy: 0.27296
	loss_value: 0.27322
	loss_reward: 0.02568
[2024-05-05 08:32:09] nn step 30600, lr: 0.015.
	loss_policy_0: 0.09629
	accuracy_policy_0: 0.75312
	loss_value_0: 0.13052
	loss_policy_1: 0.03046
	accuracy_policy_1: 0.63785
	loss_value_1: 0.02753
	loss_reward_1: 0.00493
	loss_policy_2: 0.03356
	accuracy_policy_2: 0.60656
	loss_value_2: 0.02883
	loss_reward_2: 0.00455
	loss_policy_3: 0.0365
	accuracy_policy_3: 0.5809
	loss_value_3: 0.03008
	loss_reward_3: 0.00517
	loss_policy_4: 0.03881
	accuracy_policy_4: 0.55367
	loss_value_4: 0.03127
	loss_reward_4: 0.00534
	loss_policy_5: 0.04113
	accuracy_policy_5: 0.5343
	loss_value_5: 0.03255
	loss_reward_5: 0.00677
	loss_policy: 0.27675
	loss_value: 0.2808
	loss_reward: 0.02677
Optimization_Done 30600
[2024-05-05 08:34:12] [command] train weight_iter_30600.pkl 152 154
[2024-05-05 08:34:31] nn step 30650, lr: 0.015.
	loss_policy_0: 0.13064
	accuracy_policy_0: 0.67531
	loss_value_0: 0.13422
	loss_policy_1: 0.03447
	accuracy_policy_1: 0.59297
	loss_value_1: 0.0282
	loss_reward_1: 0.00492
	loss_policy_2: 0.03665
	accuracy_policy_2: 0.57512
	loss_value_2: 0.02959
	loss_reward_2: 0.00468
	loss_policy_3: 0.03916
	accuracy_policy_3: 0.55051
	loss_value_3: 0.03093
	loss_reward_3: 0.00503
	loss_policy_4: 0.04125
	accuracy_policy_4: 0.53762
	loss_value_4: 0.03195
	loss_reward_4: 0.00535
	loss_policy_5: 0.04369
	accuracy_policy_5: 0.51
	loss_value_5: 0.03301
	loss_reward_5: 0.00644
	loss_policy: 0.32585
	loss_value: 0.2879
	loss_reward: 0.02642
[2024-05-05 08:34:48] nn step 30700, lr: 0.015.
	loss_policy_0: 0.11545
	accuracy_policy_0: 0.72
	loss_value_0: 0.14179
	loss_policy_1: 0.03408
	accuracy_policy_1: 0.62309
	loss_value_1: 0.03022
	loss_reward_1: 0.00535
	loss_policy_2: 0.03683
	accuracy_policy_2: 0.60074
	loss_value_2: 0.03179
	loss_reward_2: 0.00503
	loss_policy_3: 0.03971
	accuracy_policy_3: 0.57223
	loss_value_3: 0.03315
	loss_reward_3: 0.00538
	loss_policy_4: 0.04233
	accuracy_policy_4: 0.55734
	loss_value_4: 0.03426
	loss_reward_4: 0.00567
	loss_policy_5: 0.04464
	accuracy_policy_5: 0.53559
	loss_value_5: 0.03539
	loss_reward_5: 0.00699
	loss_policy: 0.31304
	loss_value: 0.30659
	loss_reward: 0.02842
[2024-05-05 08:35:05] nn step 30750, lr: 0.015.
	loss_policy_0: 0.1015
	accuracy_policy_0: 0.73031
	loss_value_0: 0.13041
	loss_policy_1: 0.0304
	accuracy_policy_1: 0.63137
	loss_value_1: 0.02778
	loss_reward_1: 0.00498
	loss_policy_2: 0.03295
	accuracy_policy_2: 0.61051
	loss_value_2: 0.02901
	loss_reward_2: 0.00452
	loss_policy_3: 0.03545
	accuracy_policy_3: 0.58363
	loss_value_3: 0.03011
	loss_reward_3: 0.00482
	loss_policy_4: 0.03838
	accuracy_policy_4: 0.55848
	loss_value_4: 0.03112
	loss_reward_4: 0.00525
	loss_policy_5: 0.04065
	accuracy_policy_5: 0.53836
	loss_value_5: 0.03213
	loss_reward_5: 0.00646
	loss_policy: 0.27932
	loss_value: 0.28056
	loss_reward: 0.02603
[2024-05-05 08:35:23] nn step 30800, lr: 0.015.
	loss_policy_0: 0.10779
	accuracy_policy_0: 0.73773
	loss_value_0: 0.14399
	loss_policy_1: 0.03342
	accuracy_policy_1: 0.63363
	loss_value_1: 0.03036
	loss_reward_1: 0.00546
	loss_policy_2: 0.03663
	accuracy_policy_2: 0.60719
	loss_value_2: 0.03156
	loss_reward_2: 0.00515
	loss_policy_3: 0.03941
	accuracy_policy_3: 0.5848
	loss_value_3: 0.03281
	loss_reward_3: 0.00535
	loss_policy_4: 0.04185
	accuracy_policy_4: 0.56281
	loss_value_4: 0.03417
	loss_reward_4: 0.00578
	loss_policy_5: 0.04421
	accuracy_policy_5: 0.54355
	loss_value_5: 0.03536
	loss_reward_5: 0.00733
	loss_policy: 0.30331
	loss_value: 0.30825
	loss_reward: 0.02907
Optimization_Done 30800
[2024-05-05 08:37:26] [command] train weight_iter_30800.pkl 153 155
[2024-05-05 08:37:45] nn step 30850, lr: 0.015.
	loss_policy_0: 0.11972
	accuracy_policy_0: 0.70785
	loss_value_0: 0.13068
	loss_policy_1: 0.03195
	accuracy_policy_1: 0.63023
	loss_value_1: 0.02786
	loss_reward_1: 0.00502
	loss_policy_2: 0.03436
	accuracy_policy_2: 0.61043
	loss_value_2: 0.02924
	loss_reward_2: 0.00502
	loss_policy_3: 0.03643
	accuracy_policy_3: 0.59379
	loss_value_3: 0.03036
	loss_reward_3: 0.00527
	loss_policy_4: 0.03904
	accuracy_policy_4: 0.57047
	loss_value_4: 0.03156
	loss_reward_4: 0.00565
	loss_policy_5: 0.04069
	accuracy_policy_5: 0.56344
	loss_value_5: 0.03255
	loss_reward_5: 0.00625
	loss_policy: 0.30219
	loss_value: 0.28225
	loss_reward: 0.02721
[2024-05-05 08:38:02] nn step 30900, lr: 0.015.
	loss_policy_0: 0.10173
	accuracy_policy_0: 0.75223
	loss_value_0: 0.13375
	loss_policy_1: 0.02986
	accuracy_policy_1: 0.66277
	loss_value_1: 0.02861
	loss_reward_1: 0.00504
	loss_policy_2: 0.03277
	accuracy_policy_2: 0.63992
	loss_value_2: 0.03008
	loss_reward_2: 0.00494
	loss_policy_3: 0.03516
	accuracy_policy_3: 0.62059
	loss_value_3: 0.03134
	loss_reward_3: 0.00524
	loss_policy_4: 0.03768
	accuracy_policy_4: 0.60102
	loss_value_4: 0.03246
	loss_reward_4: 0.00562
	loss_policy_5: 0.04009
	accuracy_policy_5: 0.58129
	loss_value_5: 0.03362
	loss_reward_5: 0.00642
	loss_policy: 0.27729
	loss_value: 0.28986
	loss_reward: 0.02726
[2024-05-05 08:38:20] nn step 30950, lr: 0.015.
	loss_policy_0: 0.1014
	accuracy_policy_0: 0.75879
	loss_value_0: 0.13837
	loss_policy_1: 0.03016
	accuracy_policy_1: 0.66992
	loss_value_1: 0.02935
	loss_reward_1: 0.00513
	loss_policy_2: 0.0336
	accuracy_policy_2: 0.64387
	loss_value_2: 0.03069
	loss_reward_2: 0.0051
	loss_policy_3: 0.03599
	accuracy_policy_3: 0.62246
	loss_value_3: 0.03188
	loss_reward_3: 0.00525
	loss_policy_4: 0.03791
	accuracy_policy_4: 0.61102
	loss_value_4: 0.03304
	loss_reward_4: 0.00583
	loss_policy_5: 0.0403
	accuracy_policy_5: 0.59348
	loss_value_5: 0.03406
	loss_reward_5: 0.00681
	loss_policy: 0.27936
	loss_value: 0.29738
	loss_reward: 0.02813
[2024-05-05 08:38:37] nn step 31000, lr: 0.015.
	loss_policy_0: 0.09539
	accuracy_policy_0: 0.76328
	loss_value_0: 0.13692
	loss_policy_1: 0.02922
	accuracy_policy_1: 0.67023
	loss_value_1: 0.02935
	loss_reward_1: 0.00522
	loss_policy_2: 0.03218
	accuracy_policy_2: 0.6484
	loss_value_2: 0.03073
	loss_reward_2: 0.00477
	loss_policy_3: 0.03485
	accuracy_policy_3: 0.62512
	loss_value_3: 0.03178
	loss_reward_3: 0.00508
	loss_policy_4: 0.03688
	accuracy_policy_4: 0.60656
	loss_value_4: 0.033
	loss_reward_4: 0.00571
	loss_policy_5: 0.03942
	accuracy_policy_5: 0.5932
	loss_value_5: 0.03411
	loss_reward_5: 0.00649
	loss_policy: 0.26794
	loss_value: 0.29588
	loss_reward: 0.02728
Optimization_Done 31000
[2024-05-05 08:40:43] [command] train weight_iter_31000.pkl 154 156
[2024-05-05 08:41:02] nn step 31050, lr: 0.015.
	loss_policy_0: 0.14775
	accuracy_policy_0: 0.72215
	loss_value_0: 0.17003
	loss_policy_1: 0.04127
	accuracy_policy_1: 0.63562
	loss_value_1: 0.03612
	loss_reward_1: 0.00738
	loss_policy_2: 0.0444
	accuracy_policy_2: 0.6143
	loss_value_2: 0.03786
	loss_reward_2: 0.00719
	loss_policy_3: 0.04767
	accuracy_policy_3: 0.59
	loss_value_3: 0.03934
	loss_reward_3: 0.00766
	loss_policy_4: 0.05044
	accuracy_policy_4: 0.57797
	loss_value_4: 0.0407
	loss_reward_4: 0.00828
	loss_policy_5: 0.05338
	accuracy_policy_5: 0.56613
	loss_value_5: 0.04234
	loss_reward_5: 0.00953
	loss_policy: 0.38491
	loss_value: 0.36639
	loss_reward: 0.04003
[2024-05-05 08:41:20] nn step 31100, lr: 0.015.
	loss_policy_0: 0.12156
	accuracy_policy_0: 0.75637
	loss_value_0: 0.16038
	loss_policy_1: 0.03648
	accuracy_policy_1: 0.66105
	loss_value_1: 0.03416
	loss_reward_1: 0.00706
	loss_policy_2: 0.04047
	accuracy_policy_2: 0.63785
	loss_value_2: 0.03579
	loss_reward_2: 0.00659
	loss_policy_3: 0.04357
	accuracy_policy_3: 0.61312
	loss_value_3: 0.03726
	loss_reward_3: 0.00717
	loss_policy_4: 0.0462
	accuracy_policy_4: 0.59734
	loss_value_4: 0.03885
	loss_reward_4: 0.00794
	loss_policy_5: 0.04928
	accuracy_policy_5: 0.5759
	loss_value_5: 0.04023
	loss_reward_5: 0.00935
	loss_policy: 0.33755
	loss_value: 0.34666
	loss_reward: 0.03812
[2024-05-05 08:41:38] nn step 31150, lr: 0.015.
	loss_policy_0: 0.11194
	accuracy_policy_0: 0.76406
	loss_value_0: 0.15416
	loss_policy_1: 0.03468
	accuracy_policy_1: 0.67047
	loss_value_1: 0.03294
	loss_reward_1: 0.00696
	loss_policy_2: 0.03853
	accuracy_policy_2: 0.64125
	loss_value_2: 0.0346
	loss_reward_2: 0.00638
	loss_policy_3: 0.04169
	accuracy_policy_3: 0.61914
	loss_value_3: 0.03595
	loss_reward_3: 0.00701
	loss_policy_4: 0.04466
	accuracy_policy_4: 0.60402
	loss_value_4: 0.03737
	loss_reward_4: 0.00743
	loss_policy_5: 0.04721
	accuracy_policy_5: 0.58473
	loss_value_5: 0.03877
	loss_reward_5: 0.00901
	loss_policy: 0.31871
	loss_value: 0.33379
	loss_reward: 0.03679
[2024-05-05 08:41:56] nn step 31200, lr: 0.015.
	loss_policy_0: 0.10842
	accuracy_policy_0: 0.77398
	loss_value_0: 0.15475
	loss_policy_1: 0.03443
	accuracy_policy_1: 0.66727
	loss_value_1: 0.03273
	loss_reward_1: 0.00684
	loss_policy_2: 0.03792
	accuracy_policy_2: 0.64262
	loss_value_2: 0.0342
	loss_reward_2: 0.0065
	loss_policy_3: 0.04132
	accuracy_policy_3: 0.61957
	loss_value_3: 0.03586
	loss_reward_3: 0.00691
	loss_policy_4: 0.04393
	accuracy_policy_4: 0.60633
	loss_value_4: 0.03723
	loss_reward_4: 0.00741
	loss_policy_5: 0.04655
	accuracy_policy_5: 0.58957
	loss_value_5: 0.03868
	loss_reward_5: 0.00884
	loss_policy: 0.31258
	loss_value: 0.33345
	loss_reward: 0.0365
Optimization_Done 31200
[2024-05-05 08:44:00] [command] train weight_iter_31200.pkl 155 157
[2024-05-05 08:44:19] nn step 31250, lr: 0.015.
	loss_policy_0: 0.13524
	accuracy_policy_0: 0.69559
	loss_value_0: 0.14775
	loss_policy_1: 0.03741
	accuracy_policy_1: 0.6043
	loss_value_1: 0.03119
	loss_reward_1: 0.00657
	loss_policy_2: 0.0407
	accuracy_policy_2: 0.58309
	loss_value_2: 0.03268
	loss_reward_2: 0.00601
	loss_policy_3: 0.04354
	accuracy_policy_3: 0.5548
	loss_value_3: 0.0341
	loss_reward_3: 0.00654
	loss_policy_4: 0.04538
	accuracy_policy_4: 0.53988
	loss_value_4: 0.03537
	loss_reward_4: 0.00712
	loss_policy_5: 0.04813
	accuracy_policy_5: 0.51812
	loss_value_5: 0.03662
	loss_reward_5: 0.00814
	loss_policy: 0.3504
	loss_value: 0.3177
	loss_reward: 0.03436
[2024-05-05 08:44:37] nn step 31300, lr: 0.015.
	loss_policy_0: 0.12263
	accuracy_policy_0: 0.7382
	loss_value_0: 0.15494
	loss_policy_1: 0.03759
	accuracy_policy_1: 0.63387
	loss_value_1: 0.03288
	loss_reward_1: 0.00682
	loss_policy_2: 0.04136
	accuracy_policy_2: 0.6057
	loss_value_2: 0.03444
	loss_reward_2: 0.00636
	loss_policy_3: 0.04463
	accuracy_policy_3: 0.58234
	loss_value_3: 0.03588
	loss_reward_3: 0.00689
	loss_policy_4: 0.04709
	accuracy_policy_4: 0.56297
	loss_value_4: 0.03711
	loss_reward_4: 0.00761
	loss_policy_5: 0.04899
	accuracy_policy_5: 0.55352
	loss_value_5: 0.03847
	loss_reward_5: 0.00867
	loss_policy: 0.34229
	loss_value: 0.33373
	loss_reward: 0.03635
[2024-05-05 08:44:55] nn step 31350, lr: 0.015.
	loss_policy_0: 0.11705
	accuracy_policy_0: 0.75023
	loss_value_0: 0.15489
	loss_policy_1: 0.03629
	accuracy_policy_1: 0.6418
	loss_value_1: 0.03268
	loss_reward_1: 0.00683
	loss_policy_2: 0.03987
	accuracy_policy_2: 0.62316
	loss_value_2: 0.03381
	loss_reward_2: 0.00635
	loss_policy_3: 0.04342
	accuracy_policy_3: 0.59613
	loss_value_3: 0.03535
	loss_reward_3: 0.00719
	loss_policy_4: 0.04609
	accuracy_policy_4: 0.58238
	loss_value_4: 0.03677
	loss_reward_4: 0.00737
	loss_policy_5: 0.04807
	accuracy_policy_5: 0.56461
	loss_value_5: 0.03813
	loss_reward_5: 0.00873
	loss_policy: 0.33079
	loss_value: 0.33162
	loss_reward: 0.03647
[2024-05-05 08:45:12] nn step 31400, lr: 0.015.
	loss_policy_0: 0.1179
	accuracy_policy_0: 0.75762
	loss_value_0: 0.158
	loss_policy_1: 0.03636
	accuracy_policy_1: 0.64926
	loss_value_1: 0.03327
	loss_reward_1: 0.00679
	loss_policy_2: 0.0407
	accuracy_policy_2: 0.61898
	loss_value_2: 0.03489
	loss_reward_2: 0.00651
	loss_policy_3: 0.04357
	accuracy_policy_3: 0.60246
	loss_value_3: 0.03615
	loss_reward_3: 0.00706
	loss_policy_4: 0.04654
	accuracy_policy_4: 0.58152
	loss_value_4: 0.03751
	loss_reward_4: 0.00759
	loss_policy_5: 0.04878
	accuracy_policy_5: 0.56895
	loss_value_5: 0.03882
	loss_reward_5: 0.009
	loss_policy: 0.33385
	loss_value: 0.33865
	loss_reward: 0.03695
Optimization_Done 31400
[2024-05-05 08:46:57] [command] train weight_iter_31400.pkl 156 158
[2024-05-05 08:47:16] nn step 31450, lr: 0.015.
	loss_policy_0: 0.12342
	accuracy_policy_0: 0.72973
	loss_value_0: 0.1452
	loss_policy_1: 0.03396
	accuracy_policy_1: 0.64961
	loss_value_1: 0.03056
	loss_reward_1: 0.00557
	loss_policy_2: 0.03633
	accuracy_policy_2: 0.62488
	loss_value_2: 0.03166
	loss_reward_2: 0.00524
	loss_policy_3: 0.03893
	accuracy_policy_3: 0.60305
	loss_value_3: 0.03276
	loss_reward_3: 0.00553
	loss_policy_4: 0.04081
	accuracy_policy_4: 0.58535
	loss_value_4: 0.0338
	loss_reward_4: 0.00597
	loss_policy_5: 0.04273
	accuracy_policy_5: 0.57121
	loss_value_5: 0.03483
	loss_reward_5: 0.00703
	loss_policy: 0.31619
	loss_value: 0.30881
	loss_reward: 0.02934
[2024-05-05 08:47:33] nn step 31500, lr: 0.015.
	loss_policy_0: 0.11209
	accuracy_policy_0: 0.76254
	loss_value_0: 0.15729
	loss_policy_1: 0.03422
	accuracy_policy_1: 0.66688
	loss_value_1: 0.0332
	loss_reward_1: 0.00586
	loss_policy_2: 0.03763
	accuracy_policy_2: 0.63938
	loss_value_2: 0.03457
	loss_reward_2: 0.00562
	loss_policy_3: 0.04024
	accuracy_policy_3: 0.61984
	loss_value_3: 0.03567
	loss_reward_3: 0.00626
	loss_policy_4: 0.0424
	accuracy_policy_4: 0.60488
	loss_value_4: 0.03688
	loss_reward_4: 0.00642
	loss_policy_5: 0.04495
	accuracy_policy_5: 0.58863
	loss_value_5: 0.03791
	loss_reward_5: 0.00781
	loss_policy: 0.31152
	loss_value: 0.33553
	loss_reward: 0.03197
[2024-05-05 08:47:51] nn step 31550, lr: 0.015.
	loss_policy_0: 0.10172
	accuracy_policy_0: 0.76531
	loss_value_0: 0.14482
	loss_policy_1: 0.03097
	accuracy_policy_1: 0.67523
	loss_value_1: 0.03057
	loss_reward_1: 0.00577
	loss_policy_2: 0.03431
	accuracy_policy_2: 0.64668
	loss_value_2: 0.03185
	loss_reward_2: 0.00522
	loss_policy_3: 0.03668
	accuracy_policy_3: 0.62242
	loss_value_3: 0.03295
	loss_reward_3: 0.00559
	loss_policy_4: 0.03915
	accuracy_policy_4: 0.60605
	loss_value_4: 0.03395
	loss_reward_4: 0.00619
	loss_policy_5: 0.04097
	accuracy_policy_5: 0.5877
	loss_value_5: 0.03506
	loss_reward_5: 0.00707
	loss_policy: 0.28379
	loss_value: 0.3092
	loss_reward: 0.02984
[2024-05-05 08:48:08] nn step 31600, lr: 0.015.
	loss_policy_0: 0.1017
	accuracy_policy_0: 0.7718
	loss_value_0: 0.15045
	loss_policy_1: 0.03185
	accuracy_policy_1: 0.67305
	loss_value_1: 0.03165
	loss_reward_1: 0.00569
	loss_policy_2: 0.03516
	accuracy_policy_2: 0.64629
	loss_value_2: 0.03295
	loss_reward_2: 0.00555
	loss_policy_3: 0.03793
	accuracy_policy_3: 0.62031
	loss_value_3: 0.03407
	loss_reward_3: 0.00607
	loss_policy_4: 0.03976
	accuracy_policy_4: 0.61312
	loss_value_4: 0.03533
	loss_reward_4: 0.00651
	loss_policy_5: 0.04248
	accuracy_policy_5: 0.58961
	loss_value_5: 0.0365
	loss_reward_5: 0.00749
	loss_policy: 0.28889
	loss_value: 0.32095
	loss_reward: 0.03131
Optimization_Done 31600
[2024-05-05 08:50:17] [command] train weight_iter_31600.pkl 157 159
[2024-05-05 08:50:36] nn step 31650, lr: 0.015.
	loss_policy_0: 0.13473
	accuracy_policy_0: 0.73859
	loss_value_0: 0.18551
	loss_policy_1: 0.03728
	accuracy_policy_1: 0.65988
	loss_value_1: 0.03917
	loss_reward_1: 0.00608
	loss_policy_2: 0.04014
	accuracy_policy_2: 0.6368
	loss_value_2: 0.04083
	loss_reward_2: 0.00567
	loss_policy_3: 0.04334
	accuracy_policy_3: 0.61008
	loss_value_3: 0.04221
	loss_reward_3: 0.00616
	loss_policy_4: 0.04551
	accuracy_policy_4: 0.59254
	loss_value_4: 0.04349
	loss_reward_4: 0.00678
	loss_policy_5: 0.04777
	accuracy_policy_5: 0.57699
	loss_value_5: 0.0446
	loss_reward_5: 0.00748
	loss_policy: 0.34877
	loss_value: 0.39582
	loss_reward: 0.03218
[2024-05-05 08:50:53] nn step 31700, lr: 0.015.
	loss_policy_0: 0.11104
	accuracy_policy_0: 0.76711
	loss_value_0: 0.16943
	loss_policy_1: 0.03278
	accuracy_policy_1: 0.67645
	loss_value_1: 0.03594
	loss_reward_1: 0.0056
	loss_policy_2: 0.03561
	accuracy_policy_2: 0.65824
	loss_value_2: 0.03739
	loss_reward_2: 0.00536
	loss_policy_3: 0.03896
	accuracy_policy_3: 0.62957
	loss_value_3: 0.03876
	loss_reward_3: 0.00596
	loss_policy_4: 0.04113
	accuracy_policy_4: 0.61156
	loss_value_4: 0.03968
	loss_reward_4: 0.00633
	loss_policy_5: 0.04251
	accuracy_policy_5: 0.60246
	loss_value_5: 0.04085
	loss_reward_5: 0.00703
	loss_policy: 0.30203
	loss_value: 0.36205
	loss_reward: 0.03029
[2024-05-05 08:51:11] nn step 31750, lr: 0.015.
	loss_policy_0: 0.0992
	accuracy_policy_0: 0.77586
	loss_value_0: 0.15805
	loss_policy_1: 0.03006
	accuracy_policy_1: 0.6891
	loss_value_1: 0.03349
	loss_reward_1: 0.00528
	loss_policy_2: 0.03333
	accuracy_policy_2: 0.66086
	loss_value_2: 0.03476
	loss_reward_2: 0.00505
	loss_policy_3: 0.03587
	accuracy_policy_3: 0.64184
	loss_value_3: 0.03613
	loss_reward_3: 0.00556
	loss_policy_4: 0.03854
	accuracy_policy_4: 0.61883
	loss_value_4: 0.03726
	loss_reward_4: 0.00585
	loss_policy_5: 0.03979
	accuracy_policy_5: 0.60855
	loss_value_5: 0.03831
	loss_reward_5: 0.00659
	loss_policy: 0.27679
	loss_value: 0.338
	loss_reward: 0.02832
[2024-05-05 08:51:28] nn step 31800, lr: 0.015.
	loss_policy_0: 0.09631
	accuracy_policy_0: 0.78336
	loss_value_0: 0.15812
	loss_policy_1: 0.03005
	accuracy_policy_1: 0.68609
	loss_value_1: 0.03346
	loss_reward_1: 0.00547
	loss_policy_2: 0.03277
	accuracy_policy_2: 0.66559
	loss_value_2: 0.03484
	loss_reward_2: 0.00483
	loss_policy_3: 0.03584
	accuracy_policy_3: 0.64262
	loss_value_3: 0.03599
	loss_reward_3: 0.00546
	loss_policy_4: 0.03792
	accuracy_policy_4: 0.62187
	loss_value_4: 0.03692
	loss_reward_4: 0.00583
	loss_policy_5: 0.03958
	accuracy_policy_5: 0.61082
	loss_value_5: 0.03793
	loss_reward_5: 0.00669
	loss_policy: 0.27248
	loss_value: 0.33727
	loss_reward: 0.02828
Optimization_Done 31800
[2024-05-05 08:53:34] [command] train weight_iter_31800.pkl 158 160
[2024-05-05 08:53:53] nn step 31850, lr: 0.015.
	loss_policy_0: 0.11371
	accuracy_policy_0: 0.77219
	loss_value_0: 0.15857
	loss_policy_1: 0.03252
	accuracy_policy_1: 0.69734
	loss_value_1: 0.03377
	loss_reward_1: 0.00624
	loss_policy_2: 0.0361
	accuracy_policy_2: 0.66922
	loss_value_2: 0.03535
	loss_reward_2: 0.00643
	loss_policy_3: 0.03951
	accuracy_policy_3: 0.64582
	loss_value_3: 0.03675
	loss_reward_3: 0.00659
	loss_policy_4: 0.04205
	accuracy_policy_4: 0.6282
	loss_value_4: 0.0381
	loss_reward_4: 0.00723
	loss_policy_5: 0.04345
	accuracy_policy_5: 0.62871
	loss_value_5: 0.03943
	loss_reward_5: 0.00844
	loss_policy: 0.30734
	loss_value: 0.34198
	loss_reward: 0.03494
[2024-05-05 08:54:10] nn step 31900, lr: 0.015.
	loss_policy_0: 0.09693
	accuracy_policy_0: 0.80074
	loss_value_0: 0.15284
	loss_policy_1: 0.03049
	accuracy_policy_1: 0.71426
	loss_value_1: 0.03266
	loss_reward_1: 0.0061
	loss_policy_2: 0.03429
	accuracy_policy_2: 0.68477
	loss_value_2: 0.03415
	loss_reward_2: 0.00597
	loss_policy_3: 0.037
	accuracy_policy_3: 0.66281
	loss_value_3: 0.03568
	loss_reward_3: 0.00644
	loss_policy_4: 0.03934
	accuracy_policy_4: 0.64711
	loss_value_4: 0.03698
	loss_reward_4: 0.00699
	loss_policy_5: 0.04099
	accuracy_policy_5: 0.64422
	loss_value_5: 0.03833
	loss_reward_5: 0.00833
	loss_policy: 0.27904
	loss_value: 0.33065
	loss_reward: 0.03383
[2024-05-05 08:54:28] nn step 31950, lr: 0.015.
	loss_policy_0: 0.09533
	accuracy_policy_0: 0.80789
	loss_value_0: 0.15698
	loss_policy_1: 0.03048
	accuracy_policy_1: 0.71938
	loss_value_1: 0.03342
	loss_reward_1: 0.00648
	loss_policy_2: 0.03435
	accuracy_policy_2: 0.69199
	loss_value_2: 0.03496
	loss_reward_2: 0.00613
	loss_policy_3: 0.03737
	accuracy_policy_3: 0.6723
	loss_value_3: 0.03647
	loss_reward_3: 0.00691
	loss_policy_4: 0.03955
	accuracy_policy_4: 0.65742
	loss_value_4: 0.03792
	loss_reward_4: 0.00735
	loss_policy_5: 0.0416
	accuracy_policy_5: 0.64195
	loss_value_5: 0.03937
	loss_reward_5: 0.00847
	loss_policy: 0.27868
	loss_value: 0.33911
	loss_reward: 0.03533
[2024-05-05 08:54:46] nn step 32000, lr: 0.015.
	loss_policy_0: 0.09599
	accuracy_policy_0: 0.81512
	loss_value_0: 0.16381
	loss_policy_1: 0.03124
	accuracy_policy_1: 0.72562
	loss_value_1: 0.03485
	loss_reward_1: 0.00664
	loss_policy_2: 0.03513
	accuracy_policy_2: 0.69699
	loss_value_2: 0.03675
	loss_reward_2: 0.00624
	loss_policy_3: 0.03817
	accuracy_policy_3: 0.68066
	loss_value_3: 0.03818
	loss_reward_3: 0.00692
	loss_policy_4: 0.0411
	accuracy_policy_4: 0.6573
	loss_value_4: 0.03959
	loss_reward_4: 0.0074
	loss_policy_5: 0.04316
	accuracy_policy_5: 0.6493
	loss_value_5: 0.04079
	loss_reward_5: 0.00894
	loss_policy: 0.28478
	loss_value: 0.35396
	loss_reward: 0.03613
Optimization_Done 32000
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-05 09:40:54] [command] train weight_iter_32000.pkl 159 161
[2024-05-05 09:41:21] nn step 32050, lr: 0.015.
	loss_policy_0: 0.12559
	accuracy_policy_0: 0.77301
	loss_value_0: 0.18079
	loss_policy_1: 0.03686
	accuracy_policy_1: 0.685
	loss_value_1: 0.03819
	loss_reward_1: 0.00666
	loss_policy_2: 0.04068
	accuracy_policy_2: 0.65582
	loss_value_2: 0.04009
	loss_reward_2: 0.00656
	loss_policy_3: 0.04393
	accuracy_policy_3: 0.64023
	loss_value_3: 0.04158
	loss_reward_3: 0.00698
	loss_policy_4: 0.04659
	accuracy_policy_4: 0.6225
	loss_value_4: 0.043
	loss_reward_4: 0.00766
	loss_policy_5: 0.04923
	accuracy_policy_5: 0.61062
	loss_value_5: 0.04435
	loss_reward_5: 0.00855
	loss_policy: 0.34289
	loss_value: 0.388
	loss_reward: 0.03641
[2024-05-05 09:41:38] nn step 32100, lr: 0.015.
	loss_policy_0: 0.11261
	accuracy_policy_0: 0.78781
	loss_value_0: 0.16846
	loss_policy_1: 0.03373
	accuracy_policy_1: 0.70605
	loss_value_1: 0.03587
	loss_reward_1: 0.00643
	loss_policy_2: 0.03759
	accuracy_policy_2: 0.67941
	loss_value_2: 0.03762
	loss_reward_2: 0.00638
	loss_policy_3: 0.04108
	accuracy_policy_3: 0.65832
	loss_value_3: 0.03916
	loss_reward_3: 0.00684
	loss_policy_4: 0.04344
	accuracy_policy_4: 0.64445
	loss_value_4: 0.04066
	loss_reward_4: 0.0074
	loss_policy_5: 0.0462
	accuracy_policy_5: 0.63395
	loss_value_5: 0.04191
	loss_reward_5: 0.00832
	loss_policy: 0.31465
	loss_value: 0.36367
	loss_reward: 0.03538
[2024-05-05 09:41:55] nn step 32150, lr: 0.015.
	loss_policy_0: 0.10081
	accuracy_policy_0: 0.79719
	loss_value_0: 0.15789
	loss_policy_1: 0.03157
	accuracy_policy_1: 0.71105
	loss_value_1: 0.03358
	loss_reward_1: 0.00614
	loss_policy_2: 0.03578
	accuracy_policy_2: 0.6809
	loss_value_2: 0.03515
	loss_reward_2: 0.00605
	loss_policy_3: 0.03885
	accuracy_policy_3: 0.65961
	loss_value_3: 0.03657
	loss_reward_3: 0.00649
	loss_policy_4: 0.04159
	accuracy_policy_4: 0.64172
	loss_value_4: 0.03774
	loss_reward_4: 0.0071
	loss_policy_5: 0.04421
	accuracy_policy_5: 0.62941
	loss_value_5: 0.03936
	loss_reward_5: 0.00806
	loss_policy: 0.2928
	loss_value: 0.34029
	loss_reward: 0.03384
[2024-05-05 09:42:12] nn step 32200, lr: 0.015.
	loss_policy_0: 0.10273
	accuracy_policy_0: 0.80629
	loss_value_0: 0.16665
	loss_policy_1: 0.03283
	accuracy_policy_1: 0.71793
	loss_value_1: 0.03564
	loss_reward_1: 0.00661
	loss_policy_2: 0.03704
	accuracy_policy_2: 0.69047
	loss_value_2: 0.03742
	loss_reward_2: 0.00631
	loss_policy_3: 0.04123
	accuracy_policy_3: 0.66086
	loss_value_3: 0.03927
	loss_reward_3: 0.0071
	loss_policy_4: 0.04382
	accuracy_policy_4: 0.64848
	loss_value_4: 0.04057
	loss_reward_4: 0.00759
	loss_policy_5: 0.04572
	accuracy_policy_5: 0.64117
	loss_value_5: 0.04189
	loss_reward_5: 0.00883
	loss_policy: 0.30336
	loss_value: 0.36145
	loss_reward: 0.03644
Optimization_Done 32200
[2024-05-05 09:44:15] [command] train weight_iter_32200.pkl 160 162
[2024-05-05 09:44:34] nn step 32250, lr: 0.015.
	loss_policy_0: 0.12653
	accuracy_policy_0: 0.74445
	loss_value_0: 0.1448
	loss_policy_1: 0.03405
	accuracy_policy_1: 0.67598
	loss_value_1: 0.03065
	loss_reward_1: 0.00548
	loss_policy_2: 0.0372
	accuracy_policy_2: 0.65703
	loss_value_2: 0.03235
	loss_reward_2: 0.0054
	loss_policy_3: 0.04005
	accuracy_policy_3: 0.63973
	loss_value_3: 0.03389
	loss_reward_3: 0.00581
	loss_policy_4: 0.04313
	accuracy_policy_4: 0.61504
	loss_value_4: 0.03511
	loss_reward_4: 0.00619
	loss_policy_5: 0.04561
	accuracy_policy_5: 0.59875
	loss_value_5: 0.0364
	loss_reward_5: 0.00709
	loss_policy: 0.32656
	loss_value: 0.31321
	loss_reward: 0.02997
[2024-05-05 09:44:50] nn step 32300, lr: 0.015.
	loss_policy_0: 0.11539
	accuracy_policy_0: 0.77711
	loss_value_0: 0.15251
	loss_policy_1: 0.03443
	accuracy_policy_1: 0.69832
	loss_value_1: 0.03275
	loss_reward_1: 0.00597
	loss_policy_2: 0.0379
	accuracy_policy_2: 0.67305
	loss_value_2: 0.03444
	loss_reward_2: 0.00585
	loss_policy_3: 0.04152
	accuracy_policy_3: 0.65418
	loss_value_3: 0.03593
	loss_reward_3: 0.00625
	loss_policy_4: 0.04424
	accuracy_policy_4: 0.63719
	loss_value_4: 0.03728
	loss_reward_4: 0.0067
	loss_policy_5: 0.0469
	accuracy_policy_5: 0.62098
	loss_value_5: 0.03873
	loss_reward_5: 0.00766
	loss_policy: 0.32038
	loss_value: 0.33165
	loss_reward: 0.03242
[2024-05-05 09:45:07] nn step 32350, lr: 0.015.
	loss_policy_0: 0.1133
	accuracy_policy_0: 0.78598
	loss_value_0: 0.15826
	loss_policy_1: 0.03425
	accuracy_policy_1: 0.70699
	loss_value_1: 0.03357
	loss_reward_1: 0.00648
	loss_policy_2: 0.03812
	accuracy_policy_2: 0.68641
	loss_value_2: 0.03521
	loss_reward_2: 0.00602
	loss_policy_3: 0.04184
	accuracy_policy_3: 0.66832
	loss_value_3: 0.03708
	loss_reward_3: 0.00638
	loss_policy_4: 0.04498
	accuracy_policy_4: 0.64594
	loss_value_4: 0.03839
	loss_reward_4: 0.007
	loss_policy_5: 0.04803
	accuracy_policy_5: 0.63051
	loss_value_5: 0.03993
	loss_reward_5: 0.00832
	loss_policy: 0.32052
	loss_value: 0.34244
	loss_reward: 0.0342
[2024-05-05 09:45:24] nn step 32400, lr: 0.015.
	loss_policy_0: 0.10231
	accuracy_policy_0: 0.7959
	loss_value_0: 0.14811
	loss_policy_1: 0.03231
	accuracy_policy_1: 0.70449
	loss_value_1: 0.03158
	loss_reward_1: 0.00578
	loss_policy_2: 0.0358
	accuracy_policy_2: 0.68422
	loss_value_2: 0.03307
	loss_reward_2: 0.0057
	loss_policy_3: 0.03969
	accuracy_policy_3: 0.66199
	loss_value_3: 0.03451
	loss_reward_3: 0.00597
	loss_policy_4: 0.04222
	accuracy_policy_4: 0.64281
	loss_value_4: 0.03588
	loss_reward_4: 0.0063
	loss_policy_5: 0.04504
	accuracy_policy_5: 0.62332
	loss_value_5: 0.03734
	loss_reward_5: 0.0077
	loss_policy: 0.29737
	loss_value: 0.32048
	loss_reward: 0.03144
Optimization_Done 32400
[2024-05-05 09:47:29] [command] train weight_iter_32400.pkl 161 163
[2024-05-05 09:47:47] nn step 32450, lr: 0.015.
	loss_policy_0: 0.12025
	accuracy_policy_0: 0.75219
	loss_value_0: 0.1474
	loss_policy_1: 0.03453
	accuracy_policy_1: 0.65992
	loss_value_1: 0.03127
	loss_reward_1: 0.00599
	loss_policy_2: 0.03781
	accuracy_policy_2: 0.63227
	loss_value_2: 0.03287
	loss_reward_2: 0.00577
	loss_policy_3: 0.04156
	accuracy_policy_3: 0.60699
	loss_value_3: 0.03438
	loss_reward_3: 0.00606
	loss_policy_4: 0.04465
	accuracy_policy_4: 0.58715
	loss_value_4: 0.03547
	loss_reward_4: 0.00647
	loss_policy_5: 0.04684
	accuracy_policy_5: 0.57488
	loss_value_5: 0.03678
	loss_reward_5: 0.00752
	loss_policy: 0.32564
	loss_value: 0.31818
	loss_reward: 0.03182
[2024-05-05 09:48:04] nn step 32500, lr: 0.015.
	loss_policy_0: 0.10433
	accuracy_policy_0: 0.77973
	loss_value_0: 0.14695
	loss_policy_1: 0.03191
	accuracy_policy_1: 0.68578
	loss_value_1: 0.03157
	loss_reward_1: 0.00596
	loss_policy_2: 0.03602
	accuracy_policy_2: 0.65895
	loss_value_2: 0.03318
	loss_reward_2: 0.00552
	loss_policy_3: 0.03922
	accuracy_policy_3: 0.63984
	loss_value_3: 0.03468
	loss_reward_3: 0.00622
	loss_policy_4: 0.04229
	accuracy_policy_4: 0.6098
	loss_value_4: 0.03594
	loss_reward_4: 0.00655
	loss_policy_5: 0.04491
	accuracy_policy_5: 0.59383
	loss_value_5: 0.03695
	loss_reward_5: 0.00741
	loss_policy: 0.2987
	loss_value: 0.31927
	loss_reward: 0.03165
[2024-05-05 09:48:22] nn step 32550, lr: 0.015.
	loss_policy_0: 0.0958
	accuracy_policy_0: 0.78801
	loss_value_0: 0.14473
	loss_policy_1: 0.03074
	accuracy_policy_1: 0.68906
	loss_value_1: 0.03092
	loss_reward_1: 0.00572
	loss_policy_2: 0.03455
	accuracy_policy_2: 0.66758
	loss_value_2: 0.03239
	loss_reward_2: 0.00553
	loss_policy_3: 0.03773
	accuracy_policy_3: 0.63824
	loss_value_3: 0.0337
	loss_reward_3: 0.0061
	loss_policy_4: 0.04091
	accuracy_policy_4: 0.6184
	loss_value_4: 0.03531
	loss_reward_4: 0.0064
	loss_policy_5: 0.04355
	accuracy_policy_5: 0.60098
	loss_value_5: 0.03665
	loss_reward_5: 0.00748
	loss_policy: 0.28327
	loss_value: 0.31369
	loss_reward: 0.03123
[2024-05-05 09:48:39] nn step 32600, lr: 0.015.
	loss_policy_0: 0.08987
	accuracy_policy_0: 0.795
	loss_value_0: 0.13942
	loss_policy_1: 0.02886
	accuracy_policy_1: 0.69742
	loss_value_1: 0.02968
	loss_reward_1: 0.00539
	loss_policy_2: 0.03286
	accuracy_policy_2: 0.67109
	loss_value_2: 0.03116
	loss_reward_2: 0.00535
	loss_policy_3: 0.03594
	accuracy_policy_3: 0.64848
	loss_value_3: 0.03253
	loss_reward_3: 0.00565
	loss_policy_4: 0.0387
	accuracy_policy_4: 0.62121
	loss_value_4: 0.03376
	loss_reward_4: 0.00607
	loss_policy_5: 0.0413
	accuracy_policy_5: 0.60355
	loss_value_5: 0.03489
	loss_reward_5: 0.00716
	loss_policy: 0.26752
	loss_value: 0.30145
	loss_reward: 0.02962
Optimization_Done 32600
[2024-05-05 09:50:44] [command] train weight_iter_32600.pkl 162 164
[2024-05-05 09:51:02] nn step 32650, lr: 0.015.
	loss_policy_0: 0.08597
	accuracy_policy_0: 0.77395
	loss_value_0: 0.12019
	loss_policy_1: 0.02738
	accuracy_policy_1: 0.67012
	loss_value_1: 0.02555
	loss_reward_1: 0.00468
	loss_policy_2: 0.03003
	accuracy_policy_2: 0.64188
	loss_value_2: 0.02682
	loss_reward_2: 0.00458
	loss_policy_3: 0.03272
	accuracy_policy_3: 0.62156
	loss_value_3: 0.02766
	loss_reward_3: 0.00492
	loss_policy_4: 0.03512
	accuracy_policy_4: 0.59363
	loss_value_4: 0.0288
	loss_reward_4: 0.00535
	loss_policy_5: 0.03705
	accuracy_policy_5: 0.58141
	loss_value_5: 0.02983
	loss_reward_5: 0.00598
	loss_policy: 0.24828
	loss_value: 0.25885
	loss_reward: 0.02552
[2024-05-05 09:51:19] nn step 32700, lr: 0.015.
	loss_policy_0: 0.07697
	accuracy_policy_0: 0.79785
	loss_value_0: 0.12216
	loss_policy_1: 0.0263
	accuracy_policy_1: 0.67926
	loss_value_1: 0.02565
	loss_reward_1: 0.0048
	loss_policy_2: 0.02878
	accuracy_policy_2: 0.6577
	loss_value_2: 0.02686
	loss_reward_2: 0.00455
	loss_policy_3: 0.03145
	accuracy_policy_3: 0.63516
	loss_value_3: 0.02802
	loss_reward_3: 0.00478
	loss_policy_4: 0.03371
	accuracy_policy_4: 0.61305
	loss_value_4: 0.02904
	loss_reward_4: 0.00521
	loss_policy_5: 0.03621
	accuracy_policy_5: 0.59121
	loss_value_5: 0.03014
	loss_reward_5: 0.00602
	loss_policy: 0.23341
	loss_value: 0.26186
	loss_reward: 0.02537
[2024-05-05 09:51:36] nn step 32750, lr: 0.015.
	loss_policy_0: 0.0736
	accuracy_policy_0: 0.79855
	loss_value_0: 0.12051
	loss_policy_1: 0.02473
	accuracy_policy_1: 0.68961
	loss_value_1: 0.02547
	loss_reward_1: 0.00466
	loss_policy_2: 0.02801
	accuracy_policy_2: 0.65723
	loss_value_2: 0.02679
	loss_reward_2: 0.00435
	loss_policy_3: 0.03057
	accuracy_policy_3: 0.64039
	loss_value_3: 0.02781
	loss_reward_3: 0.00481
	loss_policy_4: 0.03272
	accuracy_policy_4: 0.61484
	loss_value_4: 0.02868
	loss_reward_4: 0.00525
	loss_policy_5: 0.03502
	accuracy_policy_5: 0.59684
	loss_value_5: 0.02979
	loss_reward_5: 0.00598
	loss_policy: 0.22465
	loss_value: 0.25905
	loss_reward: 0.02506
[2024-05-05 09:51:53] nn step 32800, lr: 0.015.
	loss_policy_0: 0.0705
	accuracy_policy_0: 0.79879
	loss_value_0: 0.11717
	loss_policy_1: 0.02382
	accuracy_policy_1: 0.69422
	loss_value_1: 0.02518
	loss_reward_1: 0.00443
	loss_policy_2: 0.02693
	accuracy_policy_2: 0.65977
	loss_value_2: 0.0262
	loss_reward_2: 0.0043
	loss_policy_3: 0.02922
	accuracy_policy_3: 0.64125
	loss_value_3: 0.02699
	loss_reward_3: 0.00462
	loss_policy_4: 0.03164
	accuracy_policy_4: 0.61707
	loss_value_4: 0.02805
	loss_reward_4: 0.00494
	loss_policy_5: 0.03362
	accuracy_policy_5: 0.59863
	loss_value_5: 0.02911
	loss_reward_5: 0.00591
	loss_policy: 0.21571
	loss_value: 0.2527
	loss_reward: 0.02418
Optimization_Done 32800
[2024-05-05 09:54:02] [command] train weight_iter_32800.pkl 163 165
[2024-05-05 09:54:21] nn step 32850, lr: 0.015.
	loss_policy_0: 0.10956
	accuracy_policy_0: 0.70547
	loss_value_0: 0.11669
	loss_policy_1: 0.02963
	accuracy_policy_1: 0.62145
	loss_value_1: 0.02467
	loss_reward_1: 0.00492
	loss_policy_2: 0.03194
	accuracy_policy_2: 0.59465
	loss_value_2: 0.02595
	loss_reward_2: 0.00466
	loss_policy_3: 0.03465
	accuracy_policy_3: 0.56965
	loss_value_3: 0.02695
	loss_reward_3: 0.00497
	loss_policy_4: 0.03699
	accuracy_policy_4: 0.54504
	loss_value_4: 0.02808
	loss_reward_4: 0.00535
	loss_policy_5: 0.03878
	accuracy_policy_5: 0.52477
	loss_value_5: 0.02922
	loss_reward_5: 0.0061
	loss_policy: 0.28154
	loss_value: 0.25157
	loss_reward: 0.026
[2024-05-05 09:54:37] nn step 32900, lr: 0.015.
	loss_policy_0: 0.08372
	accuracy_policy_0: 0.7632
	loss_value_0: 0.11352
	loss_policy_1: 0.0261
	accuracy_policy_1: 0.66074
	loss_value_1: 0.02397
	loss_reward_1: 0.005
	loss_policy_2: 0.02907
	accuracy_policy_2: 0.6316
	loss_value_2: 0.02529
	loss_reward_2: 0.00474
	loss_policy_3: 0.03153
	accuracy_policy_3: 0.60465
	loss_value_3: 0.02628
	loss_reward_3: 0.00509
	loss_policy_4: 0.03421
	accuracy_policy_4: 0.57613
	loss_value_4: 0.02725
	loss_reward_4: 0.00541
	loss_policy_5: 0.03585
	accuracy_policy_5: 0.56348
	loss_value_5: 0.02818
	loss_reward_5: 0.00615
	loss_policy: 0.24049
	loss_value: 0.24449
	loss_reward: 0.0264
[2024-05-05 09:54:54] nn step 32950, lr: 0.015.
	loss_policy_0: 0.0812
	accuracy_policy_0: 0.77398
	loss_value_0: 0.11778
	loss_policy_1: 0.0263
	accuracy_policy_1: 0.66859
	loss_value_1: 0.02493
	loss_reward_1: 0.00486
	loss_policy_2: 0.02924
	accuracy_policy_2: 0.63938
	loss_value_2: 0.02607
	loss_reward_2: 0.00487
	loss_policy_3: 0.03168
	accuracy_policy_3: 0.61414
	loss_value_3: 0.02704
	loss_reward_3: 0.00511
	loss_policy_4: 0.03436
	accuracy_policy_4: 0.58871
	loss_value_4: 0.028
	loss_reward_4: 0.00552
	loss_policy_5: 0.03628
	accuracy_policy_5: 0.56609
	loss_value_5: 0.02911
	loss_reward_5: 0.00639
	loss_policy: 0.23906
	loss_value: 0.25293
	loss_reward: 0.02675
[2024-05-05 09:55:11] nn step 33000, lr: 0.015.
	loss_policy_0: 0.07049
	accuracy_policy_0: 0.7825
	loss_value_0: 0.10898
	loss_policy_1: 0.02331
	accuracy_policy_1: 0.67273
	loss_value_1: 0.02321
	loss_reward_1: 0.0046
	loss_policy_2: 0.02584
	accuracy_policy_2: 0.64195
	loss_value_2: 0.02406
	loss_reward_2: 0.00427
	loss_policy_3: 0.02811
	accuracy_policy_3: 0.6198
	loss_value_3: 0.02493
	loss_reward_3: 0.00478
	loss_policy_4: 0.03031
	accuracy_policy_4: 0.59543
	loss_value_4: 0.02599
	loss_reward_4: 0.00499
	loss_policy_5: 0.03226
	accuracy_policy_5: 0.58043
	loss_value_5: 0.02698
	loss_reward_5: 0.00584
	loss_policy: 0.21033
	loss_value: 0.23415
	loss_reward: 0.02448
Optimization_Done 33000
[2024-05-05 09:57:18] [command] train weight_iter_33000.pkl 164 166
[2024-05-05 09:57:37] nn step 33050, lr: 0.015.
	loss_policy_0: 0.10768
	accuracy_policy_0: 0.71652
	loss_value_0: 0.11966
	loss_policy_1: 0.02856
	accuracy_policy_1: 0.64082
	loss_value_1: 0.02513
	loss_reward_1: 0.00474
	loss_policy_2: 0.03081
	accuracy_policy_2: 0.61414
	loss_value_2: 0.02631
	loss_reward_2: 0.00448
	loss_policy_3: 0.03321
	accuracy_policy_3: 0.5902
	loss_value_3: 0.02733
	loss_reward_3: 0.00475
	loss_policy_4: 0.03506
	accuracy_policy_4: 0.57355
	loss_value_4: 0.02828
	loss_reward_4: 0.00513
	loss_policy_5: 0.03659
	accuracy_policy_5: 0.55508
	loss_value_5: 0.02923
	loss_reward_5: 0.0057
	loss_policy: 0.27191
	loss_value: 0.25595
	loss_reward: 0.02479
[2024-05-05 09:57:54] nn step 33100, lr: 0.015.
	loss_policy_0: 0.09459
	accuracy_policy_0: 0.7557
	loss_value_0: 0.12792
	loss_policy_1: 0.02773
	accuracy_policy_1: 0.67719
	loss_value_1: 0.02717
	loss_reward_1: 0.00487
	loss_policy_2: 0.03072
	accuracy_policy_2: 0.6468
	loss_value_2: 0.02845
	loss_reward_2: 0.00486
	loss_policy_3: 0.03366
	accuracy_policy_3: 0.6227
	loss_value_3: 0.02967
	loss_reward_3: 0.00502
	loss_policy_4: 0.036
	accuracy_policy_4: 0.59848
	loss_value_4: 0.03073
	loss_reward_4: 0.00551
	loss_policy_5: 0.03795
	accuracy_policy_5: 0.58301
	loss_value_5: 0.03188
	loss_reward_5: 0.00626
	loss_policy: 0.26065
	loss_value: 0.27581
	loss_reward: 0.02651
[2024-05-05 09:58:11] nn step 33150, lr: 0.015.
	loss_policy_0: 0.08771
	accuracy_policy_0: 0.76852
	loss_value_0: 0.12875
	loss_policy_1: 0.02614
	accuracy_policy_1: 0.68445
	loss_value_1: 0.02723
	loss_reward_1: 0.0048
	loss_policy_2: 0.02947
	accuracy_policy_2: 0.65371
	loss_value_2: 0.02855
	loss_reward_2: 0.0046
	loss_policy_3: 0.03216
	accuracy_policy_3: 0.62969
	loss_value_3: 0.02977
	loss_reward_3: 0.00514
	loss_policy_4: 0.03465
	accuracy_policy_4: 0.60648
	loss_value_4: 0.03093
	loss_reward_4: 0.00512
	loss_policy_5: 0.03654
	accuracy_policy_5: 0.59715
	loss_value_5: 0.03201
	loss_reward_5: 0.00603
	loss_policy: 0.24666
	loss_value: 0.27723
	loss_reward: 0.02569
[2024-05-05 09:58:29] nn step 33200, lr: 0.015.
	loss_policy_0: 0.08336
	accuracy_policy_0: 0.78078
	loss_value_0: 0.12997
	loss_policy_1: 0.02606
	accuracy_policy_1: 0.68734
	loss_value_1: 0.02766
	loss_reward_1: 0.00483
	loss_policy_2: 0.02957
	accuracy_policy_2: 0.65398
	loss_value_2: 0.02887
	loss_reward_2: 0.00478
	loss_policy_3: 0.03205
	accuracy_policy_3: 0.63027
	loss_value_3: 0.03023
	loss_reward_3: 0.00505
	loss_policy_4: 0.03451
	accuracy_policy_4: 0.60359
	loss_value_4: 0.03141
	loss_reward_4: 0.00537
	loss_policy_5: 0.03635
	accuracy_policy_5: 0.59512
	loss_value_5: 0.03232
	loss_reward_5: 0.00607
	loss_policy: 0.24189
	loss_value: 0.28047
	loss_reward: 0.02611
Optimization_Done 33200
[2024-05-05 10:00:33] [command] train weight_iter_33200.pkl 165 167
[2024-05-05 10:00:52] nn step 33250, lr: 0.015.
	loss_policy_0: 0.1288
	accuracy_policy_0: 0.71688
	loss_value_0: 0.1445
	loss_policy_1: 0.03544
	accuracy_policy_1: 0.6298
	loss_value_1: 0.03076
	loss_reward_1: 0.00628
	loss_policy_2: 0.03879
	accuracy_policy_2: 0.59707
	loss_value_2: 0.03244
	loss_reward_2: 0.00626
	loss_policy_3: 0.04248
	accuracy_policy_3: 0.56617
	loss_value_3: 0.0337
	loss_reward_3: 0.00679
	loss_policy_4: 0.04561
	accuracy_policy_4: 0.5352
	loss_value_4: 0.03502
	loss_reward_4: 0.00754
	loss_policy_5: 0.04812
	accuracy_policy_5: 0.52344
	loss_value_5: 0.0365
	loss_reward_5: 0.00844
	loss_policy: 0.33923
	loss_value: 0.31292
	loss_reward: 0.03532
[2024-05-05 10:01:09] nn step 33300, lr: 0.015.
	loss_policy_0: 0.10932
	accuracy_policy_0: 0.75223
	loss_value_0: 0.13562
	loss_policy_1: 0.03316
	accuracy_policy_1: 0.65473
	loss_value_1: 0.02884
	loss_reward_1: 0.00623
	loss_policy_2: 0.03665
	accuracy_policy_2: 0.62344
	loss_value_2: 0.03038
	loss_reward_2: 0.00612
	loss_policy_3: 0.04005
	accuracy_policy_3: 0.58977
	loss_value_3: 0.03183
	loss_reward_3: 0.00657
	loss_policy_4: 0.04311
	accuracy_policy_4: 0.56312
	loss_value_4: 0.03348
	loss_reward_4: 0.00687
	loss_policy_5: 0.04562
	accuracy_policy_5: 0.54379
	loss_value_5: 0.03458
	loss_reward_5: 0.00784
	loss_policy: 0.30791
	loss_value: 0.29473
	loss_reward: 0.03362
[2024-05-05 10:01:27] nn step 33350, lr: 0.015.
	loss_policy_0: 0.10131
	accuracy_policy_0: 0.76504
	loss_value_0: 0.13086
	loss_policy_1: 0.03147
	accuracy_policy_1: 0.66305
	loss_value_1: 0.0279
	loss_reward_1: 0.00613
	loss_policy_2: 0.03521
	accuracy_policy_2: 0.62629
	loss_value_2: 0.02932
	loss_reward_2: 0.00585
	loss_policy_3: 0.03903
	accuracy_policy_3: 0.59148
	loss_value_3: 0.03078
	loss_reward_3: 0.00646
	loss_policy_4: 0.04182
	accuracy_policy_4: 0.56949
	loss_value_4: 0.03191
	loss_reward_4: 0.0067
	loss_policy_5: 0.04418
	accuracy_policy_5: 0.55352
	loss_value_5: 0.0332
	loss_reward_5: 0.00794
	loss_policy: 0.29302
	loss_value: 0.28398
	loss_reward: 0.03308
[2024-05-05 10:01:44] nn step 33400, lr: 0.015.
	loss_policy_0: 0.10068
	accuracy_policy_0: 0.77516
	loss_value_0: 0.13513
	loss_policy_1: 0.0324
	accuracy_policy_1: 0.66477
	loss_value_1: 0.02869
	loss_reward_1: 0.00615
	loss_policy_2: 0.03582
	accuracy_policy_2: 0.63504
	loss_value_2: 0.0301
	loss_reward_2: 0.00618
	loss_policy_3: 0.03939
	accuracy_policy_3: 0.60375
	loss_value_3: 0.03131
	loss_reward_3: 0.00661
	loss_policy_4: 0.04227
	accuracy_policy_4: 0.57844
	loss_value_4: 0.03258
	loss_reward_4: 0.00704
	loss_policy_5: 0.04548
	accuracy_policy_5: 0.55363
	loss_value_5: 0.03385
	loss_reward_5: 0.00805
	loss_policy: 0.29604
	loss_value: 0.29166
	loss_reward: 0.03404
Optimization_Done 33400
[2024-05-05 10:03:49] [command] train weight_iter_33400.pkl 166 168
[2024-05-05 10:04:08] nn step 33450, lr: 0.015.
	loss_policy_0: 0.14445
	accuracy_policy_0: 0.7116
	loss_value_0: 0.15291
	loss_policy_1: 0.04049
	accuracy_policy_1: 0.61707
	loss_value_1: 0.03265
	loss_reward_1: 0.00694
	loss_policy_2: 0.04445
	accuracy_policy_2: 0.57871
	loss_value_2: 0.03432
	loss_reward_2: 0.00686
	loss_policy_3: 0.04773
	accuracy_policy_3: 0.55562
	loss_value_3: 0.03607
	loss_reward_3: 0.00759
	loss_policy_4: 0.05128
	accuracy_policy_4: 0.52676
	loss_value_4: 0.03745
	loss_reward_4: 0.00778
	loss_policy_5: 0.05316
	accuracy_policy_5: 0.50586
	loss_value_5: 0.03868
	loss_reward_5: 0.00921
	loss_policy: 0.38156
	loss_value: 0.33208
	loss_reward: 0.03839
[2024-05-05 10:04:26] nn step 33500, lr: 0.015.
	loss_policy_0: 0.10862
	accuracy_policy_0: 0.75414
	loss_value_0: 0.13701
	loss_policy_1: 0.03451
	accuracy_policy_1: 0.6434
	loss_value_1: 0.02915
	loss_reward_1: 0.00628
	loss_policy_2: 0.0381
	accuracy_policy_2: 0.61023
	loss_value_2: 0.03078
	loss_reward_2: 0.00618
	loss_policy_3: 0.04124
	accuracy_policy_3: 0.58441
	loss_value_3: 0.03202
	loss_reward_3: 0.00677
	loss_policy_4: 0.04401
	accuracy_policy_4: 0.56156
	loss_value_4: 0.0333
	loss_reward_4: 0.00716
	loss_policy_5: 0.04626
	accuracy_policy_5: 0.53883
	loss_value_5: 0.03444
	loss_reward_5: 0.00812
	loss_policy: 0.31275
	loss_value: 0.29672
	loss_reward: 0.03451
[2024-05-05 10:04:43] nn step 33550, lr: 0.015.
	loss_policy_0: 0.11542
	accuracy_policy_0: 0.76586
	loss_value_0: 0.14949
	loss_policy_1: 0.03689
	accuracy_policy_1: 0.65617
	loss_value_1: 0.03168
	loss_reward_1: 0.00714
	loss_policy_2: 0.04088
	accuracy_policy_2: 0.6227
	loss_value_2: 0.03344
	loss_reward_2: 0.00689
	loss_policy_3: 0.04438
	accuracy_policy_3: 0.59012
	loss_value_3: 0.03494
	loss_reward_3: 0.00775
	loss_policy_4: 0.04766
	accuracy_policy_4: 0.56578
	loss_value_4: 0.03651
	loss_reward_4: 0.00787
	loss_policy_5: 0.04988
	accuracy_policy_5: 0.55125
	loss_value_5: 0.03785
	loss_reward_5: 0.00901
	loss_policy: 0.33513
	loss_value: 0.32391
	loss_reward: 0.03865
[2024-05-05 10:05:01] nn step 33600, lr: 0.015.
	loss_policy_0: 0.1088
	accuracy_policy_0: 0.77633
	loss_value_0: 0.14841
	loss_policy_1: 0.03578
	accuracy_policy_1: 0.65957
	loss_value_1: 0.03184
	loss_reward_1: 0.00714
	loss_policy_2: 0.03988
	accuracy_policy_2: 0.62957
	loss_value_2: 0.0336
	loss_reward_2: 0.00675
	loss_policy_3: 0.04364
	accuracy_policy_3: 0.59641
	loss_value_3: 0.03506
	loss_reward_3: 0.00737
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.57359
	loss_value_4: 0.03637
	loss_reward_4: 0.00785
	loss_policy_5: 0.0492
	accuracy_policy_5: 0.55422
	loss_value_5: 0.0375
	loss_reward_5: 0.00912
	loss_policy: 0.32401
	loss_value: 0.32278
	loss_reward: 0.03823
Optimization_Done 33600
[2024-05-05 10:06:47] [command] train weight_iter_33600.pkl 167 169
[2024-05-05 10:07:05] nn step 33650, lr: 0.015.
	loss_policy_0: 0.12021
	accuracy_policy_0: 0.73371
	loss_value_0: 0.13681
	loss_policy_1: 0.03478
	accuracy_policy_1: 0.6432
	loss_value_1: 0.02885
	loss_reward_1: 0.00585
	loss_policy_2: 0.03845
	accuracy_policy_2: 0.60316
	loss_value_2: 0.0306
	loss_reward_2: 0.00586
	loss_policy_3: 0.04163
	accuracy_policy_3: 0.57352
	loss_value_3: 0.03205
	loss_reward_3: 0.00619
	loss_policy_4: 0.04426
	accuracy_policy_4: 0.54754
	loss_value_4: 0.03328
	loss_reward_4: 0.00696
	loss_policy_5: 0.04611
	accuracy_policy_5: 0.52773
	loss_value_5: 0.03443
	loss_reward_5: 0.00773
	loss_policy: 0.32544
	loss_value: 0.29603
	loss_reward: 0.03259
[2024-05-05 10:07:23] nn step 33700, lr: 0.015.
	loss_policy_0: 0.11747
	accuracy_policy_0: 0.75977
	loss_value_0: 0.14539
	loss_policy_1: 0.03642
	accuracy_policy_1: 0.65148
	loss_value_1: 0.03101
	loss_reward_1: 0.00662
	loss_policy_2: 0.04032
	accuracy_policy_2: 0.62098
	loss_value_2: 0.03277
	loss_reward_2: 0.00652
	loss_policy_3: 0.04404
	accuracy_policy_3: 0.58824
	loss_value_3: 0.03417
	loss_reward_3: 0.00703
	loss_policy_4: 0.04684
	accuracy_policy_4: 0.56586
	loss_value_4: 0.03537
	loss_reward_4: 0.00742
	loss_policy_5: 0.04984
	accuracy_policy_5: 0.54586
	loss_value_5: 0.03663
	loss_reward_5: 0.00865
	loss_policy: 0.33493
	loss_value: 0.31533
	loss_reward: 0.03623
[2024-05-05 10:07:40] nn step 33750, lr: 0.015.
	loss_policy_0: 0.10405
	accuracy_policy_0: 0.76691
	loss_value_0: 0.13505
	loss_policy_1: 0.03372
	accuracy_policy_1: 0.6532
	loss_value_1: 0.02871
	loss_reward_1: 0.00612
	loss_policy_2: 0.03729
	accuracy_policy_2: 0.62203
	loss_value_2: 0.03016
	loss_reward_2: 0.00616
	loss_policy_3: 0.04022
	accuracy_policy_3: 0.60227
	loss_value_3: 0.03137
	loss_reward_3: 0.00652
	loss_policy_4: 0.04317
	accuracy_policy_4: 0.57063
	loss_value_4: 0.03252
	loss_reward_4: 0.00701
	loss_policy_5: 0.045
	accuracy_policy_5: 0.55609
	loss_value_5: 0.03376
	loss_reward_5: 0.00796
	loss_policy: 0.30344
	loss_value: 0.29157
	loss_reward: 0.03377
[2024-05-05 10:07:57] nn step 33800, lr: 0.015.
	loss_policy_0: 0.09994
	accuracy_policy_0: 0.78086
	loss_value_0: 0.136
	loss_policy_1: 0.03299
	accuracy_policy_1: 0.66227
	loss_value_1: 0.02904
	loss_reward_1: 0.00614
	loss_policy_2: 0.03633
	accuracy_policy_2: 0.6307
	loss_value_2: 0.03053
	loss_reward_2: 0.00591
	loss_policy_3: 0.04012
	accuracy_policy_3: 0.59883
	loss_value_3: 0.03174
	loss_reward_3: 0.00643
	loss_policy_4: 0.04299
	accuracy_policy_4: 0.57473
	loss_value_4: 0.03308
	loss_reward_4: 0.00704
	loss_policy_5: 0.04531
	accuracy_policy_5: 0.55672
	loss_value_5: 0.03435
	loss_reward_5: 0.0081
	loss_policy: 0.29767
	loss_value: 0.29474
	loss_reward: 0.03361
Optimization_Done 33800
[2024-05-05 10:10:05] [command] train weight_iter_33800.pkl 168 170
[2024-05-05 10:10:23] nn step 33850, lr: 0.015.
	loss_policy_0: 0.1537
	accuracy_policy_0: 0.69152
	loss_value_0: 0.15592
	loss_policy_1: 0.04094
	accuracy_policy_1: 0.60777
	loss_value_1: 0.03287
	loss_reward_1: 0.00589
	loss_policy_2: 0.04453
	accuracy_policy_2: 0.57312
	loss_value_2: 0.03461
	loss_reward_2: 0.00557
	loss_policy_3: 0.04781
	accuracy_policy_3: 0.55426
	loss_value_3: 0.03618
	loss_reward_3: 0.00603
	loss_policy_4: 0.04985
	accuracy_policy_4: 0.53613
	loss_value_4: 0.0375
	loss_reward_4: 0.00636
	loss_policy_5: 0.05279
	accuracy_policy_5: 0.51902
	loss_value_5: 0.03885
	loss_reward_5: 0.00741
	loss_policy: 0.38962
	loss_value: 0.33592
	loss_reward: 0.03126
[2024-05-05 10:10:41] nn step 33900, lr: 0.015.
	loss_policy_0: 0.12582
	accuracy_policy_0: 0.73727
	loss_value_0: 0.15502
	loss_policy_1: 0.03759
	accuracy_policy_1: 0.63629
	loss_value_1: 0.03316
	loss_reward_1: 0.0059
	loss_policy_2: 0.04192
	accuracy_policy_2: 0.60555
	loss_value_2: 0.03468
	loss_reward_2: 0.00567
	loss_policy_3: 0.04489
	accuracy_policy_3: 0.58164
	loss_value_3: 0.03617
	loss_reward_3: 0.00614
	loss_policy_4: 0.04726
	accuracy_policy_4: 0.56824
	loss_value_4: 0.03747
	loss_reward_4: 0.00651
	loss_policy_5: 0.05031
	accuracy_policy_5: 0.54777
	loss_value_5: 0.0385
	loss_reward_5: 0.00743
	loss_policy: 0.34778
	loss_value: 0.335
	loss_reward: 0.03165
[2024-05-05 10:10:58] nn step 33950, lr: 0.015.
	loss_policy_0: 0.11582
	accuracy_policy_0: 0.75246
	loss_value_0: 0.15123
	loss_policy_1: 0.0356
	accuracy_policy_1: 0.64516
	loss_value_1: 0.03217
	loss_reward_1: 0.00565
	loss_policy_2: 0.03961
	accuracy_policy_2: 0.61457
	loss_value_2: 0.03346
	loss_reward_2: 0.00577
	loss_policy_3: 0.04288
	accuracy_policy_3: 0.59223
	loss_value_3: 0.03498
	loss_reward_3: 0.00631
	loss_policy_4: 0.04561
	accuracy_policy_4: 0.57395
	loss_value_4: 0.03636
	loss_reward_4: 0.00638
	loss_policy_5: 0.04824
	accuracy_policy_5: 0.55109
	loss_value_5: 0.03796
	loss_reward_5: 0.00753
	loss_policy: 0.32775
	loss_value: 0.32616
	loss_reward: 0.03163
[2024-05-05 10:11:16] nn step 34000, lr: 0.015.
	loss_policy_0: 0.11516
	accuracy_policy_0: 0.75934
	loss_value_0: 0.15312
	loss_policy_1: 0.0356
	accuracy_policy_1: 0.65684
	loss_value_1: 0.03283
	loss_reward_1: 0.00584
	loss_policy_2: 0.03972
	accuracy_policy_2: 0.62246
	loss_value_2: 0.0343
	loss_reward_2: 0.00571
	loss_policy_3: 0.0428
	accuracy_policy_3: 0.59352
	loss_value_3: 0.03551
	loss_reward_3: 0.00614
	loss_policy_4: 0.04599
	accuracy_policy_4: 0.56801
	loss_value_4: 0.03667
	loss_reward_4: 0.00666
	loss_policy_5: 0.04848
	accuracy_policy_5: 0.55582
	loss_value_5: 0.03816
	loss_reward_5: 0.00762
	loss_policy: 0.32775
	loss_value: 0.33059
	loss_reward: 0.03198
Optimization_Done 34000
[2024-05-05 10:13:16] [command] train weight_iter_34000.pkl 169 171
[2024-05-05 10:13:36] nn step 34050, lr: 0.015.
	loss_policy_0: 0.13513
	accuracy_policy_0: 0.71301
	loss_value_0: 0.14269
	loss_policy_1: 0.0374
	accuracy_policy_1: 0.6241
	loss_value_1: 0.03026
	loss_reward_1: 0.00555
	loss_policy_2: 0.04048
	accuracy_policy_2: 0.60254
	loss_value_2: 0.03184
	loss_reward_2: 0.00528
	loss_policy_3: 0.04356
	accuracy_policy_3: 0.57855
	loss_value_3: 0.03316
	loss_reward_3: 0.00585
	loss_policy_4: 0.0458
	accuracy_policy_4: 0.55914
	loss_value_4: 0.03428
	loss_reward_4: 0.00624
	loss_policy_5: 0.04843
	accuracy_policy_5: 0.54363
	loss_value_5: 0.03561
	loss_reward_5: 0.00735
	loss_policy: 0.35079
	loss_value: 0.30784
	loss_reward: 0.03027
[2024-05-05 10:13:53] nn step 34100, lr: 0.015.
	loss_policy_0: 0.11861
	accuracy_policy_0: 0.7457
	loss_value_0: 0.14294
	loss_policy_1: 0.03447
	accuracy_policy_1: 0.66102
	loss_value_1: 0.03048
	loss_reward_1: 0.00536
	loss_policy_2: 0.03911
	accuracy_policy_2: 0.62133
	loss_value_2: 0.03214
	loss_reward_2: 0.0052
	loss_policy_3: 0.04238
	accuracy_policy_3: 0.59543
	loss_value_3: 0.03347
	loss_reward_3: 0.00575
	loss_policy_4: 0.04514
	accuracy_policy_4: 0.5748
	loss_value_4: 0.03475
	loss_reward_4: 0.00626
	loss_policy_5: 0.04738
	accuracy_policy_5: 0.55949
	loss_value_5: 0.03601
	loss_reward_5: 0.00744
	loss_policy: 0.32709
	loss_value: 0.30978
	loss_reward: 0.03001
[2024-05-05 10:14:10] nn step 34150, lr: 0.015.
	loss_policy_0: 0.11411
	accuracy_policy_0: 0.76094
	loss_value_0: 0.14861
	loss_policy_1: 0.03521
	accuracy_policy_1: 0.6625
	loss_value_1: 0.0313
	loss_reward_1: 0.00552
	loss_policy_2: 0.03906
	accuracy_policy_2: 0.63414
	loss_value_2: 0.03278
	loss_reward_2: 0.00529
	loss_policy_3: 0.04307
	accuracy_policy_3: 0.6025
	loss_value_3: 0.03429
	loss_reward_3: 0.006
	loss_policy_4: 0.04554
	accuracy_policy_4: 0.58125
	loss_value_4: 0.03567
	loss_reward_4: 0.00643
	loss_policy_5: 0.04777
	accuracy_policy_5: 0.56504
	loss_value_5: 0.03685
	loss_reward_5: 0.00757
	loss_policy: 0.32475
	loss_value: 0.31949
	loss_reward: 0.03081
[2024-05-05 10:14:28] nn step 34200, lr: 0.015.
	loss_policy_0: 0.1062
	accuracy_policy_0: 0.76777
	loss_value_0: 0.14146
	loss_policy_1: 0.03221
	accuracy_policy_1: 0.66832
	loss_value_1: 0.03011
	loss_reward_1: 0.00548
	loss_policy_2: 0.03612
	accuracy_policy_2: 0.63758
	loss_value_2: 0.03149
	loss_reward_2: 0.00505
	loss_policy_3: 0.03934
	accuracy_policy_3: 0.61117
	loss_value_3: 0.03269
	loss_reward_3: 0.0055
	loss_policy_4: 0.04218
	accuracy_policy_4: 0.58809
	loss_value_4: 0.034
	loss_reward_4: 0.00599
	loss_policy_5: 0.04484
	accuracy_policy_5: 0.56566
	loss_value_5: 0.03515
	loss_reward_5: 0.00701
	loss_policy: 0.3009
	loss_value: 0.3049
	loss_reward: 0.02904
Optimization_Done 34200
[2024-05-05 10:16:34] [command] train weight_iter_34200.pkl 170 172
[2024-05-05 10:16:53] nn step 34250, lr: 0.015.
	loss_policy_0: 0.12661
	accuracy_policy_0: 0.71531
	loss_value_0: 0.13359
	loss_policy_1: 0.03721
	accuracy_policy_1: 0.61949
	loss_value_1: 0.0283
	loss_reward_1: 0.00582
	loss_policy_2: 0.04015
	accuracy_policy_2: 0.59441
	loss_value_2: 0.02985
	loss_reward_2: 0.00552
	loss_policy_3: 0.04392
	accuracy_policy_3: 0.56117
	loss_value_3: 0.03115
	loss_reward_3: 0.00622
	loss_policy_4: 0.04617
	accuracy_policy_4: 0.54246
	loss_value_4: 0.03233
	loss_reward_4: 0.00677
	loss_policy_5: 0.04864
	accuracy_policy_5: 0.5241
	loss_value_5: 0.0334
	loss_reward_5: 0.00767
	loss_policy: 0.3427
	loss_value: 0.28861
	loss_reward: 0.03201
[2024-05-05 10:17:10] nn step 34300, lr: 0.015.
	loss_policy_0: 0.12329
	accuracy_policy_0: 0.74488
	loss_value_0: 0.14747
	loss_policy_1: 0.03818
	accuracy_policy_1: 0.64465
	loss_value_1: 0.03138
	loss_reward_1: 0.00656
	loss_policy_2: 0.04226
	accuracy_policy_2: 0.61379
	loss_value_2: 0.03262
	loss_reward_2: 0.00609
	loss_policy_3: 0.04602
	accuracy_policy_3: 0.58465
	loss_value_3: 0.03411
	loss_reward_3: 0.00676
	loss_policy_4: 0.04881
	accuracy_policy_4: 0.56535
	loss_value_4: 0.03553
	loss_reward_4: 0.00737
	loss_policy_5: 0.05208
	accuracy_policy_5: 0.5423
	loss_value_5: 0.03672
	loss_reward_5: 0.00828
	loss_policy: 0.35064
	loss_value: 0.31783
	loss_reward: 0.03506
[2024-05-05 10:17:28] nn step 34350, lr: 0.015.
	loss_policy_0: 0.10709
	accuracy_policy_0: 0.75578
	loss_value_0: 0.13704
	loss_policy_1: 0.03406
	accuracy_policy_1: 0.65148
	loss_value_1: 0.02908
	loss_reward_1: 0.0059
	loss_policy_2: 0.03781
	accuracy_policy_2: 0.62016
	loss_value_2: 0.03027
	loss_reward_2: 0.00529
	loss_policy_3: 0.04131
	accuracy_policy_3: 0.59344
	loss_value_3: 0.03163
	loss_reward_3: 0.00603
	loss_policy_4: 0.0443
	accuracy_policy_4: 0.56727
	loss_value_4: 0.03282
	loss_reward_4: 0.00646
	loss_policy_5: 0.04694
	accuracy_policy_5: 0.54797
	loss_value_5: 0.034
	loss_reward_5: 0.00771
	loss_policy: 0.31151
	loss_value: 0.29483
	loss_reward: 0.0314
[2024-05-05 10:17:45] nn step 34400, lr: 0.015.
	loss_policy_0: 0.11517
	accuracy_policy_0: 0.76371
	loss_value_0: 0.15239
	loss_policy_1: 0.03722
	accuracy_policy_1: 0.65738
	loss_value_1: 0.03236
	loss_reward_1: 0.0065
	loss_policy_2: 0.04126
	accuracy_policy_2: 0.62617
	loss_value_2: 0.03379
	loss_reward_2: 0.00611
	loss_policy_3: 0.04559
	accuracy_policy_3: 0.59586
	loss_value_3: 0.03533
	loss_reward_3: 0.00655
	loss_policy_4: 0.04811
	accuracy_policy_4: 0.57586
	loss_value_4: 0.0367
	loss_reward_4: 0.00721
	loss_policy_5: 0.05065
	accuracy_policy_5: 0.5602
	loss_value_5: 0.03793
	loss_reward_5: 0.00849
	loss_policy: 0.338
	loss_value: 0.3285
	loss_reward: 0.03487
Optimization_Done 34400
[2024-05-05 10:19:53] [command] train weight_iter_34400.pkl 171 173
[2024-05-05 10:20:12] nn step 34450, lr: 0.015.
	loss_policy_0: 0.15511
	accuracy_policy_0: 0.67621
	loss_value_0: 0.14443
	loss_policy_1: 0.04132
	accuracy_policy_1: 0.59391
	loss_value_1: 0.03034
	loss_reward_1: 0.0061
	loss_policy_2: 0.0441
	accuracy_policy_2: 0.56758
	loss_value_2: 0.03181
	loss_reward_2: 0.00571
	loss_policy_3: 0.04691
	accuracy_policy_3: 0.54148
	loss_value_3: 0.03286
	loss_reward_3: 0.00604
	loss_policy_4: 0.04936
	accuracy_policy_4: 0.52301
	loss_value_4: 0.034
	loss_reward_4: 0.00655
	loss_policy_5: 0.05123
	accuracy_policy_5: 0.49801
	loss_value_5: 0.03491
	loss_reward_5: 0.00744
	loss_policy: 0.38803
	loss_value: 0.30835
	loss_reward: 0.03184
[2024-05-05 10:20:29] nn step 34500, lr: 0.015.
	loss_policy_0: 0.12051
	accuracy_policy_0: 0.73242
	loss_value_0: 0.14324
	loss_policy_1: 0.03618
	accuracy_policy_1: 0.63453
	loss_value_1: 0.0303
	loss_reward_1: 0.00618
	loss_policy_2: 0.04086
	accuracy_policy_2: 0.59605
	loss_value_2: 0.03144
	loss_reward_2: 0.0057
	loss_policy_3: 0.04409
	accuracy_policy_3: 0.57078
	loss_value_3: 0.03259
	loss_reward_3: 0.00628
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.55293
	loss_value_4: 0.03372
	loss_reward_4: 0.00656
	loss_policy_5: 0.04826
	accuracy_policy_5: 0.53672
	loss_value_5: 0.03474
	loss_reward_5: 0.00764
	loss_policy: 0.33661
	loss_value: 0.30602
	loss_reward: 0.03235
[2024-05-05 10:20:47] nn step 34550, lr: 0.015.
	loss_policy_0: 0.12142
	accuracy_policy_0: 0.74066
	loss_value_0: 0.15159
	loss_policy_1: 0.03749
	accuracy_policy_1: 0.64238
	loss_value_1: 0.03193
	loss_reward_1: 0.00641
	loss_policy_2: 0.04116
	accuracy_policy_2: 0.61285
	loss_value_2: 0.03317
	loss_reward_2: 0.0058
	loss_policy_3: 0.04513
	accuracy_policy_3: 0.5827
	loss_value_3: 0.03438
	loss_reward_3: 0.00656
	loss_policy_4: 0.0471
	accuracy_policy_4: 0.56824
	loss_value_4: 0.03569
	loss_reward_4: 0.00696
	loss_policy_5: 0.05016
	accuracy_policy_5: 0.54656
	loss_value_5: 0.03679
	loss_reward_5: 0.00822
	loss_policy: 0.34247
	loss_value: 0.32355
	loss_reward: 0.03395
[2024-05-05 10:21:04] nn step 34600, lr: 0.015.
	loss_policy_0: 0.11556
	accuracy_policy_0: 0.74797
	loss_value_0: 0.15002
	loss_policy_1: 0.03674
	accuracy_policy_1: 0.64402
	loss_value_1: 0.03165
	loss_reward_1: 0.00636
	loss_policy_2: 0.04029
	accuracy_policy_2: 0.61426
	loss_value_2: 0.03299
	loss_reward_2: 0.00594
	loss_policy_3: 0.04321
	accuracy_policy_3: 0.59551
	loss_value_3: 0.03421
	loss_reward_3: 0.0064
	loss_policy_4: 0.04578
	accuracy_policy_4: 0.57316
	loss_value_4: 0.03533
	loss_reward_4: 0.00678
	loss_policy_5: 0.0484
	accuracy_policy_5: 0.55273
	loss_value_5: 0.03635
	loss_reward_5: 0.00811
	loss_policy: 0.32997
	loss_value: 0.32055
	loss_reward: 0.03359
Optimization_Done 34600
[2024-05-05 10:23:13] [command] train weight_iter_34600.pkl 172 174
[2024-05-05 10:23:36] nn step 34650, lr: 0.015.
	loss_policy_0: 0.14809
	accuracy_policy_0: 0.68859
	loss_value_0: 0.1662
	loss_policy_1: 0.03973
	accuracy_policy_1: 0.6093
	loss_value_1: 0.03509
	loss_reward_1: 0.00641
	loss_policy_2: 0.04384
	accuracy_policy_2: 0.58082
	loss_value_2: 0.03655
	loss_reward_2: 0.00582
	loss_policy_3: 0.04762
	accuracy_policy_3: 0.54629
	loss_value_3: 0.03774
	loss_reward_3: 0.0065
	loss_policy_4: 0.05011
	accuracy_policy_4: 0.53188
	loss_value_4: 0.03918
	loss_reward_4: 0.00675
	loss_policy_5: 0.05245
	accuracy_policy_5: 0.50988
	loss_value_5: 0.04046
	loss_reward_5: 0.00805
	loss_policy: 0.38184
	loss_value: 0.35521
	loss_reward: 0.03351
[2024-05-05 10:23:54] nn step 34700, lr: 0.015.
	loss_policy_0: 0.11711
	accuracy_policy_0: 0.72652
	loss_value_0: 0.1499
	loss_policy_1: 0.03434
	accuracy_policy_1: 0.63352
	loss_value_1: 0.03156
	loss_reward_1: 0.00572
	loss_policy_2: 0.03855
	accuracy_policy_2: 0.59902
	loss_value_2: 0.03291
	loss_reward_2: 0.00533
	loss_policy_3: 0.04137
	accuracy_policy_3: 0.57539
	loss_value_3: 0.03418
	loss_reward_3: 0.00598
	loss_policy_4: 0.04402
	accuracy_policy_4: 0.55465
	loss_value_4: 0.03552
	loss_reward_4: 0.00615
	loss_policy_5: 0.04617
	accuracy_policy_5: 0.53535
	loss_value_5: 0.03667
	loss_reward_5: 0.00708
	loss_policy: 0.32157
	loss_value: 0.32073
	loss_reward: 0.03027
[2024-05-05 10:24:11] nn step 34750, lr: 0.015.
	loss_policy_0: 0.11478
	accuracy_policy_0: 0.73891
	loss_value_0: 0.15632
	loss_policy_1: 0.03476
	accuracy_policy_1: 0.64312
	loss_value_1: 0.03317
	loss_reward_1: 0.00595
	loss_policy_2: 0.03852
	accuracy_policy_2: 0.60902
	loss_value_2: 0.03464
	loss_reward_2: 0.00556
	loss_policy_3: 0.0422
	accuracy_policy_3: 0.57445
	loss_value_3: 0.03597
	loss_reward_3: 0.0062
	loss_policy_4: 0.0444
	accuracy_policy_4: 0.56414
	loss_value_4: 0.03713
	loss_reward_4: 0.00629
	loss_policy_5: 0.04775
	accuracy_policy_5: 0.53508
	loss_value_5: 0.03808
	loss_reward_5: 0.00745
	loss_policy: 0.32241
	loss_value: 0.33531
	loss_reward: 0.03144
[2024-05-05 10:24:29] nn step 34800, lr: 0.015.
	loss_policy_0: 0.11684
	accuracy_policy_0: 0.74562
	loss_value_0: 0.16449
	loss_policy_1: 0.03626
	accuracy_policy_1: 0.64258
	loss_value_1: 0.03471
	loss_reward_1: 0.00621
	loss_policy_2: 0.03997
	accuracy_policy_2: 0.61375
	loss_value_2: 0.03633
	loss_reward_2: 0.00589
	loss_policy_3: 0.04367
	accuracy_policy_3: 0.58453
	loss_value_3: 0.03767
	loss_reward_3: 0.0064
	loss_policy_4: 0.04683
	accuracy_policy_4: 0.5616
	loss_value_4: 0.03889
	loss_reward_4: 0.00698
	loss_policy_5: 0.04938
	accuracy_policy_5: 0.54609
	loss_value_5: 0.04011
	loss_reward_5: 0.00803
	loss_policy: 0.33296
	loss_value: 0.35219
	loss_reward: 0.0335
Optimization_Done 34800
[2024-05-05 10:26:42] [command] train weight_iter_34800.pkl 173 175
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-05 11:00:24] [command] train weight_iter_34800.pkl 174 175
[2024-05-05 11:00:50] nn step 34850, lr: 0.01.
	loss_policy_0: 0.14729
	accuracy_policy_0: 0.68613
	loss_value_0: 0.1792
	loss_policy_1: 0.03888
	accuracy_policy_1: 0.60805
	loss_value_1: 0.03787
	loss_reward_1: 0.00547
	loss_policy_2: 0.04304
	accuracy_policy_2: 0.57133
	loss_value_2: 0.03952
	loss_reward_2: 0.00549
	loss_policy_3: 0.04715
	accuracy_policy_3: 0.53617
	loss_value_3: 0.04106
	loss_reward_3: 0.00591
	loss_policy_4: 0.04996
	accuracy_policy_4: 0.51723
	loss_value_4: 0.0426
	loss_reward_4: 0.00635
	loss_policy_5: 0.05367
	accuracy_policy_5: 0.48984
	loss_value_5: 0.04408
	loss_reward_5: 0.00732
	loss_policy: 0.37999
	loss_value: 0.38432
	loss_reward: 0.03054
[2024-05-05 11:01:07] nn step 34900, lr: 0.01.
	loss_policy_0: 0.1246
	accuracy_policy_0: 0.70887
	loss_value_0: 0.16637
	loss_policy_1: 0.03438
	accuracy_policy_1: 0.62926
	loss_value_1: 0.03527
	loss_reward_1: 0.00496
	loss_policy_2: 0.03847
	accuracy_policy_2: 0.59363
	loss_value_2: 0.03678
	loss_reward_2: 0.00492
	loss_policy_3: 0.04193
	accuracy_policy_3: 0.56574
	loss_value_3: 0.03849
	loss_reward_3: 0.00539
	loss_policy_4: 0.04468
	accuracy_policy_4: 0.53895
	loss_value_4: 0.03996
	loss_reward_4: 0.00592
	loss_policy_5: 0.04781
	accuracy_policy_5: 0.52008
	loss_value_5: 0.04113
	loss_reward_5: 0.00686
	loss_policy: 0.33187
	loss_value: 0.35801
	loss_reward: 0.02806
[2024-05-05 11:01:24] nn step 34950, lr: 0.01.
	loss_policy_0: 0.12412
	accuracy_policy_0: 0.72188
	loss_value_0: 0.17207
	loss_policy_1: 0.03419
	accuracy_policy_1: 0.63957
	loss_value_1: 0.03643
	loss_reward_1: 0.00526
	loss_policy_2: 0.03849
	accuracy_policy_2: 0.60613
	loss_value_2: 0.03796
	loss_reward_2: 0.00537
	loss_policy_3: 0.04229
	accuracy_policy_3: 0.57535
	loss_value_3: 0.03938
	loss_reward_3: 0.00579
	loss_policy_4: 0.04546
	accuracy_policy_4: 0.55098
	loss_value_4: 0.04093
	loss_reward_4: 0.00618
	loss_policy_5: 0.04773
	accuracy_policy_5: 0.53902
	loss_value_5: 0.04216
	loss_reward_5: 0.00704
	loss_policy: 0.33227
	loss_value: 0.36894
	loss_reward: 0.02964
[2024-05-05 11:01:40] nn step 35000, lr: 0.01.
	loss_policy_0: 0.11941
	accuracy_policy_0: 0.72898
	loss_value_0: 0.17217
	loss_policy_1: 0.03329
	accuracy_policy_1: 0.6452
	loss_value_1: 0.03649
	loss_reward_1: 0.00539
	loss_policy_2: 0.03805
	accuracy_policy_2: 0.61293
	loss_value_2: 0.03826
	loss_reward_2: 0.00547
	loss_policy_3: 0.04163
	accuracy_policy_3: 0.58391
	loss_value_3: 0.0397
	loss_reward_3: 0.00587
	loss_policy_4: 0.04479
	accuracy_policy_4: 0.56152
	loss_value_4: 0.04113
	loss_reward_4: 0.00602
	loss_policy_5: 0.04784
	accuracy_policy_5: 0.53855
	loss_value_5: 0.04256
	loss_reward_5: 0.00679
	loss_policy: 0.32501
	loss_value: 0.37032
	loss_reward: 0.02954
Optimization_Done 35000
[2024-05-05 11:03:49] [command] train weight_iter_35000.pkl 175 176
[2024-05-05 11:04:07] nn step 35050, lr: 0.01.
	loss_policy_0: 0.15507
	accuracy_policy_0: 0.69312
	loss_value_0: 0.15431
	loss_policy_1: 0.03979
	accuracy_policy_1: 0.61172
	loss_value_1: 0.03288
	loss_reward_1: 0.00479
	loss_policy_2: 0.04241
	accuracy_policy_2: 0.59219
	loss_value_2: 0.0346
	loss_reward_2: 0.00463
	loss_policy_3: 0.04508
	accuracy_policy_3: 0.56664
	loss_value_3: 0.03619
	loss_reward_3: 0.0053
	loss_policy_4: 0.048
	accuracy_policy_4: 0.54379
	loss_value_4: 0.03736
	loss_reward_4: 0.0054
	loss_policy_5: 0.05064
	accuracy_policy_5: 0.51891
	loss_value_5: 0.0388
	loss_reward_5: 0.0065
	loss_policy: 0.38098
	loss_value: 0.33413
	loss_reward: 0.02663
[2024-05-05 11:04:24] nn step 35100, lr: 0.01.
	loss_policy_0: 0.11218
	accuracy_policy_0: 0.74785
	loss_value_0: 0.14514
	loss_policy_1: 0.03188
	accuracy_policy_1: 0.66859
	loss_value_1: 0.03102
	loss_reward_1: 0.00481
	loss_policy_2: 0.03539
	accuracy_policy_2: 0.6341
	loss_value_2: 0.03237
	loss_reward_2: 0.00461
	loss_policy_3: 0.03845
	accuracy_policy_3: 0.60695
	loss_value_3: 0.0339
	loss_reward_3: 0.00493
	loss_policy_4: 0.04129
	accuracy_policy_4: 0.58258
	loss_value_4: 0.03517
	loss_reward_4: 0.00507
	loss_policy_5: 0.04414
	accuracy_policy_5: 0.56336
	loss_value_5: 0.03658
	loss_reward_5: 0.00625
	loss_policy: 0.30333
	loss_value: 0.31418
	loss_reward: 0.02567
[2024-05-05 11:04:41] nn step 35150, lr: 0.01.
	loss_policy_0: 0.11265
	accuracy_policy_0: 0.75953
	loss_value_0: 0.15975
	loss_policy_1: 0.03325
	accuracy_policy_1: 0.67465
	loss_value_1: 0.03402
	loss_reward_1: 0.00501
	loss_policy_2: 0.03719
	accuracy_policy_2: 0.64469
	loss_value_2: 0.0357
	loss_reward_2: 0.00493
	loss_policy_3: 0.04077
	accuracy_policy_3: 0.61848
	loss_value_3: 0.03709
	loss_reward_3: 0.00511
	loss_policy_4: 0.04373
	accuracy_policy_4: 0.59445
	loss_value_4: 0.03848
	loss_reward_4: 0.0056
	loss_policy_5: 0.0466
	accuracy_policy_5: 0.57531
	loss_value_5: 0.0399
	loss_reward_5: 0.00663
	loss_policy: 0.31419
	loss_value: 0.34493
	loss_reward: 0.02728
[2024-05-05 11:04:58] nn step 35200, lr: 0.01.
	loss_policy_0: 0.09965
	accuracy_policy_0: 0.76922
	loss_value_0: 0.15307
	loss_policy_1: 0.03068
	accuracy_policy_1: 0.68395
	loss_value_1: 0.0325
	loss_reward_1: 0.00484
	loss_policy_2: 0.03463
	accuracy_policy_2: 0.65059
	loss_value_2: 0.03392
	loss_reward_2: 0.00493
	loss_policy_3: 0.03792
	accuracy_policy_3: 0.62469
	loss_value_3: 0.03554
	loss_reward_3: 0.00515
	loss_policy_4: 0.04105
	accuracy_policy_4: 0.60301
	loss_value_4: 0.03674
	loss_reward_4: 0.00535
	loss_policy_5: 0.04369
	accuracy_policy_5: 0.57973
	loss_value_5: 0.03803
	loss_reward_5: 0.00624
	loss_policy: 0.28762
	loss_value: 0.32981
	loss_reward: 0.02652
Optimization_Done 35200
[2024-05-05 11:07:07] [command] train weight_iter_35200.pkl 176 177
[2024-05-05 11:07:26] nn step 35250, lr: 0.01.
	loss_policy_0: 0.12708
	accuracy_policy_0: 0.69164
	loss_value_0: 0.12617
	loss_policy_1: 0.03549
	accuracy_policy_1: 0.59883
	loss_value_1: 0.02703
	loss_reward_1: 0.0061
	loss_policy_2: 0.03909
	accuracy_policy_2: 0.56391
	loss_value_2: 0.02861
	loss_reward_2: 0.0059
	loss_policy_3: 0.0425
	accuracy_policy_3: 0.53672
	loss_value_3: 0.03004
	loss_reward_3: 0.00639
	loss_policy_4: 0.04574
	accuracy_policy_4: 0.50137
	loss_value_4: 0.03151
	loss_reward_4: 0.00714
	loss_policy_5: 0.04755
	accuracy_policy_5: 0.48168
	loss_value_5: 0.03283
	loss_reward_5: 0.00796
	loss_policy: 0.33746
	loss_value: 0.27619
	loss_reward: 0.03349
[2024-05-05 11:07:42] nn step 35300, lr: 0.01.
	loss_policy_0: 0.10137
	accuracy_policy_0: 0.7427
	loss_value_0: 0.12372
	loss_policy_1: 0.03151
	accuracy_policy_1: 0.64031
	loss_value_1: 0.02662
	loss_reward_1: 0.00595
	loss_policy_2: 0.035
	accuracy_policy_2: 0.60207
	loss_value_2: 0.02786
	loss_reward_2: 0.00557
	loss_policy_3: 0.03824
	accuracy_policy_3: 0.57789
	loss_value_3: 0.02928
	loss_reward_3: 0.00609
	loss_policy_4: 0.04159
	accuracy_policy_4: 0.54277
	loss_value_4: 0.0304
	loss_reward_4: 0.00658
	loss_policy_5: 0.04397
	accuracy_policy_5: 0.51887
	loss_value_5: 0.03167
	loss_reward_5: 0.0076
	loss_policy: 0.29167
	loss_value: 0.26956
	loss_reward: 0.03179
[2024-05-05 11:07:59] nn step 35350, lr: 0.01.
	loss_policy_0: 0.10545
	accuracy_policy_0: 0.76039
	loss_value_0: 0.13991
	loss_policy_1: 0.03457
	accuracy_policy_1: 0.64965
	loss_value_1: 0.03003
	loss_reward_1: 0.00672
	loss_policy_2: 0.03834
	accuracy_policy_2: 0.61793
	loss_value_2: 0.0315
	loss_reward_2: 0.00625
	loss_policy_3: 0.04196
	accuracy_policy_3: 0.58785
	loss_value_3: 0.03322
	loss_reward_3: 0.00697
	loss_policy_4: 0.04538
	accuracy_policy_4: 0.56016
	loss_value_4: 0.03461
	loss_reward_4: 0.00735
	loss_policy_5: 0.04861
	accuracy_policy_5: 0.53598
	loss_value_5: 0.03602
	loss_reward_5: 0.00847
	loss_policy: 0.31431
	loss_value: 0.30529
	loss_reward: 0.03576
[2024-05-05 11:08:16] nn step 35400, lr: 0.01.
	loss_policy_0: 0.09706
	accuracy_policy_0: 0.77344
	loss_value_0: 0.1349
	loss_policy_1: 0.03186
	accuracy_policy_1: 0.65859
	loss_value_1: 0.02894
	loss_reward_1: 0.00639
	loss_policy_2: 0.03576
	accuracy_policy_2: 0.61992
	loss_value_2: 0.03048
	loss_reward_2: 0.00597
	loss_policy_3: 0.03893
	accuracy_policy_3: 0.59648
	loss_value_3: 0.03188
	loss_reward_3: 0.00654
	loss_policy_4: 0.0418
	accuracy_policy_4: 0.57375
	loss_value_4: 0.03302
	loss_reward_4: 0.00695
	loss_policy_5: 0.0446
	accuracy_policy_5: 0.54688
	loss_value_5: 0.03418
	loss_reward_5: 0.00799
	loss_policy: 0.29001
	loss_value: 0.29339
	loss_reward: 0.03384
Optimization_Done 35400
[2024-05-05 11:10:25] [command] train weight_iter_35400.pkl 177 178
[2024-05-05 11:10:43] nn step 35450, lr: 0.01.
	loss_policy_0: 0.10084
	accuracy_policy_0: 0.7159
	loss_value_0: 0.11173
	loss_policy_1: 0.0294
	accuracy_policy_1: 0.60922
	loss_value_1: 0.02371
	loss_reward_1: 0.00546
	loss_policy_2: 0.03214
	accuracy_policy_2: 0.57832
	loss_value_2: 0.02505
	loss_reward_2: 0.005
	loss_policy_3: 0.03445
	accuracy_policy_3: 0.54422
	loss_value_3: 0.02606
	loss_reward_3: 0.00571
	loss_policy_4: 0.03638
	accuracy_policy_4: 0.52203
	loss_value_4: 0.02691
	loss_reward_4: 0.00604
	loss_policy_5: 0.03825
	accuracy_policy_5: 0.50496
	loss_value_5: 0.02774
	loss_reward_5: 0.00697
	loss_policy: 0.27146
	loss_value: 0.2412
	loss_reward: 0.02919
[2024-05-05 11:11:00] nn step 35500, lr: 0.01.
	loss_policy_0: 0.08296
	accuracy_policy_0: 0.76102
	loss_value_0: 0.11714
	loss_policy_1: 0.0276
	accuracy_policy_1: 0.63938
	loss_value_1: 0.02491
	loss_reward_1: 0.00562
	loss_policy_2: 0.03056
	accuracy_policy_2: 0.60891
	loss_value_2: 0.02614
	loss_reward_2: 0.0052
	loss_policy_3: 0.0338
	accuracy_policy_3: 0.57086
	loss_value_3: 0.0271
	loss_reward_3: 0.00563
	loss_policy_4: 0.03563
	accuracy_policy_4: 0.55211
	loss_value_4: 0.02808
	loss_reward_4: 0.00622
	loss_policy_5: 0.03788
	accuracy_policy_5: 0.5282
	loss_value_5: 0.02904
	loss_reward_5: 0.00748
	loss_policy: 0.24843
	loss_value: 0.25241
	loss_reward: 0.03016
[2024-05-05 11:11:17] nn step 35550, lr: 0.01.
	loss_policy_0: 0.07419
	accuracy_policy_0: 0.77926
	loss_value_0: 0.11538
	loss_policy_1: 0.02614
	accuracy_policy_1: 0.64613
	loss_value_1: 0.02423
	loss_reward_1: 0.00538
	loss_policy_2: 0.02899
	accuracy_policy_2: 0.61172
	loss_value_2: 0.02548
	loss_reward_2: 0.00496
	loss_policy_3: 0.03175
	accuracy_policy_3: 0.58188
	loss_value_3: 0.02638
	loss_reward_3: 0.00554
	loss_policy_4: 0.03412
	accuracy_policy_4: 0.55844
	loss_value_4: 0.02725
	loss_reward_4: 0.00608
	loss_policy_5: 0.03591
	accuracy_policy_5: 0.54293
	loss_value_5: 0.02819
	loss_reward_5: 0.00709
	loss_policy: 0.2311
	loss_value: 0.2469
	loss_reward: 0.02906
[2024-05-05 11:11:34] nn step 35600, lr: 0.01.
	loss_policy_0: 0.06874
	accuracy_policy_0: 0.78871
	loss_value_0: 0.11769
	loss_policy_1: 0.02547
	accuracy_policy_1: 0.65336
	loss_value_1: 0.02488
	loss_reward_1: 0.00525
	loss_policy_2: 0.02836
	accuracy_policy_2: 0.61789
	loss_value_2: 0.0259
	loss_reward_2: 0.00483
	loss_policy_3: 0.03126
	accuracy_policy_3: 0.58668
	loss_value_3: 0.02695
	loss_reward_3: 0.00544
	loss_policy_4: 0.0335
	accuracy_policy_4: 0.55871
	loss_value_4: 0.02776
	loss_reward_4: 0.00574
	loss_policy_5: 0.03547
	accuracy_policy_5: 0.54539
	loss_value_5: 0.02882
	loss_reward_5: 0.00694
	loss_policy: 0.2228
	loss_value: 0.252
	loss_reward: 0.02821
Optimization_Done 35600
[2024-05-05 11:13:43] [command] train weight_iter_35600.pkl 178 179
[2024-05-05 11:14:01] nn step 35650, lr: 0.01.
	loss_policy_0: 0.11046
	accuracy_policy_0: 0.66457
	loss_value_0: 0.11355
	loss_policy_1: 0.02725
	accuracy_policy_1: 0.60602
	loss_value_1: 0.02405
	loss_reward_1: 0.00336
	loss_policy_2: 0.02934
	accuracy_policy_2: 0.58043
	loss_value_2: 0.02539
	loss_reward_2: 0.00332
	loss_policy_3: 0.03122
	accuracy_policy_3: 0.55879
	loss_value_3: 0.02671
	loss_reward_3: 0.00364
	loss_policy_4: 0.03287
	accuracy_policy_4: 0.5348
	loss_value_4: 0.02778
	loss_reward_4: 0.00399
	loss_policy_5: 0.03473
	accuracy_policy_5: 0.51664
	loss_value_5: 0.0289
	loss_reward_5: 0.00453
	loss_policy: 0.26588
	loss_value: 0.24637
	loss_reward: 0.01884
[2024-05-05 11:14:18] nn step 35700, lr: 0.01.
	loss_policy_0: 0.08819
	accuracy_policy_0: 0.7293
	loss_value_0: 0.11699
	loss_policy_1: 0.02534
	accuracy_policy_1: 0.64645
	loss_value_1: 0.02489
	loss_reward_1: 0.00329
	loss_policy_2: 0.02735
	accuracy_policy_2: 0.62371
	loss_value_2: 0.0261
	loss_reward_2: 0.00331
	loss_policy_3: 0.02951
	accuracy_policy_3: 0.605
	loss_value_3: 0.0272
	loss_reward_3: 0.0036
	loss_policy_4: 0.03133
	accuracy_policy_4: 0.58363
	loss_value_4: 0.02816
	loss_reward_4: 0.00384
	loss_policy_5: 0.03287
	accuracy_policy_5: 0.56523
	loss_value_5: 0.02908
	loss_reward_5: 0.00434
	loss_policy: 0.23459
	loss_value: 0.25242
	loss_reward: 0.01837
[2024-05-05 11:14:35] nn step 35750, lr: 0.01.
	loss_policy_0: 0.08697
	accuracy_policy_0: 0.75156
	loss_value_0: 0.12769
	loss_policy_1: 0.02599
	accuracy_policy_1: 0.66008
	loss_value_1: 0.02702
	loss_reward_1: 0.00361
	loss_policy_2: 0.02858
	accuracy_policy_2: 0.63238
	loss_value_2: 0.02842
	loss_reward_2: 0.0036
	loss_policy_3: 0.03055
	accuracy_policy_3: 0.61586
	loss_value_3: 0.02948
	loss_reward_3: 0.00361
	loss_policy_4: 0.03274
	accuracy_policy_4: 0.58688
	loss_value_4: 0.03055
	loss_reward_4: 0.00394
	loss_policy_5: 0.03412
	accuracy_policy_5: 0.57336
	loss_value_5: 0.03163
	loss_reward_5: 0.00449
	loss_policy: 0.23896
	loss_value: 0.27479
	loss_reward: 0.01924
[2024-05-05 11:14:52] nn step 35800, lr: 0.01.
	loss_policy_0: 0.07818
	accuracy_policy_0: 0.77301
	loss_value_0: 0.12738
	loss_policy_1: 0.02484
	accuracy_policy_1: 0.66613
	loss_value_1: 0.02687
	loss_reward_1: 0.00345
	loss_policy_2: 0.02726
	accuracy_policy_2: 0.64141
	loss_value_2: 0.02812
	loss_reward_2: 0.00335
	loss_policy_3: 0.02929
	accuracy_policy_3: 0.6143
	loss_value_3: 0.02923
	loss_reward_3: 0.00366
	loss_policy_4: 0.03116
	accuracy_policy_4: 0.59777
	loss_value_4: 0.03015
	loss_reward_4: 0.0039
	loss_policy_5: 0.03353
	accuracy_policy_5: 0.58109
	loss_value_5: 0.03106
	loss_reward_5: 0.00453
	loss_policy: 0.22425
	loss_value: 0.27282
	loss_reward: 0.01889
Optimization_Done 35800
[2024-05-05 11:17:01] [command] train weight_iter_35800.pkl 179 180
[2024-05-05 11:17:20] nn step 35850, lr: 0.01.
	loss_policy_0: 0.15049
	accuracy_policy_0: 0.65211
	loss_value_0: 0.14826
	loss_policy_1: 0.03837
	accuracy_policy_1: 0.5793
	loss_value_1: 0.0315
	loss_reward_1: 0.00532
	loss_policy_2: 0.04052
	accuracy_policy_2: 0.56488
	loss_value_2: 0.03296
	loss_reward_2: 0.0053
	loss_policy_3: 0.04338
	accuracy_policy_3: 0.53562
	loss_value_3: 0.03444
	loss_reward_3: 0.00565
	loss_policy_4: 0.04593
	accuracy_policy_4: 0.52086
	loss_value_4: 0.0357
	loss_reward_4: 0.00587
	loss_policy_5: 0.04774
	accuracy_policy_5: 0.50957
	loss_value_5: 0.03702
	loss_reward_5: 0.007
	loss_policy: 0.36642
	loss_value: 0.31988
	loss_reward: 0.02914
[2024-05-05 11:17:37] nn step 35900, lr: 0.01.
	loss_policy_0: 0.12967
	accuracy_policy_0: 0.71297
	loss_value_0: 0.1564
	loss_policy_1: 0.03671
	accuracy_policy_1: 0.62621
	loss_value_1: 0.03314
	loss_reward_1: 0.0058
	loss_policy_2: 0.03984
	accuracy_policy_2: 0.60309
	loss_value_2: 0.03477
	loss_reward_2: 0.00552
	loss_policy_3: 0.04307
	accuracy_policy_3: 0.57738
	loss_value_3: 0.03622
	loss_reward_3: 0.00606
	loss_policy_4: 0.04547
	accuracy_policy_4: 0.5559
	loss_value_4: 0.03752
	loss_reward_4: 0.0064
	loss_policy_5: 0.04817
	accuracy_policy_5: 0.53785
	loss_value_5: 0.03875
	loss_reward_5: 0.00719
	loss_policy: 0.34293
	loss_value: 0.3368
	loss_reward: 0.03097
[2024-05-05 11:17:54] nn step 35950, lr: 0.01.
	loss_policy_0: 0.10931
	accuracy_policy_0: 0.73754
	loss_value_0: 0.14711
	loss_policy_1: 0.03233
	accuracy_policy_1: 0.64699
	loss_value_1: 0.03127
	loss_reward_1: 0.00525
	loss_policy_2: 0.03596
	accuracy_policy_2: 0.61223
	loss_value_2: 0.03287
	loss_reward_2: 0.00509
	loss_policy_3: 0.03822
	accuracy_policy_3: 0.59309
	loss_value_3: 0.03401
	loss_reward_3: 0.00529
	loss_policy_4: 0.04127
	accuracy_policy_4: 0.56316
	loss_value_4: 0.03501
	loss_reward_4: 0.00571
	loss_policy_5: 0.04391
	accuracy_policy_5: 0.5484
	loss_value_5: 0.03656
	loss_reward_5: 0.00685
	loss_policy: 0.30099
	loss_value: 0.31683
	loss_reward: 0.02819
[2024-05-05 11:18:12] nn step 36000, lr: 0.01.
	loss_policy_0: 0.10184
	accuracy_policy_0: 0.745
	loss_value_0: 0.14315
	loss_policy_1: 0.0308
	accuracy_policy_1: 0.65234
	loss_value_1: 0.03046
	loss_reward_1: 0.00527
	loss_policy_2: 0.03411
	accuracy_policy_2: 0.62199
	loss_value_2: 0.03201
	loss_reward_2: 0.00498
	loss_policy_3: 0.03676
	accuracy_policy_3: 0.59344
	loss_value_3: 0.03312
	loss_reward_3: 0.00527
	loss_policy_4: 0.03934
	accuracy_policy_4: 0.57273
	loss_value_4: 0.03427
	loss_reward_4: 0.00575
	loss_policy_5: 0.04199
	accuracy_policy_5: 0.55289
	loss_value_5: 0.03526
	loss_reward_5: 0.00659
	loss_policy: 0.28484
	loss_value: 0.30825
	loss_reward: 0.02787
Optimization_Done 36000
[2024-05-05 11:20:21] [command] train weight_iter_36000.pkl 180 181
[2024-05-05 11:20:40] nn step 36050, lr: 0.01.
	loss_policy_0: 0.15596
	accuracy_policy_0: 0.68441
	loss_value_0: 0.1661
	loss_policy_1: 0.0416
	accuracy_policy_1: 0.60539
	loss_value_1: 0.03493
	loss_reward_1: 0.00728
	loss_policy_2: 0.04567
	accuracy_policy_2: 0.57715
	loss_value_2: 0.03683
	loss_reward_2: 0.00719
	loss_policy_3: 0.04901
	accuracy_policy_3: 0.55367
	loss_value_3: 0.0387
	loss_reward_3: 0.00789
	loss_policy_4: 0.05282
	accuracy_policy_4: 0.52461
	loss_value_4: 0.04021
	loss_reward_4: 0.00825
	loss_policy_5: 0.05576
	accuracy_policy_5: 0.50363
	loss_value_5: 0.04183
	loss_reward_5: 0.00948
	loss_policy: 0.40083
	loss_value: 0.35858
	loss_reward: 0.0401
[2024-05-05 11:20:57] nn step 36100, lr: 0.01.
	loss_policy_0: 0.11735
	accuracy_policy_0: 0.735
	loss_value_0: 0.15026
	loss_policy_1: 0.03583
	accuracy_policy_1: 0.63309
	loss_value_1: 0.03172
	loss_reward_1: 0.00666
	loss_policy_2: 0.03925
	accuracy_policy_2: 0.60496
	loss_value_2: 0.03363
	loss_reward_2: 0.00644
	loss_policy_3: 0.04262
	accuracy_policy_3: 0.58203
	loss_value_3: 0.03503
	loss_reward_3: 0.00722
	loss_policy_4: 0.04587
	accuracy_policy_4: 0.55414
	loss_value_4: 0.03636
	loss_reward_4: 0.00741
	loss_policy_5: 0.04847
	accuracy_policy_5: 0.5398
	loss_value_5: 0.03771
	loss_reward_5: 0.00873
	loss_policy: 0.32939
	loss_value: 0.32472
	loss_reward: 0.03646
[2024-05-05 11:21:14] nn step 36150, lr: 0.01.
	loss_policy_0: 0.11278
	accuracy_policy_0: 0.74777
	loss_value_0: 0.15038
	loss_policy_1: 0.03471
	accuracy_policy_1: 0.6466
	loss_value_1: 0.03197
	loss_reward_1: 0.00658
	loss_policy_2: 0.03864
	accuracy_policy_2: 0.61879
	loss_value_2: 0.03344
	loss_reward_2: 0.0063
	loss_policy_3: 0.04204
	accuracy_policy_3: 0.59227
	loss_value_3: 0.03479
	loss_reward_3: 0.00704
	loss_policy_4: 0.04502
	accuracy_policy_4: 0.56551
	loss_value_4: 0.03605
	loss_reward_4: 0.00742
	loss_policy_5: 0.04846
	accuracy_policy_5: 0.54609
	loss_value_5: 0.03734
	loss_reward_5: 0.00854
	loss_policy: 0.32165
	loss_value: 0.32396
	loss_reward: 0.03589
[2024-05-05 11:21:31] nn step 36200, lr: 0.01.
	loss_policy_0: 0.1065
	accuracy_policy_0: 0.75855
	loss_value_0: 0.15335
	loss_policy_1: 0.03409
	accuracy_policy_1: 0.64836
	loss_value_1: 0.03232
	loss_reward_1: 0.00659
	loss_policy_2: 0.03781
	accuracy_policy_2: 0.62211
	loss_value_2: 0.03367
	loss_reward_2: 0.00665
	loss_policy_3: 0.0413
	accuracy_policy_3: 0.59926
	loss_value_3: 0.03528
	loss_reward_3: 0.00673
	loss_policy_4: 0.04414
	accuracy_policy_4: 0.57645
	loss_value_4: 0.03674
	loss_reward_4: 0.00729
	loss_policy_5: 0.04763
	accuracy_policy_5: 0.55742
	loss_value_5: 0.03803
	loss_reward_5: 0.00883
	loss_policy: 0.31147
	loss_value: 0.32939
	loss_reward: 0.0361
Optimization_Done 36200
[2024-05-05 11:23:39] [command] train weight_iter_36200.pkl 181 182
[2024-05-05 11:23:58] nn step 36250, lr: 0.01.
	loss_policy_0: 0.15218
	accuracy_policy_0: 0.68699
	loss_value_0: 0.16134
	loss_policy_1: 0.0416
	accuracy_policy_1: 0.59773
	loss_value_1: 0.03414
	loss_reward_1: 0.00724
	loss_policy_2: 0.04548
	accuracy_policy_2: 0.56949
	loss_value_2: 0.03566
	loss_reward_2: 0.00677
	loss_policy_3: 0.049
	accuracy_policy_3: 0.53543
	loss_value_3: 0.03713
	loss_reward_3: 0.00762
	loss_policy_4: 0.05122
	accuracy_policy_4: 0.51488
	loss_value_4: 0.03859
	loss_reward_4: 0.00795
	loss_policy_5: 0.05393
	accuracy_policy_5: 0.49172
	loss_value_5: 0.03991
	loss_reward_5: 0.0093
	loss_policy: 0.39341
	loss_value: 0.34677
	loss_reward: 0.03889
[2024-05-05 11:24:15] nn step 36300, lr: 0.01.
	loss_policy_0: 0.11869
	accuracy_policy_0: 0.73383
	loss_value_0: 0.15302
	loss_policy_1: 0.03719
	accuracy_policy_1: 0.62914
	loss_value_1: 0.03226
	loss_reward_1: 0.00704
	loss_policy_2: 0.04091
	accuracy_policy_2: 0.60031
	loss_value_2: 0.03389
	loss_reward_2: 0.00657
	loss_policy_3: 0.04435
	accuracy_policy_3: 0.56684
	loss_value_3: 0.03524
	loss_reward_3: 0.0071
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.55281
	loss_value_4: 0.03651
	loss_reward_4: 0.00764
	loss_policy_5: 0.04894
	accuracy_policy_5: 0.53422
	loss_value_5: 0.03784
	loss_reward_5: 0.00875
	loss_policy: 0.33678
	loss_value: 0.32875
	loss_reward: 0.0371
[2024-05-05 11:24:32] nn step 36350, lr: 0.01.
	loss_policy_0: 0.1094
	accuracy_policy_0: 0.75664
	loss_value_0: 0.15366
	loss_policy_1: 0.03608
	accuracy_policy_1: 0.63887
	loss_value_1: 0.03239
	loss_reward_1: 0.00696
	loss_policy_2: 0.03993
	accuracy_policy_2: 0.61215
	loss_value_2: 0.03374
	loss_reward_2: 0.00656
	loss_policy_3: 0.04344
	accuracy_policy_3: 0.58082
	loss_value_3: 0.0351
	loss_reward_3: 0.00717
	loss_policy_4: 0.04667
	accuracy_policy_4: 0.55301
	loss_value_4: 0.03629
	loss_reward_4: 0.00768
	loss_policy_5: 0.0488
	accuracy_policy_5: 0.5423
	loss_value_5: 0.03754
	loss_reward_5: 0.00892
	loss_policy: 0.32432
	loss_value: 0.32871
	loss_reward: 0.03728
[2024-05-05 11:24:50] nn step 36400, lr: 0.01.
	loss_policy_0: 0.10312
	accuracy_policy_0: 0.76055
	loss_value_0: 0.15187
	loss_policy_1: 0.03411
	accuracy_policy_1: 0.65133
	loss_value_1: 0.03207
	loss_reward_1: 0.0068
	loss_policy_2: 0.03854
	accuracy_policy_2: 0.61777
	loss_value_2: 0.03321
	loss_reward_2: 0.00647
	loss_policy_3: 0.04155
	accuracy_policy_3: 0.58582
	loss_value_3: 0.03441
	loss_reward_3: 0.00699
	loss_policy_4: 0.04462
	accuracy_policy_4: 0.56621
	loss_value_4: 0.03566
	loss_reward_4: 0.00739
	loss_policy_5: 0.04734
	accuracy_policy_5: 0.54754
	loss_value_5: 0.03687
	loss_reward_5: 0.00887
	loss_policy: 0.30929
	loss_value: 0.3241
	loss_reward: 0.03652
Optimization_Done 36400
[2024-05-05 11:26:37] [command] train weight_iter_36400.pkl 182 183
[2024-05-05 11:26:55] nn step 36450, lr: 0.01.
	loss_policy_0: 0.12775
	accuracy_policy_0: 0.72824
	loss_value_0: 0.15583
	loss_policy_1: 0.03561
	accuracy_policy_1: 0.64844
	loss_value_1: 0.03273
	loss_reward_1: 0.00601
	loss_policy_2: 0.0391
	accuracy_policy_2: 0.6191
	loss_value_2: 0.03412
	loss_reward_2: 0.00569
	loss_policy_3: 0.04257
	accuracy_policy_3: 0.58602
	loss_value_3: 0.03529
	loss_reward_3: 0.00624
	loss_policy_4: 0.04443
	accuracy_policy_4: 0.56746
	loss_value_4: 0.03615
	loss_reward_4: 0.00677
	loss_policy_5: 0.04639
	accuracy_policy_5: 0.54965
	loss_value_5: 0.03719
	loss_reward_5: 0.00785
	loss_policy: 0.33585
	loss_value: 0.33131
	loss_reward: 0.03256
[2024-05-05 11:27:13] nn step 36500, lr: 0.01.
	loss_policy_0: 0.10816
	accuracy_policy_0: 0.77766
	loss_value_0: 0.1646
	loss_policy_1: 0.03461
	accuracy_policy_1: 0.67586
	loss_value_1: 0.03485
	loss_reward_1: 0.00646
	loss_policy_2: 0.03798
	accuracy_policy_2: 0.64957
	loss_value_2: 0.03594
	loss_reward_2: 0.00637
	loss_policy_3: 0.04205
	accuracy_policy_3: 0.61559
	loss_value_3: 0.03715
	loss_reward_3: 0.00696
	loss_policy_4: 0.04413
	accuracy_policy_4: 0.59594
	loss_value_4: 0.03831
	loss_reward_4: 0.00714
	loss_policy_5: 0.04618
	accuracy_policy_5: 0.57871
	loss_value_5: 0.03948
	loss_reward_5: 0.00827
	loss_policy: 0.3131
	loss_value: 0.35032
	loss_reward: 0.0352
[2024-05-05 11:27:30] nn step 36550, lr: 0.01.
	loss_policy_0: 0.09575
	accuracy_policy_0: 0.78453
	loss_value_0: 0.15415
	loss_policy_1: 0.03201
	accuracy_policy_1: 0.67656
	loss_value_1: 0.03262
	loss_reward_1: 0.00617
	loss_policy_2: 0.03555
	accuracy_policy_2: 0.64309
	loss_value_2: 0.03369
	loss_reward_2: 0.00589
	loss_policy_3: 0.03772
	accuracy_policy_3: 0.62086
	loss_value_3: 0.03476
	loss_reward_3: 0.00644
	loss_policy_4: 0.04024
	accuracy_policy_4: 0.60105
	loss_value_4: 0.03561
	loss_reward_4: 0.00669
	loss_policy_5: 0.04301
	accuracy_policy_5: 0.57281
	loss_value_5: 0.03661
	loss_reward_5: 0.00792
	loss_policy: 0.28429
	loss_value: 0.32743
	loss_reward: 0.03311
[2024-05-05 11:27:47] nn step 36600, lr: 0.01.
	loss_policy_0: 0.09334
	accuracy_policy_0: 0.79078
	loss_value_0: 0.15718
	loss_policy_1: 0.03217
	accuracy_policy_1: 0.67797
	loss_value_1: 0.0329
	loss_reward_1: 0.00611
	loss_policy_2: 0.03537
	accuracy_policy_2: 0.64887
	loss_value_2: 0.03403
	loss_reward_2: 0.00596
	loss_policy_3: 0.03834
	accuracy_policy_3: 0.62148
	loss_value_3: 0.03485
	loss_reward_3: 0.00663
	loss_policy_4: 0.04079
	accuracy_policy_4: 0.6041
	loss_value_4: 0.0359
	loss_reward_4: 0.00701
	loss_policy_5: 0.04287
	accuracy_policy_5: 0.58805
	loss_value_5: 0.03705
	loss_reward_5: 0.00817
	loss_policy: 0.28289
	loss_value: 0.33191
	loss_reward: 0.03387
Optimization_Done 36600
[2024-05-05 11:29:58] [command] train weight_iter_36600.pkl 183 184
[2024-05-05 11:30:17] nn step 36650, lr: 0.01.
	loss_policy_0: 0.15226
	accuracy_policy_0: 0.7043
	loss_value_0: 0.16711
	loss_policy_1: 0.03917
	accuracy_policy_1: 0.63633
	loss_value_1: 0.03547
	loss_reward_1: 0.00582
	loss_policy_2: 0.04238
	accuracy_policy_2: 0.60906
	loss_value_2: 0.03698
	loss_reward_2: 0.00572
	loss_policy_3: 0.04591
	accuracy_policy_3: 0.57898
	loss_value_3: 0.03845
	loss_reward_3: 0.00637
	loss_policy_4: 0.04881
	accuracy_policy_4: 0.54937
	loss_value_4: 0.03957
	loss_reward_4: 0.00673
	loss_policy_5: 0.05165
	accuracy_policy_5: 0.52535
	loss_value_5: 0.04053
	loss_reward_5: 0.0078
	loss_policy: 0.38019
	loss_value: 0.35811
	loss_reward: 0.03245
[2024-05-05 11:30:34] nn step 36700, lr: 0.01.
	loss_policy_0: 0.11765
	accuracy_policy_0: 0.76348
	loss_value_0: 0.16556
	loss_policy_1: 0.03551
	accuracy_policy_1: 0.67168
	loss_value_1: 0.03499
	loss_reward_1: 0.00596
	loss_policy_2: 0.03893
	accuracy_policy_2: 0.64098
	loss_value_2: 0.03655
	loss_reward_2: 0.00578
	loss_policy_3: 0.04222
	accuracy_policy_3: 0.62121
	loss_value_3: 0.03792
	loss_reward_3: 0.0064
	loss_policy_4: 0.04513
	accuracy_policy_4: 0.60125
	loss_value_4: 0.03928
	loss_reward_4: 0.00688
	loss_policy_5: 0.04792
	accuracy_policy_5: 0.57508
	loss_value_5: 0.04069
	loss_reward_5: 0.00794
	loss_policy: 0.32735
	loss_value: 0.35499
	loss_reward: 0.03295
[2024-05-05 11:30:52] nn step 36750, lr: 0.01.
	loss_policy_0: 0.10493
	accuracy_policy_0: 0.78078
	loss_value_0: 0.15757
	loss_policy_1: 0.0326
	accuracy_policy_1: 0.68746
	loss_value_1: 0.03325
	loss_reward_1: 0.0056
	loss_policy_2: 0.03611
	accuracy_policy_2: 0.65953
	loss_value_2: 0.03476
	loss_reward_2: 0.00553
	loss_policy_3: 0.03959
	accuracy_policy_3: 0.63355
	loss_value_3: 0.03596
	loss_reward_3: 0.00631
	loss_policy_4: 0.04197
	accuracy_policy_4: 0.60852
	loss_value_4: 0.03703
	loss_reward_4: 0.00663
	loss_policy_5: 0.04502
	accuracy_policy_5: 0.58504
	loss_value_5: 0.03822
	loss_reward_5: 0.0077
	loss_policy: 0.30023
	loss_value: 0.33678
	loss_reward: 0.03177
[2024-05-05 11:31:09] nn step 36800, lr: 0.01.
	loss_policy_0: 0.09722
	accuracy_policy_0: 0.78969
	loss_value_0: 0.15696
	loss_policy_1: 0.03195
	accuracy_policy_1: 0.68789
	loss_value_1: 0.03314
	loss_reward_1: 0.00569
	loss_policy_2: 0.0356
	accuracy_policy_2: 0.65719
	loss_value_2: 0.03447
	loss_reward_2: 0.00554
	loss_policy_3: 0.03903
	accuracy_policy_3: 0.62891
	loss_value_3: 0.03554
	loss_reward_3: 0.00616
	loss_policy_4: 0.04164
	accuracy_policy_4: 0.60957
	loss_value_4: 0.03661
	loss_reward_4: 0.00656
	loss_policy_5: 0.04378
	accuracy_policy_5: 0.58715
	loss_value_5: 0.03769
	loss_reward_5: 0.0076
	loss_policy: 0.28922
	loss_value: 0.33442
	loss_reward: 0.03155
Optimization_Done 36800
A.L.E: Arcade Learning Environment (version 0.8.0+d59d006)
[Powered by Stella]
[2024-05-05 11:49:01] [command] train weight_iter_36800.pkl 184 185
[2024-05-05 11:49:25] nn step 36850, lr: 0.005.
	loss_policy_0: 0.15139
	accuracy_policy_0: 0.71543
	loss_value_0: 0.17997
	loss_policy_1: 0.04214
	accuracy_policy_1: 0.62906
	loss_value_1: 0.03782
	loss_reward_1: 0.00622
	loss_policy_2: 0.04598
	accuracy_policy_2: 0.59996
	loss_value_2: 0.03966
	loss_reward_2: 0.00606
	loss_policy_3: 0.05008
	accuracy_policy_3: 0.56426
	loss_value_3: 0.04118
	loss_reward_3: 0.00663
	loss_policy_4: 0.05381
	accuracy_policy_4: 0.54422
	loss_value_4: 0.04257
	loss_reward_4: 0.00721
	loss_policy_5: 0.05716
	accuracy_policy_5: 0.51746
	loss_value_5: 0.04398
	loss_reward_5: 0.00824
	loss_policy: 0.40057
	loss_value: 0.38518
	loss_reward: 0.03436
[2024-05-05 11:49:40] nn step 36900, lr: 0.005.
	loss_policy_0: 0.1226
	accuracy_policy_0: 0.74539
	loss_value_0: 0.16091
	loss_policy_1: 0.03714
	accuracy_policy_1: 0.64789
	loss_value_1: 0.03408
	loss_reward_1: 0.00578
	loss_policy_2: 0.04089
	accuracy_policy_2: 0.61535
	loss_value_2: 0.03551
	loss_reward_2: 0.00558
	loss_policy_3: 0.04452
	accuracy_policy_3: 0.59648
	loss_value_3: 0.03712
	loss_reward_3: 0.00628
	loss_policy_4: 0.04779
	accuracy_policy_4: 0.56852
	loss_value_4: 0.03845
	loss_reward_4: 0.00673
	loss_policy_5: 0.0507
	accuracy_policy_5: 0.54621
	loss_value_5: 0.03963
	loss_reward_5: 0.00738
	loss_policy: 0.34364
	loss_value: 0.3457
	loss_reward: 0.03175
[2024-05-05 11:49:56] nn step 36950, lr: 0.005.
	loss_policy_0: 0.12435
	accuracy_policy_0: 0.75184
	loss_value_0: 0.16738
	loss_policy_1: 0.03805
	accuracy_policy_1: 0.64695
	loss_value_1: 0.03572
	loss_reward_1: 0.00622
	loss_policy_2: 0.04235
	accuracy_policy_2: 0.62465
	loss_value_2: 0.03713
	loss_reward_2: 0.0059
	loss_policy_3: 0.04598
	accuracy_policy_3: 0.59688
	loss_value_3: 0.03852
	loss_reward_3: 0.00656
	loss_policy_4: 0.04962
	accuracy_policy_4: 0.57176
	loss_value_4: 0.03976
	loss_reward_4: 0.00711
	loss_policy_5: 0.05309
	accuracy_policy_5: 0.54488
	loss_value_5: 0.04096
	loss_reward_5: 0.00816
	loss_policy: 0.35343
	loss_value: 0.35947
	loss_reward: 0.03394
[2024-05-05 11:50:11] nn step 37000, lr: 0.005.
	loss_policy_0: 0.12454
	accuracy_policy_0: 0.75246
	loss_value_0: 0.16948
	loss_policy_1: 0.03803
	accuracy_policy_1: 0.65578
	loss_value_1: 0.03602
	loss_reward_1: 0.00624
	loss_policy_2: 0.04247
	accuracy_policy_2: 0.62453
	loss_value_2: 0.03765
	loss_reward_2: 0.00603
	loss_policy_3: 0.04651
	accuracy_policy_3: 0.60078
	loss_value_3: 0.03907
	loss_reward_3: 0.00654
	loss_policy_4: 0.04953
	accuracy_policy_4: 0.57461
	loss_value_4: 0.04042
	loss_reward_4: 0.00705
	loss_policy_5: 0.05324
	accuracy_policy_5: 0.55156
	loss_value_5: 0.04152
	loss_reward_5: 0.00869
	loss_policy: 0.35432
	loss_value: 0.36416
	loss_reward: 0.03456
Optimization_Done 37000
[2024-05-05 11:52:15] [command] train weight_iter_37000.pkl 185 186
[2024-05-05 11:52:31] nn step 37050, lr: 0.005.
	loss_policy_0: 0.14637
	accuracy_policy_0: 0.69961
	loss_value_0: 0.15238
	loss_policy_1: 0.03909
	accuracy_policy_1: 0.60977
	loss_value_1: 0.03238
	loss_reward_1: 0.00482
	loss_policy_2: 0.0426
	accuracy_policy_2: 0.58746
	loss_value_2: 0.03394
	loss_reward_2: 0.00482
	loss_policy_3: 0.04567
	accuracy_policy_3: 0.56102
	loss_value_3: 0.0354
	loss_reward_3: 0.0052
	loss_policy_4: 0.04916
	accuracy_policy_4: 0.53258
	loss_value_4: 0.03675
	loss_reward_4: 0.00574
	loss_policy_5: 0.05211
	accuracy_policy_5: 0.51191
	loss_value_5: 0.03801
	loss_reward_5: 0.00659
	loss_policy: 0.375
	loss_value: 0.32886
	loss_reward: 0.02717
[2024-05-05 11:52:47] nn step 37100, lr: 0.005.
	loss_policy_0: 0.11311
	accuracy_policy_0: 0.74195
	loss_value_0: 0.14491
	loss_policy_1: 0.03363
	accuracy_policy_1: 0.65316
	loss_value_1: 0.03067
	loss_reward_1: 0.00455
	loss_policy_2: 0.03741
	accuracy_policy_2: 0.62371
	loss_value_2: 0.03212
	loss_reward_2: 0.0045
	loss_policy_3: 0.04105
	accuracy_policy_3: 0.5925
	loss_value_3: 0.03324
	loss_reward_3: 0.00505
	loss_policy_4: 0.04438
	accuracy_policy_4: 0.57113
	loss_value_4: 0.03455
	loss_reward_4: 0.00545
	loss_policy_5: 0.04741
	accuracy_policy_5: 0.54828
	loss_value_5: 0.03581
	loss_reward_5: 0.00661
	loss_policy: 0.317
	loss_value: 0.3113
	loss_reward: 0.02616
[2024-05-05 11:53:02] nn step 37150, lr: 0.005.
	loss_policy_0: 0.1072
	accuracy_policy_0: 0.75719
	loss_value_0: 0.14652
	loss_policy_1: 0.03386
	accuracy_policy_1: 0.65652
	loss_value_1: 0.03126
	loss_reward_1: 0.00476
	loss_policy_2: 0.03716
	accuracy_policy_2: 0.62508
	loss_value_2: 0.03269
	loss_reward_2: 0.0045
	loss_policy_3: 0.04061
	accuracy_policy_3: 0.60238
	loss_value_3: 0.03396
	loss_reward_3: 0.00522
	loss_policy_4: 0.04412
	accuracy_policy_4: 0.58039
	loss_value_4: 0.03524
	loss_reward_4: 0.00564
	loss_policy_5: 0.04731
	accuracy_policy_5: 0.55457
	loss_value_5: 0.03649
	loss_reward_5: 0.00662
	loss_policy: 0.31027
	loss_value: 0.31615
	loss_reward: 0.02674
[2024-05-05 11:53:18] nn step 37200, lr: 0.005.
	loss_policy_0: 0.10564
	accuracy_policy_0: 0.77508
	loss_value_0: 0.15632
	loss_policy_1: 0.03449
	accuracy_policy_1: 0.66934
	loss_value_1: 0.03332
	loss_reward_1: 0.00511
	loss_policy_2: 0.03848
	accuracy_policy_2: 0.63566
	loss_value_2: 0.03472
	loss_reward_2: 0.00491
	loss_policy_3: 0.04277
	accuracy_policy_3: 0.60961
	loss_value_3: 0.03635
	loss_reward_3: 0.00557
	loss_policy_4: 0.04577
	accuracy_policy_4: 0.58621
	loss_value_4: 0.03784
	loss_reward_4: 0.00619
	loss_policy_5: 0.04984
	accuracy_policy_5: 0.55559
	loss_value_5: 0.03918
	loss_reward_5: 0.0072
	loss_policy: 0.31699
	loss_value: 0.33773
	loss_reward: 0.02898
Optimization_Done 37200
[2024-05-05 11:55:13] [command] train weight_iter_37200.pkl 186 187
[2024-05-05 11:55:30] nn step 37250, lr: 0.005.
	loss_policy_0: 0.13826
	accuracy_policy_0: 0.68668
	loss_value_0: 0.14388
	loss_policy_1: 0.03634
	accuracy_policy_1: 0.60934
	loss_value_1: 0.03037
	loss_reward_1: 0.00528
	loss_policy_2: 0.03931
	accuracy_policy_2: 0.58773
	loss_value_2: 0.03187
	loss_reward_2: 0.0052
	loss_policy_3: 0.04284
	accuracy_policy_3: 0.55461
	loss_value_3: 0.03323
	loss_reward_3: 0.00549
	loss_policy_4: 0.04609
	accuracy_policy_4: 0.52809
	loss_value_4: 0.0345
	loss_reward_4: 0.00599
	loss_policy_5: 0.04927
	accuracy_policy_5: 0.50723
	loss_value_5: 0.03581
	loss_reward_5: 0.00697
	loss_policy: 0.35211
	loss_value: 0.30965
	loss_reward: 0.02893
[2024-05-05 11:55:46] nn step 37300, lr: 0.005.
	loss_policy_0: 0.11506
	accuracy_policy_0: 0.7382
	loss_value_0: 0.15018
	loss_policy_1: 0.03454
	accuracy_policy_1: 0.64914
	loss_value_1: 0.03172
	loss_reward_1: 0.00568
	loss_policy_2: 0.03782
	accuracy_policy_2: 0.62602
	loss_value_2: 0.03316
	loss_reward_2: 0.00509
	loss_policy_3: 0.04201
	accuracy_policy_3: 0.59246
	loss_value_3: 0.03453
	loss_reward_3: 0.00587
	loss_policy_4: 0.04497
	accuracy_policy_4: 0.56766
	loss_value_4: 0.03593
	loss_reward_4: 0.00636
	loss_policy_5: 0.04894
	accuracy_policy_5: 0.54059
	loss_value_5: 0.03719
	loss_reward_5: 0.00771
	loss_policy: 0.32335
	loss_value: 0.32271
	loss_reward: 0.0307
[2024-05-05 11:56:02] nn step 37350, lr: 0.005.
	loss_policy_0: 0.10668
	accuracy_policy_0: 0.75469
	loss_value_0: 0.14941
	loss_policy_1: 0.03309
	accuracy_policy_1: 0.66008
	loss_value_1: 0.03182
	loss_reward_1: 0.00571
	loss_policy_2: 0.03689
	accuracy_policy_2: 0.63211
	loss_value_2: 0.03312
	loss_reward_2: 0.0052
	loss_policy_3: 0.04115
	accuracy_policy_3: 0.60266
	loss_value_3: 0.03462
	loss_reward_3: 0.00585
	loss_policy_4: 0.04383
	accuracy_policy_4: 0.58508
	loss_value_4: 0.03592
	loss_reward_4: 0.00658
	loss_policy_5: 0.04774
	accuracy_policy_5: 0.55516
	loss_value_5: 0.03723
	loss_reward_5: 0.00771
	loss_policy: 0.30939
	loss_value: 0.32212
	loss_reward: 0.03104
[2024-05-05 11:56:18] nn step 37400, lr: 0.005.
	loss_policy_0: 0.10132
	accuracy_policy_0: 0.76168
	loss_value_0: 0.14664
	loss_policy_1: 0.03179
	accuracy_policy_1: 0.67121
	loss_value_1: 0.03114
	loss_reward_1: 0.0053
	loss_policy_2: 0.03549
	accuracy_policy_2: 0.64145
	loss_value_2: 0.03278
	loss_reward_2: 0.00519
	loss_policy_3: 0.03951
	accuracy_policy_3: 0.60992
	loss_value_3: 0.03423
	loss_reward_3: 0.00556
	loss_policy_4: 0.04273
	accuracy_policy_4: 0.58551
	loss_value_4: 0.0355
	loss_reward_4: 0.00617
	loss_policy_5: 0.04598
	accuracy_policy_5: 0.56281
	loss_value_5: 0.03689
	loss_reward_5: 0.00734
	loss_policy: 0.29681
	loss_value: 0.31718
	loss_reward: 0.02956
Optimization_Done 37400
[2024-05-05 11:58:17] [command] train weight_iter_37400.pkl 187 188
[2024-05-05 11:58:34] nn step 37450, lr: 0.005.
	loss_policy_0: 0.08156
	accuracy_policy_0: 0.75156
	loss_value_0: 0.12305
	loss_policy_1: 0.02257
	accuracy_policy_1: 0.68176
	loss_value_1: 0.0257
	loss_reward_1: 0.00438
	loss_policy_2: 0.02441
	accuracy_policy_2: 0.66602
	loss_value_2: 0.02665
	loss_reward_2: 0.00404
	loss_policy_3: 0.02709
	accuracy_policy_3: 0.64012
	loss_value_3: 0.02771
	loss_reward_3: 0.00448
	loss_policy_4: 0.02891
	accuracy_policy_4: 0.62043
	loss_value_4: 0.02853
	loss_reward_4: 0.00484
	loss_policy_5: 0.03081
	accuracy_policy_5: 0.60348
	loss_value_5: 0.0295
	loss_reward_5: 0.00581
	loss_policy: 0.21535
	loss_value: 0.26115
	loss_reward: 0.02355
[2024-05-05 11:58:51] nn step 37500, lr: 0.005.
	loss_policy_0: 0.06992
	accuracy_policy_0: 0.78785
	loss_value_0: 0.12713
	loss_policy_1: 0.02169
	accuracy_policy_1: 0.69688
	loss_value_1: 0.02657
	loss_reward_1: 0.00452
	loss_policy_2: 0.02393
	accuracy_policy_2: 0.67777
	loss_value_2: 0.02746
	loss_reward_2: 0.00426
	loss_policy_3: 0.02612
	accuracy_policy_3: 0.65961
	loss_value_3: 0.0284
	loss_reward_3: 0.00457
	loss_policy_4: 0.02772
	accuracy_policy_4: 0.64133
	loss_value_4: 0.02943
	loss_reward_4: 0.00505
	loss_policy_5: 0.02968
	accuracy_policy_5: 0.62719
	loss_value_5: 0.03035
	loss_reward_5: 0.00612
	loss_policy: 0.19906
	loss_value: 0.26935
	loss_reward: 0.02452
[2024-05-05 11:59:07] nn step 37550, lr: 0.005.
	loss_policy_0: 0.05824
	accuracy_policy_0: 0.7977
	loss_value_0: 0.11515
	loss_policy_1: 0.0189
	accuracy_policy_1: 0.70781
	loss_value_1: 0.02396
	loss_reward_1: 0.00408
	loss_policy_2: 0.02058
	accuracy_policy_2: 0.68832
	loss_value_2: 0.02485
	loss_reward_2: 0.00371
	loss_policy_3: 0.02309
	accuracy_policy_3: 0.66504
	loss_value_3: 0.02568
	loss_reward_3: 0.00407
	loss_policy_4: 0.02483
	accuracy_policy_4: 0.6477
	loss_value_4: 0.02645
	loss_reward_4: 0.00439
	loss_policy_5: 0.02654
	accuracy_policy_5: 0.6257
	loss_value_5: 0.02723
	loss_reward_5: 0.00549
	loss_policy: 0.17218
	loss_value: 0.24333
	loss_reward: 0.02174
[2024-05-05 11:59:22] nn step 37600, lr: 0.005.
	loss_policy_0: 0.05782
	accuracy_policy_0: 0.80699
	loss_value_0: 0.12101
	loss_policy_1: 0.01926
	accuracy_policy_1: 0.72023
	loss_value_1: 0.02525
	loss_reward_1: 0.00422
	loss_policy_2: 0.02147
	accuracy_policy_2: 0.69453
	loss_value_2: 0.02622
	loss_reward_2: 0.0039
	loss_policy_3: 0.02403
	accuracy_policy_3: 0.67004
	loss_value_3: 0.02708
	loss_reward_3: 0.00438
	loss_policy_4: 0.02549
	accuracy_policy_4: 0.6543
	loss_value_4: 0.02788
	loss_reward_4: 0.00483
	loss_policy_5: 0.0275
	accuracy_policy_5: 0.6323
	loss_value_5: 0.02875
	loss_reward_5: 0.00579
	loss_policy: 0.17558
	loss_value: 0.25619
	loss_reward: 0.02313
Optimization_Done 37600
[2024-05-05 12:01:25] [command] train weight_iter_37600.pkl 188 189
[2024-05-05 12:01:42] nn step 37650, lr: 0.005.
	loss_policy_0: 0.09167
	accuracy_policy_0: 0.71504
	loss_value_0: 0.12155
	loss_policy_1: 0.02443
	accuracy_policy_1: 0.64141
	loss_value_1: 0.02583
	loss_reward_1: 0.00415
	loss_policy_2: 0.02629
	accuracy_policy_2: 0.62602
	loss_value_2: 0.02714
	loss_reward_2: 0.00403
	loss_policy_3: 0.0282
	accuracy_policy_3: 0.60539
	loss_value_3: 0.02812
	loss_reward_3: 0.00449
	loss_policy_4: 0.03
	accuracy_policy_4: 0.59012
	loss_value_4: 0.02911
	loss_reward_4: 0.00492
	loss_policy_5: 0.03212
	accuracy_policy_5: 0.57039
	loss_value_5: 0.03037
	loss_reward_5: 0.00542
	loss_policy: 0.23272
	loss_value: 0.26212
	loss_reward: 0.02301
[2024-05-05 12:01:59] nn step 37700, lr: 0.005.
	loss_policy_0: 0.06692
	accuracy_policy_0: 0.77809
	loss_value_0: 0.12203
	loss_policy_1: 0.0204
	accuracy_policy_1: 0.70184
	loss_value_1: 0.0259
	loss_reward_1: 0.0039
	loss_policy_2: 0.02255
	accuracy_policy_2: 0.67941
	loss_value_2: 0.02703
	loss_reward_2: 0.00384
	loss_policy_3: 0.02445
	accuracy_policy_3: 0.65723
	loss_value_3: 0.02802
	loss_reward_3: 0.0041
	loss_policy_4: 0.02613
	accuracy_policy_4: 0.6459
	loss_value_4: 0.02899
	loss_reward_4: 0.00443
	loss_policy_5: 0.02823
	accuracy_policy_5: 0.6232
	loss_value_5: 0.03012
	loss_reward_5: 0.00512
	loss_policy: 0.18869
	loss_value: 0.26208
	loss_reward: 0.0214
[2024-05-05 12:02:15] nn step 37750, lr: 0.005.
	loss_policy_0: 0.05964
	accuracy_policy_0: 0.79793
	loss_value_0: 0.1237
	loss_policy_1: 0.01949
	accuracy_policy_1: 0.71344
	loss_value_1: 0.02613
	loss_reward_1: 0.00372
	loss_policy_2: 0.02163
	accuracy_policy_2: 0.68695
	loss_value_2: 0.02712
	loss_reward_2: 0.00375
	loss_policy_3: 0.02371
	accuracy_policy_3: 0.66945
	loss_value_3: 0.02827
	loss_reward_3: 0.00402
	loss_policy_4: 0.02523
	accuracy_policy_4: 0.65383
	loss_value_4: 0.02942
	loss_reward_4: 0.0043
	loss_policy_5: 0.02724
	accuracy_policy_5: 0.63941
	loss_value_5: 0.0303
	loss_reward_5: 0.00503
	loss_policy: 0.17695
	loss_value: 0.26494
	loss_reward: 0.02081
[2024-05-05 12:02:31] nn step 37800, lr: 0.005.
	loss_policy_0: 0.05395
	accuracy_policy_0: 0.80691
	loss_value_0: 0.11922
	loss_policy_1: 0.01825
	accuracy_policy_1: 0.71758
	loss_value_1: 0.02519
	loss_reward_1: 0.00377
	loss_policy_2: 0.02016
	accuracy_policy_2: 0.69414
	loss_value_2: 0.02629
	loss_reward_2: 0.00358
	loss_policy_3: 0.02242
	accuracy_policy_3: 0.67035
	loss_value_3: 0.02722
	loss_reward_3: 0.00381
	loss_policy_4: 0.02406
	accuracy_policy_4: 0.65273
	loss_value_4: 0.02808
	loss_reward_4: 0.00409
	loss_policy_5: 0.02564
	accuracy_policy_5: 0.64742
	loss_value_5: 0.029
	loss_reward_5: 0.00466
	loss_policy: 0.16448
	loss_value: 0.25499
	loss_reward: 0.01992
Optimization_Done 37800
[2024-05-05 12:04:31] [command] train weight_iter_37800.pkl 189 190
[2024-05-05 12:04:49] nn step 37850, lr: 0.005.
	loss_policy_0: 0.14467
	accuracy_policy_0: 0.6732
	loss_value_0: 0.15508
	loss_policy_1: 0.03804
	accuracy_policy_1: 0.59344
	loss_value_1: 0.03278
	loss_reward_1: 0.00632
	loss_policy_2: 0.04147
	accuracy_policy_2: 0.56945
	loss_value_2: 0.03437
	loss_reward_2: 0.00647
	loss_policy_3: 0.04398
	accuracy_policy_3: 0.55109
	loss_value_3: 0.03573
	loss_reward_3: 0.00698
	loss_policy_4: 0.04729
	accuracy_policy_4: 0.52812
	loss_value_4: 0.03701
	loss_reward_4: 0.00735
	loss_policy_5: 0.04992
	accuracy_policy_5: 0.50254
	loss_value_5: 0.03815
	loss_reward_5: 0.00805
	loss_policy: 0.36537
	loss_value: 0.33313
	loss_reward: 0.03519
[2024-05-05 12:05:05] nn step 37900, lr: 0.005.
	loss_policy_0: 0.11391
	accuracy_policy_0: 0.7402
	loss_value_0: 0.15881
	loss_policy_1: 0.03444
	accuracy_policy_1: 0.64355
	loss_value_1: 0.03385
	loss_reward_1: 0.00651
	loss_policy_2: 0.03883
	accuracy_policy_2: 0.61648
	loss_value_2: 0.03553
	loss_reward_2: 0.00644
	loss_policy_3: 0.0426
	accuracy_policy_3: 0.58801
	loss_value_3: 0.03713
	loss_reward_3: 0.00701
	loss_policy_4: 0.04472
	accuracy_policy_4: 0.57352
	loss_value_4: 0.0386
	loss_reward_4: 0.0074
	loss_policy_5: 0.04806
	accuracy_policy_5: 0.55023
	loss_value_5: 0.03983
	loss_reward_5: 0.00838
	loss_policy: 0.32256
	loss_value: 0.34374
	loss_reward: 0.03575
[2024-05-05 12:05:21] nn step 37950, lr: 0.005.
	loss_policy_0: 0.09162
	accuracy_policy_0: 0.75074
	loss_value_0: 0.13963
	loss_policy_1: 0.02882
	accuracy_policy_1: 0.65152
	loss_value_1: 0.02982
	loss_reward_1: 0.00559
	loss_policy_2: 0.03235
	accuracy_policy_2: 0.62344
	loss_value_2: 0.03123
	loss_reward_2: 0.00557
	loss_policy_3: 0.03566
	accuracy_policy_3: 0.59707
	loss_value_3: 0.03259
	loss_reward_3: 0.00617
	loss_policy_4: 0.03808
	accuracy_policy_4: 0.58285
	loss_value_4: 0.03392
	loss_reward_4: 0.00649
	loss_policy_5: 0.04021
	accuracy_policy_5: 0.56543
	loss_value_5: 0.03494
	loss_reward_5: 0.00723
	loss_policy: 0.26674
	loss_value: 0.30214
	loss_reward: 0.03106
[2024-05-05 12:05:37] nn step 38000, lr: 0.005.
	loss_policy_0: 0.09467
	accuracy_policy_0: 0.76531
	loss_value_0: 0.15307
	loss_policy_1: 0.03066
	accuracy_policy_1: 0.65918
	loss_value_1: 0.03251
	loss_reward_1: 0.00605
	loss_policy_2: 0.03446
	accuracy_policy_2: 0.62676
	loss_value_2: 0.03406
	loss_reward_2: 0.00623
	loss_policy_3: 0.03757
	accuracy_policy_3: 0.60586
	loss_value_3: 0.03543
	loss_reward_3: 0.00651
	loss_policy_4: 0.04016
	accuracy_policy_4: 0.58539
	loss_value_4: 0.03663
	loss_reward_4: 0.0069
	loss_policy_5: 0.04259
	accuracy_policy_5: 0.57055
	loss_value_5: 0.03792
	loss_reward_5: 0.00779
	loss_policy: 0.2801
	loss_value: 0.32961
	loss_reward: 0.03349
Optimization_Done 38000
[2024-05-05 12:07:38] [command] train weight_iter_38000.pkl 190 191
[2024-05-05 12:07:55] nn step 38050, lr: 0.005.
	loss_policy_0: 0.12146
	accuracy_policy_0: 0.76539
	loss_value_0: 0.17732
	loss_policy_1: 0.03524
	accuracy_policy_1: 0.68555
	loss_value_1: 0.03742
	loss_reward_1: 0.00776
	loss_policy_2: 0.03942
	accuracy_policy_2: 0.65113
	loss_value_2: 0.03939
	loss_reward_2: 0.00775
	loss_policy_3: 0.04351
	accuracy_policy_3: 0.6257
	loss_value_3: 0.04084
	loss_reward_3: 0.00846
	loss_policy_4: 0.04609
	accuracy_policy_4: 0.61195
	loss_value_4: 0.04222
	loss_reward_4: 0.00904
	loss_policy_5: 0.04896
	accuracy_policy_5: 0.59289
	loss_value_5: 0.04342
	loss_reward_5: 0.01014
	loss_policy: 0.33468
	loss_value: 0.38061
	loss_reward: 0.04315
[2024-05-05 12:08:12] nn step 38100, lr: 0.005.
	loss_policy_0: 0.09542
	accuracy_policy_0: 0.80277
	loss_value_0: 0.16921
	loss_policy_1: 0.03174
	accuracy_policy_1: 0.70004
	loss_value_1: 0.03588
	loss_reward_1: 0.00771
	loss_policy_2: 0.03576
	accuracy_policy_2: 0.67043
	loss_value_2: 0.0374
	loss_reward_2: 0.00729
	loss_policy_3: 0.03994
	accuracy_policy_3: 0.64258
	loss_value_3: 0.03858
	loss_reward_3: 0.00813
	loss_policy_4: 0.04268
	accuracy_policy_4: 0.62332
	loss_value_4: 0.04006
	loss_reward_4: 0.00845
	loss_policy_5: 0.04546
	accuracy_policy_5: 0.61012
	loss_value_5: 0.04145
	loss_reward_5: 0.00978
	loss_policy: 0.291
	loss_value: 0.36259
	loss_reward: 0.04137
[2024-05-05 12:08:28] nn step 38150, lr: 0.005.
	loss_policy_0: 0.08641
	accuracy_policy_0: 0.79973
	loss_value_0: 0.15708
	loss_policy_1: 0.02927
	accuracy_policy_1: 0.69762
	loss_value_1: 0.03296
	loss_reward_1: 0.00694
	loss_policy_2: 0.03324
	accuracy_policy_2: 0.66883
	loss_value_2: 0.03457
	loss_reward_2: 0.00694
	loss_policy_3: 0.03673
	accuracy_policy_3: 0.64207
	loss_value_3: 0.03587
	loss_reward_3: 0.00766
	loss_policy_4: 0.03932
	accuracy_policy_4: 0.62043
	loss_value_4: 0.03694
	loss_reward_4: 0.00805
	loss_policy_5: 0.04191
	accuracy_policy_5: 0.60438
	loss_value_5: 0.03788
	loss_reward_5: 0.00912
	loss_policy: 0.26689
	loss_value: 0.3353
	loss_reward: 0.03872
[2024-05-05 12:08:44] nn step 38200, lr: 0.005.
	loss_policy_0: 0.08915
	accuracy_policy_0: 0.80582
	loss_value_0: 0.16814
	loss_policy_1: 0.03055
	accuracy_policy_1: 0.69738
	loss_value_1: 0.03544
	loss_reward_1: 0.00738
	loss_policy_2: 0.03481
	accuracy_policy_2: 0.66324
	loss_value_2: 0.03713
	loss_reward_2: 0.00722
	loss_policy_3: 0.03862
	accuracy_policy_3: 0.64551
	loss_value_3: 0.03832
	loss_reward_3: 0.00822
	loss_policy_4: 0.04175
	accuracy_policy_4: 0.6252
	loss_value_4: 0.03942
	loss_reward_4: 0.00872
	loss_policy_5: 0.04448
	accuracy_policy_5: 0.60863
	loss_value_5: 0.0405
	loss_reward_5: 0.00975
	loss_policy: 0.27936
	loss_value: 0.35895
	loss_reward: 0.04129
Optimization_Done 38200
[2024-05-05 12:10:44] [command] train weight_iter_38200.pkl 191 192
[2024-05-05 12:11:01] nn step 38250, lr: 0.005.
	loss_policy_0: 0.137
	accuracy_policy_0: 0.72875
	loss_value_0: 0.17236
	loss_policy_1: 0.03679
	accuracy_policy_1: 0.64262
	loss_value_1: 0.0361
	loss_reward_1: 0.00737
	loss_policy_2: 0.04105
	accuracy_policy_2: 0.60984
	loss_value_2: 0.03766
	loss_reward_2: 0.00707
	loss_policy_3: 0.04434
	accuracy_policy_3: 0.57918
	loss_value_3: 0.03917
	loss_reward_3: 0.00784
	loss_policy_4: 0.04731
	accuracy_policy_4: 0.55215
	loss_value_4: 0.04043
	loss_reward_4: 0.00844
	loss_policy_5: 0.04917
	accuracy_policy_5: 0.53582
	loss_value_5: 0.04152
	loss_reward_5: 0.00957
	loss_policy: 0.35567
	loss_value: 0.36723
	loss_reward: 0.04028
[2024-05-05 12:11:18] nn step 38300, lr: 0.005.
	loss_policy_0: 0.09931
	accuracy_policy_0: 0.78246
	loss_value_0: 0.16488
	loss_policy_1: 0.03133
	accuracy_policy_1: 0.68277
	loss_value_1: 0.03447
	loss_reward_1: 0.00695
	loss_policy_2: 0.03546
	accuracy_policy_2: 0.64727
	loss_value_2: 0.036
	loss_reward_2: 0.00684
	loss_policy_3: 0.03922
	accuracy_policy_3: 0.60957
	loss_value_3: 0.03699
	loss_reward_3: 0.00769
	loss_policy_4: 0.04175
	accuracy_policy_4: 0.59273
	loss_value_4: 0.03819
	loss_reward_4: 0.00822
	loss_policy_5: 0.04441
	accuracy_policy_5: 0.57184
	loss_value_5: 0.03906
	loss_reward_5: 0.00911
	loss_policy: 0.29147
	loss_value: 0.34959
	loss_reward: 0.03881
[2024-05-05 12:11:34] nn step 38350, lr: 0.005.
	loss_policy_0: 0.09216
	accuracy_policy_0: 0.79438
	loss_value_0: 0.16598
	loss_policy_1: 0.02993
	accuracy_policy_1: 0.69867
	loss_value_1: 0.03434
	loss_reward_1: 0.00717
	loss_policy_2: 0.03434
	accuracy_policy_2: 0.66105
	loss_value_2: 0.03577
	loss_reward_2: 0.00688
	loss_policy_3: 0.03801
	accuracy_policy_3: 0.63344
	loss_value_3: 0.03676
	loss_reward_3: 0.00771
	loss_policy_4: 0.04109
	accuracy_policy_4: 0.60699
	loss_value_4: 0.0379
	loss_reward_4: 0.00818
	loss_policy_5: 0.04307
	accuracy_policy_5: 0.58562
	loss_value_5: 0.03909
	loss_reward_5: 0.00958
	loss_policy: 0.2786
	loss_value: 0.34984
	loss_reward: 0.03951
[2024-05-05 12:11:50] nn step 38400, lr: 0.005.
	loss_policy_0: 0.09203
	accuracy_policy_0: 0.80633
	loss_value_0: 0.1713
	loss_policy_1: 0.03015
	accuracy_policy_1: 0.70625
	loss_value_1: 0.03565
	loss_reward_1: 0.00748
	loss_policy_2: 0.03481
	accuracy_policy_2: 0.66816
	loss_value_2: 0.03723
	loss_reward_2: 0.00722
	loss_policy_3: 0.03915
	accuracy_policy_3: 0.63691
	loss_value_3: 0.03828
	loss_reward_3: 0.00811
	loss_policy_4: 0.04192
	accuracy_policy_4: 0.61578
	loss_value_4: 0.03949
	loss_reward_4: 0.00903
	loss_policy_5: 0.04423
	accuracy_policy_5: 0.59609
	loss_value_5: 0.04076
	loss_reward_5: 0.01005
	loss_policy: 0.28228
	loss_value: 0.3627
	loss_reward: 0.04188
Optimization_Done 38400
[2024-05-05 12:13:33] [command] train weight_iter_38400.pkl 192 193
[2024-05-05 12:13:50] nn step 38450, lr: 0.005.
	loss_policy_0: 0.1345
	accuracy_policy_0: 0.72848
	loss_value_0: 0.17475
	loss_policy_1: 0.03755
	accuracy_policy_1: 0.63863
	loss_value_1: 0.0363
	loss_reward_1: 0.00622
	loss_policy_2: 0.04075
	accuracy_policy_2: 0.60406
	loss_value_2: 0.03775
	loss_reward_2: 0.00577
	loss_policy_3: 0.04441
	accuracy_policy_3: 0.56898
	loss_value_3: 0.03902
	loss_reward_3: 0.00661
	loss_policy_4: 0.04683
	accuracy_policy_4: 0.54328
	loss_value_4: 0.04019
	loss_reward_4: 0.00699
	loss_policy_5: 0.04927
	accuracy_policy_5: 0.52258
	loss_value_5: 0.04125
	loss_reward_5: 0.00774
	loss_policy: 0.35331
	loss_value: 0.36926
	loss_reward: 0.03333
[2024-05-05 12:14:06] nn step 38500, lr: 0.005.
	loss_policy_0: 0.10943
	accuracy_policy_0: 0.76492
	loss_value_0: 0.17073
	loss_policy_1: 0.03313
	accuracy_policy_1: 0.67082
	loss_value_1: 0.03555
	loss_reward_1: 0.00627
	loss_policy_2: 0.03724
	accuracy_policy_2: 0.63418
	loss_value_2: 0.03667
	loss_reward_2: 0.00608
	loss_policy_3: 0.04091
	accuracy_policy_3: 0.60137
	loss_value_3: 0.03786
	loss_reward_3: 0.00654
	loss_policy_4: 0.04373
	accuracy_policy_4: 0.57398
	loss_value_4: 0.03892
	loss_reward_4: 0.00704
	loss_policy_5: 0.04575
	accuracy_policy_5: 0.55328
	loss_value_5: 0.03979
	loss_reward_5: 0.00788
	loss_policy: 0.31019
	loss_value: 0.35953
	loss_reward: 0.03381
[2024-05-05 12:14:23] nn step 38550, lr: 0.005.
	loss_policy_0: 0.10055
	accuracy_policy_0: 0.77945
	loss_value_0: 0.17276
	loss_policy_1: 0.0321
	accuracy_policy_1: 0.68531
	loss_value_1: 0.03582
	loss_reward_1: 0.00645
	loss_policy_2: 0.03636
	accuracy_policy_2: 0.63848
	loss_value_2: 0.03723
	loss_reward_2: 0.00611
	loss_policy_3: 0.04038
	accuracy_policy_3: 0.60336
	loss_value_3: 0.03836
	loss_reward_3: 0.00651
	loss_policy_4: 0.04324
	accuracy_policy_4: 0.5766
	loss_value_4: 0.03937
	loss_reward_4: 0.00702
	loss_policy_5: 0.04554
	accuracy_policy_5: 0.55629
	loss_value_5: 0.0404
	loss_reward_5: 0.00781
	loss_policy: 0.29818
	loss_value: 0.36394
	loss_reward: 0.0339
[2024-05-05 12:14:39] nn step 38600, lr: 0.005.
	loss_policy_0: 0.0909
	accuracy_policy_0: 0.79398
	loss_value_0: 0.1654
	loss_policy_1: 0.03037
	accuracy_policy_1: 0.68219
	loss_value_1: 0.03442
	loss_reward_1: 0.00624
	loss_policy_2: 0.03419
	accuracy_policy_2: 0.64449
	loss_value_2: 0.0356
	loss_reward_2: 0.00577
	loss_policy_3: 0.03779
	accuracy_policy_3: 0.61496
	loss_value_3: 0.03676
	loss_reward_3: 0.00628
	loss_policy_4: 0.04065
	accuracy_policy_4: 0.58574
	loss_value_4: 0.03771
	loss_reward_4: 0.0068
	loss_policy_5: 0.04297
	accuracy_policy_5: 0.56277
	loss_value_5: 0.03866
	loss_reward_5: 0.00786
	loss_policy: 0.27686
	loss_value: 0.34854
	loss_reward: 0.03295
Optimization_Done 38600
[2024-05-05 12:16:41] [command] train weight_iter_38600.pkl 193 194
[2024-05-05 12:16:58] nn step 38650, lr: 0.005.
	loss_policy_0: 0.1341
	accuracy_policy_0: 0.72711
	loss_value_0: 0.16884
	loss_policy_1: 0.03576
	accuracy_policy_1: 0.64852
	loss_value_1: 0.03547
	loss_reward_1: 0.00586
	loss_policy_2: 0.04005
	accuracy_policy_2: 0.61187
	loss_value_2: 0.03702
	loss_reward_2: 0.00584
	loss_policy_3: 0.0437
	accuracy_policy_3: 0.57582
	loss_value_3: 0.03834
	loss_reward_3: 0.00647
	loss_policy_4: 0.04642
	accuracy_policy_4: 0.55949
	loss_value_4: 0.03943
	loss_reward_4: 0.00717
	loss_policy_5: 0.04924
	accuracy_policy_5: 0.54078
	loss_value_5: 0.0407
	loss_reward_5: 0.00777
	loss_policy: 0.34928
	loss_value: 0.3598
	loss_reward: 0.03311
[2024-05-05 12:17:14] nn step 38700, lr: 0.005.
	loss_policy_0: 0.103
	accuracy_policy_0: 0.78512
	loss_value_0: 0.16571
	loss_policy_1: 0.03236
	accuracy_policy_1: 0.69156
	loss_value_1: 0.03506
	loss_reward_1: 0.00614
	loss_policy_2: 0.03745
	accuracy_policy_2: 0.64949
	loss_value_2: 0.03668
	loss_reward_2: 0.00622
	loss_policy_3: 0.04171
	accuracy_policy_3: 0.6202
	loss_value_3: 0.03787
	loss_reward_3: 0.00655
	loss_policy_4: 0.04483
	accuracy_policy_4: 0.58969
	loss_value_4: 0.03914
	loss_reward_4: 0.00722
	loss_policy_5: 0.04733
	accuracy_policy_5: 0.57402
	loss_value_5: 0.04024
	loss_reward_5: 0.00817
	loss_policy: 0.30668
	loss_value: 0.3547
	loss_reward: 0.03431
[2024-05-05 12:17:30] nn step 38750, lr: 0.005.
	loss_policy_0: 0.09697
	accuracy_policy_0: 0.78887
	loss_value_0: 0.16283
	loss_policy_1: 0.03118
	accuracy_policy_1: 0.6909
	loss_value_1: 0.03447
	loss_reward_1: 0.00621
	loss_policy_2: 0.03613
	accuracy_policy_2: 0.65828
	loss_value_2: 0.03586
	loss_reward_2: 0.00601
	loss_policy_3: 0.04013
	accuracy_policy_3: 0.62508
	loss_value_3: 0.0371
	loss_reward_3: 0.00641
	loss_policy_4: 0.04323
	accuracy_policy_4: 0.60258
	loss_value_4: 0.03831
	loss_reward_4: 0.00705
	loss_policy_5: 0.04535
	accuracy_policy_5: 0.58348
	loss_value_5: 0.03952
	loss_reward_5: 0.00805
	loss_policy: 0.29299
	loss_value: 0.3481
	loss_reward: 0.03374
[2024-05-05 12:17:46] nn step 38800, lr: 0.005.
	loss_policy_0: 0.0901
	accuracy_policy_0: 0.80059
	loss_value_0: 0.1605
	loss_policy_1: 0.02978
	accuracy_policy_1: 0.70535
	loss_value_1: 0.03387
	loss_reward_1: 0.00627
	loss_policy_2: 0.03535
	accuracy_policy_2: 0.65406
	loss_value_2: 0.03534
	loss_reward_2: 0.0059
	loss_policy_3: 0.03952
	accuracy_policy_3: 0.62547
	loss_value_3: 0.03677
	loss_reward_3: 0.00667
	loss_policy_4: 0.0424
	accuracy_policy_4: 0.60379
	loss_value_4: 0.03785
	loss_reward_4: 0.00705
	loss_policy_5: 0.0451
	accuracy_policy_5: 0.58297
	loss_value_5: 0.03902
	loss_reward_5: 0.0082
	loss_policy: 0.28224
	loss_value: 0.34336
	loss_reward: 0.03409
Optimization_Done 38800
[2024-05-05 12:19:44] [command] train weight_iter_38800.pkl 194 195
[2024-05-05 12:20:01] nn step 38850, lr: 0.005.
	loss_policy_0: 0.138
	accuracy_policy_0: 0.74898
	loss_value_0: 0.17562
	loss_policy_1: 0.03793
	accuracy_policy_1: 0.66707
	loss_value_1: 0.03698
	loss_reward_1: 0.00645
	loss_policy_2: 0.04235
	accuracy_policy_2: 0.63586
	loss_value_2: 0.0389
	loss_reward_2: 0.00637
	loss_policy_3: 0.04721
	accuracy_policy_3: 0.60031
	loss_value_3: 0.04084
	loss_reward_3: 0.00695
	loss_policy_4: 0.05017
	accuracy_policy_4: 0.57773
	loss_value_4: 0.0425
	loss_reward_4: 0.00753
	loss_policy_5: 0.05381
	accuracy_policy_5: 0.55359
	loss_value_5: 0.04402
	loss_reward_5: 0.00872
	loss_policy: 0.36948
	loss_value: 0.37886
	loss_reward: 0.03602
[2024-05-05 12:20:17] nn step 38900, lr: 0.005.
	loss_policy_0: 0.10349
	accuracy_policy_0: 0.78859
	loss_value_0: 0.16111
	loss_policy_1: 0.03263
	accuracy_policy_1: 0.69551
	loss_value_1: 0.03418
	loss_reward_1: 0.00619
	loss_policy_2: 0.03723
	accuracy_policy_2: 0.66316
	loss_value_2: 0.03608
	loss_reward_2: 0.00607
	loss_policy_3: 0.04183
	accuracy_policy_3: 0.6218
	loss_value_3: 0.03766
	loss_reward_3: 0.00664
	loss_policy_4: 0.04476
	accuracy_policy_4: 0.60781
	loss_value_4: 0.03903
	loss_reward_4: 0.00683
	loss_policy_5: 0.04774
	accuracy_policy_5: 0.58012
	loss_value_5: 0.04048
	loss_reward_5: 0.00833
	loss_policy: 0.30768
	loss_value: 0.34853
	loss_reward: 0.03406
[2024-05-05 12:20:33] nn step 38950, lr: 0.005.
	loss_policy_0: 0.09137
	accuracy_policy_0: 0.80355
	loss_value_0: 0.15367
	loss_policy_1: 0.0297
	accuracy_policy_1: 0.70383
	loss_value_1: 0.0325
	loss_reward_1: 0.00595
	loss_policy_2: 0.03439
	accuracy_policy_2: 0.66633
	loss_value_2: 0.03417
	loss_reward_2: 0.0057
	loss_policy_3: 0.03889
	accuracy_policy_3: 0.62883
	loss_value_3: 0.03551
	loss_reward_3: 0.00651
	loss_policy_4: 0.04197
	accuracy_policy_4: 0.60719
	loss_value_4: 0.03677
	loss_reward_4: 0.00697
	loss_policy_5: 0.04418
	accuracy_policy_5: 0.59203
	loss_value_5: 0.03802
	loss_reward_5: 0.00804
	loss_policy: 0.2805
	loss_value: 0.33063
	loss_reward: 0.03317
[2024-05-05 12:20:49] nn step 39000, lr: 0.005.
	loss_policy_0: 0.09611
	accuracy_policy_0: 0.8118
	loss_value_0: 0.16845
	loss_policy_1: 0.03134
	accuracy_policy_1: 0.7143
	loss_value_1: 0.03539
	loss_reward_1: 0.00638
	loss_policy_2: 0.03671
	accuracy_policy_2: 0.67496
	loss_value_2: 0.0374
	loss_reward_2: 0.0062
	loss_policy_3: 0.04143
	accuracy_policy_3: 0.63867
	loss_value_3: 0.03885
	loss_reward_3: 0.00694
	loss_policy_4: 0.04452
	accuracy_policy_4: 0.61789
	loss_value_4: 0.04036
	loss_reward_4: 0.00759
	loss_policy_5: 0.04776
	accuracy_policy_5: 0.59551
	loss_value_5: 0.04165
	loss_reward_5: 0.00877
	loss_policy: 0.29787
	loss_value: 0.3621
	loss_reward: 0.03589
Optimization_Done 39000
[2024-05-05 12:22:55] [command] train weight_iter_39000.pkl 195 196
[2024-05-05 12:23:11] nn step 39050, lr: 0.005.
	loss_policy_0: 0.10253
	accuracy_policy_0: 0.76867
	loss_value_0: 0.15465
	loss_policy_1: 0.02978
	accuracy_policy_1: 0.67883
	loss_value_1: 0.03252
	loss_reward_1: 0.00606
	loss_policy_2: 0.03283
	accuracy_policy_2: 0.6509
	loss_value_2: 0.03384
	loss_reward_2: 0.00591
	loss_policy_3: 0.037
	accuracy_policy_3: 0.61922
	loss_value_3: 0.03538
	loss_reward_3: 0.0063
	loss_policy_4: 0.03956
	accuracy_policy_4: 0.59953
	loss_value_4: 0.0367
	loss_reward_4: 0.00693
	loss_policy_5: 0.04219
	accuracy_policy_5: 0.57836
	loss_value_5: 0.03788
	loss_reward_5: 0.00779
	loss_policy: 0.28388
	loss_value: 0.33097
	loss_reward: 0.03299
[2024-05-05 12:23:27] nn step 39100, lr: 0.005.
	loss_policy_0: 0.08265
	accuracy_policy_0: 0.80109
	loss_value_0: 0.14855
	loss_policy_1: 0.02635
	accuracy_policy_1: 0.70773
	loss_value_1: 0.03148
	loss_reward_1: 0.00581
	loss_policy_2: 0.03031
	accuracy_policy_2: 0.66891
	loss_value_2: 0.03293
	loss_reward_2: 0.00538
	loss_policy_3: 0.03303
	accuracy_policy_3: 0.65273
	loss_value_3: 0.03418
	loss_reward_3: 0.00603
	loss_policy_4: 0.03563
	accuracy_policy_4: 0.62633
	loss_value_4: 0.03531
	loss_reward_4: 0.00652
	loss_policy_5: 0.03811
	accuracy_policy_5: 0.60434
	loss_value_5: 0.03656
	loss_reward_5: 0.00749
	loss_policy: 0.24609
	loss_value: 0.31902
	loss_reward: 0.03123
[2024-05-05 12:23:43] nn step 39150, lr: 0.005.
	loss_policy_0: 0.07881
	accuracy_policy_0: 0.80969
	loss_value_0: 0.1503
	loss_policy_1: 0.02575
	accuracy_policy_1: 0.71508
	loss_value_1: 0.03171
	loss_reward_1: 0.00583
	loss_policy_2: 0.02925
	accuracy_policy_2: 0.68105
	loss_value_2: 0.0331
	loss_reward_2: 0.0056
	loss_policy_3: 0.03262
	accuracy_policy_3: 0.65496
	loss_value_3: 0.03429
	loss_reward_3: 0.00592
	loss_policy_4: 0.03517
	accuracy_policy_4: 0.63582
	loss_value_4: 0.03553
	loss_reward_4: 0.00647
	loss_policy_5: 0.03802
	accuracy_policy_5: 0.61289
	loss_value_5: 0.03654
	loss_reward_5: 0.00747
	loss_policy: 0.23962
	loss_value: 0.32147
	loss_reward: 0.0313
[2024-05-05 12:23:59] nn step 39200, lr: 0.005.
	loss_policy_0: 0.0787
	accuracy_policy_0: 0.81855
	loss_value_0: 0.15645
	loss_policy_1: 0.0258
	accuracy_policy_1: 0.72312
	loss_value_1: 0.03303
	loss_reward_1: 0.00597
	loss_policy_2: 0.0301
	accuracy_policy_2: 0.68297
	loss_value_2: 0.03436
	loss_reward_2: 0.00555
	loss_policy_3: 0.03326
	accuracy_policy_3: 0.65652
	loss_value_3: 0.03578
	loss_reward_3: 0.00627
	loss_policy_4: 0.03652
	accuracy_policy_4: 0.6327
	loss_value_4: 0.03686
	loss_reward_4: 0.00649
	loss_policy_5: 0.03879
	accuracy_policy_5: 0.6157
	loss_value_5: 0.03804
	loss_reward_5: 0.0078
	loss_policy: 0.24318
	loss_value: 0.33451
	loss_reward: 0.03207
Optimization_Done 39200
[2024-05-05 12:25:39] [command] train weight_iter_39200.pkl 196 197
[2024-05-05 12:25:56] nn step 39250, lr: 0.005.
	loss_policy_0: 0.10367
	accuracy_policy_0: 0.73051
	loss_value_0: 0.14858
	loss_policy_1: 0.02858
	accuracy_policy_1: 0.64383
	loss_value_1: 0.03095
	loss_reward_1: 0.00449
	loss_policy_2: 0.03152
	accuracy_policy_2: 0.61422
	loss_value_2: 0.03221
	loss_reward_2: 0.00431
	loss_policy_3: 0.03449
	accuracy_policy_3: 0.58195
	loss_value_3: 0.03336
	loss_reward_3: 0.00478
	loss_policy_4: 0.03634
	accuracy_policy_4: 0.56012
	loss_value_4: 0.03443
	loss_reward_4: 0.00503
	loss_policy_5: 0.03832
	accuracy_policy_5: 0.5409
	loss_value_5: 0.03562
	loss_reward_5: 0.00574
	loss_policy: 0.27293
	loss_value: 0.31516
	loss_reward: 0.02435
[2024-05-05 12:26:12] nn step 39300, lr: 0.005.
	loss_policy_0: 0.08365
	accuracy_policy_0: 0.78062
	loss_value_0: 0.14851
	loss_policy_1: 0.02551
	accuracy_policy_1: 0.68891
	loss_value_1: 0.03099
	loss_reward_1: 0.00487
	loss_policy_2: 0.02893
	accuracy_policy_2: 0.65281
	loss_value_2: 0.03217
	loss_reward_2: 0.00456
	loss_policy_3: 0.03177
	accuracy_policy_3: 0.62379
	loss_value_3: 0.03311
	loss_reward_3: 0.00498
	loss_policy_4: 0.03337
	accuracy_policy_4: 0.61508
	loss_value_4: 0.034
	loss_reward_4: 0.00519
	loss_policy_5: 0.03519
	accuracy_policy_5: 0.59113
	loss_value_5: 0.03499
	loss_reward_5: 0.0061
	loss_policy: 0.23842
	loss_value: 0.31378
	loss_reward: 0.0257
[2024-05-05 12:26:28] nn step 39350, lr: 0.005.
	loss_policy_0: 0.07912
	accuracy_policy_0: 0.79586
	loss_value_0: 0.15261
	loss_policy_1: 0.02506
	accuracy_policy_1: 0.6966
	loss_value_1: 0.03184
	loss_reward_1: 0.00513
	loss_policy_2: 0.02842
	accuracy_policy_2: 0.66527
	loss_value_2: 0.03301
	loss_reward_2: 0.00488
	loss_policy_3: 0.03092
	accuracy_policy_3: 0.63938
	loss_value_3: 0.03415
	loss_reward_3: 0.00525
	loss_policy_4: 0.03325
	accuracy_policy_4: 0.61973
	loss_value_4: 0.03515
	loss_reward_4: 0.00541
	loss_policy_5: 0.03533
	accuracy_policy_5: 0.59949
	loss_value_5: 0.03624
	loss_reward_5: 0.00644
	loss_policy: 0.2321
	loss_value: 0.323
	loss_reward: 0.02711
[2024-05-05 12:26:44] nn step 39400, lr: 0.005.
	loss_policy_0: 0.07039
	accuracy_policy_0: 0.80316
	loss_value_0: 0.14601
	loss_policy_1: 0.02375
	accuracy_policy_1: 0.69926
	loss_value_1: 0.03048
	loss_reward_1: 0.00484
	loss_policy_2: 0.02711
	accuracy_policy_2: 0.66211
	loss_value_2: 0.03169
	loss_reward_2: 0.00461
	loss_policy_3: 0.02959
	accuracy_policy_3: 0.64082
	loss_value_3: 0.03261
	loss_reward_3: 0.00491
	loss_policy_4: 0.03179
	accuracy_policy_4: 0.6216
	loss_value_4: 0.03351
	loss_reward_4: 0.00527
	loss_policy_5: 0.03393
	accuracy_policy_5: 0.60113
	loss_value_5: 0.03441
	loss_reward_5: 0.00631
	loss_policy: 0.21655
	loss_value: 0.30872
	loss_reward: 0.02595
Optimization_Done 39400
[2024-05-05 12:28:52] [command] train weight_iter_39400.pkl 197 198
[2024-05-05 12:29:09] nn step 39450, lr: 0.005.
	loss_policy_0: 0.12711
	accuracy_policy_0: 0.69793
	loss_value_0: 0.15638
	loss_policy_1: 0.03309
	accuracy_policy_1: 0.6191
	loss_value_1: 0.03279
	loss_reward_1: 0.00468
	loss_policy_2: 0.03672
	accuracy_policy_2: 0.58074
	loss_value_2: 0.03407
	loss_reward_2: 0.00452
	loss_policy_3: 0.03962
	accuracy_policy_3: 0.55199
	loss_value_3: 0.03555
	loss_reward_3: 0.00497
	loss_policy_4: 0.04141
	accuracy_policy_4: 0.53398
	loss_value_4: 0.03659
	loss_reward_4: 0.00555
	loss_policy_5: 0.04446
	accuracy_policy_5: 0.50484
	loss_value_5: 0.03758
	loss_reward_5: 0.00588
	loss_policy: 0.32241
	loss_value: 0.33294
	loss_reward: 0.02561
[2024-05-05 12:29:25] nn step 39500, lr: 0.005.
	loss_policy_0: 0.09495
	accuracy_policy_0: 0.75773
	loss_value_0: 0.15298
	loss_policy_1: 0.02955
	accuracy_policy_1: 0.65645
	loss_value_1: 0.03192
	loss_reward_1: 0.00461
	loss_policy_2: 0.0332
	accuracy_policy_2: 0.62473
	loss_value_2: 0.03321
	loss_reward_2: 0.0043
	loss_policy_3: 0.03659
	accuracy_policy_3: 0.59551
	loss_value_3: 0.03447
	loss_reward_3: 0.00487
	loss_policy_4: 0.03945
	accuracy_policy_4: 0.56582
	loss_value_4: 0.03556
	loss_reward_4: 0.00532
	loss_policy_5: 0.04156
	accuracy_policy_5: 0.54613
	loss_value_5: 0.03648
	loss_reward_5: 0.00564
	loss_policy: 0.2753
	loss_value: 0.32461
	loss_reward: 0.02474
[2024-05-05 12:29:40] nn step 39550, lr: 0.005.
	loss_policy_0: 0.08559
	accuracy_policy_0: 0.77691
	loss_value_0: 0.14909
	loss_policy_1: 0.02713
	accuracy_policy_1: 0.67512
	loss_value_1: 0.03142
	loss_reward_1: 0.00428
	loss_policy_2: 0.031
	accuracy_policy_2: 0.64363
	loss_value_2: 0.0326
	loss_reward_2: 0.00433
	loss_policy_3: 0.03417
	accuracy_policy_3: 0.61242
	loss_value_3: 0.03381
	loss_reward_3: 0.0045
	loss_policy_4: 0.03664
	accuracy_policy_4: 0.58719
	loss_value_4: 0.0348
	loss_reward_4: 0.00499
	loss_policy_5: 0.03897
	accuracy_policy_5: 0.56605
	loss_value_5: 0.03581
	loss_reward_5: 0.00556
	loss_policy: 0.25349
	loss_value: 0.31753
	loss_reward: 0.02366
[2024-05-05 12:29:56] nn step 39600, lr: 0.005.
	loss_policy_0: 0.08279
	accuracy_policy_0: 0.78195
	loss_value_0: 0.15473
	loss_policy_1: 0.0272
	accuracy_policy_1: 0.6802
	loss_value_1: 0.03238
	loss_reward_1: 0.00451
	loss_policy_2: 0.03149
	accuracy_policy_2: 0.64625
	loss_value_2: 0.03375
	loss_reward_2: 0.00437
	loss_policy_3: 0.03485
	accuracy_policy_3: 0.61543
	loss_value_3: 0.0349
	loss_reward_3: 0.00466
	loss_policy_4: 0.03714
	accuracy_policy_4: 0.59379
	loss_value_4: 0.03606
	loss_reward_4: 0.00506
	loss_policy_5: 0.03944
	accuracy_policy_5: 0.57082
	loss_value_5: 0.03708
	loss_reward_5: 0.00557
	loss_policy: 0.25292
	loss_value: 0.32889
	loss_reward: 0.02417
Optimization_Done 39600
[2024-05-05 12:32:02] [command] train weight_iter_39600.pkl 198 199
[2024-05-05 12:32:19] nn step 39650, lr: 0.005.
	loss_policy_0: 0.14475
	accuracy_policy_0: 0.70543
	loss_value_0: 0.1766
	loss_policy_1: 0.038
	accuracy_policy_1: 0.6357
	loss_value_1: 0.037
	loss_reward_1: 0.00554
	loss_policy_2: 0.04199
	accuracy_policy_2: 0.6073
	loss_value_2: 0.0385
	loss_reward_2: 0.00555
	loss_policy_3: 0.04529
	accuracy_policy_3: 0.58125
	loss_value_3: 0.03998
	loss_reward_3: 0.0058
	loss_policy_4: 0.04778
	accuracy_policy_4: 0.56063
	loss_value_4: 0.04139
	loss_reward_4: 0.00638
	loss_policy_5: 0.05111
	accuracy_policy_5: 0.53082
	loss_value_5: 0.04273
	loss_reward_5: 0.00713
	loss_policy: 0.36891
	loss_value: 0.37621
	loss_reward: 0.03041
[2024-05-05 12:32:35] nn step 39700, lr: 0.005.
	loss_policy_0: 0.13217
	accuracy_policy_0: 0.74738
	loss_value_0: 0.18718
	loss_policy_1: 0.03773
	accuracy_policy_1: 0.66203
	loss_value_1: 0.03966
	loss_reward_1: 0.00602
	loss_policy_2: 0.04294
	accuracy_policy_2: 0.62426
	loss_value_2: 0.04118
	loss_reward_2: 0.00575
	loss_policy_3: 0.04685
	accuracy_policy_3: 0.60059
	loss_value_3: 0.04279
	loss_reward_3: 0.00642
	loss_policy_4: 0.04923
	accuracy_policy_4: 0.58125
	loss_value_4: 0.04416
	loss_reward_4: 0.00689
	loss_policy_5: 0.0524
	accuracy_policy_5: 0.55898
	loss_value_5: 0.04576
	loss_reward_5: 0.00809
	loss_policy: 0.36131
	loss_value: 0.40074
	loss_reward: 0.03316
[2024-05-05 12:32:51] nn step 39750, lr: 0.005.
	loss_policy_0: 0.11865
	accuracy_policy_0: 0.76367
	loss_value_0: 0.18241
	loss_policy_1: 0.03556
	accuracy_policy_1: 0.67172
	loss_value_1: 0.03837
	loss_reward_1: 0.00582
	loss_policy_2: 0.0406
	accuracy_policy_2: 0.63523
	loss_value_2: 0.04018
	loss_reward_2: 0.00579
	loss_policy_3: 0.04436
	accuracy_policy_3: 0.61562
	loss_value_3: 0.04148
	loss_reward_3: 0.00604
	loss_policy_4: 0.04742
	accuracy_policy_4: 0.5898
	loss_value_4: 0.0429
	loss_reward_4: 0.00673
	loss_policy_5: 0.05034
	accuracy_policy_5: 0.5623
	loss_value_5: 0.04422
	loss_reward_5: 0.00772
	loss_policy: 0.33693
	loss_value: 0.38957
	loss_reward: 0.0321
[2024-05-05 12:33:06] nn step 39800, lr: 0.005.
	loss_policy_0: 0.10623
	accuracy_policy_0: 0.77086
	loss_value_0: 0.16988
	loss_policy_1: 0.0331
	accuracy_policy_1: 0.66832
	loss_value_1: 0.03597
	loss_reward_1: 0.0056
	loss_policy_2: 0.03739
	accuracy_policy_2: 0.63844
	loss_value_2: 0.03749
	loss_reward_2: 0.00551
	loss_policy_3: 0.0411
	accuracy_policy_3: 0.61316
	loss_value_3: 0.03887
	loss_reward_3: 0.0056
	loss_policy_4: 0.04365
	accuracy_policy_4: 0.59348
	loss_value_4: 0.04032
	loss_reward_4: 0.00633
	loss_policy_5: 0.04679
	accuracy_policy_5: 0.56629
	loss_value_5: 0.04161
	loss_reward_5: 0.00727
	loss_policy: 0.30826
	loss_value: 0.36413
	loss_reward: 0.03031
Optimization_Done 39800
[2024-05-05 12:35:09] [command] train weight_iter_39800.pkl 199 200
[2024-05-05 12:35:26] nn step 39850, lr: 0.005.
	loss_policy_0: 0.1201
	accuracy_policy_0: 0.75066
	loss_value_0: 0.17137
	loss_policy_1: 0.03515
	accuracy_policy_1: 0.66066
	loss_value_1: 0.03615
	loss_reward_1: 0.00691
	loss_policy_2: 0.0394
	accuracy_policy_2: 0.6223
	loss_value_2: 0.03777
	loss_reward_2: 0.00629
	loss_policy_3: 0.0428
	accuracy_policy_3: 0.59656
	loss_value_3: 0.03912
	loss_reward_3: 0.00712
	loss_policy_4: 0.04589
	accuracy_policy_4: 0.56879
	loss_value_4: 0.0403
	loss_reward_4: 0.00756
	loss_policy_5: 0.04914
	accuracy_policy_5: 0.54582
	loss_value_5: 0.04178
	loss_reward_5: 0.00865
	loss_policy: 0.33249
	loss_value: 0.36648
	loss_reward: 0.03654
[2024-05-05 12:35:42] nn step 39900, lr: 0.005.
	loss_policy_0: 0.10709
	accuracy_policy_0: 0.77746
	loss_value_0: 0.17612
	loss_policy_1: 0.03401
	accuracy_policy_1: 0.6832
	loss_value_1: 0.03724
	loss_reward_1: 0.00702
	loss_policy_2: 0.03933
	accuracy_policy_2: 0.64594
	loss_value_2: 0.03899
	loss_reward_2: 0.00669
	loss_policy_3: 0.04261
	accuracy_policy_3: 0.62187
	loss_value_3: 0.0405
	loss_reward_3: 0.00744
	loss_policy_4: 0.04597
	accuracy_policy_4: 0.59246
	loss_value_4: 0.04194
	loss_reward_4: 0.00762
	loss_policy_5: 0.04917
	accuracy_policy_5: 0.56738
	loss_value_5: 0.04358
	loss_reward_5: 0.00896
	loss_policy: 0.31818
	loss_value: 0.37836
	loss_reward: 0.03773
[2024-05-05 12:35:58] nn step 39950, lr: 0.005.
	loss_policy_0: 0.08943
	accuracy_policy_0: 0.79145
	loss_value_0: 0.15487
	loss_policy_1: 0.02926
	accuracy_policy_1: 0.69312
	loss_value_1: 0.03258
	loss_reward_1: 0.00621
	loss_policy_2: 0.03337
	accuracy_policy_2: 0.65488
	loss_value_2: 0.03398
	loss_reward_2: 0.00611
	loss_policy_3: 0.03666
	accuracy_policy_3: 0.63137
	loss_value_3: 0.03521
	loss_reward_3: 0.00635
	loss_policy_4: 0.03941
	accuracy_policy_4: 0.60414
	loss_value_4: 0.03661
	loss_reward_4: 0.00679
	loss_policy_5: 0.04256
	accuracy_policy_5: 0.57922
	loss_value_5: 0.03802
	loss_reward_5: 0.00807
	loss_policy: 0.27069
	loss_value: 0.33127
	loss_reward: 0.03354
[2024-05-05 12:36:14] nn step 40000, lr: 0.005.
	loss_policy_0: 0.09558
	accuracy_policy_0: 0.79227
	loss_value_0: 0.17236
	loss_policy_1: 0.03218
	accuracy_policy_1: 0.69066
	loss_value_1: 0.03651
	loss_reward_1: 0.00691
	loss_policy_2: 0.03715
	accuracy_policy_2: 0.65129
	loss_value_2: 0.03795
	loss_reward_2: 0.0064
	loss_policy_3: 0.04051
	accuracy_policy_3: 0.63238
	loss_value_3: 0.03943
	loss_reward_3: 0.00695
	loss_policy_4: 0.04388
	accuracy_policy_4: 0.60594
	loss_value_4: 0.04081
	loss_reward_4: 0.00743
	loss_policy_5: 0.04724
	accuracy_policy_5: 0.57391
	loss_value_5: 0.04214
	loss_reward_5: 0.00891
	loss_policy: 0.29654
	loss_value: 0.3692
	loss_reward: 0.03659
Optimization_Done 40000
[2024-05-05 12:38:14] [command] train weight_iter_40000.pkl 200 201
[2024-05-05 12:38:32] nn step 40050, lr: 0.005.
	loss_policy_0: 0.12627
	accuracy_policy_0: 0.68492
	loss_value_0: 0.15017
	loss_policy_1: 0.0326
	accuracy_policy_1: 0.61355
	loss_value_1: 0.03158
	loss_reward_1: 0.00579
	loss_policy_2: 0.03634
	accuracy_policy_2: 0.57035
	loss_value_2: 0.0329
	loss_reward_2: 0.00539
	loss_policy_3: 0.0391
	accuracy_policy_3: 0.54711
	loss_value_3: 0.03408
	loss_reward_3: 0.00617
	loss_policy_4: 0.04142
	accuracy_policy_4: 0.51746
	loss_value_4: 0.03507
	loss_reward_4: 0.00617
	loss_policy_5: 0.04385
	accuracy_policy_5: 0.49738
	loss_value_5: 0.03621
	loss_reward_5: 0.00738
	loss_policy: 0.31959
	loss_value: 0.32001
	loss_reward: 0.0309
[2024-05-05 12:38:48] nn step 40100, lr: 0.005.
	loss_policy_0: 0.09879
	accuracy_policy_0: 0.74273
	loss_value_0: 0.1555
	loss_policy_1: 0.02958
	accuracy_policy_1: 0.65191
	loss_value_1: 0.03266
	loss_reward_1: 0.00612
	loss_policy_2: 0.03334
	accuracy_policy_2: 0.61434
	loss_value_2: 0.03392
	loss_reward_2: 0.00583
	loss_policy_3: 0.03651
	accuracy_policy_3: 0.58832
	loss_value_3: 0.03493
	loss_reward_3: 0.00634
	loss_policy_4: 0.03882
	accuracy_policy_4: 0.56465
	loss_value_4: 0.03614
	loss_reward_4: 0.00685
	loss_policy_5: 0.04167
	accuracy_policy_5: 0.5425
	loss_value_5: 0.0372
	loss_reward_5: 0.00765
	loss_policy: 0.27871
	loss_value: 0.33034
	loss_reward: 0.0328
[2024-05-05 12:39:04] nn step 40150, lr: 0.005.
	loss_policy_0: 0.09504
	accuracy_policy_0: 0.76043
	loss_value_0: 0.16058
	loss_policy_1: 0.0293
	accuracy_policy_1: 0.6709
	loss_value_1: 0.0337
	loss_reward_1: 0.00622
	loss_policy_2: 0.03336
	accuracy_policy_2: 0.62887
	loss_value_2: 0.03488
	loss_reward_2: 0.00589
	loss_policy_3: 0.03644
	accuracy_policy_3: 0.60285
	loss_value_3: 0.03597
	loss_reward_3: 0.00652
	loss_policy_4: 0.03878
	accuracy_policy_4: 0.57953
	loss_value_4: 0.03691
	loss_reward_4: 0.00678
	loss_policy_5: 0.04137
	accuracy_policy_5: 0.55926
	loss_value_5: 0.03813
	loss_reward_5: 0.00804
	loss_policy: 0.27429
	loss_value: 0.34016
	loss_reward: 0.03344
[2024-05-05 12:39:20] nn step 40200, lr: 0.005.
	loss_policy_0: 0.08124
	accuracy_policy_0: 0.77312
	loss_value_0: 0.1444
	loss_policy_1: 0.02575
	accuracy_policy_1: 0.6718
	loss_value_1: 0.03025
	loss_reward_1: 0.00567
	loss_policy_2: 0.0292
	accuracy_policy_2: 0.63777
	loss_value_2: 0.0314
	loss_reward_2: 0.00523
	loss_policy_3: 0.03214
	accuracy_policy_3: 0.60738
	loss_value_3: 0.03234
	loss_reward_3: 0.00588
	loss_policy_4: 0.03493
	accuracy_policy_4: 0.57973
	loss_value_4: 0.03322
	loss_reward_4: 0.00612
	loss_policy_5: 0.03712
	accuracy_policy_5: 0.56191
	loss_value_5: 0.0343
	loss_reward_5: 0.00719
	loss_policy: 0.24038
	loss_value: 0.30591
	loss_reward: 0.03009
Optimization_Done 40200
[2024-05-05 12:41:04] [command] train weight_iter_40200.pkl 201 202
[2024-05-05 12:41:21] nn step 40250, lr: 0.005.
	loss_policy_0: 0.1166
	accuracy_policy_0: 0.71008
	loss_value_0: 0.16095
	loss_policy_1: 0.03145
	accuracy_policy_1: 0.63355
	loss_value_1: 0.03378
	loss_reward_1: 0.00531
	loss_policy_2: 0.03466
	accuracy_policy_2: 0.59684
	loss_value_2: 0.03486
	loss_reward_2: 0.00525
	loss_policy_3: 0.03754
	accuracy_policy_3: 0.56773
	loss_value_3: 0.03596
	loss_reward_3: 0.00553
	loss_policy_4: 0.04047
	accuracy_policy_4: 0.54523
	loss_value_4: 0.03694
	loss_reward_4: 0.00606
	loss_policy_5: 0.0427
	accuracy_policy_5: 0.51551
	loss_value_5: 0.03801
	loss_reward_5: 0.00701
	loss_policy: 0.30342
	loss_value: 0.34051
	loss_reward: 0.02915
[2024-05-05 12:41:37] nn step 40300, lr: 0.005.
	loss_policy_0: 0.0934
	accuracy_policy_0: 0.76281
	loss_value_0: 0.16225
	loss_policy_1: 0.02869
	accuracy_policy_1: 0.6693
	loss_value_1: 0.03418
	loss_reward_1: 0.00572
	loss_policy_2: 0.03229
	accuracy_policy_2: 0.63555
	loss_value_2: 0.03548
	loss_reward_2: 0.00538
	loss_policy_3: 0.03532
	accuracy_policy_3: 0.60648
	loss_value_3: 0.03673
	loss_reward_3: 0.00575
	loss_policy_4: 0.03767
	accuracy_policy_4: 0.58531
	loss_value_4: 0.03761
	loss_reward_4: 0.00601
	loss_policy_5: 0.04055
	accuracy_policy_5: 0.55453
	loss_value_5: 0.03883
	loss_reward_5: 0.00683
	loss_policy: 0.26793
	loss_value: 0.34508
	loss_reward: 0.02968
[2024-05-05 12:41:53] nn step 40350, lr: 0.005.
	loss_policy_0: 0.08714
	accuracy_policy_0: 0.77508
	loss_value_0: 0.16493
	loss_policy_1: 0.02752
	accuracy_policy_1: 0.68312
	loss_value_1: 0.03455
	loss_reward_1: 0.00538
	loss_policy_2: 0.03169
	accuracy_policy_2: 0.64848
	loss_value_2: 0.03592
	loss_reward_2: 0.00527
	loss_policy_3: 0.03514
	accuracy_policy_3: 0.61801
	loss_value_3: 0.03695
	loss_reward_3: 0.00581
	loss_policy_4: 0.03712
	accuracy_policy_4: 0.59586
	loss_value_4: 0.03793
	loss_reward_4: 0.0062
	loss_policy_5: 0.04039
	accuracy_policy_5: 0.55801
	loss_value_5: 0.03908
	loss_reward_5: 0.00691
	loss_policy: 0.259
	loss_value: 0.34937
	loss_reward: 0.02957
[2024-05-05 12:42:09] nn step 40400, lr: 0.005.
	loss_policy_0: 0.08453
	accuracy_policy_0: 0.77473
	loss_value_0: 0.16144
	loss_policy_1: 0.02665
	accuracy_policy_1: 0.68895
	loss_value_1: 0.03375
	loss_reward_1: 0.00546
	loss_policy_2: 0.03035
	accuracy_policy_2: 0.64992
	loss_value_2: 0.03482
	loss_reward_2: 0.00515
	loss_policy_3: 0.03352
	accuracy_policy_3: 0.62605
	loss_value_3: 0.03602
	loss_reward_3: 0.00555
	loss_policy_4: 0.03605
	accuracy_policy_4: 0.60152
	loss_value_4: 0.03708
	loss_reward_4: 0.006
	loss_policy_5: 0.03891
	accuracy_policy_5: 0.57
	loss_value_5: 0.03834
	loss_reward_5: 0.00688
	loss_policy: 0.25001
	loss_value: 0.34146
	loss_reward: 0.02905
Optimization_Done 40400
[2024-05-05 12:44:12] [command] train weight_iter_40400.pkl 202 203
[2024-05-05 12:44:28] nn step 40450, lr: 0.005.
	loss_policy_0: 0.16275
	accuracy_policy_0: 0.70539
	loss_value_0: 0.1959
	loss_policy_1: 0.03945
	accuracy_policy_1: 0.6475
	loss_value_1: 0.04057
	loss_reward_1: 0.00638
	loss_policy_2: 0.04277
	accuracy_policy_2: 0.62055
	loss_value_2: 0.04207
	loss_reward_2: 0.00632
	loss_policy_3: 0.04577
	accuracy_policy_3: 0.60008
	loss_value_3: 0.04361
	loss_reward_3: 0.00656
	loss_policy_4: 0.04913
	accuracy_policy_4: 0.56961
	loss_value_4: 0.0449
	loss_reward_4: 0.0072
	loss_policy_5: 0.05234
	accuracy_policy_5: 0.54816
	loss_value_5: 0.04637
	loss_reward_5: 0.00765
	loss_policy: 0.39222
	loss_value: 0.41342
	loss_reward: 0.0341
[2024-05-05 12:44:44] nn step 40500, lr: 0.005.
	loss_policy_0: 0.11195
	accuracy_policy_0: 0.76242
	loss_value_0: 0.18594
	loss_policy_1: 0.03205
	accuracy_policy_1: 0.68871
	loss_value_1: 0.03902
	loss_reward_1: 0.00602
	loss_policy_2: 0.0361
	accuracy_policy_2: 0.66152
	loss_value_2: 0.04025
	loss_reward_2: 0.00607
	loss_policy_3: 0.0392
	accuracy_policy_3: 0.63883
	loss_value_3: 0.04145
	loss_reward_3: 0.00626
	loss_policy_4: 0.04232
	accuracy_policy_4: 0.61578
	loss_value_4: 0.04249
	loss_reward_4: 0.00703
	loss_policy_5: 0.04504
	accuracy_policy_5: 0.59309
	loss_value_5: 0.0436
	loss_reward_5: 0.00751
	loss_policy: 0.30667
	loss_value: 0.39275
	loss_reward: 0.03289
[2024-05-05 12:45:00] nn step 40550, lr: 0.005.
	loss_policy_0: 0.10108
	accuracy_policy_0: 0.78156
	loss_value_0: 0.18602
	loss_policy_1: 0.0298
	accuracy_policy_1: 0.70719
	loss_value_1: 0.03864
	loss_reward_1: 0.00582
	loss_policy_2: 0.03369
	accuracy_policy_2: 0.67508
	loss_value_2: 0.03993
	loss_reward_2: 0.00579
	loss_policy_3: 0.03754
	accuracy_policy_3: 0.6484
	loss_value_3: 0.04121
	loss_reward_3: 0.00592
	loss_policy_4: 0.04023
	accuracy_policy_4: 0.62305
	loss_value_4: 0.04239
	loss_reward_4: 0.00655
	loss_policy_5: 0.04315
	accuracy_policy_5: 0.60566
	loss_value_5: 0.04335
	loss_reward_5: 0.00734
	loss_policy: 0.28551
	loss_value: 0.39154
	loss_reward: 0.03142
[2024-05-05 12:45:16] nn step 40600, lr: 0.005.
	loss_policy_0: 0.09415
	accuracy_policy_0: 0.79098
	loss_value_0: 0.1835
	loss_policy_1: 0.02935
	accuracy_policy_1: 0.70898
	loss_value_1: 0.03844
	loss_reward_1: 0.00582
	loss_policy_2: 0.03313
	accuracy_policy_2: 0.67848
	loss_value_2: 0.03971
	loss_reward_2: 0.00568
	loss_policy_3: 0.03647
	accuracy_policy_3: 0.6548
	loss_value_3: 0.04079
	loss_reward_3: 0.00606
	loss_policy_4: 0.03983
	accuracy_policy_4: 0.62277
	loss_value_4: 0.04192
	loss_reward_4: 0.00669
	loss_policy_5: 0.04254
	accuracy_policy_5: 0.60637
	loss_value_5: 0.04309
	loss_reward_5: 0.00741
	loss_policy: 0.27547
	loss_value: 0.38745
	loss_reward: 0.03165
Optimization_Done 40600
[2024-05-05 12:47:19] [command] train weight_iter_40600.pkl 203 204
[2024-05-05 12:47:36] nn step 40650, lr: 0.005.
	loss_policy_0: 0.13507
	accuracy_policy_0: 0.73711
	loss_value_0: 0.19438
	loss_policy_1: 0.03581
	accuracy_policy_1: 0.66754
	loss_value_1: 0.04053
	loss_reward_1: 0.00694
	loss_policy_2: 0.03981
	accuracy_policy_2: 0.64031
	loss_value_2: 0.04227
	loss_reward_2: 0.00674
	loss_policy_3: 0.04442
	accuracy_policy_3: 0.60734
	loss_value_3: 0.04401
	loss_reward_3: 0.00704
	loss_policy_4: 0.04727
	accuracy_policy_4: 0.58082
	loss_value_4: 0.04559
	loss_reward_4: 0.0078
	loss_policy_5: 0.05066
	accuracy_policy_5: 0.56273
	loss_value_5: 0.0473
	loss_reward_5: 0.00866
	loss_policy: 0.35303
	loss_value: 0.41408
	loss_reward: 0.03717
[2024-05-05 12:47:52] nn step 40700, lr: 0.005.
	loss_policy_0: 0.10239
	accuracy_policy_0: 0.78469
	loss_value_0: 0.18324
	loss_policy_1: 0.03075
	accuracy_policy_1: 0.70367
	loss_value_1: 0.03853
	loss_reward_1: 0.00634
	loss_policy_2: 0.03433
	accuracy_policy_2: 0.67699
	loss_value_2: 0.04006
	loss_reward_2: 0.00634
	loss_policy_3: 0.03789
	accuracy_policy_3: 0.64973
	loss_value_3: 0.0415
	loss_reward_3: 0.00669
	loss_policy_4: 0.04137
	accuracy_policy_4: 0.62687
	loss_value_4: 0.04299
	loss_reward_4: 0.00759
	loss_policy_5: 0.04401
	accuracy_policy_5: 0.60977
	loss_value_5: 0.04436
	loss_reward_5: 0.0084
	loss_policy: 0.29075
	loss_value: 0.39068
	loss_reward: 0.03536
[2024-05-05 12:48:08] nn step 40750, lr: 0.005.
	loss_policy_0: 0.09516
	accuracy_policy_0: 0.79902
	loss_value_0: 0.18749
	loss_policy_1: 0.02969
	accuracy_policy_1: 0.72133
	loss_value_1: 0.03933
	loss_reward_1: 0.00706
	loss_policy_2: 0.03433
	accuracy_policy_2: 0.68871
	loss_value_2: 0.04085
	loss_reward_2: 0.00673
	loss_policy_3: 0.03787
	accuracy_policy_3: 0.65793
	loss_value_3: 0.04228
	loss_reward_3: 0.00705
	loss_policy_4: 0.0412
	accuracy_policy_4: 0.63281
	loss_value_4: 0.04372
	loss_reward_4: 0.00764
	loss_policy_5: 0.04402
	accuracy_policy_5: 0.61703
	loss_value_5: 0.04511
	loss_reward_5: 0.00856
	loss_policy: 0.28226
	loss_value: 0.39878
	loss_reward: 0.03705
[2024-05-05 12:48:24] nn step 40800, lr: 0.005.
	loss_policy_0: 0.09001
	accuracy_policy_0: 0.80699
	loss_value_0: 0.18972
	loss_policy_1: 0.029
	accuracy_policy_1: 0.72445
	loss_value_1: 0.03962
	loss_reward_1: 0.00653
	loss_policy_2: 0.03348
	accuracy_policy_2: 0.69258
	loss_value_2: 0.04115
	loss_reward_2: 0.00628
	loss_policy_3: 0.03763
	accuracy_policy_3: 0.66566
	loss_value_3: 0.04252
	loss_reward_3: 0.00736
	loss_policy_4: 0.04052
	accuracy_policy_4: 0.63801
	loss_value_4: 0.04407
	loss_reward_4: 0.00754
	loss_policy_5: 0.04393
	accuracy_policy_5: 0.61906
	loss_value_5: 0.04521
	loss_reward_5: 0.00872
	loss_policy: 0.27459
	loss_value: 0.40228
	loss_reward: 0.03644
Optimization_Done 40800
[2024-05-05 12:50:26] [command] train weight_iter_40800.pkl 204 205
[2024-05-05 12:50:43] nn step 40850, lr: 0.005.
	loss_policy_0: 0.13305
	accuracy_policy_0: 0.70496
	loss_value_0: 0.17277
	loss_policy_1: 0.03598
	accuracy_policy_1: 0.62121
	loss_value_1: 0.03615
	loss_reward_1: 0.00657
	loss_policy_2: 0.03963
	accuracy_policy_2: 0.59156
	loss_value_2: 0.03776
	loss_reward_2: 0.00607
	loss_policy_3: 0.04331
	accuracy_policy_3: 0.56285
	loss_value_3: 0.03946
	loss_reward_3: 0.00677
	loss_policy_4: 0.04619
	accuracy_policy_4: 0.5425
	loss_value_4: 0.04077
	loss_reward_4: 0.00718
	loss_policy_5: 0.04906
	accuracy_policy_5: 0.51715
	loss_value_5: 0.0422
	loss_reward_5: 0.00799
	loss_policy: 0.34723
	loss_value: 0.36912
	loss_reward: 0.03457
[2024-05-05 12:50:58] nn step 40900, lr: 0.005.
	loss_policy_0: 0.1077
	accuracy_policy_0: 0.74988
	loss_value_0: 0.16986
	loss_policy_1: 0.03245
	accuracy_policy_1: 0.65895
	loss_value_1: 0.03577
	loss_reward_1: 0.00654
	loss_policy_2: 0.03633
	accuracy_policy_2: 0.6291
	loss_value_2: 0.03733
	loss_reward_2: 0.00615
	loss_policy_3: 0.04012
	accuracy_policy_3: 0.59582
	loss_value_3: 0.0388
	loss_reward_3: 0.00663
	loss_policy_4: 0.04285
	accuracy_policy_4: 0.5798
	loss_value_4: 0.04013
	loss_reward_4: 0.00736
	loss_policy_5: 0.04567
	accuracy_policy_5: 0.55305
	loss_value_5: 0.04132
	loss_reward_5: 0.0083
	loss_policy: 0.30513
	loss_value: 0.36321
	loss_reward: 0.03498
[2024-05-05 12:51:14] nn step 40950, lr: 0.005.
	loss_policy_0: 0.10069
	accuracy_policy_0: 0.77125
	loss_value_0: 0.17528
	loss_policy_1: 0.03183
	accuracy_policy_1: 0.67941
	loss_value_1: 0.03693
	loss_reward_1: 0.00702
	loss_policy_2: 0.03618
	accuracy_policy_2: 0.64062
	loss_value_2: 0.03836
	loss_reward_2: 0.0062
	loss_policy_3: 0.04016
	accuracy_policy_3: 0.60871
	loss_value_3: 0.0397
	loss_reward_3: 0.00723
	loss_policy_4: 0.04277
	accuracy_policy_4: 0.59445
	loss_value_4: 0.041
	loss_reward_4: 0.0078
	loss_policy_5: 0.046
	accuracy_policy_5: 0.57176
	loss_value_5: 0.04243
	loss_reward_5: 0.00864
	loss_policy: 0.29762
	loss_value: 0.37369
	loss_reward: 0.03689
[2024-05-05 12:51:30] nn step 41000, lr: 0.005.
	loss_policy_0: 0.08998
	accuracy_policy_0: 0.78531
	loss_value_0: 0.16505
	loss_policy_1: 0.0295
	accuracy_policy_1: 0.68562
	loss_value_1: 0.03491
	loss_reward_1: 0.00648
	loss_policy_2: 0.03311
	accuracy_policy_2: 0.6557
	loss_value_2: 0.03642
	loss_reward_2: 0.00594
	loss_policy_3: 0.03719
	accuracy_policy_3: 0.62066
	loss_value_3: 0.03773
	loss_reward_3: 0.00673
	loss_policy_4: 0.03973
	accuracy_policy_4: 0.60273
	loss_value_4: 0.03899
	loss_reward_4: 0.00729
	loss_policy_5: 0.04273
	accuracy_policy_5: 0.5768
	loss_value_5: 0.04024
	loss_reward_5: 0.00819
	loss_policy: 0.27226
	loss_value: 0.35335
	loss_reward: 0.03463
Optimization_Done 41000
[2024-05-05 12:53:23] [command] train weight_iter_41000.pkl 205 206
[2024-05-05 12:53:40] nn step 41050, lr: 0.005.
	loss_policy_0: 0.14183
	accuracy_policy_0: 0.70363
	loss_value_0: 0.18363
	loss_policy_1: 0.03804
	accuracy_policy_1: 0.61687
	loss_value_1: 0.0386
	loss_reward_1: 0.00623
	loss_policy_2: 0.04178
	accuracy_policy_2: 0.58875
	loss_value_2: 0.04021
	loss_reward_2: 0.00557
	loss_policy_3: 0.04531
	accuracy_policy_3: 0.56133
	loss_value_3: 0.04162
	loss_reward_3: 0.00604
	loss_policy_4: 0.04781
	accuracy_policy_4: 0.53793
	loss_value_4: 0.04298
	loss_reward_4: 0.00687
	loss_policy_5: 0.05044
	accuracy_policy_5: 0.51246
	loss_value_5: 0.04437
	loss_reward_5: 0.00788
	loss_policy: 0.36521
	loss_value: 0.39141
	loss_reward: 0.03259
[2024-05-05 12:53:55] nn step 41100, lr: 0.005.
	loss_policy_0: 0.10969
	accuracy_policy_0: 0.75574
	loss_value_0: 0.17669
	loss_policy_1: 0.03393
	accuracy_policy_1: 0.65121
	loss_value_1: 0.03686
	loss_reward_1: 0.00584
	loss_policy_2: 0.03762
	accuracy_policy_2: 0.61773
	loss_value_2: 0.03821
	loss_reward_2: 0.00552
	loss_policy_3: 0.04141
	accuracy_policy_3: 0.5857
	loss_value_3: 0.03957
	loss_reward_3: 0.00606
	loss_policy_4: 0.04374
	accuracy_policy_4: 0.56578
	loss_value_4: 0.04088
	loss_reward_4: 0.00645
	loss_policy_5: 0.04639
	accuracy_policy_5: 0.54727
	loss_value_5: 0.04219
	loss_reward_5: 0.00761
	loss_policy: 0.31277
	loss_value: 0.3744
	loss_reward: 0.03149
[2024-05-05 12:54:11] nn step 41150, lr: 0.005.
	loss_policy_0: 0.10191
	accuracy_policy_0: 0.7668
	loss_value_0: 0.1757
	loss_policy_1: 0.03274
	accuracy_policy_1: 0.65832
	loss_value_1: 0.03712
	loss_reward_1: 0.00584
	loss_policy_2: 0.03748
	accuracy_policy_2: 0.62344
	loss_value_2: 0.03847
	loss_reward_2: 0.00556
	loss_policy_3: 0.04074
	accuracy_policy_3: 0.5941
	loss_value_3: 0.03965
	loss_reward_3: 0.00605
	loss_policy_4: 0.04299
	accuracy_policy_4: 0.57035
	loss_value_4: 0.04086
	loss_reward_4: 0.0067
	loss_policy_5: 0.04574
	accuracy_policy_5: 0.54801
	loss_value_5: 0.04227
	loss_reward_5: 0.00771
	loss_policy: 0.30161
	loss_value: 0.37406
	loss_reward: 0.03187
[2024-05-05 12:54:27] nn step 41200, lr: 0.005.
	loss_policy_0: 0.09675
	accuracy_policy_0: 0.77723
	loss_value_0: 0.17342
	loss_policy_1: 0.0314
	accuracy_policy_1: 0.67402
	loss_value_1: 0.03641
	loss_reward_1: 0.00559
	loss_policy_2: 0.03552
	accuracy_policy_2: 0.63758
	loss_value_2: 0.03777
	loss_reward_2: 0.0054
	loss_policy_3: 0.03917
	accuracy_policy_3: 0.59848
	loss_value_3: 0.03891
	loss_reward_3: 0.00604
	loss_policy_4: 0.04193
	accuracy_policy_4: 0.57898
	loss_value_4: 0.04039
	loss_reward_4: 0.00646
	loss_policy_5: 0.04503
	accuracy_policy_5: 0.5552
	loss_value_5: 0.04167
	loss_reward_5: 0.00769
	loss_policy: 0.28981
	loss_value: 0.36857
	loss_reward: 0.03119
Optimization_Done 41200
[2024-05-05 12:56:30] [command] train weight_iter_41200.pkl 206 207
[2024-05-05 12:56:48] nn step 41250, lr: 0.005.
	loss_policy_0: 0.1326
	accuracy_policy_0: 0.74523
	loss_value_0: 0.18201
	loss_policy_1: 0.0342
	accuracy_policy_1: 0.68527
	loss_value_1: 0.03772
	loss_reward_1: 0.00587
	loss_policy_2: 0.03793
	accuracy_policy_2: 0.65449
	loss_value_2: 0.03921
	loss_reward_2: 0.00598
	loss_policy_3: 0.04106
	accuracy_policy_3: 0.6325
	loss_value_3: 0.04035
	loss_reward_3: 0.00641
	loss_policy_4: 0.04414
	accuracy_policy_4: 0.60754
	loss_value_4: 0.0414
	loss_reward_4: 0.00693
	loss_policy_5: 0.04734
	accuracy_policy_5: 0.58332
	loss_value_5: 0.04241
	loss_reward_5: 0.00776
	loss_policy: 0.33727
	loss_value: 0.38309
	loss_reward: 0.03296
[2024-05-05 12:57:03] nn step 41300, lr: 0.005.
	loss_policy_0: 0.10192
	accuracy_policy_0: 0.78645
	loss_value_0: 0.18041
	loss_policy_1: 0.03011
	accuracy_policy_1: 0.71301
	loss_value_1: 0.03767
	loss_reward_1: 0.00638
	loss_policy_2: 0.03433
	accuracy_policy_2: 0.68359
	loss_value_2: 0.03893
	loss_reward_2: 0.0061
	loss_policy_3: 0.03733
	accuracy_policy_3: 0.66195
	loss_value_3: 0.03995
	loss_reward_3: 0.00637
	loss_policy_4: 0.04045
	accuracy_policy_4: 0.63797
	loss_value_4: 0.04108
	loss_reward_4: 0.00684
	loss_policy_5: 0.04325
	accuracy_policy_5: 0.61289
	loss_value_5: 0.04222
	loss_reward_5: 0.00768
	loss_policy: 0.2874
	loss_value: 0.38027
	loss_reward: 0.03337
[2024-05-05 12:57:19] nn step 41350, lr: 0.005.
	loss_policy_0: 0.09182
	accuracy_policy_0: 0.79965
	loss_value_0: 0.17663
	loss_policy_1: 0.02802
	accuracy_policy_1: 0.72316
	loss_value_1: 0.0369
	loss_reward_1: 0.0061
	loss_policy_2: 0.03237
	accuracy_policy_2: 0.69113
	loss_value_2: 0.03829
	loss_reward_2: 0.00602
	loss_policy_3: 0.03558
	accuracy_policy_3: 0.67031
	loss_value_3: 0.03932
	loss_reward_3: 0.00633
	loss_policy_4: 0.03813
	accuracy_policy_4: 0.64719
	loss_value_4: 0.04012
	loss_reward_4: 0.00697
	loss_policy_5: 0.04071
	accuracy_policy_5: 0.62426
	loss_value_5: 0.04117
	loss_reward_5: 0.00754
	loss_policy: 0.26662
	loss_value: 0.37243
	loss_reward: 0.03295
[2024-05-05 12:57:35] nn step 41400, lr: 0.005.
	loss_policy_0: 0.08686
	accuracy_policy_0: 0.8102
	loss_value_0: 0.1755
	loss_policy_1: 0.02811
	accuracy_policy_1: 0.72035
	loss_value_1: 0.03669
	loss_reward_1: 0.00611
	loss_policy_2: 0.03231
	accuracy_policy_2: 0.68707
	loss_value_2: 0.03795
	loss_reward_2: 0.00608
	loss_policy_3: 0.03549
	accuracy_policy_3: 0.66312
	loss_value_3: 0.03897
	loss_reward_3: 0.00644
	loss_policy_4: 0.03814
	accuracy_policy_4: 0.64129
	loss_value_4: 0.04002
	loss_reward_4: 0.00701
	loss_policy_5: 0.04151
	accuracy_policy_5: 0.61676
	loss_value_5: 0.0412
	loss_reward_5: 0.00795
	loss_policy: 0.26243
	loss_value: 0.37032
	loss_reward: 0.03359
Optimization_Done 41400
[2024-05-05 12:59:19] [command] train weight_iter_41400.pkl 207 208
[2024-05-05 12:59:36] nn step 41450, lr: 0.005.
	loss_policy_0: 0.13401
	accuracy_policy_0: 0.72793
	loss_value_0: 0.18787
	loss_policy_1: 0.0349
	accuracy_policy_1: 0.66305
	loss_value_1: 0.03897
	loss_reward_1: 0.00621
	loss_policy_2: 0.03789
	accuracy_policy_2: 0.63473
	loss_value_2: 0.04043
	loss_reward_2: 0.00625
	loss_policy_3: 0.04164
	accuracy_policy_3: 0.60398
	loss_value_3: 0.04164
	loss_reward_3: 0.00664
	loss_policy_4: 0.0441
	accuracy_policy_4: 0.57703
	loss_value_4: 0.04271
	loss_reward_4: 0.00683
	loss_policy_5: 0.04717
	accuracy_policy_5: 0.5577
	loss_value_5: 0.04367
	loss_reward_5: 0.00784
	loss_policy: 0.33971
	loss_value: 0.39529
	loss_reward: 0.03377
[2024-05-05 12:59:52] nn step 41500, lr: 0.005.
	loss_policy_0: 0.10755
	accuracy_policy_0: 0.78488
	loss_value_0: 0.18851
	loss_policy_1: 0.03189
	accuracy_policy_1: 0.70766
	loss_value_1: 0.03951
	loss_reward_1: 0.0067
	loss_policy_2: 0.03604
	accuracy_policy_2: 0.67371
	loss_value_2: 0.04091
	loss_reward_2: 0.00678
	loss_policy_3: 0.03937
	accuracy_policy_3: 0.64898
	loss_value_3: 0.04188
	loss_reward_3: 0.00739
	loss_policy_4: 0.04266
	accuracy_policy_4: 0.62406
	loss_value_4: 0.04288
	loss_reward_4: 0.00774
	loss_policy_5: 0.04576
	accuracy_policy_5: 0.60398
	loss_value_5: 0.04395
	loss_reward_5: 0.00869
	loss_policy: 0.30326
	loss_value: 0.39765
	loss_reward: 0.0373
[2024-05-05 13:00:08] nn step 41550, lr: 0.005.
	loss_policy_0: 0.09978
	accuracy_policy_0: 0.79348
	loss_value_0: 0.18749
	loss_policy_1: 0.02986
	accuracy_policy_1: 0.71543
	loss_value_1: 0.03893
	loss_reward_1: 0.00661
	loss_policy_2: 0.03441
	accuracy_policy_2: 0.68125
	loss_value_2: 0.04013
	loss_reward_2: 0.00675
	loss_policy_3: 0.03742
	accuracy_policy_3: 0.65945
	loss_value_3: 0.041
	loss_reward_3: 0.00713
	loss_policy_4: 0.04062
	accuracy_policy_4: 0.63586
	loss_value_4: 0.04188
	loss_reward_4: 0.00758
	loss_policy_5: 0.04421
	accuracy_policy_5: 0.61113
	loss_value_5: 0.04275
	loss_reward_5: 0.00845
	loss_policy: 0.2863
	loss_value: 0.39219
	loss_reward: 0.03652
[2024-05-05 13:00:24] nn step 41600, lr: 0.005.
	loss_policy_0: 0.09429
	accuracy_policy_0: 0.79871
	loss_value_0: 0.18415
	loss_policy_1: 0.02872
	accuracy_policy_1: 0.72164
	loss_value_1: 0.03808
	loss_reward_1: 0.00643
	loss_policy_2: 0.03271
	accuracy_policy_2: 0.68793
	loss_value_2: 0.03889
	loss_reward_2: 0.00662
	loss_policy_3: 0.03576
	accuracy_policy_3: 0.66465
	loss_value_3: 0.0397
	loss_reward_3: 0.00708
	loss_policy_4: 0.03909
	accuracy_policy_4: 0.63918
	loss_value_4: 0.04045
	loss_reward_4: 0.00727
	loss_policy_5: 0.04173
	accuracy_policy_5: 0.62383
	loss_value_5: 0.04152
	loss_reward_5: 0.00817
	loss_policy: 0.27231
	loss_value: 0.38279
	loss_reward: 0.03557
Optimization_Done 41600
[2024-05-05 13:02:26] [command] train weight_iter_41600.pkl 208 209
[2024-05-05 13:02:43] nn step 41650, lr: 0.005.
	loss_policy_0: 0.12502
	accuracy_policy_0: 0.74648
	loss_value_0: 0.17445
	loss_policy_1: 0.03579
	accuracy_policy_1: 0.65988
	loss_value_1: 0.03669
	loss_reward_1: 0.00818
	loss_policy_2: 0.03919
	accuracy_policy_2: 0.62363
	loss_value_2: 0.03817
	loss_reward_2: 0.00781
	loss_policy_3: 0.04328
	accuracy_policy_3: 0.59324
	loss_value_3: 0.03963
	loss_reward_3: 0.00845
	loss_policy_4: 0.04687
	accuracy_policy_4: 0.56125
	loss_value_4: 0.04102
	loss_reward_4: 0.00892
	loss_policy_5: 0.04987
	accuracy_policy_5: 0.53875
	loss_value_5: 0.04249
	loss_reward_5: 0.00991
	loss_policy: 0.34002
	loss_value: 0.37246
	loss_reward: 0.04327
[2024-05-05 13:02:59] nn step 41700, lr: 0.005.
	loss_policy_0: 0.09906
	accuracy_policy_0: 0.785
	loss_value_0: 0.16923
	loss_policy_1: 0.0311
	accuracy_policy_1: 0.69605
	loss_value_1: 0.03561
	loss_reward_1: 0.00795
	loss_policy_2: 0.03526
	accuracy_policy_2: 0.66473
	loss_value_2: 0.03697
	loss_reward_2: 0.00774
	loss_policy_3: 0.03912
	accuracy_policy_3: 0.6325
	loss_value_3: 0.03831
	loss_reward_3: 0.00821
	loss_policy_4: 0.04256
	accuracy_policy_4: 0.60578
	loss_value_4: 0.03967
	loss_reward_4: 0.0089
	loss_policy_5: 0.04585
	accuracy_policy_5: 0.58387
	loss_value_5: 0.04092
	loss_reward_5: 0.01007
	loss_policy: 0.29295
	loss_value: 0.36071
	loss_reward: 0.04285
[2024-05-05 13:03:15] nn step 41750, lr: 0.005.
	loss_policy_0: 0.09233
	accuracy_policy_0: 0.79488
	loss_value_0: 0.16781
	loss_policy_1: 0.02988
	accuracy_policy_1: 0.705
	loss_value_1: 0.03537
	loss_reward_1: 0.00759
	loss_policy_2: 0.03394
	accuracy_policy_2: 0.66809
	loss_value_2: 0.03658
	loss_reward_2: 0.00757
	loss_policy_3: 0.0377
	accuracy_policy_3: 0.6432
	loss_value_3: 0.03781
	loss_reward_3: 0.00813
	loss_policy_4: 0.04119
	accuracy_policy_4: 0.62055
	loss_value_4: 0.03927
	loss_reward_4: 0.00874
	loss_policy_5: 0.04441
	accuracy_policy_5: 0.59156
	loss_value_5: 0.0406
	loss_reward_5: 0.00981
	loss_policy: 0.27945
	loss_value: 0.35742
	loss_reward: 0.04184
[2024-05-05 13:03:31] nn step 41800, lr: 0.005.
	loss_policy_0: 0.08984
	accuracy_policy_0: 0.79844
	loss_value_0: 0.17306
	loss_policy_1: 0.03027
	accuracy_policy_1: 0.71008
	loss_value_1: 0.03665
	loss_reward_1: 0.00775
	loss_policy_2: 0.03372
	accuracy_policy_2: 0.68328
	loss_value_2: 0.03802
	loss_reward_2: 0.00756
	loss_policy_3: 0.03758
	accuracy_policy_3: 0.6459
	loss_value_3: 0.03934
	loss_reward_3: 0.00833
	loss_policy_4: 0.04118
	accuracy_policy_4: 0.62094
	loss_value_4: 0.04057
	loss_reward_4: 0.00855
	loss_policy_5: 0.0446
	accuracy_policy_5: 0.59453
	loss_value_5: 0.04196
	loss_reward_5: 0.00981
	loss_policy: 0.27719
	loss_value: 0.36961
	loss_reward: 0.04199
Optimization_Done 41800
[2024-05-05 13:05:23] [command] train weight_iter_41800.pkl 209 210
[2024-05-05 13:05:40] nn step 41850, lr: 0.005.
	loss_policy_0: 0.15868
	accuracy_policy_0: 0.67723
	loss_value_0: 0.17949
	loss_policy_1: 0.04081
	accuracy_policy_1: 0.59781
	loss_value_1: 0.03824
	loss_reward_1: 0.00756
	loss_policy_2: 0.04377
	accuracy_policy_2: 0.57008
	loss_value_2: 0.0399
	loss_reward_2: 0.00723
	loss_policy_3: 0.0469
	accuracy_policy_3: 0.54203
	loss_value_3: 0.04135
	loss_reward_3: 0.00761
	loss_policy_4: 0.05042
	accuracy_policy_4: 0.51613
	loss_value_4: 0.043
	loss_reward_4: 0.00831
	loss_policy_5: 0.05264
	accuracy_policy_5: 0.48852
	loss_value_5: 0.04414
	loss_reward_5: 0.00934
	loss_policy: 0.39322
	loss_value: 0.38611
	loss_reward: 0.04005
[2024-05-05 13:05:56] nn step 41900, lr: 0.005.
	loss_policy_0: 0.13205
	accuracy_policy_0: 0.73363
	loss_value_0: 0.17795
	loss_policy_1: 0.0377
	accuracy_policy_1: 0.64105
	loss_value_1: 0.0376
	loss_reward_1: 0.00772
	loss_policy_2: 0.04137
	accuracy_policy_2: 0.61676
	loss_value_2: 0.03945
	loss_reward_2: 0.00747
	loss_policy_3: 0.04497
	accuracy_policy_3: 0.58531
	loss_value_3: 0.04104
	loss_reward_3: 0.0081
	loss_policy_4: 0.0482
	accuracy_policy_4: 0.55695
	loss_value_4: 0.04247
	loss_reward_4: 0.00868
	loss_policy_5: 0.05114
	accuracy_policy_5: 0.53504
	loss_value_5: 0.04385
	loss_reward_5: 0.00984
	loss_policy: 0.35543
	loss_value: 0.38236
	loss_reward: 0.04182
[2024-05-05 13:06:12] nn step 41950, lr: 0.005.
	loss_policy_0: 0.12199
	accuracy_policy_0: 0.74453
	loss_value_0: 0.17154
	loss_policy_1: 0.03536
	accuracy_policy_1: 0.65262
	loss_value_1: 0.03664
	loss_reward_1: 0.00728
	loss_policy_2: 0.03901
	accuracy_policy_2: 0.61934
	loss_value_2: 0.03807
	loss_reward_2: 0.00709
	loss_policy_3: 0.04254
	accuracy_policy_3: 0.59117
	loss_value_3: 0.03949
	loss_reward_3: 0.0078
	loss_policy_4: 0.04504
	accuracy_policy_4: 0.56863
	loss_value_4: 0.04088
	loss_reward_4: 0.00826
	loss_policy_5: 0.04794
	accuracy_policy_5: 0.54422
	loss_value_5: 0.04206
	loss_reward_5: 0.0094
	loss_policy: 0.3319
	loss_value: 0.36868
	loss_reward: 0.03983
[2024-05-05 13:06:28] nn step 42000, lr: 0.005.
	loss_policy_0: 0.11935
	accuracy_policy_0: 0.75836
	loss_value_0: 0.17871
	loss_policy_1: 0.03621
	accuracy_policy_1: 0.65734
	loss_value_1: 0.03781
	loss_reward_1: 0.00781
	loss_policy_2: 0.03955
	accuracy_policy_2: 0.63219
	loss_value_2: 0.03943
	loss_reward_2: 0.0075
	loss_policy_3: 0.04311
	accuracy_policy_3: 0.59781
	loss_value_3: 0.04063
	loss_reward_3: 0.00809
	loss_policy_4: 0.04588
	accuracy_policy_4: 0.58195
	loss_value_4: 0.04183
	loss_reward_4: 0.00862
	loss_policy_5: 0.04864
	accuracy_policy_5: 0.55754
	loss_value_5: 0.04321
	loss_reward_5: 0.00979
	loss_policy: 0.33274
	loss_value: 0.38162
	loss_reward: 0.04181
Optimization_Done 42000
[2024-05-05 13:08:31] [command] train weight_iter_42000.pkl 210 211
[2024-05-05 13:08:48] nn step 42050, lr: 0.005.
	loss_policy_0: 0.13361
	accuracy_policy_0: 0.73297
	loss_value_0: 0.1886
	loss_policy_1: 0.03592
	accuracy_policy_1: 0.66512
	loss_value_1: 0.03971
	loss_reward_1: 0.00592
	loss_policy_2: 0.03995
	accuracy_policy_2: 0.63555
	loss_value_2: 0.04153
	loss_reward_2: 0.00602
	loss_policy_3: 0.04362
	accuracy_policy_3: 0.61012
	loss_value_3: 0.04291
	loss_reward_3: 0.00652
	loss_policy_4: 0.0467
	accuracy_policy_4: 0.58438
	loss_value_4: 0.04419
	loss_reward_4: 0.00687
	loss_policy_5: 0.05026
	accuracy_policy_5: 0.55418
	loss_value_5: 0.0456
	loss_reward_5: 0.00773
	loss_policy: 0.35007
	loss_value: 0.40254
	loss_reward: 0.03306
[2024-05-05 13:09:04] nn step 42100, lr: 0.005.
	loss_policy_0: 0.10767
	accuracy_policy_0: 0.77758
	loss_value_0: 0.18452
	loss_policy_1: 0.03258
	accuracy_policy_1: 0.69258
	loss_value_1: 0.03906
	loss_reward_1: 0.00593
	loss_policy_2: 0.03723
	accuracy_policy_2: 0.66129
	loss_value_2: 0.04063
	loss_reward_2: 0.00594
	loss_policy_3: 0.04061
	accuracy_policy_3: 0.63734
	loss_value_3: 0.04192
	loss_reward_3: 0.00629
	loss_policy_4: 0.04394
	accuracy_policy_4: 0.61473
	loss_value_4: 0.04328
	loss_reward_4: 0.00705
	loss_policy_5: 0.04714
	accuracy_policy_5: 0.58711
	loss_value_5: 0.04456
	loss_reward_5: 0.00758
	loss_policy: 0.30916
	loss_value: 0.39397
	loss_reward: 0.0328
[2024-05-05 13:09:20] nn step 42150, lr: 0.005.
	loss_policy_0: 0.09575
	accuracy_policy_0: 0.78238
	loss_value_0: 0.16977
	loss_policy_1: 0.0299
	accuracy_policy_1: 0.69891
	loss_value_1: 0.03576
	loss_reward_1: 0.0055
	loss_policy_2: 0.03342
	accuracy_policy_2: 0.67023
	loss_value_2: 0.03726
	loss_reward_2: 0.00543
	loss_policy_3: 0.03699
	accuracy_policy_3: 0.645
	loss_value_3: 0.03842
	loss_reward_3: 0.00586
	loss_policy_4: 0.03973
	accuracy_policy_4: 0.62051
	loss_value_4: 0.03978
	loss_reward_4: 0.00628
	loss_policy_5: 0.04274
	accuracy_policy_5: 0.59305
	loss_value_5: 0.04112
	loss_reward_5: 0.00698
	loss_policy: 0.27853
	loss_value: 0.36211
	loss_reward: 0.03006
[2024-05-05 13:09:35] nn step 42200, lr: 0.005.
	loss_policy_0: 0.09449
	accuracy_policy_0: 0.79609
	loss_value_0: 0.18102
	loss_policy_1: 0.03068
	accuracy_policy_1: 0.70527
	loss_value_1: 0.03815
	loss_reward_1: 0.00591
	loss_policy_2: 0.03539
	accuracy_policy_2: 0.67227
	loss_value_2: 0.03951
	loss_reward_2: 0.00585
	loss_policy_3: 0.03841
	accuracy_policy_3: 0.64309
	loss_value_3: 0.04105
	loss_reward_3: 0.00618
	loss_policy_4: 0.04191
	accuracy_policy_4: 0.61781
	loss_value_4: 0.04217
	loss_reward_4: 0.00677
	loss_policy_5: 0.045
	accuracy_policy_5: 0.5923
	loss_value_5: 0.04355
	loss_reward_5: 0.0077
	loss_policy: 0.28587
	loss_value: 0.38544
	loss_reward: 0.03241
Optimization_Done 42200
[2024-05-05 13:11:38] [command] train weight_iter_42200.pkl 211 212
[2024-05-05 13:11:55] nn step 42250, lr: 0.005.
	loss_policy_0: 0.09876
	accuracy_policy_0: 0.78535
	loss_value_0: 0.18626
	loss_policy_1: 0.02748
	accuracy_policy_1: 0.72727
	loss_value_1: 0.03891
	loss_reward_1: 0.00568
	loss_policy_2: 0.03194
	accuracy_policy_2: 0.68633
	loss_value_2: 0.04023
	loss_reward_2: 0.0055
	loss_policy_3: 0.03496
	accuracy_policy_3: 0.66699
	loss_value_3: 0.04181
	loss_reward_3: 0.00597
	loss_policy_4: 0.03775
	accuracy_policy_4: 0.64473
	loss_value_4: 0.04285
	loss_reward_4: 0.00668
	loss_policy_5: 0.04077
	accuracy_policy_5: 0.62113
	loss_value_5: 0.04383
	loss_reward_5: 0.0075
	loss_policy: 0.27166
	loss_value: 0.3939
	loss_reward: 0.03133
[2024-05-05 13:12:11] nn step 42300, lr: 0.005.
	loss_policy_0: 0.07529
	accuracy_policy_0: 0.82023
	loss_value_0: 0.17418
	loss_policy_1: 0.02372
	accuracy_policy_1: 0.75273
	loss_value_1: 0.03619
	loss_reward_1: 0.00549
	loss_policy_2: 0.02778
	accuracy_policy_2: 0.71445
	loss_value_2: 0.03745
	loss_reward_2: 0.00556
	loss_policy_3: 0.03047
	accuracy_policy_3: 0.69793
	loss_value_3: 0.03871
	loss_reward_3: 0.00608
	loss_policy_4: 0.03315
	accuracy_policy_4: 0.67801
	loss_value_4: 0.0398
	loss_reward_4: 0.00641
	loss_policy_5: 0.03637
	accuracy_policy_5: 0.64961
	loss_value_5: 0.04076
	loss_reward_5: 0.00723
	loss_policy: 0.22677
	loss_value: 0.36709
	loss_reward: 0.03078
[2024-05-05 13:12:26] nn step 42350, lr: 0.005.
	loss_policy_0: 0.075
	accuracy_policy_0: 0.82773
	loss_value_0: 0.1808
	loss_policy_1: 0.02394
	accuracy_policy_1: 0.75418
	loss_value_1: 0.03798
	loss_reward_1: 0.00581
	loss_policy_2: 0.02817
	accuracy_policy_2: 0.72562
	loss_value_2: 0.03933
	loss_reward_2: 0.00585
	loss_policy_3: 0.03112
	accuracy_policy_3: 0.70496
	loss_value_3: 0.04059
	loss_reward_3: 0.00636
	loss_policy_4: 0.0341
	accuracy_policy_4: 0.68246
	loss_value_4: 0.04178
	loss_reward_4: 0.00661
	loss_policy_5: 0.03731
	accuracy_policy_5: 0.6582
	loss_value_5: 0.04284
	loss_reward_5: 0.00747
	loss_policy: 0.22964
	loss_value: 0.38332
	loss_reward: 0.0321
[2024-05-05 13:12:42] nn step 42400, lr: 0.005.
	loss_policy_0: 0.06669
	accuracy_policy_0: 0.83898
	loss_value_0: 0.17257
	loss_policy_1: 0.02257
	accuracy_policy_1: 0.75871
	loss_value_1: 0.03636
	loss_reward_1: 0.00564
	loss_policy_2: 0.02664
	accuracy_policy_2: 0.72555
	loss_value_2: 0.03771
	loss_reward_2: 0.00549
	loss_policy_3: 0.02965
	accuracy_policy_3: 0.70781
	loss_value_3: 0.03881
	loss_reward_3: 0.00579
	loss_policy_4: 0.03202
	accuracy_policy_4: 0.68746
	loss_value_4: 0.03994
	loss_reward_4: 0.00634
	loss_policy_5: 0.03516
	accuracy_policy_5: 0.66117
	loss_value_5: 0.04096
	loss_reward_5: 0.00726
	loss_policy: 0.21274
	loss_value: 0.36634
	loss_reward: 0.03052
Optimization_Done 42400
[2024-05-05 13:14:44] [command] train weight_iter_42400.pkl 212 213
[2024-05-05 13:15:01] nn step 42450, lr: 0.005.
	loss_policy_0: 0.11876
	accuracy_policy_0: 0.7241
	loss_value_0: 0.17221
	loss_policy_1: 0.03097
	accuracy_policy_1: 0.65984
	loss_value_1: 0.03628
	loss_reward_1: 0.00697
	loss_policy_2: 0.03425
	accuracy_policy_2: 0.63246
	loss_value_2: 0.03787
	loss_reward_2: 0.00673
	loss_policy_3: 0.03748
	accuracy_policy_3: 0.6084
	loss_value_3: 0.03916
	loss_reward_3: 0.00725
	loss_policy_4: 0.04077
	accuracy_policy_4: 0.58453
	loss_value_4: 0.04036
	loss_reward_4: 0.00773
	loss_policy_5: 0.0434
	accuracy_policy_5: 0.56254
	loss_value_5: 0.04154
	loss_reward_5: 0.0086
	loss_policy: 0.30563
	loss_value: 0.36741
	loss_reward: 0.03728
[2024-05-05 13:15:17] nn step 42500, lr: 0.005.
	loss_policy_0: 0.0909
	accuracy_policy_0: 0.77738
	loss_value_0: 0.16724
	loss_policy_1: 0.02687
	accuracy_policy_1: 0.70359
	loss_value_1: 0.03513
	loss_reward_1: 0.00695
	loss_policy_2: 0.03066
	accuracy_policy_2: 0.67328
	loss_value_2: 0.03697
	loss_reward_2: 0.00665
	loss_policy_3: 0.03381
	accuracy_policy_3: 0.64641
	loss_value_3: 0.0384
	loss_reward_3: 0.00701
	loss_policy_4: 0.03719
	accuracy_policy_4: 0.62504
	loss_value_4: 0.03958
	loss_reward_4: 0.00756
	loss_policy_5: 0.03957
	accuracy_policy_5: 0.60691
	loss_value_5: 0.04067
	loss_reward_5: 0.00826
	loss_policy: 0.259
	loss_value: 0.35799
	loss_reward: 0.03644
[2024-05-05 13:15:33] nn step 42550, lr: 0.005.
	loss_policy_0: 0.08172
	accuracy_policy_0: 0.79555
	loss_value_0: 0.16078
	loss_policy_1: 0.02508
	accuracy_policy_1: 0.71527
	loss_value_1: 0.03385
	loss_reward_1: 0.00636
	loss_policy_2: 0.02873
	accuracy_policy_2: 0.67973
	loss_value_2: 0.03523
	loss_reward_2: 0.00648
	loss_policy_3: 0.03209
	accuracy_policy_3: 0.65742
	loss_value_3: 0.03639
	loss_reward_3: 0.00698
	loss_policy_4: 0.03442
	accuracy_policy_4: 0.63676
	loss_value_4: 0.03756
	loss_reward_4: 0.00734
	loss_policy_5: 0.03725
	accuracy_policy_5: 0.61242
	loss_value_5: 0.03858
	loss_reward_5: 0.00811
	loss_policy: 0.23929
	loss_value: 0.34239
	loss_reward: 0.03526
[2024-05-05 13:15:49] nn step 42600, lr: 0.005.
	loss_policy_0: 0.08209
	accuracy_policy_0: 0.80422
	loss_value_0: 0.17322
	loss_policy_1: 0.02615
	accuracy_policy_1: 0.72098
	loss_value_1: 0.0365
	loss_reward_1: 0.00693
	loss_policy_2: 0.03048
	accuracy_policy_2: 0.68715
	loss_value_2: 0.0379
	loss_reward_2: 0.00661
	loss_policy_3: 0.03358
	accuracy_policy_3: 0.65953
	loss_value_3: 0.03891
	loss_reward_3: 0.00735
	loss_policy_4: 0.03576
	accuracy_policy_4: 0.64121
	loss_value_4: 0.04029
	loss_reward_4: 0.00779
	loss_policy_5: 0.03873
	accuracy_policy_5: 0.62023
	loss_value_5: 0.04152
	loss_reward_5: 0.00888
	loss_policy: 0.24679
	loss_value: 0.36834
	loss_reward: 0.03757
Optimization_Done 42600
[2024-05-05 13:17:39] [command] train weight_iter_42600.pkl 213 214
[2024-05-05 13:17:56] nn step 42650, lr: 0.005.
	loss_policy_0: 0.1683
	accuracy_policy_0: 0.66941
	loss_value_0: 0.18357
	loss_policy_1: 0.04301
	accuracy_policy_1: 0.59688
	loss_value_1: 0.03841
	loss_reward_1: 0.00731
	loss_policy_2: 0.04636
	accuracy_policy_2: 0.57082
	loss_value_2: 0.04044
	loss_reward_2: 0.00715
	loss_policy_3: 0.04946
	accuracy_policy_3: 0.54395
	loss_value_3: 0.0421
	loss_reward_3: 0.00758
	loss_policy_4: 0.0519
	accuracy_policy_4: 0.52559
	loss_value_4: 0.04375
	loss_reward_4: 0.00842
	loss_policy_5: 0.05513
	accuracy_policy_5: 0.5
	loss_value_5: 0.04539
	loss_reward_5: 0.00949
	loss_policy: 0.41417
	loss_value: 0.39366
	loss_reward: 0.03994
[2024-05-05 13:18:12] nn step 42700, lr: 0.005.
	loss_policy_0: 0.13086
	accuracy_policy_0: 0.71672
	loss_value_0: 0.169
	loss_policy_1: 0.03662
	accuracy_policy_1: 0.62887
	loss_value_1: 0.03583
	loss_reward_1: 0.00706
	loss_policy_2: 0.04001
	accuracy_policy_2: 0.60594
	loss_value_2: 0.03736
	loss_reward_2: 0.0068
	loss_policy_3: 0.04347
	accuracy_policy_3: 0.58
	loss_value_3: 0.03891
	loss_reward_3: 0.00736
	loss_policy_4: 0.04632
	accuracy_policy_4: 0.55836
	loss_value_4: 0.04038
	loss_reward_4: 0.00778
	loss_policy_5: 0.04918
	accuracy_policy_5: 0.53441
	loss_value_5: 0.04169
	loss_reward_5: 0.00894
	loss_policy: 0.34645
	loss_value: 0.36317
	loss_reward: 0.03794
[2024-05-05 13:18:28] nn step 42750, lr: 0.005.
	loss_policy_0: 0.12433
	accuracy_policy_0: 0.73262
	loss_value_0: 0.17144
	loss_policy_1: 0.03539
	accuracy_policy_1: 0.645
	loss_value_1: 0.03636
	loss_reward_1: 0.00723
	loss_policy_2: 0.03949
	accuracy_policy_2: 0.61484
	loss_value_2: 0.03811
	loss_reward_2: 0.00675
	loss_policy_3: 0.04286
	accuracy_policy_3: 0.58633
	loss_value_3: 0.03952
	loss_reward_3: 0.0073
	loss_policy_4: 0.04601
	accuracy_policy_4: 0.56621
	loss_value_4: 0.04111
	loss_reward_4: 0.00792
	loss_policy_5: 0.0489
	accuracy_policy_5: 0.54648
	loss_value_5: 0.04236
	loss_reward_5: 0.00892
	loss_policy: 0.33697
	loss_value: 0.36889
	loss_reward: 0.03811
[2024-05-05 13:18:44] nn step 42800, lr: 0.005.
	loss_policy_0: 0.12176
	accuracy_policy_0: 0.74277
	loss_value_0: 0.1743
	loss_policy_1: 0.0351
	accuracy_policy_1: 0.6568
	loss_value_1: 0.03661
	loss_reward_1: 0.00727
	loss_policy_2: 0.03951
	accuracy_policy_2: 0.62125
	loss_value_2: 0.03841
	loss_reward_2: 0.00685
	loss_policy_3: 0.04297
	accuracy_policy_3: 0.59438
	loss_value_3: 0.03987
	loss_reward_3: 0.0075
	loss_policy_4: 0.04557
	accuracy_policy_4: 0.56965
	loss_value_4: 0.04129
	loss_reward_4: 0.00811
	loss_policy_5: 0.04867
	accuracy_policy_5: 0.55113
	loss_value_5: 0.0428
	loss_reward_5: 0.00933
	loss_policy: 0.33358
	loss_value: 0.37328
	loss_reward: 0.03906
Optimization_Done 42800
[2024-05-05 13:20:37] [command] train weight_iter_42800.pkl 214 215
[2024-05-05 13:20:54] nn step 42850, lr: 0.005.
	loss_policy_0: 0.14131
	accuracy_policy_0: 0.68566
	loss_value_0: 0.17649
	loss_policy_1: 0.03687
	accuracy_policy_1: 0.60852
	loss_value_1: 0.03704
	loss_reward_1: 0.00568
	loss_policy_2: 0.04005
	accuracy_policy_2: 0.58562
	loss_value_2: 0.03856
	loss_reward_2: 0.00531
	loss_policy_3: 0.04299
	accuracy_policy_3: 0.55816
	loss_value_3: 0.03983
	loss_reward_3: 0.00598
	loss_policy_4: 0.04546
	accuracy_policy_4: 0.53344
	loss_value_4: 0.04101
	loss_reward_4: 0.00646
	loss_policy_5: 0.04778
	accuracy_policy_5: 0.51203
	loss_value_5: 0.04228
	loss_reward_5: 0.0074
	loss_policy: 0.35445
	loss_value: 0.37521
	loss_reward: 0.03083
[2024-05-05 13:21:09] nn step 42900, lr: 0.005.
	loss_policy_0: 0.12914
	accuracy_policy_0: 0.73211
	loss_value_0: 0.18822
	loss_policy_1: 0.03713
	accuracy_policy_1: 0.64137
	loss_value_1: 0.03957
	loss_reward_1: 0.00639
	loss_policy_2: 0.04089
	accuracy_policy_2: 0.6116
	loss_value_2: 0.04114
	loss_reward_2: 0.00609
	loss_policy_3: 0.04412
	accuracy_policy_3: 0.58754
	loss_value_3: 0.0427
	loss_reward_3: 0.00659
	loss_policy_4: 0.04755
	accuracy_policy_4: 0.5616
	loss_value_4: 0.04383
	loss_reward_4: 0.0071
	loss_policy_5: 0.04993
	accuracy_policy_5: 0.5393
	loss_value_5: 0.0449
	loss_reward_5: 0.00805
	loss_policy: 0.34875
	loss_value: 0.40036
	loss_reward: 0.03423
[2024-05-05 13:21:26] nn step 42950, lr: 0.005.
	loss_policy_0: 0.11618
	accuracy_policy_0: 0.74742
	loss_value_0: 0.17944
	loss_policy_1: 0.03417
	accuracy_policy_1: 0.65609
	loss_value_1: 0.03795
	loss_reward_1: 0.00621
	loss_policy_2: 0.0382
	accuracy_policy_2: 0.61863
	loss_value_2: 0.03954
	loss_reward_2: 0.00595
	loss_policy_3: 0.04161
	accuracy_policy_3: 0.58898
	loss_value_3: 0.04075
	loss_reward_3: 0.00637
	loss_policy_4: 0.04413
	accuracy_policy_4: 0.56305
	loss_value_4: 0.04184
	loss_reward_4: 0.007
	loss_policy_5: 0.04729
	accuracy_policy_5: 0.54445
	loss_value_5: 0.04307
	loss_reward_5: 0.00784
	loss_policy: 0.32158
	loss_value: 0.38259
	loss_reward: 0.03336
[2024-05-05 13:21:41] nn step 43000, lr: 0.005.
	loss_policy_0: 0.11859
	accuracy_policy_0: 0.75215
	loss_value_0: 0.19175
	loss_policy_1: 0.03617
	accuracy_policy_1: 0.6623
	loss_value_1: 0.04053
	loss_reward_1: 0.00648
	loss_policy_2: 0.04027
	accuracy_policy_2: 0.62727
	loss_value_2: 0.04191
	loss_reward_2: 0.00644
	loss_policy_3: 0.04362
	accuracy_policy_3: 0.60074
	loss_value_3: 0.04334
	loss_reward_3: 0.00686
	loss_policy_4: 0.04652
	accuracy_policy_4: 0.57227
	loss_value_4: 0.04476
	loss_reward_4: 0.00745
	loss_policy_5: 0.04961
	accuracy_policy_5: 0.54973
	loss_value_5: 0.04608
	loss_reward_5: 0.00844
	loss_policy: 0.33477
	loss_value: 0.40837
	loss_reward: 0.03567
Optimization_Done 43000
[2024-05-05 13:23:45] [command] train weight_iter_43000.pkl 215 216
[2024-05-05 13:24:01] nn step 43050, lr: 0.005.
	loss_policy_0: 0.14035
	accuracy_policy_0: 0.68109
	loss_value_0: 0.17087
	loss_policy_1: 0.0356
	accuracy_policy_1: 0.60988
	loss_value_1: 0.03582
	loss_reward_1: 0.0052
	loss_policy_2: 0.03823
	accuracy_policy_2: 0.58609
	loss_value_2: 0.03703
	loss_reward_2: 0.00527
	loss_policy_3: 0.04095
	accuracy_policy_3: 0.56078
	loss_value_3: 0.03816
	loss_reward_3: 0.00558
	loss_policy_4: 0.04332
	accuracy_policy_4: 0.53762
	loss_value_4: 0.03907
	loss_reward_4: 0.00621
	loss_policy_5: 0.04613
	accuracy_policy_5: 0.51293
	loss_value_5: 0.03985
	loss_reward_5: 0.00678
	loss_policy: 0.34459
	loss_value: 0.36079
	loss_reward: 0.02904
[2024-05-05 13:24:17] nn step 43100, lr: 0.005.
	loss_policy_0: 0.11103
	accuracy_policy_0: 0.74414
	loss_value_0: 0.17491
	loss_policy_1: 0.03184
	accuracy_policy_1: 0.65645
	loss_value_1: 0.0366
	loss_reward_1: 0.00554
	loss_policy_2: 0.03563
	accuracy_policy_2: 0.62656
	loss_value_2: 0.03758
	loss_reward_2: 0.00555
	loss_policy_3: 0.03887
	accuracy_policy_3: 0.6009
	loss_value_3: 0.03856
	loss_reward_3: 0.00591
	loss_policy_4: 0.04094
	accuracy_policy_4: 0.58035
	loss_value_4: 0.03933
	loss_reward_4: 0.00633
	loss_policy_5: 0.04432
	accuracy_policy_5: 0.55055
	loss_value_5: 0.04027
	loss_reward_5: 0.00724
	loss_policy: 0.30263
	loss_value: 0.36725
	loss_reward: 0.03058
[2024-05-05 13:24:33] nn step 43150, lr: 0.005.
	loss_policy_0: 0.10663
	accuracy_policy_0: 0.7573
	loss_value_0: 0.17909
	loss_policy_1: 0.03138
	accuracy_policy_1: 0.66777
	loss_value_1: 0.03731
	loss_reward_1: 0.00571
	loss_policy_2: 0.03561
	accuracy_policy_2: 0.63
	loss_value_2: 0.03847
	loss_reward_2: 0.00584
	loss_policy_3: 0.0389
	accuracy_policy_3: 0.59992
	loss_value_3: 0.03948
	loss_reward_3: 0.006
	loss_policy_4: 0.04214
	accuracy_policy_4: 0.57715
	loss_value_4: 0.04039
	loss_reward_4: 0.00649
	loss_policy_5: 0.04448
	accuracy_policy_5: 0.55781
	loss_value_5: 0.04122
	loss_reward_5: 0.00736
	loss_policy: 0.29914
	loss_value: 0.37596
	loss_reward: 0.03141
[2024-05-05 13:24:49] nn step 43200, lr: 0.005.
	loss_policy_0: 0.09853
	accuracy_policy_0: 0.7607
	loss_value_0: 0.17292
	loss_policy_1: 0.02982
	accuracy_policy_1: 0.6668
	loss_value_1: 0.03629
	loss_reward_1: 0.00542
	loss_policy_2: 0.03369
	accuracy_policy_2: 0.63309
	loss_value_2: 0.03742
	loss_reward_2: 0.00556
	loss_policy_3: 0.03688
	accuracy_policy_3: 0.60477
	loss_value_3: 0.03832
	loss_reward_3: 0.00604
	loss_policy_4: 0.03971
	accuracy_policy_4: 0.58711
	loss_value_4: 0.03904
	loss_reward_4: 0.00638
	loss_policy_5: 0.04254
	accuracy_policy_5: 0.56559
	loss_value_5: 0.0399
	loss_reward_5: 0.00687
	loss_policy: 0.28116
	loss_value: 0.36388
	loss_reward: 0.03028
Optimization_Done 43200
[2024-05-05 13:26:54] [command] train weight_iter_43200.pkl 216 217
[2024-05-05 13:27:11] nn step 43250, lr: 0.005.
	loss_policy_0: 0.12339
	accuracy_policy_0: 0.72621
	loss_value_0: 0.18837
	loss_policy_1: 0.03351
	accuracy_policy_1: 0.655
	loss_value_1: 0.03981
	loss_reward_1: 0.00644
	loss_policy_2: 0.03763
	accuracy_policy_2: 0.62367
	loss_value_2: 0.04167
	loss_reward_2: 0.00644
	loss_policy_3: 0.04189
	accuracy_policy_3: 0.58715
	loss_value_3: 0.04371
	loss_reward_3: 0.00683
	loss_policy_4: 0.04527
	accuracy_policy_4: 0.56426
	loss_value_4: 0.0452
	loss_reward_4: 0.00748
	loss_policy_5: 0.049
	accuracy_policy_5: 0.53004
	loss_value_5: 0.04643
	loss_reward_5: 0.00833
	loss_policy: 0.3307
	loss_value: 0.40519
	loss_reward: 0.03551
[2024-05-05 13:27:27] nn step 43300, lr: 0.005.
	loss_policy_0: 0.08279
	accuracy_policy_0: 0.7732
	loss_value_0: 0.15663
	loss_policy_1: 0.02538
	accuracy_policy_1: 0.68969
	loss_value_1: 0.03287
	loss_reward_1: 0.00539
	loss_policy_2: 0.02907
	accuracy_policy_2: 0.65637
	loss_value_2: 0.03413
	loss_reward_2: 0.00536
	loss_policy_3: 0.03234
	accuracy_policy_3: 0.6302
	loss_value_3: 0.03534
	loss_reward_3: 0.00583
	loss_policy_4: 0.0354
	accuracy_policy_4: 0.60129
	loss_value_4: 0.03649
	loss_reward_4: 0.00611
	loss_policy_5: 0.03813
	accuracy_policy_5: 0.57824
	loss_value_5: 0.03751
	loss_reward_5: 0.00692
	loss_policy: 0.24311
	loss_value: 0.33297
	loss_reward: 0.0296
[2024-05-05 13:27:43] nn step 43350, lr: 0.005.
	loss_policy_0: 0.08174
	accuracy_policy_0: 0.78586
	loss_value_0: 0.17016
	loss_policy_1: 0.02606
	accuracy_policy_1: 0.70297
	loss_value_1: 0.03578
	loss_reward_1: 0.0057
	loss_policy_2: 0.03035
	accuracy_policy_2: 0.66699
	loss_value_2: 0.03724
	loss_reward_2: 0.00577
	loss_policy_3: 0.03422
	accuracy_policy_3: 0.63133
	loss_value_3: 0.0385
	loss_reward_3: 0.00636
	loss_policy_4: 0.03711
	accuracy_policy_4: 0.61199
	loss_value_4: 0.03971
	loss_reward_4: 0.0066
	loss_policy_5: 0.04009
	accuracy_policy_5: 0.58957
	loss_value_5: 0.04082
	loss_reward_5: 0.00739
	loss_policy: 0.24958
	loss_value: 0.36221
	loss_reward: 0.03182
[2024-05-05 13:27:59] nn step 43400, lr: 0.005.
	loss_policy_0: 0.08362
	accuracy_policy_0: 0.78664
	loss_value_0: 0.17798
	loss_policy_1: 0.02688
	accuracy_policy_1: 0.7068
	loss_value_1: 0.03752
	loss_reward_1: 0.00612
	loss_policy_2: 0.03123
	accuracy_policy_2: 0.66203
	loss_value_2: 0.03906
	loss_reward_2: 0.00612
	loss_policy_3: 0.03516
	accuracy_policy_3: 0.6316
	loss_value_3: 0.04049
	loss_reward_3: 0.00667
	loss_policy_4: 0.03861
	accuracy_policy_4: 0.60316
	loss_value_4: 0.04167
	loss_reward_4: 0.00702
	loss_policy_5: 0.04144
	accuracy_policy_5: 0.58906
	loss_value_5: 0.04298
	loss_reward_5: 0.00787
	loss_policy: 0.25694
	loss_value: 0.3797
	loss_reward: 0.0338
Optimization_Done 43400
[2024-05-05 13:30:01] [command] train weight_iter_43400.pkl 217 218
[2024-05-05 13:30:18] nn step 43450, lr: 0.005.
	loss_policy_0: 0.11853
	accuracy_policy_0: 0.7277
	loss_value_0: 0.19183
	loss_policy_1: 0.03181
	accuracy_policy_1: 0.66234
	loss_value_1: 0.04037
	loss_reward_1: 0.00752
	loss_policy_2: 0.03618
	accuracy_policy_2: 0.62223
	loss_value_2: 0.04209
	loss_reward_2: 0.00708
	loss_policy_3: 0.04011
	accuracy_policy_3: 0.59012
	loss_value_3: 0.04388
	loss_reward_3: 0.00799
	loss_policy_4: 0.0442
	accuracy_policy_4: 0.55469
	loss_value_4: 0.04553
	loss_reward_4: 0.00847
	loss_policy_5: 0.04726
	accuracy_policy_5: 0.52871
	loss_value_5: 0.04715
	loss_reward_5: 0.00921
	loss_policy: 0.3181
	loss_value: 0.41085
	loss_reward: 0.04027
[2024-05-05 13:30:34] nn step 43500, lr: 0.005.
	loss_policy_0: 0.09311
	accuracy_policy_0: 0.78512
	loss_value_0: 0.19451
	loss_policy_1: 0.02937
	accuracy_policy_1: 0.70281
	loss_value_1: 0.04103
	loss_reward_1: 0.00756
	loss_policy_2: 0.03334
	accuracy_policy_2: 0.67184
	loss_value_2: 0.04291
	loss_reward_2: 0.0072
	loss_policy_3: 0.03827
	accuracy_policy_3: 0.63441
	loss_value_3: 0.04467
	loss_reward_3: 0.00777
	loss_policy_4: 0.04214
	accuracy_policy_4: 0.60734
	loss_value_4: 0.04629
	loss_reward_4: 0.00834
	loss_policy_5: 0.04506
	accuracy_policy_5: 0.57738
	loss_value_5: 0.04757
	loss_reward_5: 0.00966
	loss_policy: 0.28128
	loss_value: 0.41698
	loss_reward: 0.04053
[2024-05-05 13:30:50] nn step 43550, lr: 0.005.
	loss_policy_0: 0.0824
	accuracy_policy_0: 0.80102
	loss_value_0: 0.19005
	loss_policy_1: 0.02782
	accuracy_policy_1: 0.70477
	loss_value_1: 0.04021
	loss_reward_1: 0.00748
	loss_policy_2: 0.03216
	accuracy_policy_2: 0.67715
	loss_value_2: 0.04203
	loss_reward_2: 0.00688
	loss_policy_3: 0.03658
	accuracy_policy_3: 0.64184
	loss_value_3: 0.04369
	loss_reward_3: 0.00763
	loss_policy_4: 0.04004
	accuracy_policy_4: 0.6173
	loss_value_4: 0.0452
	loss_reward_4: 0.00812
	loss_policy_5: 0.04347
	accuracy_policy_5: 0.58621
	loss_value_5: 0.04672
	loss_reward_5: 0.00974
	loss_policy: 0.26248
	loss_value: 0.40789
	loss_reward: 0.03986
[2024-05-05 13:31:05] nn step 43600, lr: 0.005.
	loss_policy_0: 0.07464
	accuracy_policy_0: 0.81117
	loss_value_0: 0.17927
	loss_policy_1: 0.02581
	accuracy_policy_1: 0.71754
	loss_value_1: 0.03795
	loss_reward_1: 0.007
	loss_policy_2: 0.03016
	accuracy_policy_2: 0.67773
	loss_value_2: 0.03956
	loss_reward_2: 0.00651
	loss_policy_3: 0.03427
	accuracy_policy_3: 0.64492
	loss_value_3: 0.041
	loss_reward_3: 0.00728
	loss_policy_4: 0.03766
	accuracy_policy_4: 0.6182
	loss_value_4: 0.04241
	loss_reward_4: 0.00793
	loss_policy_5: 0.04072
	accuracy_policy_5: 0.59094
	loss_value_5: 0.04391
	loss_reward_5: 0.00943
	loss_policy: 0.24327
	loss_value: 0.3841
	loss_reward: 0.03814
Optimization_Done 43600
[2024-05-05 13:32:41] [command] train weight_iter_43600.pkl 218 219
[2024-05-05 13:32:58] nn step 43650, lr: 0.005.
	loss_policy_0: 0.12271
	accuracy_policy_0: 0.74949
	loss_value_0: 0.18835
	loss_policy_1: 0.03428
	accuracy_policy_1: 0.67219
	loss_value_1: 0.03989
	loss_reward_1: 0.00699
	loss_policy_2: 0.03843
	accuracy_policy_2: 0.63531
	loss_value_2: 0.04153
	loss_reward_2: 0.00675
	loss_policy_3: 0.04256
	accuracy_policy_3: 0.59883
	loss_value_3: 0.04311
	loss_reward_3: 0.00716
	loss_policy_4: 0.04595
	accuracy_policy_4: 0.57063
	loss_value_4: 0.04448
	loss_reward_4: 0.00797
	loss_policy_5: 0.0491
	accuracy_policy_5: 0.54199
	loss_value_5: 0.04592
	loss_reward_5: 0.00912
	loss_policy: 0.33302
	loss_value: 0.40329
	loss_reward: 0.03799
[2024-05-05 13:33:14] nn step 43700, lr: 0.005.
	loss_policy_0: 0.0923
	accuracy_policy_0: 0.79043
	loss_value_0: 0.17801
	loss_policy_1: 0.02925
	accuracy_policy_1: 0.7018
	loss_value_1: 0.03768
	loss_reward_1: 0.00684
	loss_policy_2: 0.03336
	accuracy_policy_2: 0.67023
	loss_value_2: 0.0393
	loss_reward_2: 0.0063
	loss_policy_3: 0.03758
	accuracy_policy_3: 0.63211
	loss_value_3: 0.04063
	loss_reward_3: 0.00711
	loss_policy_4: 0.04024
	accuracy_policy_4: 0.61023
	loss_value_4: 0.04193
	loss_reward_4: 0.00764
	loss_policy_5: 0.04375
	accuracy_policy_5: 0.57641
	loss_value_5: 0.04323
	loss_reward_5: 0.00888
	loss_policy: 0.27648
	loss_value: 0.38077
	loss_reward: 0.03677
[2024-05-05 13:33:30] nn step 43750, lr: 0.005.
	loss_policy_0: 0.08802
	accuracy_policy_0: 0.80055
	loss_value_0: 0.18184
	loss_policy_1: 0.029
	accuracy_policy_1: 0.71305
	loss_value_1: 0.03857
	loss_reward_1: 0.00685
	loss_policy_2: 0.03342
	accuracy_policy_2: 0.67305
	loss_value_2: 0.04005
	loss_reward_2: 0.00641
	loss_policy_3: 0.0377
	accuracy_policy_3: 0.63711
	loss_value_3: 0.04151
	loss_reward_3: 0.00736
	loss_policy_4: 0.04025
	accuracy_policy_4: 0.61305
	loss_value_4: 0.04284
	loss_reward_4: 0.0078
	loss_policy_5: 0.04334
	accuracy_policy_5: 0.58203
	loss_value_5: 0.04429
	loss_reward_5: 0.00912
	loss_policy: 0.27172
	loss_value: 0.3891
	loss_reward: 0.03754
[2024-05-05 13:33:46] nn step 43800, lr: 0.005.
	loss_policy_0: 0.08443
	accuracy_policy_0: 0.80461
	loss_value_0: 0.17814
	loss_policy_1: 0.02838
	accuracy_policy_1: 0.71402
	loss_value_1: 0.03793
	loss_reward_1: 0.00691
	loss_policy_2: 0.0323
	accuracy_policy_2: 0.67871
	loss_value_2: 0.03937
	loss_reward_2: 0.00638
	loss_policy_3: 0.03612
	accuracy_policy_3: 0.64285
	loss_value_3: 0.04064
	loss_reward_3: 0.00717
	loss_policy_4: 0.03932
	accuracy_policy_4: 0.61895
	loss_value_4: 0.04185
	loss_reward_4: 0.00743
	loss_policy_5: 0.04212
	accuracy_policy_5: 0.59535
	loss_value_5: 0.04323
	loss_reward_5: 0.00902
	loss_policy: 0.26266
	loss_value: 0.38116
	loss_reward: 0.03691
Optimization_Done 43800
[2024-05-05 13:35:52] [command] train weight_iter_43800.pkl 219 220
[2024-05-05 13:36:09] nn step 43850, lr: 0.005.
	loss_policy_0: 0.13876
	accuracy_policy_0: 0.67941
	loss_value_0: 0.16636
	loss_policy_1: 0.0364
	accuracy_policy_1: 0.60117
	loss_value_1: 0.03474
	loss_reward_1: 0.00558
	loss_policy_2: 0.0402
	accuracy_policy_2: 0.5675
	loss_value_2: 0.03601
	loss_reward_2: 0.00541
	loss_policy_3: 0.04335
	accuracy_policy_3: 0.53605
	loss_value_3: 0.03719
	loss_reward_3: 0.00616
	loss_policy_4: 0.04637
	accuracy_policy_4: 0.5048
	loss_value_4: 0.03848
	loss_reward_4: 0.00638
	loss_policy_5: 0.04866
	accuracy_policy_5: 0.4807
	loss_value_5: 0.0395
	loss_reward_5: 0.00736
	loss_policy: 0.35375
	loss_value: 0.35228
	loss_reward: 0.03088
[2024-05-05 13:36:25] nn step 43900, lr: 0.005.
	loss_policy_0: 0.11061
	accuracy_policy_0: 0.73781
	loss_value_0: 0.16441
	loss_policy_1: 0.03221
	accuracy_policy_1: 0.65344
	loss_value_1: 0.03493
	loss_reward_1: 0.00574
	loss_policy_2: 0.03605
	accuracy_policy_2: 0.62391
	loss_value_2: 0.03631
	loss_reward_2: 0.0055
	loss_policy_3: 0.03955
	accuracy_policy_3: 0.59438
	loss_value_3: 0.03757
	loss_reward_3: 0.00615
	loss_policy_4: 0.04281
	accuracy_policy_4: 0.56598
	loss_value_4: 0.03884
	loss_reward_4: 0.00667
	loss_policy_5: 0.04617
	accuracy_policy_5: 0.52938
	loss_value_5: 0.03998
	loss_reward_5: 0.00719
	loss_policy: 0.3074
	loss_value: 0.35204
	loss_reward: 0.03125
[2024-05-05 13:36:41] nn step 43950, lr: 0.005.
	loss_policy_0: 0.10125
	accuracy_policy_0: 0.76027
	loss_value_0: 0.16526
	loss_policy_1: 0.03101
	accuracy_policy_1: 0.66914
	loss_value_1: 0.03492
	loss_reward_1: 0.00562
	loss_policy_2: 0.03515
	accuracy_policy_2: 0.63582
	loss_value_2: 0.03627
	loss_reward_2: 0.00557
	loss_policy_3: 0.03868
	accuracy_policy_3: 0.59984
	loss_value_3: 0.03783
	loss_reward_3: 0.00629
	loss_policy_4: 0.0413
	accuracy_policy_4: 0.5768
	loss_value_4: 0.0391
	loss_reward_4: 0.00669
	loss_policy_5: 0.04473
	accuracy_policy_5: 0.55023
	loss_value_5: 0.0402
	loss_reward_5: 0.00758
	loss_policy: 0.29213
	loss_value: 0.35359
	loss_reward: 0.03175
[2024-05-05 13:36:57] nn step 44000, lr: 0.005.
	loss_policy_0: 0.09802
	accuracy_policy_0: 0.77586
	loss_value_0: 0.1721
	loss_policy_1: 0.03116
	accuracy_policy_1: 0.67707
	loss_value_1: 0.03635
	loss_reward_1: 0.00605
	loss_policy_2: 0.03556
	accuracy_policy_2: 0.64176
	loss_value_2: 0.03802
	loss_reward_2: 0.00584
	loss_policy_3: 0.03926
	accuracy_policy_3: 0.60816
	loss_value_3: 0.03953
	loss_reward_3: 0.00632
	loss_policy_4: 0.04253
	accuracy_policy_4: 0.58625
	loss_value_4: 0.04077
	loss_reward_4: 0.00693
	loss_policy_5: 0.04544
	accuracy_policy_5: 0.55727
	loss_value_5: 0.04186
	loss_reward_5: 0.00795
	loss_policy: 0.29198
	loss_value: 0.36862
	loss_reward: 0.03308
Optimization_Done 44000
[2024-05-05 13:39:02] [command] train weight_iter_44000.pkl 220 221
[2024-05-05 13:39:19] nn step 44050, lr: 0.005.
	loss_policy_0: 0.14795
	accuracy_policy_0: 0.70074
	loss_value_0: 0.18996
	loss_policy_1: 0.03837
	accuracy_policy_1: 0.62422
	loss_value_1: 0.03977
	loss_reward_1: 0.00663
	loss_policy_2: 0.04278
	accuracy_policy_2: 0.59547
	loss_value_2: 0.04188
	loss_reward_2: 0.00643
	loss_policy_3: 0.04636
	accuracy_policy_3: 0.56039
	loss_value_3: 0.04366
	loss_reward_3: 0.00672
	loss_policy_4: 0.04984
	accuracy_policy_4: 0.53152
	loss_value_4: 0.04521
	loss_reward_4: 0.00751
	loss_policy_5: 0.05287
	accuracy_policy_5: 0.50848
	loss_value_5: 0.04642
	loss_reward_5: 0.00828
	loss_policy: 0.37817
	loss_value: 0.4069
	loss_reward: 0.03557
[2024-05-05 13:39:35] nn step 44100, lr: 0.005.
	loss_policy_0: 0.12093
	accuracy_policy_0: 0.7416
	loss_value_0: 0.18797
	loss_policy_1: 0.03457
	accuracy_policy_1: 0.66137
	loss_value_1: 0.03987
	loss_reward_1: 0.00663
	loss_policy_2: 0.04008
	accuracy_policy_2: 0.61484
	loss_value_2: 0.04135
	loss_reward_2: 0.00667
	loss_policy_3: 0.04414
	accuracy_policy_3: 0.58945
	loss_value_3: 0.0431
	loss_reward_3: 0.007
	loss_policy_4: 0.04734
	accuracy_policy_4: 0.56367
	loss_value_4: 0.04456
	loss_reward_4: 0.00755
	loss_policy_5: 0.04941
	accuracy_policy_5: 0.54723
	loss_value_5: 0.04585
	loss_reward_5: 0.00837
	loss_policy: 0.33647
	loss_value: 0.4027
	loss_reward: 0.03621
[2024-05-05 13:39:51] nn step 44150, lr: 0.005.
	loss_policy_0: 0.11411
	accuracy_policy_0: 0.75539
	loss_value_0: 0.18481
	loss_policy_1: 0.03356
	accuracy_policy_1: 0.66668
	loss_value_1: 0.03898
	loss_reward_1: 0.00656
	loss_policy_2: 0.0385
	accuracy_policy_2: 0.62785
	loss_value_2: 0.04085
	loss_reward_2: 0.00634
	loss_policy_3: 0.04227
	accuracy_policy_3: 0.5957
	loss_value_3: 0.04246
	loss_reward_3: 0.00679
	loss_policy_4: 0.04549
	accuracy_policy_4: 0.57449
	loss_value_4: 0.04411
	loss_reward_4: 0.00748
	loss_policy_5: 0.04891
	accuracy_policy_5: 0.54457
	loss_value_5: 0.04542
	loss_reward_5: 0.00833
	loss_policy: 0.32284
	loss_value: 0.39663
	loss_reward: 0.03551
[2024-05-05 13:40:06] nn step 44200, lr: 0.005.
	loss_policy_0: 0.11446
	accuracy_policy_0: 0.76223
	loss_value_0: 0.19551
	loss_policy_1: 0.03468
	accuracy_policy_1: 0.6682
	loss_value_1: 0.04124
	loss_reward_1: 0.00708
	loss_policy_2: 0.03951
	accuracy_policy_2: 0.6318
	loss_value_2: 0.04328
	loss_reward_2: 0.00657
	loss_policy_3: 0.04398
	accuracy_policy_3: 0.59559
	loss_value_3: 0.04528
	loss_reward_3: 0.00744
	loss_policy_4: 0.04761
	accuracy_policy_4: 0.57387
	loss_value_4: 0.04679
	loss_reward_4: 0.00772
	loss_policy_5: 0.05097
	accuracy_policy_5: 0.54625
	loss_value_5: 0.04813
	loss_reward_5: 0.00879
	loss_policy: 0.33121
	loss_value: 0.42024
	loss_reward: 0.0376
Optimization_Done 44200
[2024-05-05 13:42:08] [command] train weight_iter_44200.pkl 221 222
[2024-05-05 13:42:25] nn step 44250, lr: 0.005.
	loss_policy_0: 0.11324
	accuracy_policy_0: 0.73727
	loss_value_0: 0.17885
	loss_policy_1: 0.03081
	accuracy_policy_1: 0.6668
	loss_value_1: 0.0379
	loss_reward_1: 0.00747
	loss_policy_2: 0.03458
	accuracy_policy_2: 0.63727
	loss_value_2: 0.03939
	loss_reward_2: 0.00715
	loss_policy_3: 0.03778
	accuracy_policy_3: 0.6059
	loss_value_3: 0.04091
	loss_reward_3: 0.00784
	loss_policy_4: 0.04099
	accuracy_policy_4: 0.57723
	loss_value_4: 0.04248
	loss_reward_4: 0.00866
	loss_policy_5: 0.04386
	accuracy_policy_5: 0.55164
	loss_value_5: 0.04397
	loss_reward_5: 0.00936
	loss_policy: 0.30126
	loss_value: 0.3835
	loss_reward: 0.04047
[2024-05-05 13:42:41] nn step 44300, lr: 0.005.
	loss_policy_0: 0.08817
	accuracy_policy_0: 0.78012
	loss_value_0: 0.17007
	loss_policy_1: 0.02733
	accuracy_policy_1: 0.70152
	loss_value_1: 0.03599
	loss_reward_1: 0.00729
	loss_policy_2: 0.0316
	accuracy_policy_2: 0.66758
	loss_value_2: 0.03751
	loss_reward_2: 0.00711
	loss_policy_3: 0.03532
	accuracy_policy_3: 0.63207
	loss_value_3: 0.03908
	loss_reward_3: 0.00754
	loss_policy_4: 0.03823
	accuracy_policy_4: 0.60742
	loss_value_4: 0.04063
	loss_reward_4: 0.00822
	loss_policy_5: 0.04069
	accuracy_policy_5: 0.58641
	loss_value_5: 0.04184
	loss_reward_5: 0.00906
	loss_policy: 0.26134
	loss_value: 0.36511
	loss_reward: 0.03922
[2024-05-05 13:42:57] nn step 44350, lr: 0.005.
	loss_policy_0: 0.08532
	accuracy_policy_0: 0.79391
	loss_value_0: 0.17377
	loss_policy_1: 0.0275
	accuracy_policy_1: 0.70723
	loss_value_1: 0.03702
	loss_reward_1: 0.00759
	loss_policy_2: 0.03194
	accuracy_policy_2: 0.67254
	loss_value_2: 0.03883
	loss_reward_2: 0.00721
	loss_policy_3: 0.03576
	accuracy_policy_3: 0.6416
	loss_value_3: 0.04037
	loss_reward_3: 0.00791
	loss_policy_4: 0.03862
	accuracy_policy_4: 0.61398
	loss_value_4: 0.04189
	loss_reward_4: 0.00823
	loss_policy_5: 0.04156
	accuracy_policy_5: 0.5893
	loss_value_5: 0.04341
	loss_reward_5: 0.00957
	loss_policy: 0.2607
	loss_value: 0.37529
	loss_reward: 0.04051
[2024-05-05 13:43:13] nn step 44400, lr: 0.005.
	loss_policy_0: 0.08421
	accuracy_policy_0: 0.79961
	loss_value_0: 0.17987
	loss_policy_1: 0.02761
	accuracy_policy_1: 0.71492
	loss_value_1: 0.03801
	loss_reward_1: 0.00771
	loss_policy_2: 0.03185
	accuracy_policy_2: 0.67906
	loss_value_2: 0.03966
	loss_reward_2: 0.00733
	loss_policy_3: 0.03616
	accuracy_policy_3: 0.64176
	loss_value_3: 0.04117
	loss_reward_3: 0.00793
	loss_policy_4: 0.03955
	accuracy_policy_4: 0.61699
	loss_value_4: 0.0428
	loss_reward_4: 0.0086
	loss_policy_5: 0.04268
	accuracy_policy_5: 0.59277
	loss_value_5: 0.04409
	loss_reward_5: 0.0096
	loss_policy: 0.26207
	loss_value: 0.3856
	loss_reward: 0.04115
Optimization_Done 44400
[2024-05-05 13:45:05] [command] train weight_iter_44400.pkl 222 223
[2024-05-05 13:45:22] nn step 44450, lr: 0.005.
	loss_policy_0: 0.10439
	accuracy_policy_0: 0.76125
	loss_value_0: 0.17287
	loss_policy_1: 0.02981
	accuracy_policy_1: 0.68016
	loss_value_1: 0.03661
	loss_reward_1: 0.00693
	loss_policy_2: 0.03352
	accuracy_policy_2: 0.64262
	loss_value_2: 0.03805
	loss_reward_2: 0.00665
	loss_policy_3: 0.03679
	accuracy_policy_3: 0.6218
	loss_value_3: 0.03933
	loss_reward_3: 0.00706
	loss_policy_4: 0.03967
	accuracy_policy_4: 0.58957
	loss_value_4: 0.04052
	loss_reward_4: 0.00793
	loss_policy_5: 0.04176
	accuracy_policy_5: 0.57016
	loss_value_5: 0.0416
	loss_reward_5: 0.00888
	loss_policy: 0.28593
	loss_value: 0.36898
	loss_reward: 0.03745
[2024-05-05 13:45:38] nn step 44500, lr: 0.005.
	loss_policy_0: 0.08819
	accuracy_policy_0: 0.78922
	loss_value_0: 0.16409
	loss_policy_1: 0.02701
	accuracy_policy_1: 0.70711
	loss_value_1: 0.03477
	loss_reward_1: 0.00707
	loss_policy_2: 0.03085
	accuracy_policy_2: 0.67129
	loss_value_2: 0.03623
	loss_reward_2: 0.0064
	loss_policy_3: 0.03406
	accuracy_policy_3: 0.64234
	loss_value_3: 0.03735
	loss_reward_3: 0.00718
	loss_policy_4: 0.03621
	accuracy_policy_4: 0.62137
	loss_value_4: 0.03849
	loss_reward_4: 0.00766
	loss_policy_5: 0.03885
	accuracy_policy_5: 0.60035
	loss_value_5: 0.03973
	loss_reward_5: 0.00865
	loss_policy: 0.25517
	loss_value: 0.35066
	loss_reward: 0.03696
[2024-05-05 13:45:54] nn step 44550, lr: 0.005.
	loss_policy_0: 0.08035
	accuracy_policy_0: 0.80109
	loss_value_0: 0.16186
	loss_policy_1: 0.02564
	accuracy_policy_1: 0.70992
	loss_value_1: 0.03405
	loss_reward_1: 0.00669
	loss_policy_2: 0.02909
	accuracy_policy_2: 0.67836
	loss_value_2: 0.03513
	loss_reward_2: 0.0064
	loss_policy_3: 0.03253
	accuracy_policy_3: 0.64699
	loss_value_3: 0.03618
	loss_reward_3: 0.00715
	loss_policy_4: 0.03486
	accuracy_policy_4: 0.62789
	loss_value_4: 0.03733
	loss_reward_4: 0.00767
	loss_policy_5: 0.03728
	accuracy_policy_5: 0.60863
	loss_value_5: 0.03854
	loss_reward_5: 0.00877
	loss_policy: 0.23976
	loss_value: 0.3431
	loss_reward: 0.03668
[2024-05-05 13:46:10] nn step 44600, lr: 0.005.
	loss_policy_0: 0.07988
	accuracy_policy_0: 0.80672
	loss_value_0: 0.16581
	loss_policy_1: 0.0263
	accuracy_policy_1: 0.71988
	loss_value_1: 0.03489
	loss_reward_1: 0.00719
	loss_policy_2: 0.0301
	accuracy_policy_2: 0.68262
	loss_value_2: 0.03625
	loss_reward_2: 0.00666
	loss_policy_3: 0.03364
	accuracy_policy_3: 0.65117
	loss_value_3: 0.03769
	loss_reward_3: 0.00739
	loss_policy_4: 0.03595
	accuracy_policy_4: 0.63258
	loss_value_4: 0.03887
	loss_reward_4: 0.00807
	loss_policy_5: 0.03823
	accuracy_policy_5: 0.60961
	loss_value_5: 0.03999
	loss_reward_5: 0.00917
	loss_policy: 0.2441
	loss_value: 0.35351
	loss_reward: 0.03848
Optimization_Done 44600
[2024-05-05 13:48:03] [command] train weight_iter_44600.pkl 223 224
[2024-05-05 13:48:20] nn step 44650, lr: 0.005.
	loss_policy_0: 0.11717
	accuracy_policy_0: 0.75477
	loss_value_0: 0.17323
	loss_policy_1: 0.03245
	accuracy_policy_1: 0.6775
	loss_value_1: 0.03638
	loss_reward_1: 0.00612
	loss_policy_2: 0.03735
	accuracy_policy_2: 0.63039
	loss_value_2: 0.03791
	loss_reward_2: 0.00603
	loss_policy_3: 0.04019
	accuracy_policy_3: 0.59941
	loss_value_3: 0.03917
	loss_reward_3: 0.00683
	loss_policy_4: 0.04297
	accuracy_policy_4: 0.57809
	loss_value_4: 0.04055
	loss_reward_4: 0.00761
	loss_policy_5: 0.04587
	accuracy_policy_5: 0.55
	loss_value_5: 0.04171
	loss_reward_5: 0.0083
	loss_policy: 0.31599
	loss_value: 0.36895
	loss_reward: 0.03489
[2024-05-05 13:48:36] nn step 44700, lr: 0.005.
	loss_policy_0: 0.09165
	accuracy_policy_0: 0.81168
	loss_value_0: 0.18334
	loss_policy_1: 0.03036
	accuracy_policy_1: 0.71031
	loss_value_1: 0.03852
	loss_reward_1: 0.00656
	loss_policy_2: 0.0348
	accuracy_policy_2: 0.67207
	loss_value_2: 0.03984
	loss_reward_2: 0.00644
	loss_policy_3: 0.03841
	accuracy_policy_3: 0.64293
	loss_value_3: 0.041
	loss_reward_3: 0.00715
	loss_policy_4: 0.04132
	accuracy_policy_4: 0.61816
	loss_value_4: 0.04214
	loss_reward_4: 0.00789
	loss_policy_5: 0.0443
	accuracy_policy_5: 0.59582
	loss_value_5: 0.04347
	loss_reward_5: 0.00901
	loss_policy: 0.28085
	loss_value: 0.3883
	loss_reward: 0.03705
[2024-05-05 13:48:52] nn step 44750, lr: 0.005.
	loss_policy_0: 0.08005
	accuracy_policy_0: 0.81656
	loss_value_0: 0.17041
	loss_policy_1: 0.02722
	accuracy_policy_1: 0.71715
	loss_value_1: 0.03583
	loss_reward_1: 0.00626
	loss_policy_2: 0.0317
	accuracy_policy_2: 0.67758
	loss_value_2: 0.03712
	loss_reward_2: 0.00604
	loss_policy_3: 0.03479
	accuracy_policy_3: 0.6523
	loss_value_3: 0.03822
	loss_reward_3: 0.00661
	loss_policy_4: 0.03716
	accuracy_policy_4: 0.63145
	loss_value_4: 0.03923
	loss_reward_4: 0.00716
	loss_policy_5: 0.04008
	accuracy_policy_5: 0.60684
	loss_value_5: 0.0403
	loss_reward_5: 0.00834
	loss_policy: 0.251
	loss_value: 0.3611
	loss_reward: 0.03441
[2024-05-05 13:49:08] nn step 44800, lr: 0.005.
	loss_policy_0: 0.07171
	accuracy_policy_0: 0.82797
	loss_value_0: 0.16519
	loss_policy_1: 0.02593
	accuracy_policy_1: 0.72305
	loss_value_1: 0.0348
	loss_reward_1: 0.00613
	loss_policy_2: 0.03039
	accuracy_policy_2: 0.67965
	loss_value_2: 0.03595
	loss_reward_2: 0.00578
	loss_policy_3: 0.03327
	accuracy_policy_3: 0.65863
	loss_value_3: 0.03692
	loss_reward_3: 0.00639
	loss_policy_4: 0.03585
	accuracy_policy_4: 0.63082
	loss_value_4: 0.03793
	loss_reward_4: 0.00685
	loss_policy_5: 0.03852
	accuracy_policy_5: 0.6068
	loss_value_5: 0.03893
	loss_reward_5: 0.00783
	loss_policy: 0.23567
	loss_value: 0.34972
	loss_reward: 0.03298
Optimization_Done 44800
[2024-05-05 13:51:11] [command] train weight_iter_44800.pkl 224 225
[2024-05-05 13:51:29] nn step 44850, lr: 0.005.
	loss_policy_0: 0.13232
	accuracy_policy_0: 0.74312
	loss_value_0: 0.2011
	loss_policy_1: 0.03583
	accuracy_policy_1: 0.67074
	loss_value_1: 0.04202
	loss_reward_1: 0.00682
	loss_policy_2: 0.04046
	accuracy_policy_2: 0.6368
	loss_value_2: 0.04365
	loss_reward_2: 0.00671
	loss_policy_3: 0.04434
	accuracy_policy_3: 0.60195
	loss_value_3: 0.04503
	loss_reward_3: 0.00715
	loss_policy_4: 0.04681
	accuracy_policy_4: 0.58605
	loss_value_4: 0.04638
	loss_reward_4: 0.00798
	loss_policy_5: 0.04989
	accuracy_policy_5: 0.55945
	loss_value_5: 0.04769
	loss_reward_5: 0.00925
	loss_policy: 0.34966
	loss_value: 0.42587
	loss_reward: 0.03791
[2024-05-05 13:51:44] nn step 44900, lr: 0.005.
	loss_policy_0: 0.08914
	accuracy_policy_0: 0.79418
	loss_value_0: 0.17769
	loss_policy_1: 0.02858
	accuracy_policy_1: 0.69957
	loss_value_1: 0.03749
	loss_reward_1: 0.00617
	loss_policy_2: 0.03295
	accuracy_policy_2: 0.66641
	loss_value_2: 0.03873
	loss_reward_2: 0.00607
	loss_policy_3: 0.03628
	accuracy_policy_3: 0.64234
	loss_value_3: 0.03976
	loss_reward_3: 0.00668
	loss_policy_4: 0.03917
	accuracy_policy_4: 0.61906
	loss_value_4: 0.0409
	loss_reward_4: 0.00724
	loss_policy_5: 0.04165
	accuracy_policy_5: 0.59773
	loss_value_5: 0.04221
	loss_reward_5: 0.00841
	loss_policy: 0.26777
	loss_value: 0.37676
	loss_reward: 0.03457
[2024-05-05 13:52:00] nn step 44950, lr: 0.005.
	loss_policy_0: 0.08903
	accuracy_policy_0: 0.80367
	loss_value_0: 0.192
	loss_policy_1: 0.02923
	accuracy_policy_1: 0.71289
	loss_value_1: 0.0404
	loss_reward_1: 0.00665
	loss_policy_2: 0.03441
	accuracy_policy_2: 0.67648
	loss_value_2: 0.04194
	loss_reward_2: 0.00632
	loss_policy_3: 0.03815
	accuracy_policy_3: 0.64812
	loss_value_3: 0.04328
	loss_reward_3: 0.00703
	loss_policy_4: 0.04094
	accuracy_policy_4: 0.62473
	loss_value_4: 0.04459
	loss_reward_4: 0.00777
	loss_policy_5: 0.04329
	accuracy_policy_5: 0.60656
	loss_value_5: 0.04574
	loss_reward_5: 0.00874
	loss_policy: 0.27506
	loss_value: 0.40795
	loss_reward: 0.03651
[2024-05-05 13:52:16] nn step 45000, lr: 0.005.
	loss_policy_0: 0.08192
	accuracy_policy_0: 0.81039
	loss_value_0: 0.18734
	loss_policy_1: 0.02831
	accuracy_policy_1: 0.70535
	loss_value_1: 0.03938
	loss_reward_1: 0.00634
	loss_policy_2: 0.03319
	accuracy_policy_2: 0.66637
	loss_value_2: 0.04085
	loss_reward_2: 0.00622
	loss_policy_3: 0.03615
	accuracy_policy_3: 0.64602
	loss_value_3: 0.04205
	loss_reward_3: 0.00688
	loss_policy_4: 0.03897
	accuracy_policy_4: 0.6248
	loss_value_4: 0.04345
	loss_reward_4: 0.00763
	loss_policy_5: 0.04199
	accuracy_policy_5: 0.60578
	loss_value_5: 0.04468
	loss_reward_5: 0.00848
	loss_policy: 0.26053
	loss_value: 0.39775
	loss_reward: 0.03555
Optimization_Done 45000
[2024-05-05 13:54:20] [command] train weight_iter_45000.pkl 225 226
[2024-05-05 13:54:37] nn step 45050, lr: 0.005.
	loss_policy_0: 0.11104
	accuracy_policy_0: 0.73637
	loss_value_0: 0.17693
	loss_policy_1: 0.0318
	accuracy_policy_1: 0.65082
	loss_value_1: 0.03735
	loss_reward_1: 0.00655
	loss_policy_2: 0.03552
	accuracy_policy_2: 0.62137
	loss_value_2: 0.03899
	loss_reward_2: 0.00617
	loss_policy_3: 0.03933
	accuracy_policy_3: 0.59062
	loss_value_3: 0.04053
	loss_reward_3: 0.00686
	loss_policy_4: 0.04225
	accuracy_policy_4: 0.56129
	loss_value_4: 0.04189
	loss_reward_4: 0.00765
	loss_policy_5: 0.04525
	accuracy_policy_5: 0.5298
	loss_value_5: 0.0431
	loss_reward_5: 0.00865
	loss_policy: 0.30519
	loss_value: 0.37879
	loss_reward: 0.03588
[2024-05-05 13:54:53] nn step 45100, lr: 0.005.
	loss_policy_0: 0.10377
	accuracy_policy_0: 0.775
	loss_value_0: 0.19948
	loss_policy_1: 0.03288
	accuracy_policy_1: 0.6841
	loss_value_1: 0.04189
	loss_reward_1: 0.00736
	loss_policy_2: 0.03769
	accuracy_policy_2: 0.64793
	loss_value_2: 0.04384
	loss_reward_2: 0.00708
	loss_policy_3: 0.04161
	accuracy_policy_3: 0.62102
	loss_value_3: 0.0453
	loss_reward_3: 0.00779
	loss_policy_4: 0.04542
	accuracy_policy_4: 0.59504
	loss_value_4: 0.04681
	loss_reward_4: 0.00837
	loss_policy_5: 0.04872
	accuracy_policy_5: 0.56902
	loss_value_5: 0.04831
	loss_reward_5: 0.00952
	loss_policy: 0.3101
	loss_value: 0.42563
	loss_reward: 0.04013
[2024-05-05 13:55:09] nn step 45150, lr: 0.005.
	loss_policy_0: 0.09593
	accuracy_policy_0: 0.78352
	loss_value_0: 0.19391
	loss_policy_1: 0.03132
	accuracy_policy_1: 0.69508
	loss_value_1: 0.04095
	loss_reward_1: 0.00711
	loss_policy_2: 0.03574
	accuracy_policy_2: 0.65598
	loss_value_2: 0.04276
	loss_reward_2: 0.00711
	loss_policy_3: 0.03999
	accuracy_policy_3: 0.62504
	loss_value_3: 0.04438
	loss_reward_3: 0.00768
	loss_policy_4: 0.04337
	accuracy_policy_4: 0.60223
	loss_value_4: 0.04578
	loss_reward_4: 0.00832
	loss_policy_5: 0.04678
	accuracy_policy_5: 0.5759
	loss_value_5: 0.04736
	loss_reward_5: 0.00949
	loss_policy: 0.29312
	loss_value: 0.41513
	loss_reward: 0.03971
[2024-05-05 13:55:25] nn step 45200, lr: 0.005.
	loss_policy_0: 0.08412
	accuracy_policy_0: 0.79707
	loss_value_0: 0.18249
	loss_policy_1: 0.02817
	accuracy_policy_1: 0.7027
	loss_value_1: 0.03849
	loss_reward_1: 0.00683
	loss_policy_2: 0.03287
	accuracy_policy_2: 0.65945
	loss_value_2: 0.04014
	loss_reward_2: 0.00648
	loss_policy_3: 0.0366
	accuracy_policy_3: 0.63277
	loss_value_3: 0.04164
	loss_reward_3: 0.00707
	loss_policy_4: 0.03998
	accuracy_policy_4: 0.61141
	loss_value_4: 0.04278
	loss_reward_4: 0.00773
	loss_policy_5: 0.0426
	accuracy_policy_5: 0.58953
	loss_value_5: 0.04425
	loss_reward_5: 0.00889
	loss_policy: 0.26435
	loss_value: 0.38979
	loss_reward: 0.03701
Optimization_Done 45200
[2024-05-05 13:57:28] [command] train weight_iter_45200.pkl 226 227
[2024-05-05 13:57:45] nn step 45250, lr: 0.005.
	loss_policy_0: 0.10694
	accuracy_policy_0: 0.75008
	loss_value_0: 0.16959
	loss_policy_1: 0.02972
	accuracy_policy_1: 0.6782
	loss_value_1: 0.03543
	loss_reward_1: 0.00626
	loss_policy_2: 0.03339
	accuracy_policy_2: 0.6473
	loss_value_2: 0.03697
	loss_reward_2: 0.00625
	loss_policy_3: 0.03707
	accuracy_policy_3: 0.61406
	loss_value_3: 0.03846
	loss_reward_3: 0.0068
	loss_policy_4: 0.04018
	accuracy_policy_4: 0.58234
	loss_value_4: 0.03973
	loss_reward_4: 0.00743
	loss_policy_5: 0.04259
	accuracy_policy_5: 0.56422
	loss_value_5: 0.04083
	loss_reward_5: 0.00837
	loss_policy: 0.28989
	loss_value: 0.36101
	loss_reward: 0.0351
[2024-05-05 13:58:01] nn step 45300, lr: 0.005.
	loss_policy_0: 0.08458
	accuracy_policy_0: 0.79277
	loss_value_0: 0.16692
	loss_policy_1: 0.027
	accuracy_policy_1: 0.70289
	loss_value_1: 0.03522
	loss_reward_1: 0.00651
	loss_policy_2: 0.0309
	accuracy_policy_2: 0.67074
	loss_value_2: 0.03646
	loss_reward_2: 0.0061
	loss_policy_3: 0.03431
	accuracy_policy_3: 0.64344
	loss_value_3: 0.03753
	loss_reward_3: 0.00664
	loss_policy_4: 0.03735
	accuracy_policy_4: 0.6141
	loss_value_4: 0.03872
	loss_reward_4: 0.00704
	loss_policy_5: 0.03979
	accuracy_policy_5: 0.5968
	loss_value_5: 0.03975
	loss_reward_5: 0.00828
	loss_policy: 0.25393
	loss_value: 0.3546
	loss_reward: 0.03458
[2024-05-05 13:58:17] nn step 45350, lr: 0.005.
	loss_policy_0: 0.08066
	accuracy_policy_0: 0.80289
	loss_value_0: 0.1753
	loss_policy_1: 0.02678
	accuracy_policy_1: 0.70773
	loss_value_1: 0.03681
	loss_reward_1: 0.00664
	loss_policy_2: 0.0311
	accuracy_policy_2: 0.67273
	loss_value_2: 0.03808
	loss_reward_2: 0.00606
	loss_policy_3: 0.0341
	accuracy_policy_3: 0.65027
	loss_value_3: 0.03949
	loss_reward_3: 0.007
	loss_policy_4: 0.03722
	accuracy_policy_4: 0.62789
	loss_value_4: 0.04064
	loss_reward_4: 0.00733
	loss_policy_5: 0.03993
	accuracy_policy_5: 0.60055
	loss_value_5: 0.04162
	loss_reward_5: 0.00869
	loss_policy: 0.24979
	loss_value: 0.37195
	loss_reward: 0.03572
[2024-05-05 13:58:33] nn step 45400, lr: 0.005.
	loss_policy_0: 0.06869
	accuracy_policy_0: 0.8143
	loss_value_0: 0.16306
	loss_policy_1: 0.02346
	accuracy_policy_1: 0.71773
	loss_value_1: 0.0342
	loss_reward_1: 0.00593
	loss_policy_2: 0.02721
	accuracy_policy_2: 0.68336
	loss_value_2: 0.03551
	loss_reward_2: 0.00595
	loss_policy_3: 0.03067
	accuracy_policy_3: 0.65602
	loss_value_3: 0.03662
	loss_reward_3: 0.00633
	loss_policy_4: 0.03353
	accuracy_policy_4: 0.63168
	loss_value_4: 0.03774
	loss_reward_4: 0.00688
	loss_policy_5: 0.03605
	accuracy_policy_5: 0.61219
	loss_value_5: 0.0387
	loss_reward_5: 0.00783
	loss_policy: 0.21959
	loss_value: 0.34583
	loss_reward: 0.03291
Optimization_Done 45400
[2024-05-05 14:00:27] [command] train weight_iter_45400.pkl 227 228
[2024-05-05 14:00:44] nn step 45450, lr: 0.005.
	loss_policy_0: 0.10798
	accuracy_policy_0: 0.72625
	loss_value_0: 0.16061
	loss_policy_1: 0.02833
	accuracy_policy_1: 0.66504
	loss_value_1: 0.03343
	loss_reward_1: 0.00499
	loss_policy_2: 0.03147
	accuracy_policy_2: 0.63258
	loss_value_2: 0.03439
	loss_reward_2: 0.00501
	loss_policy_3: 0.03364
	accuracy_policy_3: 0.61297
	loss_value_3: 0.03536
	loss_reward_3: 0.00521
	loss_policy_4: 0.03568
	accuracy_policy_4: 0.59305
	loss_value_4: 0.03631
	loss_reward_4: 0.00573
	loss_policy_5: 0.03816
	accuracy_policy_5: 0.57441
	loss_value_5: 0.03716
	loss_reward_5: 0.00647
	loss_policy: 0.27527
	loss_value: 0.33726
	loss_reward: 0.0274
[2024-05-05 14:01:00] nn step 45500, lr: 0.005.
	loss_policy_0: 0.09034
	accuracy_policy_0: 0.78324
	loss_value_0: 0.17419
	loss_policy_1: 0.02682
	accuracy_policy_1: 0.70805
	loss_value_1: 0.03627
	loss_reward_1: 0.0056
	loss_policy_2: 0.03091
	accuracy_policy_2: 0.67051
	loss_value_2: 0.0373
	loss_reward_2: 0.00537
	loss_policy_3: 0.03381
	accuracy_policy_3: 0.64566
	loss_value_3: 0.03836
	loss_reward_3: 0.00584
	loss_policy_4: 0.03639
	accuracy_policy_4: 0.62203
	loss_value_4: 0.03919
	loss_reward_4: 0.00629
	loss_policy_5: 0.03861
	accuracy_policy_5: 0.60223
	loss_value_5: 0.04025
	loss_reward_5: 0.00723
	loss_policy: 0.25689
	loss_value: 0.36556
	loss_reward: 0.03034
[2024-05-05 14:01:16] nn step 45550, lr: 0.005.
	loss_policy_0: 0.07932
	accuracy_policy_0: 0.79246
	loss_value_0: 0.1642
	loss_policy_1: 0.02455
	accuracy_policy_1: 0.71867
	loss_value_1: 0.03445
	loss_reward_1: 0.00536
	loss_policy_2: 0.02818
	accuracy_policy_2: 0.68195
	loss_value_2: 0.03535
	loss_reward_2: 0.00525
	loss_policy_3: 0.03071
	accuracy_policy_3: 0.65863
	loss_value_3: 0.03645
	loss_reward_3: 0.00546
	loss_policy_4: 0.03296
	accuracy_policy_4: 0.63473
	loss_value_4: 0.03743
	loss_reward_4: 0.00611
	loss_policy_5: 0.03538
	accuracy_policy_5: 0.6148
	loss_value_5: 0.03831
	loss_reward_5: 0.00693
	loss_policy: 0.23109
	loss_value: 0.34618
	loss_reward: 0.02911
[2024-05-05 14:01:32] nn step 45600, lr: 0.005.
	loss_policy_0: 0.0744
	accuracy_policy_0: 0.80426
	loss_value_0: 0.16436
	loss_policy_1: 0.02345
	accuracy_policy_1: 0.72
	loss_value_1: 0.03423
	loss_reward_1: 0.00524
	loss_policy_2: 0.02722
	accuracy_policy_2: 0.685
	loss_value_2: 0.03509
	loss_reward_2: 0.00512
	loss_policy_3: 0.03003
	accuracy_policy_3: 0.66008
	loss_value_3: 0.03611
	loss_reward_3: 0.00553
	loss_policy_4: 0.03278
	accuracy_policy_4: 0.63195
	loss_value_4: 0.03709
	loss_reward_4: 0.00599
	loss_policy_5: 0.03513
	accuracy_policy_5: 0.61258
	loss_value_5: 0.03812
	loss_reward_5: 0.00681
	loss_policy: 0.223
	loss_value: 0.345
	loss_reward: 0.0287
Optimization_Done 45600
[2024-05-05 14:03:35] [command] train weight_iter_45600.pkl 228 229
[2024-05-05 14:03:52] nn step 45650, lr: 0.005.
	loss_policy_0: 0.11174
	accuracy_policy_0: 0.75145
	loss_value_0: 0.17852
	loss_policy_1: 0.03015
	accuracy_policy_1: 0.68469
	loss_value_1: 0.03726
	loss_reward_1: 0.0055
	loss_policy_2: 0.0342
	accuracy_policy_2: 0.64621
	loss_value_2: 0.03868
	loss_reward_2: 0.00539
	loss_policy_3: 0.03739
	accuracy_policy_3: 0.61566
	loss_value_3: 0.04009
	loss_reward_3: 0.0057
	loss_policy_4: 0.04084
	accuracy_policy_4: 0.5843
	loss_value_4: 0.04135
	loss_reward_4: 0.00641
	loss_policy_5: 0.0438
	accuracy_policy_5: 0.55395
	loss_value_5: 0.04258
	loss_reward_5: 0.00726
	loss_policy: 0.29811
	loss_value: 0.37849
	loss_reward: 0.03027
[2024-05-05 14:04:08] nn step 45700, lr: 0.005.
	loss_policy_0: 0.09295
	accuracy_policy_0: 0.78801
	loss_value_0: 0.17841
	loss_policy_1: 0.02829
	accuracy_policy_1: 0.71066
	loss_value_1: 0.03742
	loss_reward_1: 0.0056
	loss_policy_2: 0.03263
	accuracy_policy_2: 0.67406
	loss_value_2: 0.03881
	loss_reward_2: 0.00555
	loss_policy_3: 0.03641
	accuracy_policy_3: 0.63734
	loss_value_3: 0.04025
	loss_reward_3: 0.00633
	loss_policy_4: 0.03926
	accuracy_policy_4: 0.6102
	loss_value_4: 0.04155
	loss_reward_4: 0.0067
	loss_policy_5: 0.04227
	accuracy_policy_5: 0.58609
	loss_value_5: 0.04278
	loss_reward_5: 0.00739
	loss_policy: 0.2718
	loss_value: 0.37923
	loss_reward: 0.03157
[2024-05-05 14:04:24] nn step 45750, lr: 0.005.
	loss_policy_0: 0.08712
	accuracy_policy_0: 0.79277
	loss_value_0: 0.1743
	loss_policy_1: 0.02723
	accuracy_policy_1: 0.70613
	loss_value_1: 0.03669
	loss_reward_1: 0.00567
	loss_policy_2: 0.03151
	accuracy_policy_2: 0.67117
	loss_value_2: 0.03827
	loss_reward_2: 0.00551
	loss_policy_3: 0.03451
	accuracy_policy_3: 0.64359
	loss_value_3: 0.03933
	loss_reward_3: 0.006
	loss_policy_4: 0.03732
	accuracy_policy_4: 0.62328
	loss_value_4: 0.04056
	loss_reward_4: 0.00649
	loss_policy_5: 0.04024
	accuracy_policy_5: 0.59648
	loss_value_5: 0.04184
	loss_reward_5: 0.00763
	loss_policy: 0.25792
	loss_value: 0.37098
	loss_reward: 0.03131
[2024-05-05 14:04:39] nn step 45800, lr: 0.005.
	loss_policy_0: 0.08346
	accuracy_policy_0: 0.79988
	loss_value_0: 0.17844
	loss_policy_1: 0.027
	accuracy_policy_1: 0.715
	loss_value_1: 0.03763
	loss_reward_1: 0.0057
	loss_policy_2: 0.03124
	accuracy_policy_2: 0.68348
	loss_value_2: 0.03906
	loss_reward_2: 0.00554
	loss_policy_3: 0.03445
	accuracy_policy_3: 0.66004
	loss_value_3: 0.04031
	loss_reward_3: 0.00607
	loss_policy_4: 0.03718
	accuracy_policy_4: 0.63363
	loss_value_4: 0.04137
	loss_reward_4: 0.00644
	loss_policy_5: 0.04036
	accuracy_policy_5: 0.60844
	loss_value_5: 0.04253
	loss_reward_5: 0.00773
	loss_policy: 0.25369
	loss_value: 0.37933
	loss_reward: 0.03149
Optimization_Done 45800
[2024-05-05 14:06:43] [command] train weight_iter_45800.pkl 229 230
[2024-05-05 14:07:00] nn step 45850, lr: 0.005.
	loss_policy_0: 0.11145
	accuracy_policy_0: 0.74156
	loss_value_0: 0.18548
	loss_policy_1: 0.03332
	accuracy_policy_1: 0.65699
	loss_value_1: 0.03897
	loss_reward_1: 0.00659
	loss_policy_2: 0.038
	accuracy_policy_2: 0.61938
	loss_value_2: 0.04083
	loss_reward_2: 0.00658
	loss_policy_3: 0.04172
	accuracy_policy_3: 0.58906
	loss_value_3: 0.04256
	loss_reward_3: 0.00731
	loss_policy_4: 0.04586
	accuracy_policy_4: 0.54988
	loss_value_4: 0.04409
	loss_reward_4: 0.00798
	loss_policy_5: 0.04944
	accuracy_policy_5: 0.51703
	loss_value_5: 0.04562
	loss_reward_5: 0.00874
	loss_policy: 0.31979
	loss_value: 0.39755
	loss_reward: 0.03719
[2024-05-05 14:07:16] nn step 45900, lr: 0.005.
	loss_policy_0: 0.08885
	accuracy_policy_0: 0.78559
	loss_value_0: 0.17628
	loss_policy_1: 0.02873
	accuracy_policy_1: 0.6957
	loss_value_1: 0.03736
	loss_reward_1: 0.00653
	loss_policy_2: 0.0336
	accuracy_policy_2: 0.65336
	loss_value_2: 0.03897
	loss_reward_2: 0.00607
	loss_policy_3: 0.03724
	accuracy_policy_3: 0.62613
	loss_value_3: 0.04049
	loss_reward_3: 0.0069
	loss_policy_4: 0.0408
	accuracy_policy_4: 0.59676
	loss_value_4: 0.04202
	loss_reward_4: 0.00748
	loss_policy_5: 0.0452
	accuracy_policy_5: 0.55586
	loss_value_5: 0.04363
	loss_reward_5: 0.00835
	loss_policy: 0.27442
	loss_value: 0.37874
	loss_reward: 0.03533
[2024-05-05 14:07:32] nn step 45950, lr: 0.005.
	loss_policy_0: 0.08492
	accuracy_policy_0: 0.79973
	loss_value_0: 0.18713
	loss_policy_1: 0.02946
	accuracy_policy_1: 0.70164
	loss_value_1: 0.03971
	loss_reward_1: 0.00663
	loss_policy_2: 0.03422
	accuracy_policy_2: 0.66594
	loss_value_2: 0.04117
	loss_reward_2: 0.00644
	loss_policy_3: 0.03875
	accuracy_policy_3: 0.6332
	loss_value_3: 0.04248
	loss_reward_3: 0.00719
	loss_policy_4: 0.04236
	accuracy_policy_4: 0.60391
	loss_value_4: 0.04389
	loss_reward_4: 0.00786
	loss_policy_5: 0.04599
	accuracy_policy_5: 0.57484
	loss_value_5: 0.0454
	loss_reward_5: 0.00871
	loss_policy: 0.2757
	loss_value: 0.39978
	loss_reward: 0.03682
[2024-05-05 14:07:48] nn step 46000, lr: 0.005.
	loss_policy_0: 0.07975
	accuracy_policy_0: 0.80508
	loss_value_0: 0.18444
	loss_policy_1: 0.0283
	accuracy_policy_1: 0.70852
	loss_value_1: 0.03888
	loss_reward_1: 0.0068
	loss_policy_2: 0.03301
	accuracy_policy_2: 0.66855
	loss_value_2: 0.04057
	loss_reward_2: 0.0066
	loss_policy_3: 0.03674
	accuracy_policy_3: 0.64477
	loss_value_3: 0.04211
	loss_reward_3: 0.00724
	loss_policy_4: 0.04071
	accuracy_policy_4: 0.61059
	loss_value_4: 0.04379
	loss_reward_4: 0.00753
	loss_policy_5: 0.04464
	accuracy_policy_5: 0.58148
	loss_value_5: 0.04533
	loss_reward_5: 0.00907
	loss_policy: 0.26316
	loss_value: 0.39511
	loss_reward: 0.03724
Optimization_Done 46000
[2024-05-05 14:09:31] [command] train weight_iter_46000.pkl 230 231
[2024-05-05 14:09:48] nn step 46050, lr: 0.005.
	loss_policy_0: 0.10334
	accuracy_policy_0: 0.75016
	loss_value_0: 0.18437
	loss_policy_1: 0.03105
	accuracy_policy_1: 0.66414
	loss_value_1: 0.03859
	loss_reward_1: 0.00679
	loss_policy_2: 0.03559
	accuracy_policy_2: 0.62254
	loss_value_2: 0.04003
	loss_reward_2: 0.00629
	loss_policy_3: 0.0397
	accuracy_policy_3: 0.58824
	loss_value_3: 0.04134
	loss_reward_3: 0.00726
	loss_policy_4: 0.04319
	accuracy_policy_4: 0.55609
	loss_value_4: 0.04247
	loss_reward_4: 0.00764
	loss_policy_5: 0.04656
	accuracy_policy_5: 0.5273
	loss_value_5: 0.04358
	loss_reward_5: 0.00869
	loss_policy: 0.29943
	loss_value: 0.39038
	loss_reward: 0.03667
[2024-05-05 14:10:04] nn step 46100, lr: 0.005.
	loss_policy_0: 0.0802
	accuracy_policy_0: 0.78723
	loss_value_0: 0.17623
	loss_policy_1: 0.02763
	accuracy_policy_1: 0.69168
	loss_value_1: 0.03721
	loss_reward_1: 0.00669
	loss_policy_2: 0.03228
	accuracy_policy_2: 0.64926
	loss_value_2: 0.0388
	loss_reward_2: 0.00654
	loss_policy_3: 0.03617
	accuracy_policy_3: 0.61637
	loss_value_3: 0.04011
	loss_reward_3: 0.00719
	loss_policy_4: 0.03955
	accuracy_policy_4: 0.59078
	loss_value_4: 0.04134
	loss_reward_4: 0.00761
	loss_policy_5: 0.04298
	accuracy_policy_5: 0.55902
	loss_value_5: 0.0426
	loss_reward_5: 0.00857
	loss_policy: 0.2588
	loss_value: 0.37629
	loss_reward: 0.0366
[2024-05-05 14:10:20] nn step 46150, lr: 0.005.
	loss_policy_0: 0.0772
	accuracy_policy_0: 0.79887
	loss_value_0: 0.18437
	loss_policy_1: 0.02755
	accuracy_policy_1: 0.69805
	loss_value_1: 0.03876
	loss_reward_1: 0.00713
	loss_policy_2: 0.03288
	accuracy_policy_2: 0.65723
	loss_value_2: 0.04019
	loss_reward_2: 0.00698
	loss_policy_3: 0.03686
	accuracy_policy_3: 0.62699
	loss_value_3: 0.04149
	loss_reward_3: 0.00732
	loss_policy_4: 0.04058
	accuracy_policy_4: 0.59406
	loss_value_4: 0.04279
	loss_reward_4: 0.00787
	loss_policy_5: 0.04387
	accuracy_policy_5: 0.56695
	loss_value_5: 0.04422
	loss_reward_5: 0.00913
	loss_policy: 0.25894
	loss_value: 0.39183
	loss_reward: 0.03843
[2024-05-05 14:10:36] nn step 46200, lr: 0.005.
	loss_policy_0: 0.06986
	accuracy_policy_0: 0.81184
	loss_value_0: 0.17592
	loss_policy_1: 0.02617
	accuracy_policy_1: 0.70191
	loss_value_1: 0.03725
	loss_reward_1: 0.00667
	loss_policy_2: 0.0311
	accuracy_policy_2: 0.6616
	loss_value_2: 0.03859
	loss_reward_2: 0.00641
	loss_policy_3: 0.03505
	accuracy_policy_3: 0.62633
	loss_value_3: 0.03967
	loss_reward_3: 0.0071
	loss_policy_4: 0.03866
	accuracy_policy_4: 0.5918
	loss_value_4: 0.04082
	loss_reward_4: 0.00769
	loss_policy_5: 0.04183
	accuracy_policy_5: 0.57082
	loss_value_5: 0.04214
	loss_reward_5: 0.00877
	loss_policy: 0.24268
	loss_value: 0.37439
	loss_reward: 0.03664
Optimization_Done 46200
[2024-05-05 14:12:29] [command] train weight_iter_46200.pkl 231 232
[2024-05-05 14:12:46] nn step 46250, lr: 0.005.
	loss_policy_0: 0.09538
	accuracy_policy_0: 0.76965
	loss_value_0: 0.16769
	loss_policy_1: 0.02756
	accuracy_policy_1: 0.69238
	loss_value_1: 0.03504
	loss_reward_1: 0.00552
	loss_policy_2: 0.03162
	accuracy_policy_2: 0.65125
	loss_value_2: 0.03617
	loss_reward_2: 0.0056
	loss_policy_3: 0.03448
	accuracy_policy_3: 0.62219
	loss_value_3: 0.03732
	loss_reward_3: 0.00586
	loss_policy_4: 0.03692
	accuracy_policy_4: 0.59719
	loss_value_4: 0.03825
	loss_reward_4: 0.0065
	loss_policy_5: 0.03935
	accuracy_policy_5: 0.57277
	loss_value_5: 0.03937
	loss_reward_5: 0.00729
	loss_policy: 0.26531
	loss_value: 0.35384
	loss_reward: 0.03077
[2024-05-05 14:13:02] nn step 46300, lr: 0.005.
	loss_policy_0: 0.07914
	accuracy_policy_0: 0.79812
	loss_value_0: 0.16867
	loss_policy_1: 0.02537
	accuracy_policy_1: 0.71344
	loss_value_1: 0.03533
	loss_reward_1: 0.00564
	loss_policy_2: 0.03006
	accuracy_policy_2: 0.67102
	loss_value_2: 0.03658
	loss_reward_2: 0.00546
	loss_policy_3: 0.0334
	accuracy_policy_3: 0.64098
	loss_value_3: 0.03768
	loss_reward_3: 0.00596
	loss_policy_4: 0.03581
	accuracy_policy_4: 0.615
	loss_value_4: 0.03853
	loss_reward_4: 0.00662
	loss_policy_5: 0.03801
	accuracy_policy_5: 0.59078
	loss_value_5: 0.03959
	loss_reward_5: 0.00736
	loss_policy: 0.24179
	loss_value: 0.35639
	loss_reward: 0.03103
[2024-05-05 14:13:18] nn step 46350, lr: 0.005.
	loss_policy_0: 0.07668
	accuracy_policy_0: 0.80883
	loss_value_0: 0.17518
	loss_policy_1: 0.02557
	accuracy_policy_1: 0.72281
	loss_value_1: 0.03668
	loss_reward_1: 0.00587
	loss_policy_2: 0.03014
	accuracy_policy_2: 0.67758
	loss_value_2: 0.03763
	loss_reward_2: 0.00577
	loss_policy_3: 0.03325
	accuracy_policy_3: 0.64707
	loss_value_3: 0.03874
	loss_reward_3: 0.00643
	loss_policy_4: 0.03626
	accuracy_policy_4: 0.62328
	loss_value_4: 0.03982
	loss_reward_4: 0.00687
	loss_policy_5: 0.03852
	accuracy_policy_5: 0.59926
	loss_value_5: 0.04088
	loss_reward_5: 0.00772
	loss_policy: 0.24043
	loss_value: 0.36892
	loss_reward: 0.03265
[2024-05-05 14:13:34] nn step 46400, lr: 0.005.
	loss_policy_0: 0.06694
	accuracy_policy_0: 0.81508
	loss_value_0: 0.16126
	loss_policy_1: 0.02327
	accuracy_policy_1: 0.72254
	loss_value_1: 0.03363
	loss_reward_1: 0.00539
	loss_policy_2: 0.02744
	accuracy_policy_2: 0.68492
	loss_value_2: 0.03469
	loss_reward_2: 0.00523
	loss_policy_3: 0.03046
	accuracy_policy_3: 0.6491
	loss_value_3: 0.03568
	loss_reward_3: 0.00584
	loss_policy_4: 0.0326
	accuracy_policy_4: 0.62859
	loss_value_4: 0.03663
	loss_reward_4: 0.00628
	loss_policy_5: 0.03524
	accuracy_policy_5: 0.60195
	loss_value_5: 0.03771
	loss_reward_5: 0.0072
	loss_policy: 0.21596
	loss_value: 0.33959
	loss_reward: 0.02994
Optimization_Done 46400
[2024-05-05 14:15:36] [command] train weight_iter_46400.pkl 232 233
[2024-05-05 14:15:54] nn step 46450, lr: 0.005.
	loss_policy_0: 0.1102
	accuracy_policy_0: 0.75195
	loss_value_0: 0.15905
	loss_policy_1: 0.02928
	accuracy_policy_1: 0.69324
	loss_value_1: 0.03316
	loss_reward_1: 0.00506
	loss_policy_2: 0.03239
	accuracy_policy_2: 0.65855
	loss_value_2: 0.03468
	loss_reward_2: 0.00519
	loss_policy_3: 0.03499
	accuracy_policy_3: 0.63301
	loss_value_3: 0.03596
	loss_reward_3: 0.00568
	loss_policy_4: 0.03772
	accuracy_policy_4: 0.61004
	loss_value_4: 0.03714
	loss_reward_4: 0.00606
	loss_policy_5: 0.04044
	accuracy_policy_5: 0.57668
	loss_value_5: 0.0384
	loss_reward_5: 0.00687
	loss_policy: 0.28503
	loss_value: 0.33839
	loss_reward: 0.02885
[2024-05-05 14:16:09] nn step 46500, lr: 0.005.
	loss_policy_0: 0.0877
	accuracy_policy_0: 0.79898
	loss_value_0: 0.16419
	loss_policy_1: 0.02691
	accuracy_policy_1: 0.72555
	loss_value_1: 0.03445
	loss_reward_1: 0.00533
	loss_policy_2: 0.0309
	accuracy_policy_2: 0.69539
	loss_value_2: 0.03591
	loss_reward_2: 0.0054
	loss_policy_3: 0.03403
	accuracy_policy_3: 0.66605
	loss_value_3: 0.03717
	loss_reward_3: 0.00582
	loss_policy_4: 0.03712
	accuracy_policy_4: 0.63391
	loss_value_4: 0.03838
	loss_reward_4: 0.00638
	loss_policy_5: 0.04011
	accuracy_policy_5: 0.60883
	loss_value_5: 0.03937
	loss_reward_5: 0.00715
	loss_policy: 0.25677
	loss_value: 0.34947
	loss_reward: 0.03009
[2024-05-05 14:16:25] nn step 46550, lr: 0.005.
	loss_policy_0: 0.08263
	accuracy_policy_0: 0.81027
	loss_value_0: 0.16454
	loss_policy_1: 0.02662
	accuracy_policy_1: 0.72867
	loss_value_1: 0.03474
	loss_reward_1: 0.00548
	loss_policy_2: 0.03079
	accuracy_policy_2: 0.69148
	loss_value_2: 0.0362
	loss_reward_2: 0.00538
	loss_policy_3: 0.03349
	accuracy_policy_3: 0.66406
	loss_value_3: 0.03733
	loss_reward_3: 0.00583
	loss_policy_4: 0.03667
	accuracy_policy_4: 0.64133
	loss_value_4: 0.03845
	loss_reward_4: 0.00624
	loss_policy_5: 0.03978
	accuracy_policy_5: 0.60867
	loss_value_5: 0.03958
	loss_reward_5: 0.00746
	loss_policy: 0.24997
	loss_value: 0.35085
	loss_reward: 0.03039
[2024-05-05 14:16:41] nn step 46600, lr: 0.005.
	loss_policy_0: 0.07774
	accuracy_policy_0: 0.82082
	loss_value_0: 0.16686
	loss_policy_1: 0.02569
	accuracy_policy_1: 0.73684
	loss_value_1: 0.03536
	loss_reward_1: 0.00565
	loss_policy_2: 0.03001
	accuracy_policy_2: 0.69852
	loss_value_2: 0.03665
	loss_reward_2: 0.0054
	loss_policy_3: 0.03323
	accuracy_policy_3: 0.66859
	loss_value_3: 0.03798
	loss_reward_3: 0.00579
	loss_policy_4: 0.03606
	accuracy_policy_4: 0.64535
	loss_value_4: 0.03918
	loss_reward_4: 0.00619
	loss_policy_5: 0.03916
	accuracy_policy_5: 0.61348
	loss_value_5: 0.04039
	loss_reward_5: 0.00744
	loss_policy: 0.24188
	loss_value: 0.35643
	loss_reward: 0.03046
Optimization_Done 46600
[2024-05-05 14:18:44] [command] train weight_iter_46600.pkl 233 234
[2024-05-05 14:19:01] nn step 46650, lr: 0.005.
	loss_policy_0: 0.12261
	accuracy_policy_0: 0.71852
	loss_value_0: 0.17921
	loss_policy_1: 0.03264
	accuracy_policy_1: 0.65367
	loss_value_1: 0.03761
	loss_reward_1: 0.00624
	loss_policy_2: 0.03614
	accuracy_policy_2: 0.62227
	loss_value_2: 0.03924
	loss_reward_2: 0.00612
	loss_policy_3: 0.03954
	accuracy_policy_3: 0.59734
	loss_value_3: 0.04069
	loss_reward_3: 0.0065
	loss_policy_4: 0.04213
	accuracy_policy_4: 0.57359
	loss_value_4: 0.04213
	loss_reward_4: 0.00716
	loss_policy_5: 0.04538
	accuracy_policy_5: 0.54887
	loss_value_5: 0.0435
	loss_reward_5: 0.0078
	loss_policy: 0.31844
	loss_value: 0.38238
	loss_reward: 0.03382
[2024-05-05 14:19:17] nn step 46700, lr: 0.005.
	loss_policy_0: 0.11074
	accuracy_policy_0: 0.75418
	loss_value_0: 0.19085
	loss_policy_1: 0.03226
	accuracy_policy_1: 0.68324
	loss_value_1: 0.04018
	loss_reward_1: 0.00673
	loss_policy_2: 0.03611
	accuracy_policy_2: 0.65957
	loss_value_2: 0.04198
	loss_reward_2: 0.00664
	loss_policy_3: 0.03992
	accuracy_policy_3: 0.62652
	loss_value_3: 0.0436
	loss_reward_3: 0.0071
	loss_policy_4: 0.04296
	accuracy_policy_4: 0.61
	loss_value_4: 0.04499
	loss_reward_4: 0.00762
	loss_policy_5: 0.04641
	accuracy_policy_5: 0.58359
	loss_value_5: 0.04648
	loss_reward_5: 0.00845
	loss_policy: 0.30839
	loss_value: 0.40808
	loss_reward: 0.03654
[2024-05-05 14:19:32] nn step 46750, lr: 0.005.
	loss_policy_0: 0.10625
	accuracy_policy_0: 0.76598
	loss_value_0: 0.19255
	loss_policy_1: 0.03154
	accuracy_policy_1: 0.68879
	loss_value_1: 0.04065
	loss_reward_1: 0.00682
	loss_policy_2: 0.03632
	accuracy_policy_2: 0.65695
	loss_value_2: 0.04229
	loss_reward_2: 0.00678
	loss_policy_3: 0.0398
	accuracy_policy_3: 0.63375
	loss_value_3: 0.04412
	loss_reward_3: 0.00703
	loss_policy_4: 0.04279
	accuracy_policy_4: 0.61348
	loss_value_4: 0.04554
	loss_reward_4: 0.00779
	loss_policy_5: 0.04568
	accuracy_policy_5: 0.59527
	loss_value_5: 0.0471
	loss_reward_5: 0.00863
	loss_policy: 0.30238
	loss_value: 0.41225
	loss_reward: 0.03705
[2024-05-05 14:19:48] nn step 46800, lr: 0.005.
	loss_policy_0: 0.09696
	accuracy_policy_0: 0.77305
	loss_value_0: 0.18586
	loss_policy_1: 0.02977
	accuracy_policy_1: 0.69457
	loss_value_1: 0.03936
	loss_reward_1: 0.00664
	loss_policy_2: 0.03419
	accuracy_policy_2: 0.66555
	loss_value_2: 0.04106
	loss_reward_2: 0.00639
	loss_policy_3: 0.03768
	accuracy_policy_3: 0.63656
	loss_value_3: 0.04268
	loss_reward_3: 0.00679
	loss_policy_4: 0.04043
	accuracy_policy_4: 0.61312
	loss_value_4: 0.04403
	loss_reward_4: 0.0076
	loss_policy_5: 0.04355
	accuracy_policy_5: 0.59543
	loss_value_5: 0.04534
	loss_reward_5: 0.00861
	loss_policy: 0.28257
	loss_value: 0.39833
	loss_reward: 0.03602
Optimization_Done 46800
[2024-05-05 14:21:48] [command] train weight_iter_46800.pkl 234 235
[2024-05-05 14:22:05] nn step 46850, lr: 0.005.
	loss_policy_0: 0.11568
	accuracy_policy_0: 0.69672
	loss_value_0: 0.17587
	loss_policy_1: 0.03251
	accuracy_policy_1: 0.60512
	loss_value_1: 0.03672
	loss_reward_1: 0.00707
	loss_policy_2: 0.03666
	accuracy_policy_2: 0.5677
	loss_value_2: 0.03843
	loss_reward_2: 0.00659
	loss_policy_3: 0.0402
	accuracy_policy_3: 0.53406
	loss_value_3: 0.03987
	loss_reward_3: 0.00736
	loss_policy_4: 0.04391
	accuracy_policy_4: 0.50254
	loss_value_4: 0.04121
	loss_reward_4: 0.00797
	loss_policy_5: 0.04729
	accuracy_policy_5: 0.47473
	loss_value_5: 0.04241
	loss_reward_5: 0.00892
	loss_policy: 0.31626
	loss_value: 0.37452
	loss_reward: 0.0379
[2024-05-05 14:22:21] nn step 46900, lr: 0.005.
	loss_policy_0: 0.09543
	accuracy_policy_0: 0.75148
	loss_value_0: 0.1837
	loss_policy_1: 0.03016
	accuracy_policy_1: 0.6568
	loss_value_1: 0.03862
	loss_reward_1: 0.00737
	loss_policy_2: 0.0351
	accuracy_policy_2: 0.61848
	loss_value_2: 0.04021
	loss_reward_2: 0.00684
	loss_policy_3: 0.03884
	accuracy_policy_3: 0.58164
	loss_value_3: 0.04175
	loss_reward_3: 0.00787
	loss_policy_4: 0.04219
	accuracy_policy_4: 0.56074
	loss_value_4: 0.04321
	loss_reward_4: 0.00831
	loss_policy_5: 0.0454
	accuracy_policy_5: 0.53785
	loss_value_5: 0.04468
	loss_reward_5: 0.00919
	loss_policy: 0.28713
	loss_value: 0.39216
	loss_reward: 0.03959
[2024-05-05 14:22:37] nn step 46950, lr: 0.005.
	loss_policy_0: 0.08782
	accuracy_policy_0: 0.7641
	loss_value_0: 0.17785
	loss_policy_1: 0.02843
	accuracy_policy_1: 0.66547
	loss_value_1: 0.03754
	loss_reward_1: 0.00724
	loss_policy_2: 0.03345
	accuracy_policy_2: 0.62687
	loss_value_2: 0.03898
	loss_reward_2: 0.00698
	loss_policy_3: 0.03688
	accuracy_policy_3: 0.59504
	loss_value_3: 0.04047
	loss_reward_3: 0.00728
	loss_policy_4: 0.04018
	accuracy_policy_4: 0.57652
	loss_value_4: 0.04186
	loss_reward_4: 0.00794
	loss_policy_5: 0.04351
	accuracy_policy_5: 0.54449
	loss_value_5: 0.04319
	loss_reward_5: 0.00899
	loss_policy: 0.27028
	loss_value: 0.37989
	loss_reward: 0.03842
[2024-05-05 14:22:53] nn step 47000, lr: 0.005.
	loss_policy_0: 0.08689
	accuracy_policy_0: 0.77059
	loss_value_0: 0.18343
	loss_policy_1: 0.02913
	accuracy_policy_1: 0.66695
	loss_value_1: 0.03887
	loss_reward_1: 0.00755
	loss_policy_2: 0.03362
	accuracy_policy_2: 0.63086
	loss_value_2: 0.04022
	loss_reward_2: 0.00721
	loss_policy_3: 0.03793
	accuracy_policy_3: 0.59559
	loss_value_3: 0.04163
	loss_reward_3: 0.00754
	loss_policy_4: 0.04104
	accuracy_policy_4: 0.56926
	loss_value_4: 0.04307
	loss_reward_4: 0.00803
	loss_policy_5: 0.04436
	accuracy_policy_5: 0.54742
	loss_value_5: 0.0445
	loss_reward_5: 0.00945
	loss_policy: 0.27296
	loss_value: 0.39171
	loss_reward: 0.03979
Optimization_Done 47000
[2024-05-05 14:24:47] [command] train weight_iter_47000.pkl 235 236
[2024-05-05 14:25:04] nn step 47050, lr: 0.005.
	loss_policy_0: 0.1095
	accuracy_policy_0: 0.70273
	loss_value_0: 0.16837
	loss_policy_1: 0.03135
	accuracy_policy_1: 0.61703
	loss_value_1: 0.03532
	loss_reward_1: 0.00647
	loss_policy_2: 0.03617
	accuracy_policy_2: 0.56789
	loss_value_2: 0.03661
	loss_reward_2: 0.00598
	loss_policy_3: 0.03943
	accuracy_policy_3: 0.53879
	loss_value_3: 0.03776
	loss_reward_3: 0.00651
	loss_policy_4: 0.04235
	accuracy_policy_4: 0.51047
	loss_value_4: 0.03889
	loss_reward_4: 0.00685
	loss_policy_5: 0.04427
	accuracy_policy_5: 0.48754
	loss_value_5: 0.04002
	loss_reward_5: 0.00798
	loss_policy: 0.30306
	loss_value: 0.35695
	loss_reward: 0.0338
[2024-05-05 14:25:20] nn step 47100, lr: 0.005.
	loss_policy_0: 0.08572
	accuracy_policy_0: 0.75062
	loss_value_0: 0.15715
	loss_policy_1: 0.02767
	accuracy_policy_1: 0.64266
	loss_value_1: 0.03306
	loss_reward_1: 0.00625
	loss_policy_2: 0.03213
	accuracy_policy_2: 0.59777
	loss_value_2: 0.0343
	loss_reward_2: 0.00573
	loss_policy_3: 0.03492
	accuracy_policy_3: 0.57094
	loss_value_3: 0.03511
	loss_reward_3: 0.00628
	loss_policy_4: 0.03785
	accuracy_policy_4: 0.53926
	loss_value_4: 0.03608
	loss_reward_4: 0.00676
	loss_policy_5: 0.03999
	accuracy_policy_5: 0.52195
	loss_value_5: 0.03707
	loss_reward_5: 0.00793
	loss_policy: 0.25828
	loss_value: 0.33276
	loss_reward: 0.03295
[2024-05-05 14:25:36] nn step 47150, lr: 0.005.
	loss_policy_0: 0.08248
	accuracy_policy_0: 0.75961
	loss_value_0: 0.15756
	loss_policy_1: 0.02744
	accuracy_policy_1: 0.65297
	loss_value_1: 0.03302
	loss_reward_1: 0.00606
	loss_policy_2: 0.03144
	accuracy_policy_2: 0.60984
	loss_value_2: 0.03412
	loss_reward_2: 0.00592
	loss_policy_3: 0.03427
	accuracy_policy_3: 0.5773
	loss_value_3: 0.03519
	loss_reward_3: 0.00639
	loss_policy_4: 0.03726
	accuracy_policy_4: 0.55273
	loss_value_4: 0.03601
	loss_reward_4: 0.00658
	loss_policy_5: 0.03967
	accuracy_policy_5: 0.53559
	loss_value_5: 0.03725
	loss_reward_5: 0.00796
	loss_policy: 0.25257
	loss_value: 0.33315
	loss_reward: 0.03291
[2024-05-05 14:25:52] nn step 47200, lr: 0.005.
	loss_policy_0: 0.08086
	accuracy_policy_0: 0.76793
	loss_value_0: 0.15934
	loss_policy_1: 0.0274
	accuracy_policy_1: 0.65664
	loss_value_1: 0.03338
	loss_reward_1: 0.00612
	loss_policy_2: 0.03127
	accuracy_policy_2: 0.61727
	loss_value_2: 0.03454
	loss_reward_2: 0.00586
	loss_policy_3: 0.03458
	accuracy_policy_3: 0.58492
	loss_value_3: 0.03557
	loss_reward_3: 0.00658
	loss_policy_4: 0.03744
	accuracy_policy_4: 0.55527
	loss_value_4: 0.03647
	loss_reward_4: 0.00698
	loss_policy_5: 0.03942
	accuracy_policy_5: 0.54238
	loss_value_5: 0.03748
	loss_reward_5: 0.00825
	loss_policy: 0.25097
	loss_value: 0.33679
	loss_reward: 0.03378
Optimization_Done 47200
[2024-05-05 14:27:40] [command] train weight_iter_47200.pkl 236 237
[2024-05-05 14:27:57] nn step 47250, lr: 0.005.
	loss_policy_0: 0.10056
	accuracy_policy_0: 0.73109
	loss_value_0: 0.15576
	loss_policy_1: 0.02737
	accuracy_policy_1: 0.66578
	loss_value_1: 0.03261
	loss_reward_1: 0.00486
	loss_policy_2: 0.03036
	accuracy_policy_2: 0.63414
	loss_value_2: 0.0339
	loss_reward_2: 0.00456
	loss_policy_3: 0.03331
	accuracy_policy_3: 0.60172
	loss_value_3: 0.03508
	loss_reward_3: 0.00507
	loss_policy_4: 0.03584
	accuracy_policy_4: 0.57758
	loss_value_4: 0.03628
	loss_reward_4: 0.00536
	loss_policy_5: 0.03816
	accuracy_policy_5: 0.54828
	loss_value_5: 0.03726
	loss_reward_5: 0.00626
	loss_policy: 0.26561
	loss_value: 0.33089
	loss_reward: 0.02612
[2024-05-05 14:28:13] nn step 47300, lr: 0.005.
	loss_policy_0: 0.07618
	accuracy_policy_0: 0.78125
	loss_value_0: 0.15274
	loss_policy_1: 0.02409
	accuracy_policy_1: 0.7002
	loss_value_1: 0.03211
	loss_reward_1: 0.00478
	loss_policy_2: 0.02799
	accuracy_policy_2: 0.66535
	loss_value_2: 0.03338
	loss_reward_2: 0.00475
	loss_policy_3: 0.03077
	accuracy_policy_3: 0.63352
	loss_value_3: 0.03461
	loss_reward_3: 0.00508
	loss_policy_4: 0.03324
	accuracy_policy_4: 0.60922
	loss_value_4: 0.03564
	loss_reward_4: 0.0053
	loss_policy_5: 0.03554
	accuracy_policy_5: 0.58523
	loss_value_5: 0.03666
	loss_reward_5: 0.00615
	loss_policy: 0.22781
	loss_value: 0.32513
	loss_reward: 0.02605
[2024-05-05 14:28:29] nn step 47350, lr: 0.005.
	loss_policy_0: 0.07434
	accuracy_policy_0: 0.79066
	loss_value_0: 0.1587
	loss_policy_1: 0.02451
	accuracy_policy_1: 0.70891
	loss_value_1: 0.03356
	loss_reward_1: 0.00494
	loss_policy_2: 0.02871
	accuracy_policy_2: 0.66785
	loss_value_2: 0.03487
	loss_reward_2: 0.00477
	loss_policy_3: 0.03184
	accuracy_policy_3: 0.63535
	loss_value_3: 0.03594
	loss_reward_3: 0.00505
	loss_policy_4: 0.03425
	accuracy_policy_4: 0.61348
	loss_value_4: 0.03711
	loss_reward_4: 0.00554
	loss_policy_5: 0.03676
	accuracy_policy_5: 0.58641
	loss_value_5: 0.0382
	loss_reward_5: 0.00654
	loss_policy: 0.23041
	loss_value: 0.33838
	loss_reward: 0.02683
[2024-05-05 14:28:45] nn step 47400, lr: 0.005.
	loss_policy_0: 0.07062
	accuracy_policy_0: 0.79523
	loss_value_0: 0.15702
	loss_policy_1: 0.02413
	accuracy_policy_1: 0.70312
	loss_value_1: 0.03298
	loss_reward_1: 0.00491
	loss_policy_2: 0.02813
	accuracy_policy_2: 0.6657
	loss_value_2: 0.03437
	loss_reward_2: 0.00475
	loss_policy_3: 0.03101
	accuracy_policy_3: 0.63445
	loss_value_3: 0.03562
	loss_reward_3: 0.00498
	loss_policy_4: 0.03333
	accuracy_policy_4: 0.61137
	loss_value_4: 0.03661
	loss_reward_4: 0.00536
	loss_policy_5: 0.03557
	accuracy_policy_5: 0.59477
	loss_value_5: 0.03759
	loss_reward_5: 0.00647
	loss_policy: 0.22279
	loss_value: 0.33418
	loss_reward: 0.02645
Optimization_Done 47400
[2024-05-05 14:30:44] [command] train weight_iter_47400.pkl 237 238
[2024-05-05 14:31:01] nn step 47450, lr: 0.005.
	loss_policy_0: 0.11624
	accuracy_policy_0: 0.73645
	loss_value_0: 0.18455
	loss_policy_1: 0.03106
	accuracy_policy_1: 0.66832
	loss_value_1: 0.03858
	loss_reward_1: 0.00583
	loss_policy_2: 0.0345
	accuracy_policy_2: 0.64238
	loss_value_2: 0.04043
	loss_reward_2: 0.00591
	loss_policy_3: 0.03805
	accuracy_policy_3: 0.60613
	loss_value_3: 0.04231
	loss_reward_3: 0.00612
	loss_policy_4: 0.04073
	accuracy_policy_4: 0.58121
	loss_value_4: 0.0438
	loss_reward_4: 0.00682
	loss_policy_5: 0.04367
	accuracy_policy_5: 0.55043
	loss_value_5: 0.04522
	loss_reward_5: 0.00732
	loss_policy: 0.30424
	loss_value: 0.3949
	loss_reward: 0.03201
[2024-05-05 14:31:17] nn step 47500, lr: 0.005.
	loss_policy_0: 0.08235
	accuracy_policy_0: 0.7907
	loss_value_0: 0.16636
	loss_policy_1: 0.02489
	accuracy_policy_1: 0.71992
	loss_value_1: 0.0352
	loss_reward_1: 0.0053
	loss_policy_2: 0.02904
	accuracy_policy_2: 0.68199
	loss_value_2: 0.03696
	loss_reward_2: 0.00537
	loss_policy_3: 0.03235
	accuracy_policy_3: 0.65062
	loss_value_3: 0.03829
	loss_reward_3: 0.00586
	loss_policy_4: 0.03499
	accuracy_policy_4: 0.62766
	loss_value_4: 0.03964
	loss_reward_4: 0.00635
	loss_policy_5: 0.03807
	accuracy_policy_5: 0.59918
	loss_value_5: 0.04097
	loss_reward_5: 0.00703
	loss_policy: 0.24168
	loss_value: 0.35742
	loss_reward: 0.02991
[2024-05-05 14:31:32] nn step 47550, lr: 0.005.
	loss_policy_0: 0.08414
	accuracy_policy_0: 0.79777
	loss_value_0: 0.18199
	loss_policy_1: 0.0262
	accuracy_policy_1: 0.72527
	loss_value_1: 0.03889
	loss_reward_1: 0.00594
	loss_policy_2: 0.03069
	accuracy_policy_2: 0.69281
	loss_value_2: 0.04056
	loss_reward_2: 0.00576
	loss_policy_3: 0.03412
	accuracy_policy_3: 0.6607
	loss_value_3: 0.04201
	loss_reward_3: 0.00621
	loss_policy_4: 0.03724
	accuracy_policy_4: 0.63719
	loss_value_4: 0.04331
	loss_reward_4: 0.00672
	loss_policy_5: 0.04041
	accuracy_policy_5: 0.61656
	loss_value_5: 0.04473
	loss_reward_5: 0.0076
	loss_policy: 0.2528
	loss_value: 0.3915
	loss_reward: 0.03223
[2024-05-05 14:31:48] nn step 47600, lr: 0.005.
	loss_policy_0: 0.07961
	accuracy_policy_0: 0.81551
	loss_value_0: 0.18385
	loss_policy_1: 0.02623
	accuracy_policy_1: 0.72898
	loss_value_1: 0.03895
	loss_reward_1: 0.006
	loss_policy_2: 0.03039
	accuracy_policy_2: 0.69523
	loss_value_2: 0.04056
	loss_reward_2: 0.00583
	loss_policy_3: 0.03382
	accuracy_policy_3: 0.67117
	loss_value_3: 0.04213
	loss_reward_3: 0.00631
	loss_policy_4: 0.03711
	accuracy_policy_4: 0.64676
	loss_value_4: 0.04349
	loss_reward_4: 0.00687
	loss_policy_5: 0.04011
	accuracy_policy_5: 0.62297
	loss_value_5: 0.04498
	loss_reward_5: 0.00781
	loss_policy: 0.24726
	loss_value: 0.39395
	loss_reward: 0.03282
Optimization_Done 47600
[2024-05-05 14:33:50] [command] train weight_iter_47600.pkl 238 239
[2024-05-05 14:34:08] nn step 47650, lr: 0.005.
	loss_policy_0: 0.10617
	accuracy_policy_0: 0.72301
	loss_value_0: 0.1765
	loss_policy_1: 0.0308
	accuracy_policy_1: 0.63914
	loss_value_1: 0.03703
	loss_reward_1: 0.00698
	loss_policy_2: 0.03517
	accuracy_policy_2: 0.60398
	loss_value_2: 0.03852
	loss_reward_2: 0.00675
	loss_policy_3: 0.03914
	accuracy_policy_3: 0.57098
	loss_value_3: 0.04008
	loss_reward_3: 0.00746
	loss_policy_4: 0.04242
	accuracy_policy_4: 0.54293
	loss_value_4: 0.04158
	loss_reward_4: 0.00803
	loss_policy_5: 0.0459
	accuracy_policy_5: 0.5132
	loss_value_5: 0.04288
	loss_reward_5: 0.00896
	loss_policy: 0.2996
	loss_value: 0.37659
	loss_reward: 0.03819
[2024-05-05 14:34:24] nn step 47700, lr: 0.005.
	loss_policy_0: 0.08767
	accuracy_policy_0: 0.77887
	loss_value_0: 0.18017
	loss_policy_1: 0.02837
	accuracy_policy_1: 0.68875
	loss_value_1: 0.03847
	loss_reward_1: 0.00721
	loss_policy_2: 0.03315
	accuracy_policy_2: 0.65141
	loss_value_2: 0.04046
	loss_reward_2: 0.00706
	loss_policy_3: 0.03687
	accuracy_policy_3: 0.62016
	loss_value_3: 0.04184
	loss_reward_3: 0.00751
	loss_policy_4: 0.04107
	accuracy_policy_4: 0.58973
	loss_value_4: 0.0433
	loss_reward_4: 0.00809
	loss_policy_5: 0.04443
	accuracy_policy_5: 0.5623
	loss_value_5: 0.04485
	loss_reward_5: 0.0093
	loss_policy: 0.27156
	loss_value: 0.38909
	loss_reward: 0.03917
[2024-05-05 14:34:40] nn step 47750, lr: 0.005.
	loss_policy_0: 0.08379
	accuracy_policy_0: 0.79355
	loss_value_0: 0.1846
	loss_policy_1: 0.02871
	accuracy_policy_1: 0.695
	loss_value_1: 0.03903
	loss_reward_1: 0.00742
	loss_policy_2: 0.03321
	accuracy_policy_2: 0.65871
	loss_value_2: 0.04067
	loss_reward_2: 0.00746
	loss_policy_3: 0.03705
	accuracy_policy_3: 0.6291
	loss_value_3: 0.04226
	loss_reward_3: 0.00776
	loss_policy_4: 0.04093
	accuracy_policy_4: 0.60324
	loss_value_4: 0.04378
	loss_reward_4: 0.0085
	loss_policy_5: 0.0443
	accuracy_policy_5: 0.58211
	loss_value_5: 0.04555
	loss_reward_5: 0.00957
	loss_policy: 0.26798
	loss_value: 0.39589
	loss_reward: 0.04071
[2024-05-05 14:34:56] nn step 47800, lr: 0.005.
	loss_policy_0: 0.07589
	accuracy_policy_0: 0.80098
	loss_value_0: 0.17429
	loss_policy_1: 0.02678
	accuracy_policy_1: 0.7057
	loss_value_1: 0.03685
	loss_reward_1: 0.00709
	loss_policy_2: 0.0311
	accuracy_policy_2: 0.66082
	loss_value_2: 0.03855
	loss_reward_2: 0.00691
	loss_policy_3: 0.03515
	accuracy_policy_3: 0.63418
	loss_value_3: 0.04009
	loss_reward_3: 0.0074
	loss_policy_4: 0.0382
	accuracy_policy_4: 0.60812
	loss_value_4: 0.04156
	loss_reward_4: 0.00799
	loss_policy_5: 0.04148
	accuracy_policy_5: 0.58363
	loss_value_5: 0.04314
	loss_reward_5: 0.0091
	loss_policy: 0.24861
	loss_value: 0.37448
	loss_reward: 0.03849
Optimization_Done 47800
[2024-05-05 14:36:58] [command] train weight_iter_47800.pkl 239 240
[2024-05-05 14:37:15] nn step 47850, lr: 0.005.
	loss_policy_0: 0.10133
	accuracy_policy_0: 0.74324
	loss_value_0: 0.15893
	loss_policy_1: 0.03027
	accuracy_policy_1: 0.65281
	loss_value_1: 0.03341
	loss_reward_1: 0.00632
	loss_policy_2: 0.0356
	accuracy_policy_2: 0.60477
	loss_value_2: 0.03507
	loss_reward_2: 0.00587
	loss_policy_3: 0.03933
	accuracy_policy_3: 0.56895
	loss_value_3: 0.0365
	loss_reward_3: 0.00651
	loss_policy_4: 0.04254
	accuracy_policy_4: 0.54023
	loss_value_4: 0.03785
	loss_reward_4: 0.00721
	loss_policy_5: 0.04535
	accuracy_policy_5: 0.51426
	loss_value_5: 0.03926
	loss_reward_5: 0.00809
	loss_policy: 0.29441
	loss_value: 0.34101
	loss_reward: 0.03401
[2024-05-05 14:37:31] nn step 47900, lr: 0.005.
	loss_policy_0: 0.08865
	accuracy_policy_0: 0.77836
	loss_value_0: 0.16489
	loss_policy_1: 0.03048
	accuracy_policy_1: 0.67348
	loss_value_1: 0.03488
	loss_reward_1: 0.00732
	loss_policy_2: 0.03531
	accuracy_policy_2: 0.63418
	loss_value_2: 0.03632
	loss_reward_2: 0.00651
	loss_policy_3: 0.03992
	accuracy_policy_3: 0.59867
	loss_value_3: 0.03784
	loss_reward_3: 0.00708
	loss_policy_4: 0.04339
	accuracy_policy_4: 0.56902
	loss_value_4: 0.03923
	loss_reward_4: 0.00782
	loss_policy_5: 0.04719
	accuracy_policy_5: 0.53832
	loss_value_5: 0.04071
	loss_reward_5: 0.00899
	loss_policy: 0.28493
	loss_value: 0.35386
	loss_reward: 0.03772
[2024-05-05 14:37:47] nn step 47950, lr: 0.005.
	loss_policy_0: 0.08222
	accuracy_policy_0: 0.78195
	loss_value_0: 0.16189
	loss_policy_1: 0.02852
	accuracy_policy_1: 0.67766
	loss_value_1: 0.03391
	loss_reward_1: 0.00696
	loss_policy_2: 0.03379
	accuracy_policy_2: 0.63262
	loss_value_2: 0.03579
	loss_reward_2: 0.00631
	loss_policy_3: 0.03771
	accuracy_policy_3: 0.60461
	loss_value_3: 0.0371
	loss_reward_3: 0.00705
	loss_policy_4: 0.04151
	accuracy_policy_4: 0.57156
	loss_value_4: 0.03858
	loss_reward_4: 0.00756
	loss_policy_5: 0.04533
	accuracy_policy_5: 0.54027
	loss_value_5: 0.04011
	loss_reward_5: 0.00886
	loss_policy: 0.26909
	loss_value: 0.34739
	loss_reward: 0.03673
[2024-05-05 14:38:03] nn step 48000, lr: 0.005.
	loss_policy_0: 0.07534
	accuracy_policy_0: 0.79578
	loss_value_0: 0.15733
	loss_policy_1: 0.02769
	accuracy_policy_1: 0.6823
	loss_value_1: 0.03321
	loss_reward_1: 0.00678
	loss_policy_2: 0.03288
	accuracy_policy_2: 0.63957
	loss_value_2: 0.03481
	loss_reward_2: 0.00639
	loss_policy_3: 0.0368
	accuracy_policy_3: 0.60496
	loss_value_3: 0.03615
	loss_reward_3: 0.00701
	loss_policy_4: 0.0402
	accuracy_policy_4: 0.58043
	loss_value_4: 0.0375
	loss_reward_4: 0.00752
	loss_policy_5: 0.04365
	accuracy_policy_5: 0.5523
	loss_value_5: 0.03891
	loss_reward_5: 0.00873
	loss_policy: 0.25657
	loss_value: 0.33791
	loss_reward: 0.03643
Optimization_Done 48000
[2024-05-05 14:39:56] [command] train weight_iter_48000.pkl 240 241
[2024-05-05 14:40:13] nn step 48050, lr: 0.005.
	loss_policy_0: 0.10959
	accuracy_policy_0: 0.72879
	loss_value_0: 0.16273
	loss_policy_1: 0.03002
	accuracy_policy_1: 0.65098
	loss_value_1: 0.03417
	loss_reward_1: 0.00508
	loss_policy_2: 0.0345
	accuracy_policy_2: 0.61707
	loss_value_2: 0.03565
	loss_reward_2: 0.00485
	loss_policy_3: 0.03771
	accuracy_policy_3: 0.58926
	loss_value_3: 0.03695
	loss_reward_3: 0.00531
	loss_policy_4: 0.04075
	accuracy_policy_4: 0.56063
	loss_value_4: 0.03809
	loss_reward_4: 0.00552
	loss_policy_5: 0.04358
	accuracy_policy_5: 0.53691
	loss_value_5: 0.0392
	loss_reward_5: 0.00657
	loss_policy: 0.29615
	loss_value: 0.34679
	loss_reward: 0.02732
[2024-05-05 14:40:29] nn step 48100, lr: 0.005.
	loss_policy_0: 0.09292
	accuracy_policy_0: 0.77516
	loss_value_0: 0.17063
	loss_policy_1: 0.02916
	accuracy_policy_1: 0.68988
	loss_value_1: 0.0358
	loss_reward_1: 0.00566
	loss_policy_2: 0.03415
	accuracy_policy_2: 0.65004
	loss_value_2: 0.03724
	loss_reward_2: 0.00523
	loss_policy_3: 0.03774
	accuracy_policy_3: 0.6234
	loss_value_3: 0.03865
	loss_reward_3: 0.00548
	loss_policy_4: 0.04063
	accuracy_policy_4: 0.59492
	loss_value_4: 0.03978
	loss_reward_4: 0.00606
	loss_policy_5: 0.04353
	accuracy_policy_5: 0.5725
	loss_value_5: 0.04077
	loss_reward_5: 0.00717
	loss_policy: 0.27813
	loss_value: 0.36286
	loss_reward: 0.02959
[2024-05-05 14:40:45] nn step 48150, lr: 0.005.
	loss_policy_0: 0.08211
	accuracy_policy_0: 0.7891
	loss_value_0: 0.16354
	loss_policy_1: 0.02651
	accuracy_policy_1: 0.70137
	loss_value_1: 0.03426
	loss_reward_1: 0.0052
	loss_policy_2: 0.03135
	accuracy_policy_2: 0.65785
	loss_value_2: 0.03572
	loss_reward_2: 0.00512
	loss_policy_3: 0.03487
	accuracy_policy_3: 0.6259
	loss_value_3: 0.03678
	loss_reward_3: 0.00534
	loss_policy_4: 0.03745
	accuracy_policy_4: 0.60656
	loss_value_4: 0.0378
	loss_reward_4: 0.00578
	loss_policy_5: 0.03982
	accuracy_policy_5: 0.58109
	loss_value_5: 0.03892
	loss_reward_5: 0.00668
	loss_policy: 0.2521
	loss_value: 0.34702
	loss_reward: 0.02813
[2024-05-05 14:41:01] nn step 48200, lr: 0.005.
	loss_policy_0: 0.07629
	accuracy_policy_0: 0.79879
	loss_value_0: 0.16057
	loss_policy_1: 0.02581
	accuracy_policy_1: 0.70293
	loss_value_1: 0.03361
	loss_reward_1: 0.00514
	loss_policy_2: 0.03009
	accuracy_policy_2: 0.66457
	loss_value_2: 0.03503
	loss_reward_2: 0.00486
	loss_policy_3: 0.03361
	accuracy_policy_3: 0.63203
	loss_value_3: 0.03607
	loss_reward_3: 0.00504
	loss_policy_4: 0.03642
	accuracy_policy_4: 0.60848
	loss_value_4: 0.0374
	loss_reward_4: 0.00542
	loss_policy_5: 0.03907
	accuracy_policy_5: 0.58656
	loss_value_5: 0.03861
	loss_reward_5: 0.00657
	loss_policy: 0.24128
	loss_value: 0.34128
	loss_reward: 0.02703
Optimization_Done 48200
[2024-05-05 14:43:03] [command] train weight_iter_48200.pkl 241 242
[2024-05-05 14:43:20] nn step 48250, lr: 0.005.
	loss_policy_0: 0.10995
	accuracy_policy_0: 0.72789
	loss_value_0: 0.16311
	loss_policy_1: 0.02863
	accuracy_policy_1: 0.66609
	loss_value_1: 0.03395
	loss_reward_1: 0.00509
	loss_policy_2: 0.03212
	accuracy_policy_2: 0.63617
	loss_value_2: 0.03518
	loss_reward_2: 0.00498
	loss_policy_3: 0.03495
	accuracy_policy_3: 0.61258
	loss_value_3: 0.03627
	loss_reward_3: 0.00552
	loss_policy_4: 0.03759
	accuracy_policy_4: 0.59605
	loss_value_4: 0.03744
	loss_reward_4: 0.00583
	loss_policy_5: 0.04065
	accuracy_policy_5: 0.56574
	loss_value_5: 0.03836
	loss_reward_5: 0.00644
	loss_policy: 0.2839
	loss_value: 0.34431
	loss_reward: 0.02786
[2024-05-05 14:43:36] nn step 48300, lr: 0.005.
	loss_policy_0: 0.09453
	accuracy_policy_0: 0.78109
	loss_value_0: 0.17559
	loss_policy_1: 0.02788
	accuracy_policy_1: 0.70734
	loss_value_1: 0.03687
	loss_reward_1: 0.00584
	loss_policy_2: 0.0324
	accuracy_policy_2: 0.67625
	loss_value_2: 0.0383
	loss_reward_2: 0.00567
	loss_policy_3: 0.0357
	accuracy_policy_3: 0.64887
	loss_value_3: 0.03948
	loss_reward_3: 0.00598
	loss_policy_4: 0.03827
	accuracy_policy_4: 0.63258
	loss_value_4: 0.04052
	loss_reward_4: 0.00632
	loss_policy_5: 0.04127
	accuracy_policy_5: 0.61023
	loss_value_5: 0.04142
	loss_reward_5: 0.00711
	loss_policy: 0.27006
	loss_value: 0.37218
	loss_reward: 0.03092
[2024-05-05 14:43:52] nn step 48350, lr: 0.005.
	loss_policy_0: 0.08389
	accuracy_policy_0: 0.7927
	loss_value_0: 0.16792
	loss_policy_1: 0.02568
	accuracy_policy_1: 0.71711
	loss_value_1: 0.03536
	loss_reward_1: 0.00547
	loss_policy_2: 0.03001
	accuracy_policy_2: 0.68266
	loss_value_2: 0.03658
	loss_reward_2: 0.00536
	loss_policy_3: 0.03287
	accuracy_policy_3: 0.6584
	loss_value_3: 0.03774
	loss_reward_3: 0.00589
	loss_policy_4: 0.03579
	accuracy_policy_4: 0.6384
	loss_value_4: 0.03867
	loss_reward_4: 0.00643
	loss_policy_5: 0.03817
	accuracy_policy_5: 0.61973
	loss_value_5: 0.03954
	loss_reward_5: 0.00695
	loss_policy: 0.24641
	loss_value: 0.35581
	loss_reward: 0.0301
[2024-05-05 14:44:08] nn step 48400, lr: 0.005.
	loss_policy_0: 0.07717
	accuracy_policy_0: 0.80047
	loss_value_0: 0.16376
	loss_policy_1: 0.02457
	accuracy_policy_1: 0.72332
	loss_value_1: 0.03456
	loss_reward_1: 0.0054
	loss_policy_2: 0.02838
	accuracy_policy_2: 0.69117
	loss_value_2: 0.03562
	loss_reward_2: 0.00528
	loss_policy_3: 0.03213
	accuracy_policy_3: 0.66277
	loss_value_3: 0.03671
	loss_reward_3: 0.00559
	loss_policy_4: 0.03443
	accuracy_policy_4: 0.64555
	loss_value_4: 0.03783
	loss_reward_4: 0.00608
	loss_policy_5: 0.03687
	accuracy_policy_5: 0.62254
	loss_value_5: 0.039
	loss_reward_5: 0.00682
	loss_policy: 0.23355
	loss_value: 0.34748
	loss_reward: 0.02917
Optimization_Done 48400
[2024-05-05 14:46:03] [command] train weight_iter_48400.pkl 242 243
[2024-05-05 14:46:20] nn step 48450, lr: 0.005.
	loss_policy_0: 0.11353
	accuracy_policy_0: 0.73836
	loss_value_0: 0.18183
	loss_policy_1: 0.03149
	accuracy_policy_1: 0.66562
	loss_value_1: 0.03814
	loss_reward_1: 0.00637
	loss_policy_2: 0.03628
	accuracy_policy_2: 0.6225
	loss_value_2: 0.03987
	loss_reward_2: 0.00633
	loss_policy_3: 0.0396
	accuracy_policy_3: 0.59684
	loss_value_3: 0.04156
	loss_reward_3: 0.00702
	loss_policy_4: 0.04317
	accuracy_policy_4: 0.56828
	loss_value_4: 0.04283
	loss_reward_4: 0.00739
	loss_policy_5: 0.04676
	accuracy_policy_5: 0.54059
	loss_value_5: 0.04408
	loss_reward_5: 0.00819
	loss_policy: 0.31084
	loss_value: 0.38831
	loss_reward: 0.03531
[2024-05-05 14:46:36] nn step 48500, lr: 0.005.
	loss_policy_0: 0.09184
	accuracy_policy_0: 0.78043
	loss_value_0: 0.17298
	loss_policy_1: 0.02813
	accuracy_policy_1: 0.70035
	loss_value_1: 0.03655
	loss_reward_1: 0.00635
	loss_policy_2: 0.03312
	accuracy_policy_2: 0.66406
	loss_value_2: 0.03824
	loss_reward_2: 0.00624
	loss_policy_3: 0.03639
	accuracy_policy_3: 0.6407
	loss_value_3: 0.03961
	loss_reward_3: 0.00679
	loss_policy_4: 0.04014
	accuracy_policy_4: 0.61262
	loss_value_4: 0.04095
	loss_reward_4: 0.00719
	loss_policy_5: 0.04329
	accuracy_policy_5: 0.5852
	loss_value_5: 0.04236
	loss_reward_5: 0.00825
	loss_policy: 0.27293
	loss_value: 0.37067
	loss_reward: 0.03482
[2024-05-05 14:46:52] nn step 48550, lr: 0.005.
	loss_policy_0: 0.09057
	accuracy_policy_0: 0.79637
	loss_value_0: 0.18228
	loss_policy_1: 0.02852
	accuracy_policy_1: 0.70793
	loss_value_1: 0.03858
	loss_reward_1: 0.00688
	loss_policy_2: 0.03308
	accuracy_policy_2: 0.67758
	loss_value_2: 0.04041
	loss_reward_2: 0.00642
	loss_policy_3: 0.03666
	accuracy_policy_3: 0.65238
	loss_value_3: 0.04176
	loss_reward_3: 0.00698
	loss_policy_4: 0.0403
	accuracy_policy_4: 0.62871
	loss_value_4: 0.04281
	loss_reward_4: 0.00748
	loss_policy_5: 0.04388
	accuracy_policy_5: 0.59832
	loss_value_5: 0.04391
	loss_reward_5: 0.00864
	loss_policy: 0.27301
	loss_value: 0.38975
	loss_reward: 0.0364
[2024-05-05 14:47:08] nn step 48600, lr: 0.005.
	loss_policy_0: 0.08624
	accuracy_policy_0: 0.8066
	loss_value_0: 0.18887
	loss_policy_1: 0.02827
	accuracy_policy_1: 0.71699
	loss_value_1: 0.03963
	loss_reward_1: 0.00685
	loss_policy_2: 0.0332
	accuracy_policy_2: 0.68188
	loss_value_2: 0.04134
	loss_reward_2: 0.00672
	loss_policy_3: 0.03738
	accuracy_policy_3: 0.65527
	loss_value_3: 0.0428
	loss_reward_3: 0.00735
	loss_policy_4: 0.04038
	accuracy_policy_4: 0.63523
	loss_value_4: 0.04405
	loss_reward_4: 0.00762
	loss_policy_5: 0.04435
	accuracy_policy_5: 0.6066
	loss_value_5: 0.04536
	loss_reward_5: 0.00885
	loss_policy: 0.26982
	loss_value: 0.40205
	loss_reward: 0.03739
Optimization_Done 48600
[2024-05-05 14:49:01] [command] train weight_iter_48600.pkl 243 244
[2024-05-05 14:49:18] nn step 48650, lr: 0.005.
	loss_policy_0: 0.10711
	accuracy_policy_0: 0.72191
	loss_value_0: 0.16643
	loss_policy_1: 0.03033
	accuracy_policy_1: 0.64074
	loss_value_1: 0.03483
	loss_reward_1: 0.00621
	loss_policy_2: 0.03512
	accuracy_policy_2: 0.58969
	loss_value_2: 0.03613
	loss_reward_2: 0.00618
	loss_policy_3: 0.03861
	accuracy_policy_3: 0.55863
	loss_value_3: 0.03764
	loss_reward_3: 0.00645
	loss_policy_4: 0.04223
	accuracy_policy_4: 0.5275
	loss_value_4: 0.03896
	loss_reward_4: 0.00708
	loss_policy_5: 0.04528
	accuracy_policy_5: 0.49863
	loss_value_5: 0.04009
	loss_reward_5: 0.00819
	loss_policy: 0.29869
	loss_value: 0.35408
	loss_reward: 0.03412
[2024-05-05 14:49:33] nn step 48700, lr: 0.005.
	loss_policy_0: 0.09333
	accuracy_policy_0: 0.76516
	loss_value_0: 0.17086
	loss_policy_1: 0.02949
	accuracy_policy_1: 0.67375
	loss_value_1: 0.03614
	loss_reward_1: 0.00685
	loss_policy_2: 0.03484
	accuracy_policy_2: 0.6266
	loss_value_2: 0.03787
	loss_reward_2: 0.00637
	loss_policy_3: 0.03867
	accuracy_policy_3: 0.59359
	loss_value_3: 0.03946
	loss_reward_3: 0.00706
	loss_policy_4: 0.04232
	accuracy_policy_4: 0.5634
	loss_value_4: 0.0407
	loss_reward_4: 0.00768
	loss_policy_5: 0.04575
	accuracy_policy_5: 0.53121
	loss_value_5: 0.042
	loss_reward_5: 0.00879
	loss_policy: 0.28439
	loss_value: 0.36705
	loss_reward: 0.03674
[2024-05-05 14:49:49] nn step 48750, lr: 0.005.
	loss_policy_0: 0.08724
	accuracy_policy_0: 0.77848
	loss_value_0: 0.17329
	loss_policy_1: 0.02869
	accuracy_policy_1: 0.68164
	loss_value_1: 0.03631
	loss_reward_1: 0.00685
	loss_policy_2: 0.03396
	accuracy_policy_2: 0.63121
	loss_value_2: 0.0382
	loss_reward_2: 0.00633
	loss_policy_3: 0.03734
	accuracy_policy_3: 0.60609
	loss_value_3: 0.03949
	loss_reward_3: 0.0069
	loss_policy_4: 0.04115
	accuracy_policy_4: 0.57719
	loss_value_4: 0.04076
	loss_reward_4: 0.00738
	loss_policy_5: 0.04523
	accuracy_policy_5: 0.54535
	loss_value_5: 0.04197
	loss_reward_5: 0.00855
	loss_policy: 0.27361
	loss_value: 0.37001
	loss_reward: 0.03602
[2024-05-05 14:50:05] nn step 48800, lr: 0.005.
	loss_policy_0: 0.08427
	accuracy_policy_0: 0.78887
	loss_value_0: 0.17484
	loss_policy_1: 0.0289
	accuracy_policy_1: 0.68117
	loss_value_1: 0.03666
	loss_reward_1: 0.00694
	loss_policy_2: 0.03352
	accuracy_policy_2: 0.64355
	loss_value_2: 0.03834
	loss_reward_2: 0.00635
	loss_policy_3: 0.03735
	accuracy_policy_3: 0.61359
	loss_value_3: 0.03969
	loss_reward_3: 0.00711
	loss_policy_4: 0.04077
	accuracy_policy_4: 0.58801
	loss_value_4: 0.04106
	loss_reward_4: 0.00774
	loss_policy_5: 0.0446
	accuracy_policy_5: 0.5541
	loss_value_5: 0.04236
	loss_reward_5: 0.00889
	loss_policy: 0.26941
	loss_value: 0.37295
	loss_reward: 0.03702
Optimization_Done 48800
[2024-05-05 14:51:55] [command] train weight_iter_48800.pkl 244 245
[2024-05-05 14:52:12] nn step 48850, lr: 0.005.
	loss_policy_0: 0.09469
	accuracy_policy_0: 0.71164
	loss_value_0: 0.14983
	loss_policy_1: 0.02748
	accuracy_policy_1: 0.63414
	loss_value_1: 0.03151
	loss_reward_1: 0.00495
	loss_policy_2: 0.03152
	accuracy_policy_2: 0.59273
	loss_value_2: 0.03262
	loss_reward_2: 0.00477
	loss_policy_3: 0.03453
	accuracy_policy_3: 0.5634
	loss_value_3: 0.03362
	loss_reward_3: 0.00522
	loss_policy_4: 0.03734
	accuracy_policy_4: 0.53066
	loss_value_4: 0.03453
	loss_reward_4: 0.00565
	loss_policy_5: 0.03974
	accuracy_policy_5: 0.51086
	loss_value_5: 0.03535
	loss_reward_5: 0.00657
	loss_policy: 0.2653
	loss_value: 0.31746
	loss_reward: 0.02717
[2024-05-05 14:52:28] nn step 48900, lr: 0.005.
	loss_policy_0: 0.07903
	accuracy_policy_0: 0.75332
	loss_value_0: 0.1453
	loss_policy_1: 0.02536
	accuracy_policy_1: 0.65734
	loss_value_1: 0.03048
	loss_reward_1: 0.00523
	loss_policy_2: 0.02986
	accuracy_policy_2: 0.60977
	loss_value_2: 0.03182
	loss_reward_2: 0.00485
	loss_policy_3: 0.03284
	accuracy_policy_3: 0.57965
	loss_value_3: 0.03295
	loss_reward_3: 0.00535
	loss_policy_4: 0.03548
	accuracy_policy_4: 0.5559
	loss_value_4: 0.03379
	loss_reward_4: 0.00578
	loss_policy_5: 0.03813
	accuracy_policy_5: 0.52918
	loss_value_5: 0.03467
	loss_reward_5: 0.00673
	loss_policy: 0.24069
	loss_value: 0.30901
	loss_reward: 0.02794
[2024-05-05 14:52:44] nn step 48950, lr: 0.005.
	loss_policy_0: 0.07922
	accuracy_policy_0: 0.77062
	loss_value_0: 0.15952
	loss_policy_1: 0.02669
	accuracy_policy_1: 0.66828
	loss_value_1: 0.03342
	loss_reward_1: 0.00573
	loss_policy_2: 0.03126
	accuracy_policy_2: 0.62289
	loss_value_2: 0.0348
	loss_reward_2: 0.00533
	loss_policy_3: 0.03489
	accuracy_policy_3: 0.59406
	loss_value_3: 0.03611
	loss_reward_3: 0.00559
	loss_policy_4: 0.03799
	accuracy_policy_4: 0.5625
	loss_value_4: 0.03702
	loss_reward_4: 0.00615
	loss_policy_5: 0.0409
	accuracy_policy_5: 0.53727
	loss_value_5: 0.03813
	loss_reward_5: 0.00757
	loss_policy: 0.25096
	loss_value: 0.339
	loss_reward: 0.03038
[2024-05-05 14:53:00] nn step 49000, lr: 0.005.
	loss_policy_0: 0.07168
	accuracy_policy_0: 0.7809
	loss_value_0: 0.15263
	loss_policy_1: 0.02479
	accuracy_policy_1: 0.67645
	loss_value_1: 0.03196
	loss_reward_1: 0.00565
	loss_policy_2: 0.0296
	accuracy_policy_2: 0.62422
	loss_value_2: 0.03317
	loss_reward_2: 0.00488
	loss_policy_3: 0.03299
	accuracy_policy_3: 0.5925
	loss_value_3: 0.03423
	loss_reward_3: 0.00534
	loss_policy_4: 0.03581
	accuracy_policy_4: 0.56656
	loss_value_4: 0.03522
	loss_reward_4: 0.00584
	loss_policy_5: 0.03873
	accuracy_policy_5: 0.5432
	loss_value_5: 0.03635
	loss_reward_5: 0.00714
	loss_policy: 0.23359
	loss_value: 0.32356
	loss_reward: 0.02884
Optimization_Done 49000
[2024-05-05 14:55:02] [command] train weight_iter_49000.pkl 245 246
[2024-05-05 14:55:19] nn step 49050, lr: 0.005.
	loss_policy_0: 0.10368
	accuracy_policy_0: 0.72086
	loss_value_0: 0.15526
	loss_policy_1: 0.02759
	accuracy_policy_1: 0.65066
	loss_value_1: 0.03226
	loss_reward_1: 0.0048
	loss_policy_2: 0.03132
	accuracy_policy_2: 0.6209
	loss_value_2: 0.03336
	loss_reward_2: 0.00489
	loss_policy_3: 0.03433
	accuracy_policy_3: 0.59469
	loss_value_3: 0.03467
	loss_reward_3: 0.00523
	loss_policy_4: 0.03692
	accuracy_policy_4: 0.5691
	loss_value_4: 0.0358
	loss_reward_4: 0.00552
	loss_policy_5: 0.03997
	accuracy_policy_5: 0.53906
	loss_value_5: 0.03678
	loss_reward_5: 0.00633
	loss_policy: 0.2738
	loss_value: 0.32814
	loss_reward: 0.02677
[2024-05-05 14:55:35] nn step 49100, lr: 0.005.
	loss_policy_0: 0.08626
	accuracy_policy_0: 0.76152
	loss_value_0: 0.16218
	loss_policy_1: 0.02707
	accuracy_policy_1: 0.68371
	loss_value_1: 0.03415
	loss_reward_1: 0.0055
	loss_policy_2: 0.0314
	accuracy_policy_2: 0.64461
	loss_value_2: 0.03547
	loss_reward_2: 0.00528
	loss_policy_3: 0.03478
	accuracy_policy_3: 0.6168
	loss_value_3: 0.03676
	loss_reward_3: 0.00566
	loss_policy_4: 0.03806
	accuracy_policy_4: 0.59297
	loss_value_4: 0.03783
	loss_reward_4: 0.00615
	loss_policy_5: 0.0413
	accuracy_policy_5: 0.56465
	loss_value_5: 0.03911
	loss_reward_5: 0.00717
	loss_policy: 0.25887
	loss_value: 0.3455
	loss_reward: 0.02976
[2024-05-05 14:55:51] nn step 49150, lr: 0.005.
	loss_policy_0: 0.08063
	accuracy_policy_0: 0.77234
	loss_value_0: 0.1624
	loss_policy_1: 0.02592
	accuracy_policy_1: 0.69512
	loss_value_1: 0.03415
	loss_reward_1: 0.00535
	loss_policy_2: 0.03014
	accuracy_policy_2: 0.65742
	loss_value_2: 0.03541
	loss_reward_2: 0.00514
	loss_policy_3: 0.03357
	accuracy_policy_3: 0.625
	loss_value_3: 0.03666
	loss_reward_3: 0.00541
	loss_policy_4: 0.03681
	accuracy_policy_4: 0.5977
	loss_value_4: 0.03779
	loss_reward_4: 0.00588
	loss_policy_5: 0.03979
	accuracy_policy_5: 0.56926
	loss_value_5: 0.03889
	loss_reward_5: 0.00707
	loss_policy: 0.24687
	loss_value: 0.34529
	loss_reward: 0.02885
[2024-05-05 14:56:07] nn step 49200, lr: 0.005.
	loss_policy_0: 0.07496
	accuracy_policy_0: 0.78598
	loss_value_0: 0.16249
	loss_policy_1: 0.02506
	accuracy_policy_1: 0.69809
	loss_value_1: 0.03414
	loss_reward_1: 0.00525
	loss_policy_2: 0.0292
	accuracy_policy_2: 0.65898
	loss_value_2: 0.03559
	loss_reward_2: 0.00499
	loss_policy_3: 0.03274
	accuracy_policy_3: 0.62863
	loss_value_3: 0.03673
	loss_reward_3: 0.00553
	loss_policy_4: 0.03604
	accuracy_policy_4: 0.60211
	loss_value_4: 0.03785
	loss_reward_4: 0.00601
	loss_policy_5: 0.03894
	accuracy_policy_5: 0.57418
	loss_value_5: 0.03899
	loss_reward_5: 0.00695
	loss_policy: 0.23695
	loss_value: 0.3458
	loss_reward: 0.02873
Optimization_Done 49200
[2024-05-05 14:58:11] [command] train weight_iter_49200.pkl 246 247
[2024-05-05 14:58:29] nn step 49250, lr: 0.005.
	loss_policy_0: 0.12119
	accuracy_policy_0: 0.71324
	loss_value_0: 0.18239
	loss_policy_1: 0.03267
	accuracy_policy_1: 0.64312
	loss_value_1: 0.03829
	loss_reward_1: 0.00619
	loss_policy_2: 0.03722
	accuracy_policy_2: 0.60625
	loss_value_2: 0.03994
	loss_reward_2: 0.00621
	loss_policy_3: 0.04091
	accuracy_policy_3: 0.57453
	loss_value_3: 0.04154
	loss_reward_3: 0.00668
	loss_policy_4: 0.04461
	accuracy_policy_4: 0.54195
	loss_value_4: 0.04295
	loss_reward_4: 0.00729
	loss_policy_5: 0.04792
	accuracy_policy_5: 0.51215
	loss_value_5: 0.04437
	loss_reward_5: 0.00831
	loss_policy: 0.32453
	loss_value: 0.38949
	loss_reward: 0.03468
[2024-05-05 14:58:45] nn step 49300, lr: 0.005.
	loss_policy_0: 0.10251
	accuracy_policy_0: 0.74637
	loss_value_0: 0.17699
	loss_policy_1: 0.03005
	accuracy_policy_1: 0.67406
	loss_value_1: 0.03712
	loss_reward_1: 0.00635
	loss_policy_2: 0.03463
	accuracy_policy_2: 0.63629
	loss_value_2: 0.03877
	loss_reward_2: 0.00608
	loss_policy_3: 0.03889
	accuracy_policy_3: 0.60098
	loss_value_3: 0.0401
	loss_reward_3: 0.00677
	loss_policy_4: 0.04226
	accuracy_policy_4: 0.57449
	loss_value_4: 0.0416
	loss_reward_4: 0.00726
	loss_policy_5: 0.04559
	accuracy_policy_5: 0.5466
	loss_value_5: 0.04291
	loss_reward_5: 0.00813
	loss_policy: 0.29394
	loss_value: 0.37748
	loss_reward: 0.0346
[2024-05-05 14:59:01] nn step 49350, lr: 0.005.
	loss_policy_0: 0.09649
	accuracy_policy_0: 0.7609
	loss_value_0: 0.18007
	loss_policy_1: 0.02956
	accuracy_policy_1: 0.6802
	loss_value_1: 0.03807
	loss_reward_1: 0.00653
	loss_policy_2: 0.03384
	accuracy_policy_2: 0.64824
	loss_value_2: 0.0397
	loss_reward_2: 0.00631
	loss_policy_3: 0.03805
	accuracy_policy_3: 0.61387
	loss_value_3: 0.04125
	loss_reward_3: 0.00657
	loss_policy_4: 0.04151
	accuracy_policy_4: 0.58789
	loss_value_4: 0.04275
	loss_reward_4: 0.00708
	loss_policy_5: 0.04522
	accuracy_policy_5: 0.5602
	loss_value_5: 0.04416
	loss_reward_5: 0.00817
	loss_policy: 0.28466
	loss_value: 0.38601
	loss_reward: 0.03467
[2024-05-05 14:59:17] nn step 49400, lr: 0.005.
	loss_policy_0: 0.09558
	accuracy_policy_0: 0.76977
	loss_value_0: 0.18532
	loss_policy_1: 0.02999
	accuracy_policy_1: 0.68008
	loss_value_1: 0.03904
	loss_reward_1: 0.00667
	loss_policy_2: 0.03445
	accuracy_policy_2: 0.64504
	loss_value_2: 0.04048
	loss_reward_2: 0.00625
	loss_policy_3: 0.03839
	accuracy_policy_3: 0.61492
	loss_value_3: 0.04201
	loss_reward_3: 0.00682
	loss_policy_4: 0.04211
	accuracy_policy_4: 0.58648
	loss_value_4: 0.04351
	loss_reward_4: 0.0073
	loss_policy_5: 0.0458
	accuracy_policy_5: 0.56508
	loss_value_5: 0.04488
	loss_reward_5: 0.00864
	loss_policy: 0.28633
	loss_value: 0.39524
	loss_reward: 0.03568
Optimization_Done 49400
[2024-05-05 15:01:20] [command] train weight_iter_49400.pkl 247 248
[2024-05-05 15:01:37] nn step 49450, lr: 0.005.
	loss_policy_0: 0.09639
	accuracy_policy_0: 0.7716
	loss_value_0: 0.18225
	loss_policy_1: 0.02974
	accuracy_policy_1: 0.68281
	loss_value_1: 0.03823
	loss_reward_1: 0.00739
	loss_policy_2: 0.03489
	accuracy_policy_2: 0.63367
	loss_value_2: 0.03992
	loss_reward_2: 0.00723
	loss_policy_3: 0.03895
	accuracy_policy_3: 0.59688
	loss_value_3: 0.04154
	loss_reward_3: 0.00814
	loss_policy_4: 0.04265
	accuracy_policy_4: 0.55895
	loss_value_4: 0.0433
	loss_reward_4: 0.00877
	loss_policy_5: 0.04643
	accuracy_policy_5: 0.52246
	loss_value_5: 0.04466
	loss_reward_5: 0.00969
	loss_policy: 0.28905
	loss_value: 0.38989
	loss_reward: 0.04122
[2024-05-05 15:01:53] nn step 49500, lr: 0.005.
	loss_policy_0: 0.07936
	accuracy_policy_0: 0.79047
	loss_value_0: 0.16825
	loss_policy_1: 0.02702
	accuracy_policy_1: 0.6948
	loss_value_1: 0.03586
	loss_reward_1: 0.00702
	loss_policy_2: 0.03176
	accuracy_policy_2: 0.65004
	loss_value_2: 0.03746
	loss_reward_2: 0.00683
	loss_policy_3: 0.03538
	accuracy_policy_3: 0.60973
	loss_value_3: 0.03902
	loss_reward_3: 0.0072
	loss_policy_4: 0.03885
	accuracy_policy_4: 0.58727
	loss_value_4: 0.04048
	loss_reward_4: 0.0079
	loss_policy_5: 0.04235
	accuracy_policy_5: 0.55402
	loss_value_5: 0.04175
	loss_reward_5: 0.00938
	loss_policy: 0.25473
	loss_value: 0.36282
	loss_reward: 0.03833
[2024-05-05 15:02:09] nn step 49550, lr: 0.005.
	loss_policy_0: 0.07902
	accuracy_policy_0: 0.79996
	loss_value_0: 0.17398
	loss_policy_1: 0.02757
	accuracy_policy_1: 0.69457
	loss_value_1: 0.03687
	loss_reward_1: 0.00749
	loss_policy_2: 0.03193
	accuracy_policy_2: 0.65109
	loss_value_2: 0.03842
	loss_reward_2: 0.00709
	loss_policy_3: 0.03608
	accuracy_policy_3: 0.61836
	loss_value_3: 0.04016
	loss_reward_3: 0.00784
	loss_policy_4: 0.03975
	accuracy_policy_4: 0.5907
	loss_value_4: 0.04161
	loss_reward_4: 0.00823
	loss_policy_5: 0.04287
	accuracy_policy_5: 0.56008
	loss_value_5: 0.04311
	loss_reward_5: 0.00965
	loss_policy: 0.25724
	loss_value: 0.37415
	loss_reward: 0.04032
[2024-05-05 15:02:25] nn step 49600, lr: 0.005.
	loss_policy_0: 0.07558
	accuracy_policy_0: 0.80406
	loss_value_0: 0.17697
	loss_policy_1: 0.02745
	accuracy_policy_1: 0.70039
	loss_value_1: 0.03759
	loss_reward_1: 0.00743
	loss_policy_2: 0.03252
	accuracy_policy_2: 0.65047
	loss_value_2: 0.03935
	loss_reward_2: 0.00676
	loss_policy_3: 0.03653
	accuracy_policy_3: 0.6184
	loss_value_3: 0.04086
	loss_reward_3: 0.00777
	loss_policy_4: 0.04006
	accuracy_policy_4: 0.59324
	loss_value_4: 0.04233
	loss_reward_4: 0.00819
	loss_policy_5: 0.04347
	accuracy_policy_5: 0.5643
	loss_value_5: 0.04395
	loss_reward_5: 0.00973
	loss_policy: 0.25562
	loss_value: 0.38104
	loss_reward: 0.03988
Optimization_Done 49600
[2024-05-05 15:04:18] [command] train weight_iter_49600.pkl 248 249
[2024-05-05 15:04:35] nn step 49650, lr: 0.005.
	loss_policy_0: 0.08938
	accuracy_policy_0: 0.76641
	loss_value_0: 0.15614
	loss_policy_1: 0.02795
	accuracy_policy_1: 0.66941
	loss_value_1: 0.03292
	loss_reward_1: 0.00639
	loss_policy_2: 0.03189
	accuracy_policy_2: 0.62363
	loss_value_2: 0.03426
	loss_reward_2: 0.00604
	loss_policy_3: 0.03575
	accuracy_policy_3: 0.5852
	loss_value_3: 0.03542
	loss_reward_3: 0.00679
	loss_policy_4: 0.03827
	accuracy_policy_4: 0.55398
	loss_value_4: 0.03652
	loss_reward_4: 0.0071
	loss_policy_5: 0.04064
	accuracy_policy_5: 0.5266
	loss_value_5: 0.03759
	loss_reward_5: 0.00831
	loss_policy: 0.26388
	loss_value: 0.33285
	loss_reward: 0.03464
[2024-05-05 15:04:51] nn step 49700, lr: 0.005.
	loss_policy_0: 0.07443
	accuracy_policy_0: 0.79629
	loss_value_0: 0.15494
	loss_policy_1: 0.02652
	accuracy_policy_1: 0.69109
	loss_value_1: 0.03273
	loss_reward_1: 0.00655
	loss_policy_2: 0.03097
	accuracy_policy_2: 0.64266
	loss_value_2: 0.03404
	loss_reward_2: 0.00608
	loss_policy_3: 0.03409
	accuracy_policy_3: 0.60871
	loss_value_3: 0.03504
	loss_reward_3: 0.00681
	loss_policy_4: 0.03686
	accuracy_policy_4: 0.57785
	loss_value_4: 0.0362
	loss_reward_4: 0.00742
	loss_policy_5: 0.0393
	accuracy_policy_5: 0.55445
	loss_value_5: 0.03727
	loss_reward_5: 0.00858
	loss_policy: 0.24217
	loss_value: 0.33023
	loss_reward: 0.03543
[2024-05-05 15:05:07] nn step 49750, lr: 0.005.
	loss_policy_0: 0.06707
	accuracy_policy_0: 0.81605
	loss_value_0: 0.15636
	loss_policy_1: 0.02556
	accuracy_policy_1: 0.69781
	loss_value_1: 0.03294
	loss_reward_1: 0.00664
	loss_policy_2: 0.02972
	accuracy_policy_2: 0.65211
	loss_value_2: 0.0342
	loss_reward_2: 0.00632
	loss_policy_3: 0.03333
	accuracy_policy_3: 0.6173
	loss_value_3: 0.03552
	loss_reward_3: 0.00694
	loss_policy_4: 0.03638
	accuracy_policy_4: 0.59188
	loss_value_4: 0.03657
	loss_reward_4: 0.00751
	loss_policy_5: 0.03891
	accuracy_policy_5: 0.56309
	loss_value_5: 0.03776
	loss_reward_5: 0.00871
	loss_policy: 0.23097
	loss_value: 0.33335
	loss_reward: 0.03612
[2024-05-05 15:05:23] nn step 49800, lr: 0.005.
	loss_policy_0: 0.06586
	accuracy_policy_0: 0.81516
	loss_value_0: 0.15395
	loss_policy_1: 0.02501
	accuracy_policy_1: 0.69859
	loss_value_1: 0.03241
	loss_reward_1: 0.00652
	loss_policy_2: 0.02898
	accuracy_policy_2: 0.6559
	loss_value_2: 0.03377
	loss_reward_2: 0.00607
	loss_policy_3: 0.03214
	accuracy_policy_3: 0.62145
	loss_value_3: 0.03474
	loss_reward_3: 0.00691
	loss_policy_4: 0.03526
	accuracy_policy_4: 0.58492
	loss_value_4: 0.03604
	loss_reward_4: 0.00754
	loss_policy_5: 0.03794
	accuracy_policy_5: 0.56379
	loss_value_5: 0.03717
	loss_reward_5: 0.00884
	loss_policy: 0.22521
	loss_value: 0.32808
	loss_reward: 0.03588
Optimization_Done 49800
[2024-05-05 15:07:26] [command] train weight_iter_49800.pkl 249 250
[2024-05-05 15:07:43] nn step 49850, lr: 0.005.
	loss_policy_0: 0.10683
	accuracy_policy_0: 0.7068
	loss_value_0: 0.15766
	loss_policy_1: 0.0295
	accuracy_policy_1: 0.62082
	loss_value_1: 0.03301
	loss_reward_1: 0.0049
	loss_policy_2: 0.03316
	accuracy_policy_2: 0.58441
	loss_value_2: 0.0344
	loss_reward_2: 0.00479
	loss_policy_3: 0.03654
	accuracy_policy_3: 0.54688
	loss_value_3: 0.03567
	loss_reward_3: 0.0051
	loss_policy_4: 0.03953
	accuracy_policy_4: 0.51336
	loss_value_4: 0.03694
	loss_reward_4: 0.00558
	loss_policy_5: 0.04231
	accuracy_policy_5: 0.49156
	loss_value_5: 0.03782
	loss_reward_5: 0.00635
	loss_policy: 0.28788
	loss_value: 0.33551
	loss_reward: 0.02672
[2024-05-05 15:07:59] nn step 49900, lr: 0.005.
	loss_policy_0: 0.08109
	accuracy_policy_0: 0.76488
	loss_value_0: 0.16186
	loss_policy_1: 0.02694
	accuracy_policy_1: 0.66734
	loss_value_1: 0.03405
	loss_reward_1: 0.00484
	loss_policy_2: 0.03145
	accuracy_policy_2: 0.62223
	loss_value_2: 0.03537
	loss_reward_2: 0.00491
	loss_policy_3: 0.03477
	accuracy_policy_3: 0.59258
	loss_value_3: 0.03663
	loss_reward_3: 0.00511
	loss_policy_4: 0.03749
	accuracy_policy_4: 0.56059
	loss_value_4: 0.03784
	loss_reward_4: 0.00549
	loss_policy_5: 0.04033
	accuracy_policy_5: 0.53191
	loss_value_5: 0.03894
	loss_reward_5: 0.00639
	loss_policy: 0.25207
	loss_value: 0.34469
	loss_reward: 0.02674
[2024-05-05 15:08:15] nn step 49950, lr: 0.005.
	loss_policy_0: 0.07582
	accuracy_policy_0: 0.7825
	loss_value_0: 0.16609
	loss_policy_1: 0.02598
	accuracy_policy_1: 0.67852
	loss_value_1: 0.03515
	loss_reward_1: 0.00513
	loss_policy_2: 0.03079
	accuracy_policy_2: 0.64172
	loss_value_2: 0.03642
	loss_reward_2: 0.00496
	loss_policy_3: 0.0345
	accuracy_policy_3: 0.60078
	loss_value_3: 0.03752
	loss_reward_3: 0.0053
	loss_policy_4: 0.03713
	accuracy_policy_4: 0.57184
	loss_value_4: 0.03841
	loss_reward_4: 0.00563
	loss_policy_5: 0.04025
	accuracy_policy_5: 0.54066
	loss_value_5: 0.03952
	loss_reward_5: 0.00665
	loss_policy: 0.24447
	loss_value: 0.35311
	loss_reward: 0.02766
[2024-05-05 15:08:31] nn step 50000, lr: 0.005.
	loss_policy_0: 0.07281
	accuracy_policy_0: 0.78996
	loss_value_0: 0.16803
	loss_policy_1: 0.02618
	accuracy_policy_1: 0.6852
	loss_value_1: 0.03528
	loss_reward_1: 0.00512
	loss_policy_2: 0.0306
	accuracy_policy_2: 0.6409
	loss_value_2: 0.03659
	loss_reward_2: 0.00508
	loss_policy_3: 0.03454
	accuracy_policy_3: 0.60434
	loss_value_3: 0.03777
	loss_reward_3: 0.00516
	loss_policy_4: 0.03688
	accuracy_policy_4: 0.58246
	loss_value_4: 0.03873
	loss_reward_4: 0.0056
	loss_policy_5: 0.04014
	accuracy_policy_5: 0.55363
	loss_value_5: 0.03979
	loss_reward_5: 0.00696
	loss_policy: 0.24115
	loss_value: 0.35619
	loss_reward: 0.02793
Optimization_Done 50000
[2024-05-05 15:10:35] [command] train weight_iter_50000.pkl 250 251
[2024-05-05 15:10:52] nn step 50050, lr: 0.005.
	loss_policy_0: 0.13315
	accuracy_policy_0: 0.70027
	loss_value_0: 0.18788
	loss_policy_1: 0.03533
	accuracy_policy_1: 0.62988
	loss_value_1: 0.03951
	loss_reward_1: 0.00624
	loss_policy_2: 0.03996
	accuracy_policy_2: 0.59242
	loss_value_2: 0.0414
	loss_reward_2: 0.00599
	loss_policy_3: 0.0435
	accuracy_policy_3: 0.56512
	loss_value_3: 0.04315
	loss_reward_3: 0.00643
	loss_policy_4: 0.04675
	accuracy_policy_4: 0.53625
	loss_value_4: 0.04456
	loss_reward_4: 0.00689
	loss_policy_5: 0.05011
	accuracy_policy_5: 0.5123
	loss_value_5: 0.04598
	loss_reward_5: 0.00786
	loss_policy: 0.3488
	loss_value: 0.40248
	loss_reward: 0.0334
[2024-05-05 15:11:08] nn step 50100, lr: 0.005.
	loss_policy_0: 0.10372
	accuracy_policy_0: 0.74484
	loss_value_0: 0.17573
	loss_policy_1: 0.03088
	accuracy_policy_1: 0.66617
	loss_value_1: 0.03721
	loss_reward_1: 0.00563
	loss_policy_2: 0.03539
	accuracy_policy_2: 0.62492
	loss_value_2: 0.03904
	loss_reward_2: 0.00588
	loss_policy_3: 0.03927
	accuracy_policy_3: 0.5941
	loss_value_3: 0.04072
	loss_reward_3: 0.00618
	loss_policy_4: 0.0427
	accuracy_policy_4: 0.56777
	loss_value_4: 0.04211
	loss_reward_4: 0.00658
	loss_policy_5: 0.04562
	accuracy_policy_5: 0.53816
	loss_value_5: 0.04375
	loss_reward_5: 0.00754
	loss_policy: 0.29758
	loss_value: 0.37856
	loss_reward: 0.03182
[2024-05-05 15:11:24] nn step 50150, lr: 0.005.
	loss_policy_0: 0.09506
	accuracy_policy_0: 0.76504
	loss_value_0: 0.17404
	loss_policy_1: 0.02987
	accuracy_policy_1: 0.67113
	loss_value_1: 0.03706
	loss_reward_1: 0.00583
	loss_policy_2: 0.03432
	accuracy_policy_2: 0.63496
	loss_value_2: 0.03857
	loss_reward_2: 0.00576
	loss_policy_3: 0.0381
	accuracy_policy_3: 0.60531
	loss_value_3: 0.04026
	loss_reward_3: 0.00607
	loss_policy_4: 0.04164
	accuracy_policy_4: 0.57891
	loss_value_4: 0.04166
	loss_reward_4: 0.00642
	loss_policy_5: 0.04506
	accuracy_policy_5: 0.54297
	loss_value_5: 0.04318
	loss_reward_5: 0.00724
	loss_policy: 0.28404
	loss_value: 0.37476
	loss_reward: 0.03133
[2024-05-05 15:11:40] nn step 50200, lr: 0.005.
	loss_policy_0: 0.09092
	accuracy_policy_0: 0.76996
	loss_value_0: 0.17677
	loss_policy_1: 0.02988
	accuracy_policy_1: 0.67473
	loss_value_1: 0.03757
	loss_reward_1: 0.00579
	loss_policy_2: 0.03419
	accuracy_policy_2: 0.64242
	loss_value_2: 0.03917
	loss_reward_2: 0.00573
	loss_policy_3: 0.03826
	accuracy_policy_3: 0.60961
	loss_value_3: 0.04074
	loss_reward_3: 0.00615
	loss_policy_4: 0.0414
	accuracy_policy_4: 0.58195
	loss_value_4: 0.0423
	loss_reward_4: 0.00634
	loss_policy_5: 0.04472
	accuracy_policy_5: 0.55531
	loss_value_5: 0.04373
	loss_reward_5: 0.00759
	loss_policy: 0.27936
	loss_value: 0.38027
	loss_reward: 0.03159
Optimization_Done 50200
[2024-05-05 15:13:33] [command] train weight_iter_50200.pkl 251 252
[2024-05-05 15:13:50] nn step 50250, lr: 0.005.
	loss_policy_0: 0.08919
	accuracy_policy_0: 0.79902
	loss_value_0: 0.17264
	loss_policy_1: 0.0282
	accuracy_policy_1: 0.71168
	loss_value_1: 0.03641
	loss_reward_1: 0.00766
	loss_policy_2: 0.03284
	accuracy_policy_2: 0.67191
	loss_value_2: 0.03831
	loss_reward_2: 0.00758
	loss_policy_3: 0.03685
	accuracy_policy_3: 0.63656
	loss_value_3: 0.03991
	loss_reward_3: 0.0085
	loss_policy_4: 0.04031
	accuracy_policy_4: 0.60328
	loss_value_4: 0.04128
	loss_reward_4: 0.00932
	loss_policy_5: 0.0435
	accuracy_policy_5: 0.5723
	loss_value_5: 0.04274
	loss_reward_5: 0.01002
	loss_policy: 0.27088
	loss_value: 0.3713
	loss_reward: 0.04307
[2024-05-05 15:14:06] nn step 50300, lr: 0.005.
	loss_policy_0: 0.08347
	accuracy_policy_0: 0.81488
	loss_value_0: 0.17764
	loss_policy_1: 0.02871
	accuracy_policy_1: 0.71824
	loss_value_1: 0.03784
	loss_reward_1: 0.00826
	loss_policy_2: 0.03371
	accuracy_policy_2: 0.6791
	loss_value_2: 0.03976
	loss_reward_2: 0.00789
	loss_policy_3: 0.03744
	accuracy_policy_3: 0.65246
	loss_value_3: 0.04158
	loss_reward_3: 0.0086
	loss_policy_4: 0.04136
	accuracy_policy_4: 0.6243
	loss_value_4: 0.04312
	loss_reward_4: 0.00896
	loss_policy_5: 0.04496
	accuracy_policy_5: 0.58949
	loss_value_5: 0.04473
	loss_reward_5: 0.01038
	loss_policy: 0.26964
	loss_value: 0.38467
	loss_reward: 0.0441
[2024-05-05 15:14:22] nn step 50350, lr: 0.005.
	loss_policy_0: 0.0766
	accuracy_policy_0: 0.82656
	loss_value_0: 0.17678
	loss_policy_1: 0.02717
	accuracy_policy_1: 0.72629
	loss_value_1: 0.03753
	loss_reward_1: 0.00822
	loss_policy_2: 0.03224
	accuracy_policy_2: 0.69109
	loss_value_2: 0.03918
	loss_reward_2: 0.00789
	loss_policy_3: 0.03705
	accuracy_policy_3: 0.65469
	loss_value_3: 0.0409
	loss_reward_3: 0.00833
	loss_policy_4: 0.04054
	accuracy_policy_4: 0.62813
	loss_value_4: 0.04246
	loss_reward_4: 0.00902
	loss_policy_5: 0.04437
	accuracy_policy_5: 0.59461
	loss_value_5: 0.04407
	loss_reward_5: 0.01029
	loss_policy: 0.25798
	loss_value: 0.38091
	loss_reward: 0.04376
[2024-05-05 15:14:38] nn step 50400, lr: 0.005.
	loss_policy_0: 0.07506
	accuracy_policy_0: 0.82715
	loss_value_0: 0.17275
	loss_policy_1: 0.02661
	accuracy_policy_1: 0.73035
	loss_value_1: 0.03672
	loss_reward_1: 0.00793
	loss_policy_2: 0.03139
	accuracy_policy_2: 0.69434
	loss_value_2: 0.03824
	loss_reward_2: 0.00743
	loss_policy_3: 0.03547
	accuracy_policy_3: 0.66027
	loss_value_3: 0.04017
	loss_reward_3: 0.00817
	loss_policy_4: 0.03887
	accuracy_policy_4: 0.63289
	loss_value_4: 0.04189
	loss_reward_4: 0.00879
	loss_policy_5: 0.04277
	accuracy_policy_5: 0.60137
	loss_value_5: 0.04365
	loss_reward_5: 0.01015
	loss_policy: 0.25017
	loss_value: 0.37343
	loss_reward: 0.04247
Optimization_Done 50400
[2024-05-05 15:16:40] [command] train weight_iter_50400.pkl 252 253
[2024-05-05 15:16:58] nn step 50450, lr: 0.005.
	loss_policy_0: 0.08891
	accuracy_policy_0: 0.80066
	loss_value_0: 0.15567
	loss_policy_1: 0.02791
	accuracy_policy_1: 0.70516
	loss_value_1: 0.03286
	loss_reward_1: 0.00717
	loss_policy_2: 0.03207
	accuracy_policy_2: 0.66867
	loss_value_2: 0.03458
	loss_reward_2: 0.00686
	loss_policy_3: 0.03568
	accuracy_policy_3: 0.63035
	loss_value_3: 0.03608
	loss_reward_3: 0.00753
	loss_policy_4: 0.03844
	accuracy_policy_4: 0.60246
	loss_value_4: 0.03739
	loss_reward_4: 0.00825
	loss_policy_5: 0.04147
	accuracy_policy_5: 0.5732
	loss_value_5: 0.03881
	loss_reward_5: 0.00913
	loss_policy: 0.26448
	loss_value: 0.33538
	loss_reward: 0.03893
[2024-05-05 15:17:14] nn step 50500, lr: 0.005.
	loss_policy_0: 0.06481
	accuracy_policy_0: 0.83152
	loss_value_0: 0.14894
	loss_policy_1: 0.02362
	accuracy_policy_1: 0.73594
	loss_value_1: 0.03157
	loss_reward_1: 0.0071
	loss_policy_2: 0.0274
	accuracy_policy_2: 0.70129
	loss_value_2: 0.03311
	loss_reward_2: 0.00661
	loss_policy_3: 0.03109
	accuracy_policy_3: 0.66477
	loss_value_3: 0.03444
	loss_reward_3: 0.00743
	loss_policy_4: 0.03443
	accuracy_policy_4: 0.62793
	loss_value_4: 0.03567
	loss_reward_4: 0.00778
	loss_policy_5: 0.03716
	accuracy_policy_5: 0.6016
	loss_value_5: 0.03709
	loss_reward_5: 0.00912
	loss_policy: 0.21851
	loss_value: 0.32082
	loss_reward: 0.03804
[2024-05-05 15:17:30] nn step 50550, lr: 0.005.
	loss_policy_0: 0.06179
	accuracy_policy_0: 0.84543
	loss_value_0: 0.15606
	loss_policy_1: 0.02357
	accuracy_policy_1: 0.74098
	loss_value_1: 0.03301
	loss_reward_1: 0.00735
	loss_policy_2: 0.02797
	accuracy_policy_2: 0.70781
	loss_value_2: 0.03487
	loss_reward_2: 0.00699
	loss_policy_3: 0.03194
	accuracy_policy_3: 0.66598
	loss_value_3: 0.03631
	loss_reward_3: 0.00759
	loss_policy_4: 0.03471
	accuracy_policy_4: 0.64207
	loss_value_4: 0.03762
	loss_reward_4: 0.00818
	loss_policy_5: 0.03796
	accuracy_policy_5: 0.60992
	loss_value_5: 0.0391
	loss_reward_5: 0.00971
	loss_policy: 0.21795
	loss_value: 0.33697
	loss_reward: 0.03982
[2024-05-05 15:17:46] nn step 50600, lr: 0.005.
	loss_policy_0: 0.05905
	accuracy_policy_0: 0.8516
	loss_value_0: 0.1552
	loss_policy_1: 0.02301
	accuracy_policy_1: 0.75168
	loss_value_1: 0.03285
	loss_reward_1: 0.00752
	loss_policy_2: 0.02733
	accuracy_policy_2: 0.71039
	loss_value_2: 0.03434
	loss_reward_2: 0.00696
	loss_policy_3: 0.03095
	accuracy_policy_3: 0.67898
	loss_value_3: 0.03583
	loss_reward_3: 0.00748
	loss_policy_4: 0.03435
	accuracy_policy_4: 0.64492
	loss_value_4: 0.03715
	loss_reward_4: 0.00815
	loss_policy_5: 0.03737
	accuracy_policy_5: 0.61887
	loss_value_5: 0.03854
	loss_reward_5: 0.00952
	loss_policy: 0.21207
	loss_value: 0.33391
	loss_reward: 0.03964
Optimization_Done 50600
[2024-05-05 15:19:51] [command] train weight_iter_50600.pkl 253 254
[2024-05-05 15:20:09] nn step 50650, lr: 0.005.
	loss_policy_0: 0.10411
	accuracy_policy_0: 0.72957
	loss_value_0: 0.15895
	loss_policy_1: 0.02923
	accuracy_policy_1: 0.65859
	loss_value_1: 0.03337
	loss_reward_1: 0.00493
	loss_policy_2: 0.03289
	accuracy_policy_2: 0.62191
	loss_value_2: 0.03455
	loss_reward_2: 0.00461
	loss_policy_3: 0.03556
	accuracy_policy_3: 0.59957
	loss_value_3: 0.0359
	loss_reward_3: 0.00496
	loss_policy_4: 0.03789
	accuracy_policy_4: 0.57945
	loss_value_4: 0.03708
	loss_reward_4: 0.0055
	loss_policy_5: 0.03996
	accuracy_policy_5: 0.55434
	loss_value_5: 0.03822
	loss_reward_5: 0.00637
	loss_policy: 0.27964
	loss_value: 0.33807
	loss_reward: 0.02637
[2024-05-05 15:20:25] nn step 50700, lr: 0.005.
	loss_policy_0: 0.08248
	accuracy_policy_0: 0.78055
	loss_value_0: 0.16049
	loss_policy_1: 0.02589
	accuracy_policy_1: 0.70023
	loss_value_1: 0.03374
	loss_reward_1: 0.00513
	loss_policy_2: 0.03008
	accuracy_policy_2: 0.66637
	loss_value_2: 0.03507
	loss_reward_2: 0.00465
	loss_policy_3: 0.03311
	accuracy_policy_3: 0.63332
	loss_value_3: 0.03614
	loss_reward_3: 0.00512
	loss_policy_4: 0.0356
	accuracy_policy_4: 0.60879
	loss_value_4: 0.03729
	loss_reward_4: 0.00563
	loss_policy_5: 0.03779
	accuracy_policy_5: 0.58332
	loss_value_5: 0.03837
	loss_reward_5: 0.00663
	loss_policy: 0.24494
	loss_value: 0.3411
	loss_reward: 0.02716
[2024-05-05 15:20:41] nn step 50750, lr: 0.005.
	loss_policy_0: 0.07198
	accuracy_policy_0: 0.79816
	loss_value_0: 0.15582
	loss_policy_1: 0.02457
	accuracy_policy_1: 0.70848
	loss_value_1: 0.03287
	loss_reward_1: 0.00505
	loss_policy_2: 0.02845
	accuracy_policy_2: 0.67043
	loss_value_2: 0.0342
	loss_reward_2: 0.00444
	loss_policy_3: 0.03117
	accuracy_policy_3: 0.64918
	loss_value_3: 0.03547
	loss_reward_3: 0.00489
	loss_policy_4: 0.03338
	accuracy_policy_4: 0.62305
	loss_value_4: 0.03663
	loss_reward_4: 0.00547
	loss_policy_5: 0.03586
	accuracy_policy_5: 0.59461
	loss_value_5: 0.0375
	loss_reward_5: 0.00631
	loss_policy: 0.22541
	loss_value: 0.33248
	loss_reward: 0.02617
[2024-05-05 15:20:56] nn step 50800, lr: 0.005.
	loss_policy_0: 0.072
	accuracy_policy_0: 0.80457
	loss_value_0: 0.16154
	loss_policy_1: 0.02526
	accuracy_policy_1: 0.70816
	loss_value_1: 0.03398
	loss_reward_1: 0.00512
	loss_policy_2: 0.02896
	accuracy_policy_2: 0.67934
	loss_value_2: 0.03535
	loss_reward_2: 0.00471
	loss_policy_3: 0.03235
	accuracy_policy_3: 0.64637
	loss_value_3: 0.03663
	loss_reward_3: 0.00516
	loss_policy_4: 0.03481
	accuracy_policy_4: 0.62012
	loss_value_4: 0.03766
	loss_reward_4: 0.00573
	loss_policy_5: 0.03671
	accuracy_policy_5: 0.60008
	loss_value_5: 0.03871
	loss_reward_5: 0.0066
	loss_policy: 0.23008
	loss_value: 0.34386
	loss_reward: 0.02732
Optimization_Done 50800
[2024-05-05 15:22:51] [command] train weight_iter_50800.pkl 254 255
[2024-05-05 15:23:08] nn step 50850, lr: 0.005.
	loss_policy_0: 0.11577
	accuracy_policy_0: 0.73582
	loss_value_0: 0.17083
	loss_policy_1: 0.03023
	accuracy_policy_1: 0.67859
	loss_value_1: 0.03584
	loss_reward_1: 0.0053
	loss_policy_2: 0.03344
	accuracy_policy_2: 0.65262
	loss_value_2: 0.03756
	loss_reward_2: 0.00513
	loss_policy_3: 0.03654
	accuracy_policy_3: 0.62441
	loss_value_3: 0.03896
	loss_reward_3: 0.00538
	loss_policy_4: 0.03927
	accuracy_policy_4: 0.6002
	loss_value_4: 0.04034
	loss_reward_4: 0.00609
	loss_policy_5: 0.04206
	accuracy_policy_5: 0.58039
	loss_value_5: 0.04164
	loss_reward_5: 0.00683
	loss_policy: 0.29731
	loss_value: 0.36518
	loss_reward: 0.02874
[2024-05-05 15:23:24] nn step 50900, lr: 0.005.
	loss_policy_0: 0.09164
	accuracy_policy_0: 0.77879
	loss_value_0: 0.16542
	loss_policy_1: 0.02731
	accuracy_policy_1: 0.70367
	loss_value_1: 0.03514
	loss_reward_1: 0.00504
	loss_policy_2: 0.03106
	accuracy_policy_2: 0.67746
	loss_value_2: 0.03668
	loss_reward_2: 0.00503
	loss_policy_3: 0.03388
	accuracy_policy_3: 0.65742
	loss_value_3: 0.038
	loss_reward_3: 0.00534
	loss_policy_4: 0.03671
	accuracy_policy_4: 0.62969
	loss_value_4: 0.03936
	loss_reward_4: 0.00572
	loss_policy_5: 0.03948
	accuracy_policy_5: 0.605
	loss_value_5: 0.04044
	loss_reward_5: 0.00664
	loss_policy: 0.26008
	loss_value: 0.35505
	loss_reward: 0.02777
[2024-05-05 15:23:40] nn step 50950, lr: 0.005.
	loss_policy_0: 0.09146
	accuracy_policy_0: 0.78266
	loss_value_0: 0.17441
	loss_policy_1: 0.02761
	accuracy_policy_1: 0.71426
	loss_value_1: 0.03713
	loss_reward_1: 0.00542
	loss_policy_2: 0.03147
	accuracy_policy_2: 0.68387
	loss_value_2: 0.03874
	loss_reward_2: 0.00529
	loss_policy_3: 0.03445
	accuracy_policy_3: 0.66133
	loss_value_3: 0.04005
	loss_reward_3: 0.00547
	loss_policy_4: 0.03731
	accuracy_policy_4: 0.63789
	loss_value_4: 0.04136
	loss_reward_4: 0.00592
	loss_policy_5: 0.04007
	accuracy_policy_5: 0.61441
	loss_value_5: 0.04272
	loss_reward_5: 0.0071
	loss_policy: 0.26238
	loss_value: 0.37441
	loss_reward: 0.02921
[2024-05-05 15:23:56] nn step 51000, lr: 0.005.
	loss_policy_0: 0.08024
	accuracy_policy_0: 0.79332
	loss_value_0: 0.16347
	loss_policy_1: 0.02548
	accuracy_policy_1: 0.71109
	loss_value_1: 0.03459
	loss_reward_1: 0.00481
	loss_policy_2: 0.02906
	accuracy_policy_2: 0.68742
	loss_value_2: 0.03609
	loss_reward_2: 0.00482
	loss_policy_3: 0.03252
	accuracy_policy_3: 0.65801
	loss_value_3: 0.03743
	loss_reward_3: 0.00539
	loss_policy_4: 0.03506
	accuracy_policy_4: 0.63473
	loss_value_4: 0.03853
	loss_reward_4: 0.00562
	loss_policy_5: 0.03736
	accuracy_policy_5: 0.61543
	loss_value_5: 0.0397
	loss_reward_5: 0.00657
	loss_policy: 0.23972
	loss_value: 0.34981
	loss_reward: 0.0272
Optimization_Done 51000
[2024-05-05 15:26:01] [command] train weight_iter_51000.pkl 255 256
[2024-05-05 15:26:18] nn step 51050, lr: 0.005.
	loss_policy_0: 0.10386
	accuracy_policy_0: 0.76531
	loss_value_0: 0.18026
	loss_policy_1: 0.03178
	accuracy_policy_1: 0.67695
	loss_value_1: 0.03824
	loss_reward_1: 0.00699
	loss_policy_2: 0.03633
	accuracy_policy_2: 0.63863
	loss_value_2: 0.04015
	loss_reward_2: 0.00687
	loss_policy_3: 0.03982
	accuracy_policy_3: 0.61344
	loss_value_3: 0.04183
	loss_reward_3: 0.00785
	loss_policy_4: 0.0434
	accuracy_policy_4: 0.57883
	loss_value_4: 0.04345
	loss_reward_4: 0.00823
	loss_policy_5: 0.04692
	accuracy_policy_5: 0.5443
	loss_value_5: 0.04499
	loss_reward_5: 0.00944
	loss_policy: 0.30212
	loss_value: 0.38892
	loss_reward: 0.03938
[2024-05-05 15:26:34] nn step 51100, lr: 0.005.
	loss_policy_0: 0.08995
	accuracy_policy_0: 0.79879
	loss_value_0: 0.18351
	loss_policy_1: 0.03022
	accuracy_policy_1: 0.70418
	loss_value_1: 0.03919
	loss_reward_1: 0.00713
	loss_policy_2: 0.0347
	accuracy_policy_2: 0.67094
	loss_value_2: 0.04126
	loss_reward_2: 0.00716
	loss_policy_3: 0.03913
	accuracy_policy_3: 0.63195
	loss_value_3: 0.04308
	loss_reward_3: 0.0077
	loss_policy_4: 0.04265
	accuracy_policy_4: 0.60664
	loss_value_4: 0.04464
	loss_reward_4: 0.00871
	loss_policy_5: 0.04553
	accuracy_policy_5: 0.58383
	loss_value_5: 0.04619
	loss_reward_5: 0.00928
	loss_policy: 0.28218
	loss_value: 0.39786
	loss_reward: 0.03998
[2024-05-05 15:26:50] nn step 51150, lr: 0.005.
	loss_policy_0: 0.08536
	accuracy_policy_0: 0.8052
	loss_value_0: 0.18255
	loss_policy_1: 0.02904
	accuracy_policy_1: 0.71551
	loss_value_1: 0.03874
	loss_reward_1: 0.00729
	loss_policy_2: 0.03397
	accuracy_policy_2: 0.6727
	loss_value_2: 0.04073
	loss_reward_2: 0.00698
	loss_policy_3: 0.03805
	accuracy_policy_3: 0.64176
	loss_value_3: 0.04244
	loss_reward_3: 0.00794
	loss_policy_4: 0.04141
	accuracy_policy_4: 0.62047
	loss_value_4: 0.04413
	loss_reward_4: 0.00838
	loss_policy_5: 0.04484
	accuracy_policy_5: 0.59133
	loss_value_5: 0.04561
	loss_reward_5: 0.00919
	loss_policy: 0.27268
	loss_value: 0.3942
	loss_reward: 0.03978
[2024-05-05 15:27:07] nn step 51200, lr: 0.005.
	loss_policy_0: 0.06828
	accuracy_policy_0: 0.81293
	loss_value_0: 0.15958
	loss_policy_1: 0.02432
	accuracy_policy_1: 0.71938
	loss_value_1: 0.034
	loss_reward_1: 0.00639
	loss_policy_2: 0.02895
	accuracy_policy_2: 0.67754
	loss_value_2: 0.03554
	loss_reward_2: 0.00596
	loss_policy_3: 0.03249
	accuracy_policy_3: 0.64578
	loss_value_3: 0.03685
	loss_reward_3: 0.00665
	loss_policy_4: 0.0356
	accuracy_policy_4: 0.62117
	loss_value_4: 0.03821
	loss_reward_4: 0.0072
	loss_policy_5: 0.03826
	accuracy_policy_5: 0.59586
	loss_value_5: 0.03966
	loss_reward_5: 0.00805
	loss_policy: 0.2279
	loss_value: 0.34385
	loss_reward: 0.03426
Optimization_Done 51200
[2024-05-05 15:29:16] [command] train weight_iter_51200.pkl 256 257
[2024-05-05 15:29:34] nn step 51250, lr: 0.005.
	loss_policy_0: 0.08977
	accuracy_policy_0: 0.77324
	loss_value_0: 0.16352
	loss_policy_1: 0.02998
	accuracy_policy_1: 0.67367
	loss_value_1: 0.03456
	loss_reward_1: 0.00673
	loss_policy_2: 0.03431
	accuracy_policy_2: 0.63258
	loss_value_2: 0.03635
	loss_reward_2: 0.00658
	loss_policy_3: 0.03745
	accuracy_policy_3: 0.60082
	loss_value_3: 0.03801
	loss_reward_3: 0.00702
	loss_policy_4: 0.04019
	accuracy_policy_4: 0.57781
	loss_value_4: 0.03961
	loss_reward_4: 0.00771
	loss_policy_5: 0.04319
	accuracy_policy_5: 0.5507
	loss_value_5: 0.04087
	loss_reward_5: 0.00865
	loss_policy: 0.27489
	loss_value: 0.35294
	loss_reward: 0.03668
[2024-05-05 15:29:50] nn step 51300, lr: 0.005.
	loss_policy_0: 0.08171
	accuracy_policy_0: 0.80324
	loss_value_0: 0.17913
	loss_policy_1: 0.03016
	accuracy_policy_1: 0.70438
	loss_value_1: 0.03792
	loss_reward_1: 0.00783
	loss_policy_2: 0.03547
	accuracy_policy_2: 0.66148
	loss_value_2: 0.04
	loss_reward_2: 0.00749
	loss_policy_3: 0.04024
	accuracy_policy_3: 0.62398
	loss_value_3: 0.04194
	loss_reward_3: 0.0083
	loss_policy_4: 0.04317
	accuracy_policy_4: 0.60305
	loss_value_4: 0.04349
	loss_reward_4: 0.00867
	loss_policy_5: 0.0463
	accuracy_policy_5: 0.57789
	loss_value_5: 0.04521
	loss_reward_5: 0.01004
	loss_policy: 0.27704
	loss_value: 0.38768
	loss_reward: 0.04232
[2024-05-05 15:30:06] nn step 51350, lr: 0.005.
	loss_policy_0: 0.06718
	accuracy_policy_0: 0.81637
	loss_value_0: 0.1608
	loss_policy_1: 0.02646
	accuracy_policy_1: 0.70574
	loss_value_1: 0.03405
	loss_reward_1: 0.00706
	loss_policy_2: 0.03167
	accuracy_policy_2: 0.65531
	loss_value_2: 0.03583
	loss_reward_2: 0.00684
	loss_policy_3: 0.0354
	accuracy_policy_3: 0.62301
	loss_value_3: 0.0374
	loss_reward_3: 0.00713
	loss_policy_4: 0.03855
	accuracy_policy_4: 0.59906
	loss_value_4: 0.03876
	loss_reward_4: 0.0078
	loss_policy_5: 0.04099
	accuracy_policy_5: 0.5834
	loss_value_5: 0.04021
	loss_reward_5: 0.00889
	loss_policy: 0.24024
	loss_value: 0.34706
	loss_reward: 0.0377
[2024-05-05 15:30:22] nn step 51400, lr: 0.005.
	loss_policy_0: 0.06456
	accuracy_policy_0: 0.82766
	loss_value_0: 0.1632
	loss_policy_1: 0.02605
	accuracy_policy_1: 0.71664
	loss_value_1: 0.03484
	loss_reward_1: 0.00733
	loss_policy_2: 0.03123
	accuracy_policy_2: 0.66965
	loss_value_2: 0.03658
	loss_reward_2: 0.00687
	loss_policy_3: 0.03558
	accuracy_policy_3: 0.62844
	loss_value_3: 0.03829
	loss_reward_3: 0.00758
	loss_policy_4: 0.03848
	accuracy_policy_4: 0.60809
	loss_value_4: 0.03975
	loss_reward_4: 0.0077
	loss_policy_5: 0.04096
	accuracy_policy_5: 0.59289
	loss_value_5: 0.04124
	loss_reward_5: 0.00929
	loss_policy: 0.23685
	loss_value: 0.35391
	loss_reward: 0.03876
Optimization_Done 51400
[2024-05-05 15:32:18] [command] train weight_iter_51400.pkl 257 258
[2024-05-05 15:32:35] nn step 51450, lr: 0.005.
	loss_policy_0: 0.08805
	accuracy_policy_0: 0.78859
	loss_value_0: 0.17079
	loss_policy_1: 0.0286
	accuracy_policy_1: 0.69734
	loss_value_1: 0.0359
	loss_reward_1: 0.0058
	loss_policy_2: 0.03359
	accuracy_policy_2: 0.64961
	loss_value_2: 0.03751
	loss_reward_2: 0.00561
	loss_policy_3: 0.03645
	accuracy_policy_3: 0.62313
	loss_value_3: 0.03893
	loss_reward_3: 0.00592
	loss_policy_4: 0.03926
	accuracy_policy_4: 0.59953
	loss_value_4: 0.04013
	loss_reward_4: 0.00655
	loss_policy_5: 0.04146
	accuracy_policy_5: 0.5775
	loss_value_5: 0.04127
	loss_reward_5: 0.00755
	loss_policy: 0.2674
	loss_value: 0.36453
	loss_reward: 0.03142
[2024-05-05 15:32:51] nn step 51500, lr: 0.005.
	loss_policy_0: 0.07211
	accuracy_policy_0: 0.81598
	loss_value_0: 0.16569
	loss_policy_1: 0.02626
	accuracy_policy_1: 0.71797
	loss_value_1: 0.03522
	loss_reward_1: 0.00594
	loss_policy_2: 0.03122
	accuracy_policy_2: 0.67238
	loss_value_2: 0.03685
	loss_reward_2: 0.00555
	loss_policy_3: 0.03421
	accuracy_policy_3: 0.63598
	loss_value_3: 0.03816
	loss_reward_3: 0.00614
	loss_policy_4: 0.03682
	accuracy_policy_4: 0.62004
	loss_value_4: 0.03924
	loss_reward_4: 0.00627
	loss_policy_5: 0.03858
	accuracy_policy_5: 0.59918
	loss_value_5: 0.04032
	loss_reward_5: 0.00755
	loss_policy: 0.23919
	loss_value: 0.35548
	loss_reward: 0.03145
[2024-05-05 15:33:07] nn step 51550, lr: 0.005.
	loss_policy_0: 0.06641
	accuracy_policy_0: 0.81941
	loss_value_0: 0.16146
	loss_policy_1: 0.02525
	accuracy_policy_1: 0.7152
	loss_value_1: 0.03422
	loss_reward_1: 0.00571
	loss_policy_2: 0.02982
	accuracy_policy_2: 0.67539
	loss_value_2: 0.03576
	loss_reward_2: 0.00556
	loss_policy_3: 0.03253
	accuracy_policy_3: 0.6448
	loss_value_3: 0.03707
	loss_reward_3: 0.00597
	loss_policy_4: 0.03519
	accuracy_policy_4: 0.61602
	loss_value_4: 0.03815
	loss_reward_4: 0.00639
	loss_policy_5: 0.03728
	accuracy_policy_5: 0.60035
	loss_value_5: 0.03932
	loss_reward_5: 0.00754
	loss_policy: 0.22647
	loss_value: 0.34599
	loss_reward: 0.03116
[2024-05-05 15:33:23] nn step 51600, lr: 0.005.
	loss_policy_0: 0.06751
	accuracy_policy_0: 0.82723
	loss_value_0: 0.17758
	loss_policy_1: 0.02707
	accuracy_policy_1: 0.71746
	loss_value_1: 0.03736
	loss_reward_1: 0.00629
	loss_policy_2: 0.03182
	accuracy_policy_2: 0.67961
	loss_value_2: 0.03872
	loss_reward_2: 0.00581
	loss_policy_3: 0.03526
	accuracy_policy_3: 0.64562
	loss_value_3: 0.04008
	loss_reward_3: 0.00647
	loss_policy_4: 0.03771
	accuracy_policy_4: 0.62684
	loss_value_4: 0.04127
	loss_reward_4: 0.00679
	loss_policy_5: 0.04033
	accuracy_policy_5: 0.60496
	loss_value_5: 0.04257
	loss_reward_5: 0.00795
	loss_policy: 0.2397
	loss_value: 0.37758
	loss_reward: 0.03332
Optimization_Done 51600
[2024-05-05 15:35:32] [command] train weight_iter_51600.pkl 258 259
[2024-05-05 15:35:49] nn step 51650, lr: 0.005.
	loss_policy_0: 0.11719
	accuracy_policy_0: 0.73562
	loss_value_0: 0.17436
	loss_policy_1: 0.033
	accuracy_policy_1: 0.65746
	loss_value_1: 0.03666
	loss_reward_1: 0.00536
	loss_policy_2: 0.03732
	accuracy_policy_2: 0.62277
	loss_value_2: 0.03799
	loss_reward_2: 0.00529
	loss_policy_3: 0.04053
	accuracy_policy_3: 0.59184
	loss_value_3: 0.03937
	loss_reward_3: 0.00568
	loss_policy_4: 0.0433
	accuracy_policy_4: 0.56844
	loss_value_4: 0.04043
	loss_reward_4: 0.00596
	loss_policy_5: 0.04608
	accuracy_policy_5: 0.54141
	loss_value_5: 0.04176
	loss_reward_5: 0.00703
	loss_policy: 0.31743
	loss_value: 0.37057
	loss_reward: 0.02933
[2024-05-05 15:36:05] nn step 51700, lr: 0.005.
	loss_policy_0: 0.08948
	accuracy_policy_0: 0.78996
	loss_value_0: 0.1763
	loss_policy_1: 0.0297
	accuracy_policy_1: 0.69797
	loss_value_1: 0.03728
	loss_reward_1: 0.00532
	loss_policy_2: 0.03423
	accuracy_policy_2: 0.66273
	loss_value_2: 0.0387
	loss_reward_2: 0.00545
	loss_policy_3: 0.0375
	accuracy_policy_3: 0.63211
	loss_value_3: 0.04017
	loss_reward_3: 0.00589
	loss_policy_4: 0.04054
	accuracy_policy_4: 0.60734
	loss_value_4: 0.04137
	loss_reward_4: 0.00625
	loss_policy_5: 0.04325
	accuracy_policy_5: 0.58301
	loss_value_5: 0.04253
	loss_reward_5: 0.00736
	loss_policy: 0.2747
	loss_value: 0.37635
	loss_reward: 0.03028
[2024-05-05 15:36:21] nn step 51750, lr: 0.005.
	loss_policy_0: 0.07735
	accuracy_policy_0: 0.79766
	loss_value_0: 0.16348
	loss_policy_1: 0.02652
	accuracy_policy_1: 0.70539
	loss_value_1: 0.03454
	loss_reward_1: 0.00493
	loss_policy_2: 0.03078
	accuracy_policy_2: 0.6652
	loss_value_2: 0.0359
	loss_reward_2: 0.0048
	loss_policy_3: 0.03427
	accuracy_policy_3: 0.63094
	loss_value_3: 0.03715
	loss_reward_3: 0.00524
	loss_policy_4: 0.03649
	accuracy_policy_4: 0.61238
	loss_value_4: 0.03822
	loss_reward_4: 0.00579
	loss_policy_5: 0.03902
	accuracy_policy_5: 0.5907
	loss_value_5: 0.03937
	loss_reward_5: 0.00689
	loss_policy: 0.24442
	loss_value: 0.34864
	loss_reward: 0.02766
[2024-05-05 15:36:37] nn step 51800, lr: 0.005.
	loss_policy_0: 0.07335
	accuracy_policy_0: 0.81062
	loss_value_0: 0.16611
	loss_policy_1: 0.0262
	accuracy_policy_1: 0.71062
	loss_value_1: 0.03513
	loss_reward_1: 0.00529
	loss_policy_2: 0.03093
	accuracy_policy_2: 0.66816
	loss_value_2: 0.0363
	loss_reward_2: 0.00507
	loss_policy_3: 0.0343
	accuracy_policy_3: 0.63418
	loss_value_3: 0.03752
	loss_reward_3: 0.00531
	loss_policy_4: 0.03691
	accuracy_policy_4: 0.61027
	loss_value_4: 0.03846
	loss_reward_4: 0.00601
	loss_policy_5: 0.03911
	accuracy_policy_5: 0.58949
	loss_value_5: 0.03937
	loss_reward_5: 0.0069
	loss_policy: 0.24081
	loss_value: 0.35289
	loss_reward: 0.02858
Optimization_Done 51800
[2024-05-05 15:38:47] [command] train weight_iter_51800.pkl 259 260
[2024-05-05 15:39:04] nn step 51850, lr: 0.005.
	loss_policy_0: 0.10992
	accuracy_policy_0: 0.73676
	loss_value_0: 0.18068
	loss_policy_1: 0.03237
	accuracy_policy_1: 0.64738
	loss_value_1: 0.03832
	loss_reward_1: 0.0059
	loss_policy_2: 0.03745
	accuracy_policy_2: 0.61074
	loss_value_2: 0.04016
	loss_reward_2: 0.00576
	loss_policy_3: 0.04107
	accuracy_policy_3: 0.57898
	loss_value_3: 0.04182
	loss_reward_3: 0.00645
	loss_policy_4: 0.04398
	accuracy_policy_4: 0.55344
	loss_value_4: 0.04318
	loss_reward_4: 0.00698
	loss_policy_5: 0.04662
	accuracy_policy_5: 0.53453
	loss_value_5: 0.04447
	loss_reward_5: 0.00791
	loss_policy: 0.31141
	loss_value: 0.38863
	loss_reward: 0.03299
[2024-05-05 15:39:20] nn step 51900, lr: 0.005.
	loss_policy_0: 0.09492
	accuracy_policy_0: 0.76555
	loss_value_0: 0.18279
	loss_policy_1: 0.03078
	accuracy_policy_1: 0.67555
	loss_value_1: 0.03866
	loss_reward_1: 0.00614
	loss_policy_2: 0.0353
	accuracy_policy_2: 0.63871
	loss_value_2: 0.04036
	loss_reward_2: 0.00602
	loss_policy_3: 0.03921
	accuracy_policy_3: 0.61055
	loss_value_3: 0.0419
	loss_reward_3: 0.00633
	loss_policy_4: 0.04272
	accuracy_policy_4: 0.58469
	loss_value_4: 0.04326
	loss_reward_4: 0.00688
	loss_policy_5: 0.04577
	accuracy_policy_5: 0.56043
	loss_value_5: 0.04459
	loss_reward_5: 0.00803
	loss_policy: 0.28869
	loss_value: 0.39157
	loss_reward: 0.0334
[2024-05-05 15:39:36] nn step 51950, lr: 0.005.
	loss_policy_0: 0.07987
	accuracy_policy_0: 0.77613
	loss_value_0: 0.16379
	loss_policy_1: 0.02672
	accuracy_policy_1: 0.68281
	loss_value_1: 0.03447
	loss_reward_1: 0.00551
	loss_policy_2: 0.03098
	accuracy_policy_2: 0.64578
	loss_value_2: 0.03618
	loss_reward_2: 0.00538
	loss_policy_3: 0.03474
	accuracy_policy_3: 0.61687
	loss_value_3: 0.03768
	loss_reward_3: 0.0057
	loss_policy_4: 0.03763
	accuracy_policy_4: 0.59027
	loss_value_4: 0.03897
	loss_reward_4: 0.00622
	loss_policy_5: 0.0403
	accuracy_policy_5: 0.56609
	loss_value_5: 0.04006
	loss_reward_5: 0.00721
	loss_policy: 0.25024
	loss_value: 0.35115
	loss_reward: 0.03003
[2024-05-05 15:39:52] nn step 52000, lr: 0.005.
	loss_policy_0: 0.07922
	accuracy_policy_0: 0.79199
	loss_value_0: 0.17419
	loss_policy_1: 0.02742
	accuracy_policy_1: 0.69188
	loss_value_1: 0.03679
	loss_reward_1: 0.00565
	loss_policy_2: 0.03232
	accuracy_policy_2: 0.64938
	loss_value_2: 0.0383
	loss_reward_2: 0.00569
	loss_policy_3: 0.0359
	accuracy_policy_3: 0.6225
	loss_value_3: 0.03966
	loss_reward_3: 0.00603
	loss_policy_4: 0.03933
	accuracy_policy_4: 0.59684
	loss_value_4: 0.04092
	loss_reward_4: 0.0066
	loss_policy_5: 0.04155
	accuracy_policy_5: 0.5777
	loss_value_5: 0.04228
	loss_reward_5: 0.00747
	loss_policy: 0.25574
	loss_value: 0.37214
	loss_reward: 0.03145
Optimization_Done 52000
[2024-05-05 15:42:01] [command] train weight_iter_52000.pkl 260 261
[2024-05-05 15:42:18] nn step 52050, lr: 0.005.
	loss_policy_0: 0.09445
	accuracy_policy_0: 0.75508
	loss_value_0: 0.17736
	loss_policy_1: 0.02959
	accuracy_policy_1: 0.66719
	loss_value_1: 0.03759
	loss_reward_1: 0.00707
	loss_policy_2: 0.03388
	accuracy_policy_2: 0.63164
	loss_value_2: 0.03936
	loss_reward_2: 0.00661
	loss_policy_3: 0.0376
	accuracy_policy_3: 0.59754
	loss_value_3: 0.0409
	loss_reward_3: 0.00715
	loss_policy_4: 0.04099
	accuracy_policy_4: 0.5743
	loss_value_4: 0.04238
	loss_reward_4: 0.00789
	loss_policy_5: 0.04338
	accuracy_policy_5: 0.55875
	loss_value_5: 0.04379
	loss_reward_5: 0.00862
	loss_policy: 0.27989
	loss_value: 0.38138
	loss_reward: 0.03734
[2024-05-05 15:42:34] nn step 52100, lr: 0.005.
	loss_policy_0: 0.07424
	accuracy_policy_0: 0.7925
	loss_value_0: 0.1662
	loss_policy_1: 0.026
	accuracy_policy_1: 0.69781
	loss_value_1: 0.03531
	loss_reward_1: 0.00656
	loss_policy_2: 0.0305
	accuracy_policy_2: 0.65777
	loss_value_2: 0.03704
	loss_reward_2: 0.00639
	loss_policy_3: 0.03415
	accuracy_policy_3: 0.62332
	loss_value_3: 0.03856
	loss_reward_3: 0.00685
	loss_policy_4: 0.03723
	accuracy_policy_4: 0.60316
	loss_value_4: 0.03991
	loss_reward_4: 0.00745
	loss_policy_5: 0.03964
	accuracy_policy_5: 0.58586
	loss_value_5: 0.04143
	loss_reward_5: 0.0085
	loss_policy: 0.24177
	loss_value: 0.35846
	loss_reward: 0.03576
[2024-05-05 15:42:50] nn step 52150, lr: 0.005.
	loss_policy_0: 0.07109
	accuracy_policy_0: 0.79848
	loss_value_0: 0.16602
	loss_policy_1: 0.02513
	accuracy_policy_1: 0.70301
	loss_value_1: 0.03538
	loss_reward_1: 0.00669
	loss_policy_2: 0.02946
	accuracy_policy_2: 0.67047
	loss_value_2: 0.03695
	loss_reward_2: 0.0064
	loss_policy_3: 0.0333
	accuracy_policy_3: 0.63688
	loss_value_3: 0.03858
	loss_reward_3: 0.00688
	loss_policy_4: 0.03605
	accuracy_policy_4: 0.61684
	loss_value_4: 0.04013
	loss_reward_4: 0.00747
	loss_policy_5: 0.03902
	accuracy_policy_5: 0.59309
	loss_value_5: 0.04164
	loss_reward_5: 0.00864
	loss_policy: 0.23405
	loss_value: 0.35871
	loss_reward: 0.03609
[2024-05-05 15:43:06] nn step 52200, lr: 0.005.
	loss_policy_0: 0.06937
	accuracy_policy_0: 0.80285
	loss_value_0: 0.16645
	loss_policy_1: 0.02529
	accuracy_policy_1: 0.70207
	loss_value_1: 0.03523
	loss_reward_1: 0.00674
	loss_policy_2: 0.02953
	accuracy_policy_2: 0.66566
	loss_value_2: 0.03692
	loss_reward_2: 0.00651
	loss_policy_3: 0.03359
	accuracy_policy_3: 0.6327
	loss_value_3: 0.03843
	loss_reward_3: 0.00697
	loss_policy_4: 0.03681
	accuracy_policy_4: 0.60902
	loss_value_4: 0.03984
	loss_reward_4: 0.00733
	loss_policy_5: 0.0393
	accuracy_policy_5: 0.59449
	loss_value_5: 0.04138
	loss_reward_5: 0.00868
	loss_policy: 0.2339
	loss_value: 0.35824
	loss_reward: 0.03623
Optimization_Done 52200
[2024-05-05 15:45:04] [command] train weight_iter_52200.pkl 261 262
[2024-05-05 15:45:21] nn step 52250, lr: 0.005.
	loss_policy_0: 0.09523
	accuracy_policy_0: 0.76883
	loss_value_0: 0.16888
	loss_policy_1: 0.02837
	accuracy_policy_1: 0.68848
	loss_value_1: 0.03562
	loss_reward_1: 0.00598
	loss_policy_2: 0.03254
	accuracy_policy_2: 0.65398
	loss_value_2: 0.03719
	loss_reward_2: 0.00553
	loss_policy_3: 0.03529
	accuracy_policy_3: 0.62941
	loss_value_3: 0.03866
	loss_reward_3: 0.00617
	loss_policy_4: 0.03737
	accuracy_policy_4: 0.61359
	loss_value_4: 0.04002
	loss_reward_4: 0.00638
	loss_policy_5: 0.03992
	accuracy_policy_5: 0.59539
	loss_value_5: 0.04141
	loss_reward_5: 0.00758
	loss_policy: 0.26872
	loss_value: 0.36178
	loss_reward: 0.03165
[2024-05-05 15:45:37] nn step 52300, lr: 0.005.
	loss_policy_0: 0.07565
	accuracy_policy_0: 0.7984
	loss_value_0: 0.16428
	loss_policy_1: 0.02565
	accuracy_policy_1: 0.71379
	loss_value_1: 0.03454
	loss_reward_1: 0.00621
	loss_policy_2: 0.02954
	accuracy_policy_2: 0.68289
	loss_value_2: 0.03623
	loss_reward_2: 0.00588
	loss_policy_3: 0.03277
	accuracy_policy_3: 0.64781
	loss_value_3: 0.03774
	loss_reward_3: 0.00594
	loss_policy_4: 0.03506
	accuracy_policy_4: 0.63488
	loss_value_4: 0.0389
	loss_reward_4: 0.00678
	loss_policy_5: 0.03744
	accuracy_policy_5: 0.62062
	loss_value_5: 0.04023
	loss_reward_5: 0.0077
	loss_policy: 0.2361
	loss_value: 0.35193
	loss_reward: 0.03251
[2024-05-05 15:45:53] nn step 52350, lr: 0.005.
	loss_policy_0: 0.07066
	accuracy_policy_0: 0.8068
	loss_value_0: 0.16179
	loss_policy_1: 0.02475
	accuracy_policy_1: 0.71578
	loss_value_1: 0.03386
	loss_reward_1: 0.00611
	loss_policy_2: 0.02896
	accuracy_policy_2: 0.67945
	loss_value_2: 0.03521
	loss_reward_2: 0.00569
	loss_policy_3: 0.03191
	accuracy_policy_3: 0.65398
	loss_value_3: 0.03661
	loss_reward_3: 0.00606
	loss_policy_4: 0.03454
	accuracy_policy_4: 0.63688
	loss_value_4: 0.03799
	loss_reward_4: 0.00649
	loss_policy_5: 0.03684
	accuracy_policy_5: 0.61691
	loss_value_5: 0.03934
	loss_reward_5: 0.00772
	loss_policy: 0.22767
	loss_value: 0.3448
	loss_reward: 0.03208
[2024-05-05 15:46:09] nn step 52400, lr: 0.005.
	loss_policy_0: 0.06277
	accuracy_policy_0: 0.81703
	loss_value_0: 0.15134
	loss_policy_1: 0.0229
	accuracy_policy_1: 0.71902
	loss_value_1: 0.03178
	loss_reward_1: 0.00593
	loss_policy_2: 0.02675
	accuracy_policy_2: 0.68422
	loss_value_2: 0.03307
	loss_reward_2: 0.00553
	loss_policy_3: 0.02981
	accuracy_policy_3: 0.6625
	loss_value_3: 0.03437
	loss_reward_3: 0.00576
	loss_policy_4: 0.03233
	accuracy_policy_4: 0.63836
	loss_value_4: 0.03581
	loss_reward_4: 0.00634
	loss_policy_5: 0.03427
	accuracy_policy_5: 0.62234
	loss_value_5: 0.03694
	loss_reward_5: 0.00724
	loss_policy: 0.20882
	loss_value: 0.32331
	loss_reward: 0.03081
Optimization_Done 52400
[2024-05-05 15:48:20] [command] train weight_iter_52400.pkl 262 263
[2024-05-05 15:48:38] nn step 52450, lr: 0.005.
	loss_policy_0: 0.10984
	accuracy_policy_0: 0.74883
	loss_value_0: 0.1764
	loss_policy_1: 0.03081
	accuracy_policy_1: 0.67914
	loss_value_1: 0.03702
	loss_reward_1: 0.00578
	loss_policy_2: 0.03472
	accuracy_policy_2: 0.64465
	loss_value_2: 0.0385
	loss_reward_2: 0.00524
	loss_policy_3: 0.03824
	accuracy_policy_3: 0.61914
	loss_value_3: 0.03995
	loss_reward_3: 0.0059
	loss_policy_4: 0.04151
	accuracy_policy_4: 0.58812
	loss_value_4: 0.04124
	loss_reward_4: 0.00642
	loss_policy_5: 0.0443
	accuracy_policy_5: 0.57035
	loss_value_5: 0.04242
	loss_reward_5: 0.00726
	loss_policy: 0.29942
	loss_value: 0.37554
	loss_reward: 0.03061
[2024-05-05 15:48:54] nn step 52500, lr: 0.005.
	loss_policy_0: 0.08484
	accuracy_policy_0: 0.79098
	loss_value_0: 0.18238
	loss_policy_1: 0.02793
	accuracy_policy_1: 0.71449
	loss_value_1: 0.03843
	loss_reward_1: 0.00584
	loss_policy_2: 0.03285
	accuracy_policy_2: 0.67527
	loss_value_2: 0.03985
	loss_reward_2: 0.00569
	loss_policy_3: 0.03608
	accuracy_policy_3: 0.64906
	loss_value_3: 0.04112
	loss_reward_3: 0.00601
	loss_policy_4: 0.039
	accuracy_policy_4: 0.62707
	loss_value_4: 0.0424
	loss_reward_4: 0.00648
	loss_policy_5: 0.04189
	accuracy_policy_5: 0.60391
	loss_value_5: 0.04361
	loss_reward_5: 0.00752
	loss_policy: 0.26259
	loss_value: 0.3878
	loss_reward: 0.03153
[2024-05-05 15:49:09] nn step 52550, lr: 0.005.
	loss_policy_0: 0.07157
	accuracy_policy_0: 0.80621
	loss_value_0: 0.16684
	loss_policy_1: 0.02476
	accuracy_policy_1: 0.71246
	loss_value_1: 0.0351
	loss_reward_1: 0.00538
	loss_policy_2: 0.02862
	accuracy_policy_2: 0.67957
	loss_value_2: 0.03642
	loss_reward_2: 0.00511
	loss_policy_3: 0.03212
	accuracy_policy_3: 0.65
	loss_value_3: 0.03775
	loss_reward_3: 0.00544
	loss_policy_4: 0.0345
	accuracy_policy_4: 0.62613
	loss_value_4: 0.03878
	loss_reward_4: 0.00581
	loss_policy_5: 0.03714
	accuracy_policy_5: 0.60977
	loss_value_5: 0.03997
	loss_reward_5: 0.00696
	loss_policy: 0.2287
	loss_value: 0.35486
	loss_reward: 0.0287
[2024-05-05 15:49:25] nn step 52600, lr: 0.005.
	loss_policy_0: 0.06744
	accuracy_policy_0: 0.81633
	loss_value_0: 0.17012
	loss_policy_1: 0.02468
	accuracy_policy_1: 0.72664
	loss_value_1: 0.03575
	loss_reward_1: 0.00522
	loss_policy_2: 0.02871
	accuracy_policy_2: 0.68609
	loss_value_2: 0.03738
	loss_reward_2: 0.00509
	loss_policy_3: 0.03215
	accuracy_policy_3: 0.65492
	loss_value_3: 0.0385
	loss_reward_3: 0.00555
	loss_policy_4: 0.0346
	accuracy_policy_4: 0.63293
	loss_value_4: 0.03951
	loss_reward_4: 0.006
	loss_policy_5: 0.0375
	accuracy_policy_5: 0.61594
	loss_value_5: 0.0406
	loss_reward_5: 0.00718
	loss_policy: 0.22507
	loss_value: 0.36185
	loss_reward: 0.02903
Optimization_Done 52600
[2024-05-05 15:51:26] [command] train weight_iter_52600.pkl 263 264
[2024-05-05 15:51:43] nn step 52650, lr: 0.005.
	loss_policy_0: 0.1098
	accuracy_policy_0: 0.74602
	loss_value_0: 0.17905
	loss_policy_1: 0.03065
	accuracy_policy_1: 0.67129
	loss_value_1: 0.03748
	loss_reward_1: 0.00586
	loss_policy_2: 0.03463
	accuracy_policy_2: 0.64023
	loss_value_2: 0.03929
	loss_reward_2: 0.00586
	loss_policy_3: 0.03767
	accuracy_policy_3: 0.60977
	loss_value_3: 0.04074
	loss_reward_3: 0.00638
	loss_policy_4: 0.04143
	accuracy_policy_4: 0.58367
	loss_value_4: 0.04205
	loss_reward_4: 0.00682
	loss_policy_5: 0.04399
	accuracy_policy_5: 0.55668
	loss_value_5: 0.04321
	loss_reward_5: 0.008
	loss_policy: 0.29818
	loss_value: 0.38181
	loss_reward: 0.03292
[2024-05-05 15:51:59] nn step 52700, lr: 0.005.
	loss_policy_0: 0.09111
	accuracy_policy_0: 0.78359
	loss_value_0: 0.18071
	loss_policy_1: 0.0284
	accuracy_policy_1: 0.70277
	loss_value_1: 0.03811
	loss_reward_1: 0.00616
	loss_policy_2: 0.03277
	accuracy_policy_2: 0.66984
	loss_value_2: 0.03979
	loss_reward_2: 0.00585
	loss_policy_3: 0.03645
	accuracy_policy_3: 0.6382
	loss_value_3: 0.0412
	loss_reward_3: 0.00659
	loss_policy_4: 0.03955
	accuracy_policy_4: 0.60832
	loss_value_4: 0.0425
	loss_reward_4: 0.0069
	loss_policy_5: 0.0425
	accuracy_policy_5: 0.59148
	loss_value_5: 0.04382
	loss_reward_5: 0.00805
	loss_policy: 0.27077
	loss_value: 0.38613
	loss_reward: 0.03354
[2024-05-05 15:52:15] nn step 52750, lr: 0.005.
	loss_policy_0: 0.08522
	accuracy_policy_0: 0.79414
	loss_value_0: 0.17826
	loss_policy_1: 0.02756
	accuracy_policy_1: 0.70895
	loss_value_1: 0.03755
	loss_reward_1: 0.00599
	loss_policy_2: 0.03143
	accuracy_policy_2: 0.6775
	loss_value_2: 0.03908
	loss_reward_2: 0.00589
	loss_policy_3: 0.03576
	accuracy_policy_3: 0.64168
	loss_value_3: 0.04048
	loss_reward_3: 0.00651
	loss_policy_4: 0.03893
	accuracy_policy_4: 0.61738
	loss_value_4: 0.04184
	loss_reward_4: 0.0069
	loss_policy_5: 0.04202
	accuracy_policy_5: 0.59812
	loss_value_5: 0.04292
	loss_reward_5: 0.00813
	loss_policy: 0.26091
	loss_value: 0.38013
	loss_reward: 0.03343
[2024-05-05 15:52:31] nn step 52800, lr: 0.005.
	loss_policy_0: 0.07737
	accuracy_policy_0: 0.80289
	loss_value_0: 0.1703
	loss_policy_1: 0.02563
	accuracy_policy_1: 0.71141
	loss_value_1: 0.03596
	loss_reward_1: 0.00572
	loss_policy_2: 0.03007
	accuracy_policy_2: 0.67508
	loss_value_2: 0.0376
	loss_reward_2: 0.00559
	loss_policy_3: 0.03358
	accuracy_policy_3: 0.64379
	loss_value_3: 0.03912
	loss_reward_3: 0.00614
	loss_policy_4: 0.0364
	accuracy_policy_4: 0.6243
	loss_value_4: 0.04031
	loss_reward_4: 0.00647
	loss_policy_5: 0.03899
	accuracy_policy_5: 0.60859
	loss_value_5: 0.04174
	loss_reward_5: 0.00782
	loss_policy: 0.24204
	loss_value: 0.36502
	loss_reward: 0.03174
Optimization_Done 52800
[2024-05-05 15:54:25] [command] train weight_iter_52800.pkl 264 265
[2024-05-05 15:54:42] nn step 52850, lr: 0.005.
	loss_policy_0: 0.09304
	accuracy_policy_0: 0.76867
	loss_value_0: 0.17302
	loss_policy_1: 0.03014
	accuracy_policy_1: 0.67625
	loss_value_1: 0.03637
	loss_reward_1: 0.00655
	loss_policy_2: 0.03458
	accuracy_policy_2: 0.63746
	loss_value_2: 0.03798
	loss_reward_2: 0.00631
	loss_policy_3: 0.03793
	accuracy_policy_3: 0.61602
	loss_value_3: 0.03947
	loss_reward_3: 0.007
	loss_policy_4: 0.04159
	accuracy_policy_4: 0.5791
	loss_value_4: 0.04098
	loss_reward_4: 0.00763
	loss_policy_5: 0.04538
	accuracy_policy_5: 0.54773
	loss_value_5: 0.04256
	loss_reward_5: 0.00887
	loss_policy: 0.28266
	loss_value: 0.37038
	loss_reward: 0.03635
[2024-05-05 15:54:59] nn step 52900, lr: 0.005.
	loss_policy_0: 0.07455
	accuracy_policy_0: 0.79484
	loss_value_0: 0.16387
	loss_policy_1: 0.02625
	accuracy_policy_1: 0.70285
	loss_value_1: 0.03472
	loss_reward_1: 0.00642
	loss_policy_2: 0.03091
	accuracy_policy_2: 0.66328
	loss_value_2: 0.0364
	loss_reward_2: 0.0062
	loss_policy_3: 0.0351
	accuracy_policy_3: 0.63105
	loss_value_3: 0.03787
	loss_reward_3: 0.00659
	loss_policy_4: 0.0383
	accuracy_policy_4: 0.60367
	loss_value_4: 0.03934
	loss_reward_4: 0.00718
	loss_policy_5: 0.04164
	accuracy_policy_5: 0.57465
	loss_value_5: 0.04075
	loss_reward_5: 0.00831
	loss_policy: 0.24675
	loss_value: 0.35295
	loss_reward: 0.03471
[2024-05-05 15:55:15] nn step 52950, lr: 0.005.
	loss_policy_0: 0.07692
	accuracy_policy_0: 0.79805
	loss_value_0: 0.17144
	loss_policy_1: 0.02737
	accuracy_policy_1: 0.70293
	loss_value_1: 0.03606
	loss_reward_1: 0.00675
	loss_policy_2: 0.03202
	accuracy_policy_2: 0.67082
	loss_value_2: 0.03774
	loss_reward_2: 0.00642
	loss_policy_3: 0.03617
	accuracy_policy_3: 0.63461
	loss_value_3: 0.03917
	loss_reward_3: 0.00687
	loss_policy_4: 0.03951
	accuracy_policy_4: 0.61781
	loss_value_4: 0.04066
	loss_reward_4: 0.00771
	loss_policy_5: 0.04298
	accuracy_policy_5: 0.58426
	loss_value_5: 0.04212
	loss_reward_5: 0.00897
	loss_policy: 0.25496
	loss_value: 0.36718
	loss_reward: 0.03672
[2024-05-05 15:55:31] nn step 53000, lr: 0.005.
	loss_policy_0: 0.07164
	accuracy_policy_0: 0.80895
	loss_value_0: 0.16621
	loss_policy_1: 0.02609
	accuracy_policy_1: 0.71129
	loss_value_1: 0.03536
	loss_reward_1: 0.00659
	loss_policy_2: 0.03086
	accuracy_policy_2: 0.6741
	loss_value_2: 0.03727
	loss_reward_2: 0.0061
	loss_policy_3: 0.0346
	accuracy_policy_3: 0.6459
	loss_value_3: 0.03884
	loss_reward_3: 0.00675
	loss_policy_4: 0.0379
	accuracy_policy_4: 0.61828
	loss_value_4: 0.04014
	loss_reward_4: 0.0071
	loss_policy_5: 0.04132
	accuracy_policy_5: 0.5891
	loss_value_5: 0.04158
	loss_reward_5: 0.00855
	loss_policy: 0.2424
	loss_value: 0.3594
	loss_reward: 0.03509
Optimization_Done 53000
[2024-05-05 15:57:29] [command] train weight_iter_53000.pkl 265 266
[2024-05-05 15:57:46] nn step 53050, lr: 0.005.
	loss_policy_0: 0.09225
	accuracy_policy_0: 0.75887
	loss_value_0: 0.15909
	loss_policy_1: 0.02751
	accuracy_policy_1: 0.68004
	loss_value_1: 0.03329
	loss_reward_1: 0.00522
	loss_policy_2: 0.03189
	accuracy_policy_2: 0.63582
	loss_value_2: 0.03478
	loss_reward_2: 0.00523
	loss_policy_3: 0.0351
	accuracy_policy_3: 0.60711
	loss_value_3: 0.03604
	loss_reward_3: 0.00559
	loss_policy_4: 0.03777
	accuracy_policy_4: 0.58742
	loss_value_4: 0.0373
	loss_reward_4: 0.00609
	loss_policy_5: 0.03993
	accuracy_policy_5: 0.56457
	loss_value_5: 0.03849
	loss_reward_5: 0.00719
	loss_policy: 0.26445
	loss_value: 0.339
	loss_reward: 0.02932
[2024-05-05 15:58:02] nn step 53100, lr: 0.005.
	loss_policy_0: 0.07712
	accuracy_policy_0: 0.78465
	loss_value_0: 0.15635
	loss_policy_1: 0.0257
	accuracy_policy_1: 0.69199
	loss_value_1: 0.0328
	loss_reward_1: 0.00596
	loss_policy_2: 0.03025
	accuracy_policy_2: 0.65473
	loss_value_2: 0.03408
	loss_reward_2: 0.00542
	loss_policy_3: 0.03336
	accuracy_policy_3: 0.62816
	loss_value_3: 0.03539
	loss_reward_3: 0.00573
	loss_policy_4: 0.0362
	accuracy_policy_4: 0.60762
	loss_value_4: 0.03666
	loss_reward_4: 0.00619
	loss_policy_5: 0.03893
	accuracy_policy_5: 0.58285
	loss_value_5: 0.03793
	loss_reward_5: 0.0072
	loss_policy: 0.24156
	loss_value: 0.33321
	loss_reward: 0.03049
[2024-05-05 15:58:17] nn step 53150, lr: 0.005.
	loss_policy_0: 0.07368
	accuracy_policy_0: 0.79148
	loss_value_0: 0.15917
	loss_policy_1: 0.02579
	accuracy_policy_1: 0.69828
	loss_value_1: 0.03354
	loss_reward_1: 0.00588
	loss_policy_2: 0.02954
	accuracy_policy_2: 0.66699
	loss_value_2: 0.03489
	loss_reward_2: 0.0055
	loss_policy_3: 0.03291
	accuracy_policy_3: 0.63785
	loss_value_3: 0.03624
	loss_reward_3: 0.00574
	loss_policy_4: 0.03577
	accuracy_policy_4: 0.62199
	loss_value_4: 0.03752
	loss_reward_4: 0.00648
	loss_policy_5: 0.03877
	accuracy_policy_5: 0.59547
	loss_value_5: 0.03882
	loss_reward_5: 0.00754
	loss_policy: 0.23646
	loss_value: 0.34019
	loss_reward: 0.03113
[2024-05-05 15:58:34] nn step 53200, lr: 0.005.
	loss_policy_0: 0.06686
	accuracy_policy_0: 0.80277
	loss_value_0: 0.15421
	loss_policy_1: 0.02416
	accuracy_policy_1: 0.70852
	loss_value_1: 0.03267
	loss_reward_1: 0.0058
	loss_policy_2: 0.02865
	accuracy_policy_2: 0.66672
	loss_value_2: 0.03409
	loss_reward_2: 0.0053
	loss_policy_3: 0.03142
	accuracy_policy_3: 0.64289
	loss_value_3: 0.03545
	loss_reward_3: 0.00559
	loss_policy_4: 0.03417
	accuracy_policy_4: 0.62539
	loss_value_4: 0.03665
	loss_reward_4: 0.00619
	loss_policy_5: 0.03642
	accuracy_policy_5: 0.59969
	loss_value_5: 0.03787
	loss_reward_5: 0.00744
	loss_policy: 0.22168
	loss_value: 0.33093
	loss_reward: 0.03032
Optimization_Done 53200
[2024-05-05 16:00:26] [command] train weight_iter_53200.pkl 266 267
[2024-05-05 16:00:43] nn step 53250, lr: 0.005.
	loss_policy_0: 0.08675
	accuracy_policy_0: 0.7684
	loss_value_0: 0.15668
	loss_policy_1: 0.02627
	accuracy_policy_1: 0.68789
	loss_value_1: 0.03308
	loss_reward_1: 0.00513
	loss_policy_2: 0.03016
	accuracy_policy_2: 0.65547
	loss_value_2: 0.03438
	loss_reward_2: 0.00485
	loss_policy_3: 0.03275
	accuracy_policy_3: 0.62395
	loss_value_3: 0.03568
	loss_reward_3: 0.0053
	loss_policy_4: 0.03551
	accuracy_policy_4: 0.60168
	loss_value_4: 0.03659
	loss_reward_4: 0.0057
	loss_policy_5: 0.03831
	accuracy_policy_5: 0.56637
	loss_value_5: 0.03757
	loss_reward_5: 0.00649
	loss_policy: 0.24976
	loss_value: 0.33398
	loss_reward: 0.02747
[2024-05-05 16:00:59] nn step 53300, lr: 0.005.
	loss_policy_0: 0.0746
	accuracy_policy_0: 0.79531
	loss_value_0: 0.16566
	loss_policy_1: 0.02562
	accuracy_policy_1: 0.71043
	loss_value_1: 0.03496
	loss_reward_1: 0.0055
	loss_policy_2: 0.02987
	accuracy_policy_2: 0.67625
	loss_value_2: 0.03627
	loss_reward_2: 0.00521
	loss_policy_3: 0.03309
	accuracy_policy_3: 0.65031
	loss_value_3: 0.03735
	loss_reward_3: 0.00554
	loss_policy_4: 0.03615
	accuracy_policy_4: 0.62059
	loss_value_4: 0.03836
	loss_reward_4: 0.00607
	loss_policy_5: 0.03884
	accuracy_policy_5: 0.60086
	loss_value_5: 0.03959
	loss_reward_5: 0.00701
	loss_policy: 0.23817
	loss_value: 0.35218
	loss_reward: 0.02933
[2024-05-05 16:01:15] nn step 53350, lr: 0.005.
	loss_policy_0: 0.06143
	accuracy_policy_0: 0.80684
	loss_value_0: 0.14612
	loss_policy_1: 0.02242
	accuracy_policy_1: 0.71535
	loss_value_1: 0.03057
	loss_reward_1: 0.00479
	loss_policy_2: 0.02621
	accuracy_policy_2: 0.67566
	loss_value_2: 0.03185
	loss_reward_2: 0.00463
	loss_policy_3: 0.02903
	accuracy_policy_3: 0.6541
	loss_value_3: 0.03285
	loss_reward_3: 0.00486
	loss_policy_4: 0.03172
	accuracy_policy_4: 0.62832
	loss_value_4: 0.03391
	loss_reward_4: 0.0054
	loss_policy_5: 0.03391
	accuracy_policy_5: 0.60867
	loss_value_5: 0.03498
	loss_reward_5: 0.00627
	loss_policy: 0.20472
	loss_value: 0.31027
	loss_reward: 0.02595
[2024-05-05 16:01:31] nn step 53400, lr: 0.005.
	loss_policy_0: 0.06733
	accuracy_policy_0: 0.81055
	loss_value_0: 0.16444
	loss_policy_1: 0.02506
	accuracy_policy_1: 0.71941
	loss_value_1: 0.03461
	loss_reward_1: 0.00554
	loss_policy_2: 0.02939
	accuracy_policy_2: 0.6752
	loss_value_2: 0.03618
	loss_reward_2: 0.00526
	loss_policy_3: 0.03242
	accuracy_policy_3: 0.65059
	loss_value_3: 0.03731
	loss_reward_3: 0.00546
	loss_policy_4: 0.03492
	accuracy_policy_4: 0.63242
	loss_value_4: 0.03823
	loss_reward_4: 0.00596
	loss_policy_5: 0.03742
	accuracy_policy_5: 0.61508
	loss_value_5: 0.03929
	loss_reward_5: 0.00708
	loss_policy: 0.22654
	loss_value: 0.35008
	loss_reward: 0.0293
Optimization_Done 53400
[2024-05-05 16:03:29] [command] train weight_iter_53400.pkl 267 268
[2024-05-05 16:03:46] nn step 53450, lr: 0.005.
	loss_policy_0: 0.11537
	accuracy_policy_0: 0.7225
	loss_value_0: 0.17013
	loss_policy_1: 0.03107
	accuracy_policy_1: 0.65605
	loss_value_1: 0.03588
	loss_reward_1: 0.00604
	loss_policy_2: 0.03491
	accuracy_policy_2: 0.62902
	loss_value_2: 0.03748
	loss_reward_2: 0.0061
	loss_policy_3: 0.03857
	accuracy_policy_3: 0.59328
	loss_value_3: 0.03887
	loss_reward_3: 0.00635
	loss_policy_4: 0.04099
	accuracy_policy_4: 0.57395
	loss_value_4: 0.0402
	loss_reward_4: 0.00709
	loss_policy_5: 0.0435
	accuracy_policy_5: 0.55383
	loss_value_5: 0.04146
	loss_reward_5: 0.00823
	loss_policy: 0.30442
	loss_value: 0.36403
	loss_reward: 0.03381
[2024-05-05 16:04:02] nn step 53500, lr: 0.005.
	loss_policy_0: 0.08938
	accuracy_policy_0: 0.77355
	loss_value_0: 0.17389
	loss_policy_1: 0.02747
	accuracy_policy_1: 0.69953
	loss_value_1: 0.03698
	loss_reward_1: 0.00664
	loss_policy_2: 0.0318
	accuracy_policy_2: 0.66336
	loss_value_2: 0.03885
	loss_reward_2: 0.00604
	loss_policy_3: 0.03605
	accuracy_policy_3: 0.63578
	loss_value_3: 0.04026
	loss_reward_3: 0.00657
	loss_policy_4: 0.0394
	accuracy_policy_4: 0.61312
	loss_value_4: 0.04164
	loss_reward_4: 0.00745
	loss_policy_5: 0.04189
	accuracy_policy_5: 0.58945
	loss_value_5: 0.04303
	loss_reward_5: 0.00861
	loss_policy: 0.26599
	loss_value: 0.37465
	loss_reward: 0.03531
[2024-05-05 16:04:18] nn step 53550, lr: 0.005.
	loss_policy_0: 0.08091
	accuracy_policy_0: 0.78812
	loss_value_0: 0.17202
	loss_policy_1: 0.0267
	accuracy_policy_1: 0.70578
	loss_value_1: 0.03637
	loss_reward_1: 0.00638
	loss_policy_2: 0.03117
	accuracy_policy_2: 0.66871
	loss_value_2: 0.03804
	loss_reward_2: 0.0062
	loss_policy_3: 0.03487
	accuracy_policy_3: 0.63883
	loss_value_3: 0.03969
	loss_reward_3: 0.00661
	loss_policy_4: 0.03749
	accuracy_policy_4: 0.6232
	loss_value_4: 0.04108
	loss_reward_4: 0.00727
	loss_policy_5: 0.04095
	accuracy_policy_5: 0.60082
	loss_value_5: 0.04255
	loss_reward_5: 0.00826
	loss_policy: 0.2521
	loss_value: 0.36975
	loss_reward: 0.03472
[2024-05-05 16:04:34] nn step 53600, lr: 0.005.
	loss_policy_0: 0.06985
	accuracy_policy_0: 0.80125
	loss_value_0: 0.16238
	loss_policy_1: 0.02401
	accuracy_policy_1: 0.71555
	loss_value_1: 0.03443
	loss_reward_1: 0.00601
	loss_policy_2: 0.0288
	accuracy_policy_2: 0.67492
	loss_value_2: 0.03595
	loss_reward_2: 0.00566
	loss_policy_3: 0.03234
	accuracy_policy_3: 0.64723
	loss_value_3: 0.03741
	loss_reward_3: 0.0063
	loss_policy_4: 0.03593
	accuracy_policy_4: 0.62223
	loss_value_4: 0.03868
	loss_reward_4: 0.00685
	loss_policy_5: 0.03765
	accuracy_policy_5: 0.6057
	loss_value_5: 0.0398
	loss_reward_5: 0.00786
	loss_policy: 0.22858
	loss_value: 0.34865
	loss_reward: 0.03268
Optimization_Done 53600
[2024-05-05 16:06:33] [command] train weight_iter_53600.pkl 268 269
[2024-05-05 16:06:50] nn step 53650, lr: 0.005.
	loss_policy_0: 0.09208
	accuracy_policy_0: 0.77891
	loss_value_0: 0.17669
	loss_policy_1: 0.03006
	accuracy_policy_1: 0.68824
	loss_value_1: 0.03759
	loss_reward_1: 0.00761
	loss_policy_2: 0.03504
	accuracy_policy_2: 0.65055
	loss_value_2: 0.03942
	loss_reward_2: 0.00738
	loss_policy_3: 0.03957
	accuracy_policy_3: 0.61543
	loss_value_3: 0.04095
	loss_reward_3: 0.00813
	loss_policy_4: 0.04377
	accuracy_policy_4: 0.58348
	loss_value_4: 0.04254
	loss_reward_4: 0.00875
	loss_policy_5: 0.04684
	accuracy_policy_5: 0.55637
	loss_value_5: 0.04412
	loss_reward_5: 0.01021
	loss_policy: 0.28736
	loss_value: 0.38131
	loss_reward: 0.04208
[2024-05-05 16:07:06] nn step 53700, lr: 0.005.
	loss_policy_0: 0.07176
	accuracy_policy_0: 0.80676
	loss_value_0: 0.16221
	loss_policy_1: 0.02585
	accuracy_policy_1: 0.70844
	loss_value_1: 0.03455
	loss_reward_1: 0.00702
	loss_policy_2: 0.0307
	accuracy_policy_2: 0.66891
	loss_value_2: 0.03634
	loss_reward_2: 0.00671
	loss_policy_3: 0.03437
	accuracy_policy_3: 0.63508
	loss_value_3: 0.03789
	loss_reward_3: 0.00743
	loss_policy_4: 0.03789
	accuracy_policy_4: 0.60891
	loss_value_4: 0.03922
	loss_reward_4: 0.00766
	loss_policy_5: 0.0406
	accuracy_policy_5: 0.58418
	loss_value_5: 0.04045
	loss_reward_5: 0.009
	loss_policy: 0.24119
	loss_value: 0.35067
	loss_reward: 0.03783
[2024-05-05 16:07:22] nn step 53750, lr: 0.005.
	loss_policy_0: 0.06786
	accuracy_policy_0: 0.80914
	loss_value_0: 0.16282
	loss_policy_1: 0.02499
	accuracy_policy_1: 0.70953
	loss_value_1: 0.03456
	loss_reward_1: 0.00668
	loss_policy_2: 0.02966
	accuracy_policy_2: 0.67168
	loss_value_2: 0.03607
	loss_reward_2: 0.00655
	loss_policy_3: 0.03377
	accuracy_policy_3: 0.63602
	loss_value_3: 0.03757
	loss_reward_3: 0.00737
	loss_policy_4: 0.03712
	accuracy_policy_4: 0.61254
	loss_value_4: 0.03913
	loss_reward_4: 0.00781
	loss_policy_5: 0.03995
	accuracy_policy_5: 0.58715
	loss_value_5: 0.04071
	loss_reward_5: 0.00903
	loss_policy: 0.23334
	loss_value: 0.35085
	loss_reward: 0.03745
[2024-05-05 16:07:38] nn step 53800, lr: 0.005.
	loss_policy_0: 0.06082
	accuracy_policy_0: 0.82285
	loss_value_0: 0.15681
	loss_policy_1: 0.0235
	accuracy_policy_1: 0.71961
	loss_value_1: 0.03304
	loss_reward_1: 0.00679
	loss_policy_2: 0.02804
	accuracy_policy_2: 0.68098
	loss_value_2: 0.03475
	loss_reward_2: 0.00638
	loss_policy_3: 0.03157
	accuracy_policy_3: 0.64891
	loss_value_3: 0.03624
	loss_reward_3: 0.00702
	loss_policy_4: 0.03531
	accuracy_policy_4: 0.61855
	loss_value_4: 0.03759
	loss_reward_4: 0.00755
	loss_policy_5: 0.03805
	accuracy_policy_5: 0.59227
	loss_value_5: 0.03888
	loss_reward_5: 0.00888
	loss_policy: 0.2173
	loss_value: 0.33731
	loss_reward: 0.03663
Optimization_Done 53800
[2024-05-05 16:09:38] [command] train weight_iter_53800.pkl 269 270
[2024-05-05 16:09:55] nn step 53850, lr: 0.005.
	loss_policy_0: 0.07274
	accuracy_policy_0: 0.76961
	loss_value_0: 0.14166
	loss_policy_1: 0.02363
	accuracy_policy_1: 0.6875
	loss_value_1: 0.0297
	loss_reward_1: 0.00546
	loss_policy_2: 0.02715
	accuracy_policy_2: 0.64332
	loss_value_2: 0.03113
	loss_reward_2: 0.00524
	loss_policy_3: 0.02988
	accuracy_policy_3: 0.6182
	loss_value_3: 0.03237
	loss_reward_3: 0.00552
	loss_policy_4: 0.0326
	accuracy_policy_4: 0.59371
	loss_value_4: 0.03342
	loss_reward_4: 0.00605
	loss_policy_5: 0.0348
	accuracy_policy_5: 0.56898
	loss_value_5: 0.03459
	loss_reward_5: 0.007
	loss_policy: 0.22081
	loss_value: 0.30287
	loss_reward: 0.02927
[2024-05-05 16:10:11] nn step 53900, lr: 0.005.
	loss_policy_0: 0.06321
	accuracy_policy_0: 0.80656
	loss_value_0: 0.15031
	loss_policy_1: 0.02326
	accuracy_policy_1: 0.70668
	loss_value_1: 0.03157
	loss_reward_1: 0.00591
	loss_policy_2: 0.02709
	accuracy_policy_2: 0.66348
	loss_value_2: 0.03286
	loss_reward_2: 0.00546
	loss_policy_3: 0.03033
	accuracy_policy_3: 0.63523
	loss_value_3: 0.0343
	loss_reward_3: 0.00599
	loss_policy_4: 0.03364
	accuracy_policy_4: 0.61156
	loss_value_4: 0.03545
	loss_reward_4: 0.00647
	loss_policy_5: 0.03571
	accuracy_policy_5: 0.5907
	loss_value_5: 0.03676
	loss_reward_5: 0.00755
	loss_policy: 0.21325
	loss_value: 0.32125
	loss_reward: 0.03138
[2024-05-05 16:10:27] nn step 53950, lr: 0.005.
	loss_policy_0: 0.0567
	accuracy_policy_0: 0.81402
	loss_value_0: 0.14844
	loss_policy_1: 0.02187
	accuracy_policy_1: 0.71777
	loss_value_1: 0.03123
	loss_reward_1: 0.00581
	loss_policy_2: 0.02581
	accuracy_policy_2: 0.66875
	loss_value_2: 0.03261
	loss_reward_2: 0.00534
	loss_policy_3: 0.02865
	accuracy_policy_3: 0.64953
	loss_value_3: 0.03387
	loss_reward_3: 0.0059
	loss_policy_4: 0.03156
	accuracy_policy_4: 0.62391
	loss_value_4: 0.03491
	loss_reward_4: 0.00618
	loss_policy_5: 0.03414
	accuracy_policy_5: 0.59797
	loss_value_5: 0.03623
	loss_reward_5: 0.0074
	loss_policy: 0.19873
	loss_value: 0.31729
	loss_reward: 0.03062
[2024-05-05 16:10:43] nn step 54000, lr: 0.005.
	loss_policy_0: 0.05616
	accuracy_policy_0: 0.82668
	loss_value_0: 0.15584
	loss_policy_1: 0.02282
	accuracy_policy_1: 0.72062
	loss_value_1: 0.03278
	loss_reward_1: 0.0062
	loss_policy_2: 0.02645
	accuracy_policy_2: 0.67492
	loss_value_2: 0.03409
	loss_reward_2: 0.00569
	loss_policy_3: 0.0304
	accuracy_policy_3: 0.65062
	loss_value_3: 0.03539
	loss_reward_3: 0.00622
	loss_policy_4: 0.03335
	accuracy_policy_4: 0.62402
	loss_value_4: 0.0366
	loss_reward_4: 0.00659
	loss_policy_5: 0.03558
	accuracy_policy_5: 0.60434
	loss_value_5: 0.03788
	loss_reward_5: 0.00793
	loss_policy: 0.20476
	loss_value: 0.33259
	loss_reward: 0.03263
Optimization_Done 54000
[2024-05-05 16:12:32] [command] train weight_iter_54000.pkl 270 271
[2024-05-05 16:12:49] nn step 54050, lr: 0.005.
	loss_policy_0: 0.08459
	accuracy_policy_0: 0.74148
	loss_value_0: 0.14666
	loss_policy_1: 0.02509
	accuracy_policy_1: 0.66234
	loss_value_1: 0.03089
	loss_reward_1: 0.00462
	loss_policy_2: 0.02849
	accuracy_policy_2: 0.62535
	loss_value_2: 0.03202
	loss_reward_2: 0.00451
	loss_policy_3: 0.03124
	accuracy_policy_3: 0.59844
	loss_value_3: 0.03316
	loss_reward_3: 0.00478
	loss_policy_4: 0.03376
	accuracy_policy_4: 0.56598
	loss_value_4: 0.03406
	loss_reward_4: 0.00515
	loss_policy_5: 0.03608
	accuracy_policy_5: 0.54336
	loss_value_5: 0.035
	loss_reward_5: 0.00554
	loss_policy: 0.23925
	loss_value: 0.31178
	loss_reward: 0.0246
[2024-05-05 16:13:05] nn step 54100, lr: 0.005.
	loss_policy_0: 0.0727
	accuracy_policy_0: 0.77742
	loss_value_0: 0.16063
	loss_policy_1: 0.02509
	accuracy_policy_1: 0.68789
	loss_value_1: 0.03373
	loss_reward_1: 0.00491
	loss_policy_2: 0.02881
	accuracy_policy_2: 0.6475
	loss_value_2: 0.03508
	loss_reward_2: 0.00479
	loss_policy_3: 0.03191
	accuracy_policy_3: 0.62133
	loss_value_3: 0.0363
	loss_reward_3: 0.0052
	loss_policy_4: 0.03453
	accuracy_policy_4: 0.59746
	loss_value_4: 0.03717
	loss_reward_4: 0.00545
	loss_policy_5: 0.03689
	accuracy_policy_5: 0.58316
	loss_value_5: 0.03809
	loss_reward_5: 0.00625
	loss_policy: 0.22992
	loss_value: 0.34099
	loss_reward: 0.0266
[2024-05-05 16:13:20] nn step 54150, lr: 0.005.
	loss_policy_0: 0.06104
	accuracy_policy_0: 0.7927
	loss_value_0: 0.14682
	loss_policy_1: 0.02178
	accuracy_policy_1: 0.69621
	loss_value_1: 0.03094
	loss_reward_1: 0.00448
	loss_policy_2: 0.02505
	accuracy_policy_2: 0.65992
	loss_value_2: 0.03207
	loss_reward_2: 0.00416
	loss_policy_3: 0.02804
	accuracy_policy_3: 0.6366
	loss_value_3: 0.03298
	loss_reward_3: 0.00449
	loss_policy_4: 0.03022
	accuracy_policy_4: 0.61066
	loss_value_4: 0.0339
	loss_reward_4: 0.00489
	loss_policy_5: 0.03285
	accuracy_policy_5: 0.58773
	loss_value_5: 0.03483
	loss_reward_5: 0.00538
	loss_policy: 0.19898
	loss_value: 0.31153
	loss_reward: 0.02341
[2024-05-05 16:13:37] nn step 54200, lr: 0.005.
	loss_policy_0: 0.05706
	accuracy_policy_0: 0.80504
	loss_value_0: 0.14861
	loss_policy_1: 0.02106
	accuracy_policy_1: 0.70633
	loss_value_1: 0.03104
	loss_reward_1: 0.00467
	loss_policy_2: 0.02451
	accuracy_policy_2: 0.66109
	loss_value_2: 0.03215
	loss_reward_2: 0.00433
	loss_policy_3: 0.02773
	accuracy_policy_3: 0.63379
	loss_value_3: 0.03302
	loss_reward_3: 0.00464
	loss_policy_4: 0.0301
	accuracy_policy_4: 0.6118
	loss_value_4: 0.03396
	loss_reward_4: 0.00502
	loss_policy_5: 0.03268
	accuracy_policy_5: 0.5916
	loss_value_5: 0.03494
	loss_reward_5: 0.00591
	loss_policy: 0.19314
	loss_value: 0.31373
	loss_reward: 0.02457
Optimization_Done 54200
[2024-05-05 16:15:37] [command] train weight_iter_54200.pkl 271 272
[2024-05-05 16:15:54] nn step 54250, lr: 0.005.
	loss_policy_0: 0.10681
	accuracy_policy_0: 0.72883
	loss_value_0: 0.16774
	loss_policy_1: 0.03004
	accuracy_policy_1: 0.64477
	loss_value_1: 0.03524
	loss_reward_1: 0.00571
	loss_policy_2: 0.03439
	accuracy_policy_2: 0.60191
	loss_value_2: 0.03663
	loss_reward_2: 0.00569
	loss_policy_3: 0.03751
	accuracy_policy_3: 0.57551
	loss_value_3: 0.03807
	loss_reward_3: 0.00612
	loss_policy_4: 0.04058
	accuracy_policy_4: 0.54211
	loss_value_4: 0.0391
	loss_reward_4: 0.00646
	loss_policy_5: 0.04306
	accuracy_policy_5: 0.52016
	loss_value_5: 0.04005
	loss_reward_5: 0.00747
	loss_policy: 0.29239
	loss_value: 0.35682
	loss_reward: 0.03146
[2024-05-05 16:16:10] nn step 54300, lr: 0.005.
	loss_policy_0: 0.07968
	accuracy_policy_0: 0.77582
	loss_value_0: 0.16044
	loss_policy_1: 0.02635
	accuracy_policy_1: 0.68309
	loss_value_1: 0.03402
	loss_reward_1: 0.0057
	loss_policy_2: 0.03055
	accuracy_policy_2: 0.63977
	loss_value_2: 0.03545
	loss_reward_2: 0.00543
	loss_policy_3: 0.03383
	accuracy_policy_3: 0.61273
	loss_value_3: 0.03657
	loss_reward_3: 0.00596
	loss_policy_4: 0.03688
	accuracy_policy_4: 0.58215
	loss_value_4: 0.03762
	loss_reward_4: 0.00619
	loss_policy_5: 0.03969
	accuracy_policy_5: 0.55891
	loss_value_5: 0.03855
	loss_reward_5: 0.00712
	loss_policy: 0.24699
	loss_value: 0.34265
	loss_reward: 0.03039
[2024-05-05 16:16:26] nn step 54350, lr: 0.005.
	loss_policy_0: 0.07837
	accuracy_policy_0: 0.79125
	loss_value_0: 0.17146
	loss_policy_1: 0.02692
	accuracy_policy_1: 0.69285
	loss_value_1: 0.03604
	loss_reward_1: 0.00623
	loss_policy_2: 0.03217
	accuracy_policy_2: 0.64414
	loss_value_2: 0.03748
	loss_reward_2: 0.00608
	loss_policy_3: 0.03574
	accuracy_policy_3: 0.61746
	loss_value_3: 0.03866
	loss_reward_3: 0.00636
	loss_policy_4: 0.03919
	accuracy_policy_4: 0.58461
	loss_value_4: 0.03984
	loss_reward_4: 0.00705
	loss_policy_5: 0.04196
	accuracy_policy_5: 0.56156
	loss_value_5: 0.041
	loss_reward_5: 0.0078
	loss_policy: 0.25434
	loss_value: 0.36448
	loss_reward: 0.03354
[2024-05-05 16:16:41] nn step 54400, lr: 0.005.
	loss_policy_0: 0.07294
	accuracy_policy_0: 0.80348
	loss_value_0: 0.16988
	loss_policy_1: 0.02661
	accuracy_policy_1: 0.69285
	loss_value_1: 0.03543
	loss_reward_1: 0.00643
	loss_policy_2: 0.03087
	accuracy_policy_2: 0.6518
	loss_value_2: 0.03698
	loss_reward_2: 0.00599
	loss_policy_3: 0.03426
	accuracy_policy_3: 0.6266
	loss_value_3: 0.038
	loss_reward_3: 0.00637
	loss_policy_4: 0.03751
	accuracy_policy_4: 0.59672
	loss_value_4: 0.0391
	loss_reward_4: 0.00681
	loss_policy_5: 0.04044
	accuracy_policy_5: 0.56871
	loss_value_5: 0.04034
	loss_reward_5: 0.00766
	loss_policy: 0.24263
	loss_value: 0.35974
	loss_reward: 0.03327
Optimization_Done 54400
[2024-05-05 16:18:44] [command] train weight_iter_54400.pkl 272 273
[2024-05-05 16:19:01] nn step 54450, lr: 0.005.
	loss_policy_0: 0.09509
	accuracy_policy_0: 0.76586
	loss_value_0: 0.18837
	loss_policy_1: 0.02981
	accuracy_policy_1: 0.66918
	loss_value_1: 0.0397
	loss_reward_1: 0.00676
	loss_policy_2: 0.03476
	accuracy_policy_2: 0.62164
	loss_value_2: 0.04129
	loss_reward_2: 0.00687
	loss_policy_3: 0.03878
	accuracy_policy_3: 0.58395
	loss_value_3: 0.04283
	loss_reward_3: 0.00723
	loss_policy_4: 0.04219
	accuracy_policy_4: 0.5552
	loss_value_4: 0.04429
	loss_reward_4: 0.00764
	loss_policy_5: 0.04514
	accuracy_policy_5: 0.52961
	loss_value_5: 0.04577
	loss_reward_5: 0.00861
	loss_policy: 0.28577
	loss_value: 0.40224
	loss_reward: 0.03711
[2024-05-05 16:19:17] nn step 54500, lr: 0.005.
	loss_policy_0: 0.08378
	accuracy_policy_0: 0.79617
	loss_value_0: 0.19559
	loss_policy_1: 0.02959
	accuracy_policy_1: 0.68785
	loss_value_1: 0.04118
	loss_reward_1: 0.00744
	loss_policy_2: 0.03511
	accuracy_policy_2: 0.64363
	loss_value_2: 0.04278
	loss_reward_2: 0.00712
	loss_policy_3: 0.03895
	accuracy_policy_3: 0.61457
	loss_value_3: 0.04418
	loss_reward_3: 0.00769
	loss_policy_4: 0.04316
	accuracy_policy_4: 0.5832
	loss_value_4: 0.04555
	loss_reward_4: 0.00798
	loss_policy_5: 0.04671
	accuracy_policy_5: 0.55254
	loss_value_5: 0.04723
	loss_reward_5: 0.00918
	loss_policy: 0.27731
	loss_value: 0.41651
	loss_reward: 0.03941
[2024-05-05 16:19:33] nn step 54550, lr: 0.005.
	loss_policy_0: 0.07351
	accuracy_policy_0: 0.80969
	loss_value_0: 0.18583
	loss_policy_1: 0.02749
	accuracy_policy_1: 0.69664
	loss_value_1: 0.03929
	loss_reward_1: 0.00699
	loss_policy_2: 0.03304
	accuracy_policy_2: 0.64402
	loss_value_2: 0.04093
	loss_reward_2: 0.00661
	loss_policy_3: 0.03704
	accuracy_policy_3: 0.61148
	loss_value_3: 0.04266
	loss_reward_3: 0.00733
	loss_policy_4: 0.04068
	accuracy_policy_4: 0.58625
	loss_value_4: 0.04405
	loss_reward_4: 0.00788
	loss_policy_5: 0.04359
	accuracy_policy_5: 0.56219
	loss_value_5: 0.04563
	loss_reward_5: 0.00893
	loss_policy: 0.25535
	loss_value: 0.39839
	loss_reward: 0.03774
[2024-05-05 16:19:49] nn step 54600, lr: 0.005.
	loss_policy_0: 0.0734
	accuracy_policy_0: 0.81309
	loss_value_0: 0.19011
	loss_policy_1: 0.02818
	accuracy_policy_1: 0.6975
	loss_value_1: 0.04032
	loss_reward_1: 0.00699
	loss_policy_2: 0.03327
	accuracy_policy_2: 0.64883
	loss_value_2: 0.04208
	loss_reward_2: 0.00681
	loss_policy_3: 0.03727
	accuracy_policy_3: 0.61641
	loss_value_3: 0.04347
	loss_reward_3: 0.00765
	loss_policy_4: 0.04048
	accuracy_policy_4: 0.59383
	loss_value_4: 0.04484
	loss_reward_4: 0.00798
	loss_policy_5: 0.04392
	accuracy_policy_5: 0.56684
	loss_value_5: 0.04622
	loss_reward_5: 0.0091
	loss_policy: 0.25651
	loss_value: 0.40704
	loss_reward: 0.03854
Optimization_Done 54600
[2024-05-05 16:21:41] [command] train weight_iter_54600.pkl 273 274
[2024-05-05 16:21:58] nn step 54650, lr: 0.005.
	loss_policy_0: 0.09812
	accuracy_policy_0: 0.77734
	loss_value_0: 0.17683
	loss_policy_1: 0.03039
	accuracy_policy_1: 0.68965
	loss_value_1: 0.03692
	loss_reward_1: 0.00727
	loss_policy_2: 0.03536
	accuracy_policy_2: 0.64297
	loss_value_2: 0.03854
	loss_reward_2: 0.00679
	loss_policy_3: 0.03942
	accuracy_policy_3: 0.61539
	loss_value_3: 0.03986
	loss_reward_3: 0.00755
	loss_policy_4: 0.04279
	accuracy_policy_4: 0.58355
	loss_value_4: 0.04112
	loss_reward_4: 0.00797
	loss_policy_5: 0.04648
	accuracy_policy_5: 0.55508
	loss_value_5: 0.04244
	loss_reward_5: 0.00903
	loss_policy: 0.29257
	loss_value: 0.37572
	loss_reward: 0.03861
[2024-05-05 16:22:14] nn step 54700, lr: 0.005.
	loss_policy_0: 0.07261
	accuracy_policy_0: 0.82129
	loss_value_0: 0.17209
	loss_policy_1: 0.02657
	accuracy_policy_1: 0.71551
	loss_value_1: 0.03611
	loss_reward_1: 0.00719
	loss_policy_2: 0.03149
	accuracy_policy_2: 0.67355
	loss_value_2: 0.03761
	loss_reward_2: 0.00662
	loss_policy_3: 0.03574
	accuracy_policy_3: 0.64371
	loss_value_3: 0.03898
	loss_reward_3: 0.00746
	loss_policy_4: 0.0388
	accuracy_policy_4: 0.62051
	loss_value_4: 0.04041
	loss_reward_4: 0.00769
	loss_policy_5: 0.04179
	accuracy_policy_5: 0.59375
	loss_value_5: 0.04189
	loss_reward_5: 0.00906
	loss_policy: 0.24701
	loss_value: 0.36709
	loss_reward: 0.03803
[2024-05-05 16:22:30] nn step 54750, lr: 0.005.
	loss_policy_0: 0.06751
	accuracy_policy_0: 0.82953
	loss_value_0: 0.17561
	loss_policy_1: 0.0258
	accuracy_policy_1: 0.72512
	loss_value_1: 0.03701
	loss_reward_1: 0.0073
	loss_policy_2: 0.03092
	accuracy_policy_2: 0.67934
	loss_value_2: 0.03857
	loss_reward_2: 0.00677
	loss_policy_3: 0.03526
	accuracy_policy_3: 0.64859
	loss_value_3: 0.03989
	loss_reward_3: 0.00749
	loss_policy_4: 0.03848
	accuracy_policy_4: 0.62668
	loss_value_4: 0.04107
	loss_reward_4: 0.00817
	loss_policy_5: 0.04163
	accuracy_policy_5: 0.60504
	loss_value_5: 0.04249
	loss_reward_5: 0.00902
	loss_policy: 0.2396
	loss_value: 0.37463
	loss_reward: 0.03875
[2024-05-05 16:22:46] nn step 54800, lr: 0.005.
	loss_policy_0: 0.06594
	accuracy_policy_0: 0.83445
	loss_value_0: 0.17614
	loss_policy_1: 0.02606
	accuracy_policy_1: 0.72844
	loss_value_1: 0.03716
	loss_reward_1: 0.00731
	loss_policy_2: 0.03082
	accuracy_policy_2: 0.68102
	loss_value_2: 0.03862
	loss_reward_2: 0.00704
	loss_policy_3: 0.03487
	accuracy_policy_3: 0.65105
	loss_value_3: 0.04008
	loss_reward_3: 0.00769
	loss_policy_4: 0.03862
	accuracy_policy_4: 0.62488
	loss_value_4: 0.04165
	loss_reward_4: 0.00821
	loss_policy_5: 0.04153
	accuracy_policy_5: 0.60473
	loss_value_5: 0.04318
	loss_reward_5: 0.00948
	loss_policy: 0.23784
	loss_value: 0.37684
	loss_reward: 0.03972
Optimization_Done 54800
[2024-05-05 16:24:36] [command] train weight_iter_54800.pkl 274 275
[2024-05-05 16:24:53] nn step 54850, lr: 0.005.
	loss_policy_0: 0.10171
	accuracy_policy_0: 0.76961
	loss_value_0: 0.16923
	loss_policy_1: 0.03003
	accuracy_policy_1: 0.68965
	loss_value_1: 0.03556
	loss_reward_1: 0.00606
	loss_policy_2: 0.03396
	accuracy_policy_2: 0.64555
	loss_value_2: 0.03689
	loss_reward_2: 0.00563
	loss_policy_3: 0.03726
	accuracy_policy_3: 0.61781
	loss_value_3: 0.03825
	loss_reward_3: 0.00613
	loss_policy_4: 0.04018
	accuracy_policy_4: 0.59168
	loss_value_4: 0.03917
	loss_reward_4: 0.00649
	loss_policy_5: 0.04294
	accuracy_policy_5: 0.5666
	loss_value_5: 0.04024
	loss_reward_5: 0.00738
	loss_policy: 0.28608
	loss_value: 0.35934
	loss_reward: 0.0317
[2024-05-05 16:25:09] nn step 54900, lr: 0.005.
	loss_policy_0: 0.079
	accuracy_policy_0: 0.81324
	loss_value_0: 0.16868
	loss_policy_1: 0.0274
	accuracy_policy_1: 0.71461
	loss_value_1: 0.03565
	loss_reward_1: 0.00603
	loss_policy_2: 0.03187
	accuracy_policy_2: 0.67742
	loss_value_2: 0.03687
	loss_reward_2: 0.00571
	loss_policy_3: 0.03499
	accuracy_policy_3: 0.64641
	loss_value_3: 0.03777
	loss_reward_3: 0.00633
	loss_policy_4: 0.03812
	accuracy_policy_4: 0.62285
	loss_value_4: 0.03876
	loss_reward_4: 0.00686
	loss_policy_5: 0.04106
	accuracy_policy_5: 0.59789
	loss_value_5: 0.03982
	loss_reward_5: 0.00797
	loss_policy: 0.25244
	loss_value: 0.35755
	loss_reward: 0.0329
[2024-05-05 16:25:25] nn step 54950, lr: 0.005.
	loss_policy_0: 0.07666
	accuracy_policy_0: 0.8282
	loss_value_0: 0.17689
	loss_policy_1: 0.02743
	accuracy_policy_1: 0.73352
	loss_value_1: 0.03711
	loss_reward_1: 0.00671
	loss_policy_2: 0.03259
	accuracy_policy_2: 0.68844
	loss_value_2: 0.03847
	loss_reward_2: 0.00615
	loss_policy_3: 0.0361
	accuracy_policy_3: 0.65605
	loss_value_3: 0.03956
	loss_reward_3: 0.00689
	loss_policy_4: 0.03916
	accuracy_policy_4: 0.63254
	loss_value_4: 0.04056
	loss_reward_4: 0.00716
	loss_policy_5: 0.0419
	accuracy_policy_5: 0.61102
	loss_value_5: 0.04169
	loss_reward_5: 0.00823
	loss_policy: 0.25385
	loss_value: 0.37429
	loss_reward: 0.03513
[2024-05-05 16:25:41] nn step 55000, lr: 0.005.
	loss_policy_0: 0.06943
	accuracy_policy_0: 0.83477
	loss_value_0: 0.16815
	loss_policy_1: 0.02589
	accuracy_policy_1: 0.73152
	loss_value_1: 0.03542
	loss_reward_1: 0.00609
	loss_policy_2: 0.03054
	accuracy_policy_2: 0.68848
	loss_value_2: 0.03664
	loss_reward_2: 0.00598
	loss_policy_3: 0.03415
	accuracy_policy_3: 0.65566
	loss_value_3: 0.03795
	loss_reward_3: 0.00642
	loss_policy_4: 0.03728
	accuracy_policy_4: 0.63379
	loss_value_4: 0.03897
	loss_reward_4: 0.00691
	loss_policy_5: 0.03958
	accuracy_policy_5: 0.6127
	loss_value_5: 0.03997
	loss_reward_5: 0.00778
	loss_policy: 0.23687
	loss_value: 0.3571
	loss_reward: 0.03318
Optimization_Done 55000
[2024-05-05 16:27:42] [command] train weight_iter_55000.pkl 275 276
[2024-05-05 16:27:59] nn step 55050, lr: 0.005.
	loss_policy_0: 0.12572
	accuracy_policy_0: 0.71594
	loss_value_0: 0.17642
	loss_policy_1: 0.03614
	accuracy_policy_1: 0.61824
	loss_value_1: 0.03716
	loss_reward_1: 0.00629
	loss_policy_2: 0.04081
	accuracy_policy_2: 0.57801
	loss_value_2: 0.03853
	loss_reward_2: 0.00598
	loss_policy_3: 0.0449
	accuracy_policy_3: 0.53836
	loss_value_3: 0.0399
	loss_reward_3: 0.00666
	loss_policy_4: 0.04821
	accuracy_policy_4: 0.51047
	loss_value_4: 0.04124
	loss_reward_4: 0.00707
	loss_policy_5: 0.05129
	accuracy_policy_5: 0.48863
	loss_value_5: 0.04245
	loss_reward_5: 0.00809
	loss_policy: 0.34705
	loss_value: 0.37571
	loss_reward: 0.0341
[2024-05-05 16:28:15] nn step 55100, lr: 0.005.
	loss_policy_0: 0.09372
	accuracy_policy_0: 0.76602
	loss_value_0: 0.1695
	loss_policy_1: 0.03119
	accuracy_policy_1: 0.66145
	loss_value_1: 0.03562
	loss_reward_1: 0.0059
	loss_policy_2: 0.03606
	accuracy_policy_2: 0.62078
	loss_value_2: 0.03708
	loss_reward_2: 0.0058
	loss_policy_3: 0.03975
	accuracy_policy_3: 0.58445
	loss_value_3: 0.03828
	loss_reward_3: 0.0062
	loss_policy_4: 0.0433
	accuracy_policy_4: 0.55723
	loss_value_4: 0.0394
	loss_reward_4: 0.00679
	loss_policy_5: 0.04625
	accuracy_policy_5: 0.53203
	loss_value_5: 0.04054
	loss_reward_5: 0.00775
	loss_policy: 0.29027
	loss_value: 0.36041
	loss_reward: 0.03243
[2024-05-05 16:28:31] nn step 55150, lr: 0.005.
	loss_policy_0: 0.08544
	accuracy_policy_0: 0.78789
	loss_value_0: 0.16889
	loss_policy_1: 0.02997
	accuracy_policy_1: 0.67523
	loss_value_1: 0.03562
	loss_reward_1: 0.00596
	loss_policy_2: 0.03511
	accuracy_policy_2: 0.63418
	loss_value_2: 0.03696
	loss_reward_2: 0.00594
	loss_policy_3: 0.03893
	accuracy_policy_3: 0.59855
	loss_value_3: 0.03816
	loss_reward_3: 0.00598
	loss_policy_4: 0.04237
	accuracy_policy_4: 0.56973
	loss_value_4: 0.0394
	loss_reward_4: 0.00667
	loss_policy_5: 0.0451
	accuracy_policy_5: 0.54367
	loss_value_5: 0.04064
	loss_reward_5: 0.00779
	loss_policy: 0.27693
	loss_value: 0.35966
	loss_reward: 0.03233
[2024-05-05 16:28:47] nn step 55200, lr: 0.005.
	loss_policy_0: 0.08292
	accuracy_policy_0: 0.79449
	loss_value_0: 0.16841
	loss_policy_1: 0.0296
	accuracy_policy_1: 0.68039
	loss_value_1: 0.03551
	loss_reward_1: 0.00592
	loss_policy_2: 0.03458
	accuracy_policy_2: 0.63762
	loss_value_2: 0.03693
	loss_reward_2: 0.00593
	loss_policy_3: 0.03912
	accuracy_policy_3: 0.60312
	loss_value_3: 0.03828
	loss_reward_3: 0.00637
	loss_policy_4: 0.04271
	accuracy_policy_4: 0.5691
	loss_value_4: 0.03952
	loss_reward_4: 0.00683
	loss_policy_5: 0.04497
	accuracy_policy_5: 0.55352
	loss_value_5: 0.04059
	loss_reward_5: 0.0078
	loss_policy: 0.2739
	loss_value: 0.35924
	loss_reward: 0.03285
Optimization_Done 55200
[2024-05-05 16:30:49] [command] train weight_iter_55200.pkl 276 277
[2024-05-05 16:31:06] nn step 55250, lr: 0.005.
	loss_policy_0: 0.14326
	accuracy_policy_0: 0.68949
	loss_value_0: 0.18756
	loss_policy_1: 0.04004
	accuracy_policy_1: 0.60336
	loss_value_1: 0.03923
	loss_reward_1: 0.00682
	loss_policy_2: 0.04493
	accuracy_policy_2: 0.5632
	loss_value_2: 0.04102
	loss_reward_2: 0.00651
	loss_policy_3: 0.04876
	accuracy_policy_3: 0.53707
	loss_value_3: 0.04272
	loss_reward_3: 0.00727
	loss_policy_4: 0.05212
	accuracy_policy_4: 0.5182
	loss_value_4: 0.04417
	loss_reward_4: 0.00785
	loss_policy_5: 0.05507
	accuracy_policy_5: 0.4973
	loss_value_5: 0.04553
	loss_reward_5: 0.00865
	loss_policy: 0.38417
	loss_value: 0.40024
	loss_reward: 0.0371
[2024-05-05 16:31:22] nn step 55300, lr: 0.005.
	loss_policy_0: 0.11517
	accuracy_policy_0: 0.73129
	loss_value_0: 0.17523
	loss_policy_1: 0.03517
	accuracy_policy_1: 0.63145
	loss_value_1: 0.03686
	loss_reward_1: 0.00652
	loss_policy_2: 0.04006
	accuracy_policy_2: 0.59641
	loss_value_2: 0.03851
	loss_reward_2: 0.00645
	loss_policy_3: 0.04417
	accuracy_policy_3: 0.5584
	loss_value_3: 0.03998
	loss_reward_3: 0.00688
	loss_policy_4: 0.04764
	accuracy_policy_4: 0.53574
	loss_value_4: 0.04138
	loss_reward_4: 0.00724
	loss_policy_5: 0.05037
	accuracy_policy_5: 0.51738
	loss_value_5: 0.04264
	loss_reward_5: 0.00836
	loss_policy: 0.33258
	loss_value: 0.37462
	loss_reward: 0.03545
[2024-05-05 16:31:38] nn step 55350, lr: 0.005.
	loss_policy_0: 0.10773
	accuracy_policy_0: 0.73637
	loss_value_0: 0.17096
	loss_policy_1: 0.03388
	accuracy_policy_1: 0.64066
	loss_value_1: 0.03606
	loss_reward_1: 0.00667
	loss_policy_2: 0.03882
	accuracy_policy_2: 0.59531
	loss_value_2: 0.03749
	loss_reward_2: 0.00626
	loss_policy_3: 0.04205
	accuracy_policy_3: 0.57141
	loss_value_3: 0.03895
	loss_reward_3: 0.00683
	loss_policy_4: 0.04531
	accuracy_policy_4: 0.55125
	loss_value_4: 0.04041
	loss_reward_4: 0.00721
	loss_policy_5: 0.04842
	accuracy_policy_5: 0.52277
	loss_value_5: 0.04168
	loss_reward_5: 0.00839
	loss_policy: 0.31621
	loss_value: 0.36555
	loss_reward: 0.03536
[2024-05-05 16:31:54] nn step 55400, lr: 0.005.
	loss_policy_0: 0.09943
	accuracy_policy_0: 0.74723
	loss_value_0: 0.16523
	loss_policy_1: 0.03182
	accuracy_policy_1: 0.64281
	loss_value_1: 0.03485
	loss_reward_1: 0.00621
	loss_policy_2: 0.03686
	accuracy_policy_2: 0.60457
	loss_value_2: 0.03631
	loss_reward_2: 0.00592
	loss_policy_3: 0.04066
	accuracy_policy_3: 0.57457
	loss_value_3: 0.0379
	loss_reward_3: 0.00662
	loss_policy_4: 0.04359
	accuracy_policy_4: 0.55148
	loss_value_4: 0.03919
	loss_reward_4: 0.00648
	loss_policy_5: 0.04624
	accuracy_policy_5: 0.53523
	loss_value_5: 0.04056
	loss_reward_5: 0.00789
	loss_policy: 0.29859
	loss_value: 0.35404
	loss_reward: 0.03313
Optimization_Done 55400
[2024-05-05 16:33:56] [command] train weight_iter_55400.pkl 277 278
[2024-05-05 16:34:13] nn step 55450, lr: 0.005.
	loss_policy_0: 0.12129
	accuracy_policy_0: 0.75301
	loss_value_0: 0.17798
	loss_policy_1: 0.0354
	accuracy_policy_1: 0.66039
	loss_value_1: 0.03739
	loss_reward_1: 0.00753
	loss_policy_2: 0.03958
	accuracy_policy_2: 0.6248
	loss_value_2: 0.03913
	loss_reward_2: 0.00715
	loss_policy_3: 0.04324
	accuracy_policy_3: 0.59473
	loss_value_3: 0.04092
	loss_reward_3: 0.00773
	loss_policy_4: 0.04647
	accuracy_policy_4: 0.56852
	loss_value_4: 0.04269
	loss_reward_4: 0.00841
	loss_policy_5: 0.04886
	accuracy_policy_5: 0.54656
	loss_value_5: 0.04422
	loss_reward_5: 0.00903
	loss_policy: 0.33484
	loss_value: 0.38232
	loss_reward: 0.03985
[2024-05-05 16:34:29] nn step 55500, lr: 0.005.
	loss_policy_0: 0.09954
	accuracy_policy_0: 0.78
	loss_value_0: 0.17453
	loss_policy_1: 0.03214
	accuracy_policy_1: 0.68699
	loss_value_1: 0.03692
	loss_reward_1: 0.0075
	loss_policy_2: 0.03702
	accuracy_policy_2: 0.64148
	loss_value_2: 0.03864
	loss_reward_2: 0.00714
	loss_policy_3: 0.0408
	accuracy_policy_3: 0.61309
	loss_value_3: 0.04012
	loss_reward_3: 0.00762
	loss_policy_4: 0.04387
	accuracy_policy_4: 0.59145
	loss_value_4: 0.04168
	loss_reward_4: 0.00802
	loss_policy_5: 0.04697
	accuracy_policy_5: 0.56676
	loss_value_5: 0.04328
	loss_reward_5: 0.00917
	loss_policy: 0.30034
	loss_value: 0.37518
	loss_reward: 0.03944
[2024-05-05 16:34:45] nn step 55550, lr: 0.005.
	loss_policy_0: 0.09648
	accuracy_policy_0: 0.78875
	loss_value_0: 0.17791
	loss_policy_1: 0.03236
	accuracy_policy_1: 0.68867
	loss_value_1: 0.03752
	loss_reward_1: 0.00758
	loss_policy_2: 0.03725
	accuracy_policy_2: 0.64504
	loss_value_2: 0.03924
	loss_reward_2: 0.00709
	loss_policy_3: 0.04145
	accuracy_policy_3: 0.60977
	loss_value_3: 0.04076
	loss_reward_3: 0.00766
	loss_policy_4: 0.04414
	accuracy_policy_4: 0.59238
	loss_value_4: 0.0421
	loss_reward_4: 0.00811
	loss_policy_5: 0.0474
	accuracy_policy_5: 0.56516
	loss_value_5: 0.04375
	loss_reward_5: 0.00926
	loss_policy: 0.29909
	loss_value: 0.38126
	loss_reward: 0.0397
[2024-05-05 16:35:01] nn step 55600, lr: 0.005.
	loss_policy_0: 0.08665
	accuracy_policy_0: 0.79242
	loss_value_0: 0.17034
	loss_policy_1: 0.03016
	accuracy_policy_1: 0.68977
	loss_value_1: 0.03614
	loss_reward_1: 0.00737
	loss_policy_2: 0.03495
	accuracy_policy_2: 0.64781
	loss_value_2: 0.03756
	loss_reward_2: 0.00679
	loss_policy_3: 0.03908
	accuracy_policy_3: 0.6184
	loss_value_3: 0.03895
	loss_reward_3: 0.00731
	loss_policy_4: 0.04209
	accuracy_policy_4: 0.59613
	loss_value_4: 0.04055
	loss_reward_4: 0.00798
	loss_policy_5: 0.04436
	accuracy_policy_5: 0.57215
	loss_value_5: 0.04192
	loss_reward_5: 0.00893
	loss_policy: 0.27729
	loss_value: 0.36546
	loss_reward: 0.03837
Optimization_Done 55600
[2024-05-05 16:36:56] [command] train weight_iter_55600.pkl 278 279
[2024-05-05 16:37:13] nn step 55650, lr: 0.005.
	loss_policy_0: 0.12491
	accuracy_policy_0: 0.73965
	loss_value_0: 0.16824
	loss_policy_1: 0.03386
	accuracy_policy_1: 0.65895
	loss_value_1: 0.03542
	loss_reward_1: 0.00642
	loss_policy_2: 0.03722
	accuracy_policy_2: 0.63129
	loss_value_2: 0.03693
	loss_reward_2: 0.00574
	loss_policy_3: 0.04045
	accuracy_policy_3: 0.5993
	loss_value_3: 0.03828
	loss_reward_3: 0.00624
	loss_policy_4: 0.04309
	accuracy_policy_4: 0.57883
	loss_value_4: 0.0395
	loss_reward_4: 0.00695
	loss_policy_5: 0.04536
	accuracy_policy_5: 0.55645
	loss_value_5: 0.04098
	loss_reward_5: 0.00759
	loss_policy: 0.3249
	loss_value: 0.35936
	loss_reward: 0.03294
[2024-05-05 16:37:29] nn step 55700, lr: 0.005.
	loss_policy_0: 0.08872
	accuracy_policy_0: 0.77324
	loss_value_0: 0.14475
	loss_policy_1: 0.02721
	accuracy_policy_1: 0.68383
	loss_value_1: 0.03042
	loss_reward_1: 0.00563
	loss_policy_2: 0.03115
	accuracy_policy_2: 0.64703
	loss_value_2: 0.03173
	loss_reward_2: 0.00511
	loss_policy_3: 0.03357
	accuracy_policy_3: 0.61645
	loss_value_3: 0.03292
	loss_reward_3: 0.0057
	loss_policy_4: 0.03569
	accuracy_policy_4: 0.60035
	loss_value_4: 0.03403
	loss_reward_4: 0.00608
	loss_policy_5: 0.03785
	accuracy_policy_5: 0.57656
	loss_value_5: 0.03516
	loss_reward_5: 0.00684
	loss_policy: 0.25419
	loss_value: 0.30901
	loss_reward: 0.02935
[2024-05-05 16:37:45] nn step 55750, lr: 0.005.
	loss_policy_0: 0.0916
	accuracy_policy_0: 0.78828
	loss_value_0: 0.16068
	loss_policy_1: 0.02938
	accuracy_policy_1: 0.69145
	loss_value_1: 0.03363
	loss_reward_1: 0.00626
	loss_policy_2: 0.03305
	accuracy_policy_2: 0.65598
	loss_value_2: 0.03503
	loss_reward_2: 0.00579
	loss_policy_3: 0.03614
	accuracy_policy_3: 0.62891
	loss_value_3: 0.03627
	loss_reward_3: 0.00616
	loss_policy_4: 0.0389
	accuracy_policy_4: 0.60875
	loss_value_4: 0.03759
	loss_reward_4: 0.00662
	loss_policy_5: 0.04115
	accuracy_policy_5: 0.5859
	loss_value_5: 0.03888
	loss_reward_5: 0.00771
	loss_policy: 0.27023
	loss_value: 0.34209
	loss_reward: 0.03254
[2024-05-05 16:38:01] nn step 55800, lr: 0.005.
	loss_policy_0: 0.08537
	accuracy_policy_0: 0.79988
	loss_value_0: 0.15943
	loss_policy_1: 0.02831
	accuracy_policy_1: 0.6941
	loss_value_1: 0.03361
	loss_reward_1: 0.00621
	loss_policy_2: 0.03233
	accuracy_policy_2: 0.65641
	loss_value_2: 0.03501
	loss_reward_2: 0.0056
	loss_policy_3: 0.03525
	accuracy_policy_3: 0.63031
	loss_value_3: 0.0364
	loss_reward_3: 0.00619
	loss_policy_4: 0.03855
	accuracy_policy_4: 0.60742
	loss_value_4: 0.0374
	loss_reward_4: 0.00668
	loss_policy_5: 0.04024
	accuracy_policy_5: 0.58543
	loss_value_5: 0.0385
	loss_reward_5: 0.0076
	loss_policy: 0.26005
	loss_value: 0.34034
	loss_reward: 0.03227
Optimization_Done 55800
[2024-05-05 16:40:09] [command] train weight_iter_55800.pkl 279 280
[2024-05-05 16:40:27] nn step 55850, lr: 0.005.
	loss_policy_0: 0.11825
	accuracy_policy_0: 0.71922
	loss_value_0: 0.14653
	loss_policy_1: 0.03275
	accuracy_policy_1: 0.63086
	loss_value_1: 0.03069
	loss_reward_1: 0.00491
	loss_policy_2: 0.03658
	accuracy_policy_2: 0.59379
	loss_value_2: 0.03227
	loss_reward_2: 0.00484
	loss_policy_3: 0.03955
	accuracy_policy_3: 0.56531
	loss_value_3: 0.03362
	loss_reward_3: 0.00524
	loss_policy_4: 0.04238
	accuracy_policy_4: 0.53535
	loss_value_4: 0.0349
	loss_reward_4: 0.00573
	loss_policy_5: 0.04507
	accuracy_policy_5: 0.50938
	loss_value_5: 0.03633
	loss_reward_5: 0.00637
	loss_policy: 0.31458
	loss_value: 0.31433
	loss_reward: 0.0271
[2024-05-05 16:40:43] nn step 55900, lr: 0.005.
	loss_policy_0: 0.0959
	accuracy_policy_0: 0.76762
	loss_value_0: 0.15606
	loss_policy_1: 0.03088
	accuracy_policy_1: 0.66848
	loss_value_1: 0.03266
	loss_reward_1: 0.00547
	loss_policy_2: 0.03576
	accuracy_policy_2: 0.62535
	loss_value_2: 0.03431
	loss_reward_2: 0.00515
	loss_policy_3: 0.03861
	accuracy_policy_3: 0.59961
	loss_value_3: 0.03563
	loss_reward_3: 0.00569
	loss_policy_4: 0.0417
	accuracy_policy_4: 0.57496
	loss_value_4: 0.03671
	loss_reward_4: 0.00599
	loss_policy_5: 0.04462
	accuracy_policy_5: 0.55129
	loss_value_5: 0.03795
	loss_reward_5: 0.00674
	loss_policy: 0.28746
	loss_value: 0.33332
	loss_reward: 0.02904
[2024-05-05 16:40:59] nn step 55950, lr: 0.005.
	loss_policy_0: 0.08487
	accuracy_policy_0: 0.78352
	loss_value_0: 0.15104
	loss_policy_1: 0.02882
	accuracy_policy_1: 0.68109
	loss_value_1: 0.03169
	loss_reward_1: 0.00525
	loss_policy_2: 0.03309
	accuracy_policy_2: 0.645
	loss_value_2: 0.0331
	loss_reward_2: 0.00481
	loss_policy_3: 0.03633
	accuracy_policy_3: 0.61809
	loss_value_3: 0.03447
	loss_reward_3: 0.00528
	loss_policy_4: 0.04
	accuracy_policy_4: 0.58762
	loss_value_4: 0.03587
	loss_reward_4: 0.00594
	loss_policy_5: 0.04267
	accuracy_policy_5: 0.56613
	loss_value_5: 0.03737
	loss_reward_5: 0.00684
	loss_policy: 0.26578
	loss_value: 0.32354
	loss_reward: 0.02812
[2024-05-05 16:41:14] nn step 56000, lr: 0.005.
	loss_policy_0: 0.08298
	accuracy_policy_0: 0.78773
	loss_value_0: 0.15271
	loss_policy_1: 0.02823
	accuracy_policy_1: 0.68641
	loss_value_1: 0.03208
	loss_reward_1: 0.00539
	loss_policy_2: 0.03287
	accuracy_policy_2: 0.64961
	loss_value_2: 0.03355
	loss_reward_2: 0.0051
	loss_policy_3: 0.03616
	accuracy_policy_3: 0.62062
	loss_value_3: 0.03489
	loss_reward_3: 0.00531
	loss_policy_4: 0.03939
	accuracy_policy_4: 0.59473
	loss_value_4: 0.036
	loss_reward_4: 0.00581
	loss_policy_5: 0.04239
	accuracy_policy_5: 0.57082
	loss_value_5: 0.0372
	loss_reward_5: 0.00668
	loss_policy: 0.26202
	loss_value: 0.32643
	loss_reward: 0.02829
Optimization_Done 56000
[2024-05-05 16:43:10] [command] train weight_iter_56000.pkl 280 281
[2024-05-05 16:43:27] nn step 56050, lr: 0.005.
	loss_policy_0: 0.13589
	accuracy_policy_0: 0.67758
	loss_value_0: 0.16249
	loss_policy_1: 0.03506
	accuracy_policy_1: 0.6123
	loss_value_1: 0.03375
	loss_reward_1: 0.00503
	loss_policy_2: 0.03903
	accuracy_policy_2: 0.57965
	loss_value_2: 0.03539
	loss_reward_2: 0.00481
	loss_policy_3: 0.04259
	accuracy_policy_3: 0.55109
	loss_value_3: 0.037
	loss_reward_3: 0.00531
	loss_policy_4: 0.04483
	accuracy_policy_4: 0.5282
	loss_value_4: 0.03859
	loss_reward_4: 0.00573
	loss_policy_5: 0.04731
	accuracy_policy_5: 0.50371
	loss_value_5: 0.03997
	loss_reward_5: 0.00639
	loss_policy: 0.34471
	loss_value: 0.34718
	loss_reward: 0.02726
[2024-05-05 16:43:43] nn step 56100, lr: 0.005.
	loss_policy_0: 0.11205
	accuracy_policy_0: 0.75141
	loss_value_0: 0.17265
	loss_policy_1: 0.03355
	accuracy_policy_1: 0.66016
	loss_value_1: 0.03642
	loss_reward_1: 0.00551
	loss_policy_2: 0.03936
	accuracy_policy_2: 0.61871
	loss_value_2: 0.03821
	loss_reward_2: 0.00535
	loss_policy_3: 0.04355
	accuracy_policy_3: 0.58684
	loss_value_3: 0.03996
	loss_reward_3: 0.0059
	loss_policy_4: 0.04704
	accuracy_policy_4: 0.56547
	loss_value_4: 0.04167
	loss_reward_4: 0.00626
	loss_policy_5: 0.04952
	accuracy_policy_5: 0.54262
	loss_value_5: 0.04313
	loss_reward_5: 0.00714
	loss_policy: 0.32508
	loss_value: 0.37205
	loss_reward: 0.03015
[2024-05-05 16:43:59] nn step 56150, lr: 0.005.
	loss_policy_0: 0.09942
	accuracy_policy_0: 0.76512
	loss_value_0: 0.16667
	loss_policy_1: 0.03142
	accuracy_policy_1: 0.66961
	loss_value_1: 0.035
	loss_reward_1: 0.00545
	loss_policy_2: 0.03655
	accuracy_policy_2: 0.63195
	loss_value_2: 0.03673
	loss_reward_2: 0.00531
	loss_policy_3: 0.04031
	accuracy_policy_3: 0.59664
	loss_value_3: 0.03836
	loss_reward_3: 0.00574
	loss_policy_4: 0.04366
	accuracy_policy_4: 0.57023
	loss_value_4: 0.03984
	loss_reward_4: 0.00605
	loss_policy_5: 0.0465
	accuracy_policy_5: 0.55074
	loss_value_5: 0.04133
	loss_reward_5: 0.00694
	loss_policy: 0.29786
	loss_value: 0.35793
	loss_reward: 0.02949
[2024-05-05 16:44:15] nn step 56200, lr: 0.005.
	loss_policy_0: 0.09604
	accuracy_policy_0: 0.77551
	loss_value_0: 0.17153
	loss_policy_1: 0.03117
	accuracy_policy_1: 0.67617
	loss_value_1: 0.03627
	loss_reward_1: 0.00547
	loss_policy_2: 0.03659
	accuracy_policy_2: 0.63578
	loss_value_2: 0.03777
	loss_reward_2: 0.00531
	loss_policy_3: 0.04029
	accuracy_policy_3: 0.60879
	loss_value_3: 0.03942
	loss_reward_3: 0.00585
	loss_policy_4: 0.04371
	accuracy_policy_4: 0.57992
	loss_value_4: 0.04093
	loss_reward_4: 0.00587
	loss_policy_5: 0.04664
	accuracy_policy_5: 0.55148
	loss_value_5: 0.04263
	loss_reward_5: 0.0071
	loss_policy: 0.29443
	loss_value: 0.36855
	loss_reward: 0.02961
Optimization_Done 56200
[2024-05-05 16:46:15] [command] train weight_iter_56200.pkl 281 282
[2024-05-05 16:46:32] nn step 56250, lr: 0.005.
	loss_policy_0: 0.13614
	accuracy_policy_0: 0.70801
	loss_value_0: 0.16873
	loss_policy_1: 0.03728
	accuracy_policy_1: 0.63121
	loss_value_1: 0.03592
	loss_reward_1: 0.00648
	loss_policy_2: 0.04142
	accuracy_policy_2: 0.60422
	loss_value_2: 0.03781
	loss_reward_2: 0.00613
	loss_policy_3: 0.04521
	accuracy_policy_3: 0.57473
	loss_value_3: 0.0394
	loss_reward_3: 0.00665
	loss_policy_4: 0.04862
	accuracy_policy_4: 0.55676
	loss_value_4: 0.04087
	loss_reward_4: 0.00677
	loss_policy_5: 0.05188
	accuracy_policy_5: 0.53676
	loss_value_5: 0.04231
	loss_reward_5: 0.00779
	loss_policy: 0.36055
	loss_value: 0.36504
	loss_reward: 0.03381
[2024-05-05 16:46:48] nn step 56300, lr: 0.005.
	loss_policy_0: 0.11162
	accuracy_policy_0: 0.7466
	loss_value_0: 0.16391
	loss_policy_1: 0.03311
	accuracy_policy_1: 0.66301
	loss_value_1: 0.03483
	loss_reward_1: 0.00619
	loss_policy_2: 0.03851
	accuracy_policy_2: 0.62508
	loss_value_2: 0.03661
	loss_reward_2: 0.00591
	loss_policy_3: 0.04228
	accuracy_policy_3: 0.59863
	loss_value_3: 0.03809
	loss_reward_3: 0.00629
	loss_policy_4: 0.0449
	accuracy_policy_4: 0.58102
	loss_value_4: 0.03963
	loss_reward_4: 0.00692
	loss_policy_5: 0.04804
	accuracy_policy_5: 0.56094
	loss_value_5: 0.04114
	loss_reward_5: 0.00779
	loss_policy: 0.31845
	loss_value: 0.35422
	loss_reward: 0.03308
[2024-05-05 16:47:04] nn step 56350, lr: 0.005.
	loss_policy_0: 0.10229
	accuracy_policy_0: 0.76039
	loss_value_0: 0.162
	loss_policy_1: 0.03154
	accuracy_policy_1: 0.66957
	loss_value_1: 0.03417
	loss_reward_1: 0.00627
	loss_policy_2: 0.03617
	accuracy_policy_2: 0.63766
	loss_value_2: 0.03594
	loss_reward_2: 0.00579
	loss_policy_3: 0.03977
	accuracy_policy_3: 0.61262
	loss_value_3: 0.0375
	loss_reward_3: 0.00621
	loss_policy_4: 0.04268
	accuracy_policy_4: 0.59414
	loss_value_4: 0.03879
	loss_reward_4: 0.00673
	loss_policy_5: 0.04617
	accuracy_policy_5: 0.57145
	loss_value_5: 0.04023
	loss_reward_5: 0.00761
	loss_policy: 0.29862
	loss_value: 0.34864
	loss_reward: 0.03262
[2024-05-05 16:47:20] nn step 56400, lr: 0.005.
	loss_policy_0: 0.10125
	accuracy_policy_0: 0.76938
	loss_value_0: 0.16952
	loss_policy_1: 0.03256
	accuracy_policy_1: 0.67082
	loss_value_1: 0.0358
	loss_reward_1: 0.00631
	loss_policy_2: 0.03744
	accuracy_policy_2: 0.64254
	loss_value_2: 0.03753
	loss_reward_2: 0.006
	loss_policy_3: 0.0415
	accuracy_policy_3: 0.60992
	loss_value_3: 0.03905
	loss_reward_3: 0.00648
	loss_policy_4: 0.04444
	accuracy_policy_4: 0.58934
	loss_value_4: 0.04062
	loss_reward_4: 0.00665
	loss_policy_5: 0.04774
	accuracy_policy_5: 0.57152
	loss_value_5: 0.04237
	loss_reward_5: 0.00813
	loss_policy: 0.30493
	loss_value: 0.3649
	loss_reward: 0.03356
Optimization_Done 56400
[2024-05-05 16:49:17] [command] train weight_iter_56400.pkl 282 283
[2024-05-05 16:49:34] nn step 56450, lr: 0.005.
	loss_policy_0: 0.13421
	accuracy_policy_0: 0.70949
	loss_value_0: 0.17143
	loss_policy_1: 0.03631
	accuracy_policy_1: 0.6368
	loss_value_1: 0.03656
	loss_reward_1: 0.00613
	loss_policy_2: 0.0404
	accuracy_policy_2: 0.60613
	loss_value_2: 0.0383
	loss_reward_2: 0.00576
	loss_policy_3: 0.04445
	accuracy_policy_3: 0.58039
	loss_value_3: 0.03988
	loss_reward_3: 0.00613
	loss_policy_4: 0.04747
	accuracy_policy_4: 0.55574
	loss_value_4: 0.04142
	loss_reward_4: 0.00651
	loss_policy_5: 0.05034
	accuracy_policy_5: 0.54301
	loss_value_5: 0.0426
	loss_reward_5: 0.00743
	loss_policy: 0.35318
	loss_value: 0.37018
	loss_reward: 0.03197
[2024-05-05 16:49:50] nn step 56500, lr: 0.005.
	loss_policy_0: 0.09765
	accuracy_policy_0: 0.75457
	loss_value_0: 0.15445
	loss_policy_1: 0.02961
	accuracy_policy_1: 0.6775
	loss_value_1: 0.0327
	loss_reward_1: 0.00556
	loss_policy_2: 0.0345
	accuracy_policy_2: 0.63426
	loss_value_2: 0.03424
	loss_reward_2: 0.00525
	loss_policy_3: 0.03777
	accuracy_policy_3: 0.60988
	loss_value_3: 0.03555
	loss_reward_3: 0.00568
	loss_policy_4: 0.04024
	accuracy_policy_4: 0.58844
	loss_value_4: 0.03684
	loss_reward_4: 0.00594
	loss_policy_5: 0.04291
	accuracy_policy_5: 0.57477
	loss_value_5: 0.03798
	loss_reward_5: 0.00683
	loss_policy: 0.28269
	loss_value: 0.33178
	loss_reward: 0.02926
[2024-05-05 16:50:06] nn step 56550, lr: 0.005.
	loss_policy_0: 0.09483
	accuracy_policy_0: 0.77332
	loss_value_0: 0.16077
	loss_policy_1: 0.0305
	accuracy_policy_1: 0.68453
	loss_value_1: 0.03423
	loss_reward_1: 0.00607
	loss_policy_2: 0.03478
	accuracy_policy_2: 0.64539
	loss_value_2: 0.03568
	loss_reward_2: 0.00545
	loss_policy_3: 0.03823
	accuracy_policy_3: 0.62141
	loss_value_3: 0.03711
	loss_reward_3: 0.00592
	loss_policy_4: 0.04174
	accuracy_policy_4: 0.59551
	loss_value_4: 0.03841
	loss_reward_4: 0.00643
	loss_policy_5: 0.04461
	accuracy_policy_5: 0.57574
	loss_value_5: 0.03976
	loss_reward_5: 0.00738
	loss_policy: 0.28468
	loss_value: 0.34595
	loss_reward: 0.03125
[2024-05-05 16:50:22] nn step 56600, lr: 0.005.
	loss_policy_0: 0.0905
	accuracy_policy_0: 0.78012
	loss_value_0: 0.16071
	loss_policy_1: 0.02977
	accuracy_policy_1: 0.68289
	loss_value_1: 0.03396
	loss_reward_1: 0.0059
	loss_policy_2: 0.03403
	accuracy_policy_2: 0.64977
	loss_value_2: 0.03548
	loss_reward_2: 0.00535
	loss_policy_3: 0.03751
	accuracy_policy_3: 0.62375
	loss_value_3: 0.03693
	loss_reward_3: 0.00591
	loss_policy_4: 0.04089
	accuracy_policy_4: 0.595
	loss_value_4: 0.03809
	loss_reward_4: 0.00619
	loss_policy_5: 0.04402
	accuracy_policy_5: 0.57617
	loss_value_5: 0.03969
	loss_reward_5: 0.00752
	loss_policy: 0.27673
	loss_value: 0.34485
	loss_reward: 0.03087
Optimization_Done 56600
[2024-05-05 16:52:12] [command] train weight_iter_56600.pkl 283 284
[2024-05-05 16:52:29] nn step 56650, lr: 0.005.
	loss_policy_0: 0.1262
	accuracy_policy_0: 0.73887
	loss_value_0: 0.16923
	loss_policy_1: 0.03377
	accuracy_policy_1: 0.66105
	loss_value_1: 0.03578
	loss_reward_1: 0.00535
	loss_policy_2: 0.03819
	accuracy_policy_2: 0.6225
	loss_value_2: 0.03713
	loss_reward_2: 0.00543
	loss_policy_3: 0.04081
	accuracy_policy_3: 0.59586
	loss_value_3: 0.03824
	loss_reward_3: 0.00589
	loss_policy_4: 0.04388
	accuracy_policy_4: 0.56922
	loss_value_4: 0.03928
	loss_reward_4: 0.00622
	loss_policy_5: 0.04578
	accuracy_policy_5: 0.55188
	loss_value_5: 0.04034
	loss_reward_5: 0.00701
	loss_policy: 0.32862
	loss_value: 0.36
	loss_reward: 0.02989
[2024-05-05 16:52:45] nn step 56700, lr: 0.005.
	loss_policy_0: 0.09486
	accuracy_policy_0: 0.78035
	loss_value_0: 0.1678
	loss_policy_1: 0.02974
	accuracy_policy_1: 0.69121
	loss_value_1: 0.03553
	loss_reward_1: 0.00552
	loss_policy_2: 0.03438
	accuracy_policy_2: 0.65426
	loss_value_2: 0.03687
	loss_reward_2: 0.00519
	loss_policy_3: 0.03776
	accuracy_policy_3: 0.62434
	loss_value_3: 0.0379
	loss_reward_3: 0.00574
	loss_policy_4: 0.04064
	accuracy_policy_4: 0.59902
	loss_value_4: 0.03903
	loss_reward_4: 0.00607
	loss_policy_5: 0.04292
	accuracy_policy_5: 0.5773
	loss_value_5: 0.04017
	loss_reward_5: 0.00681
	loss_policy: 0.28029
	loss_value: 0.35731
	loss_reward: 0.02933
[2024-05-05 16:53:01] nn step 56750, lr: 0.005.
	loss_policy_0: 0.08903
	accuracy_policy_0: 0.79406
	loss_value_0: 0.17003
	loss_policy_1: 0.0292
	accuracy_policy_1: 0.69695
	loss_value_1: 0.03566
	loss_reward_1: 0.00548
	loss_policy_2: 0.03346
	accuracy_policy_2: 0.66438
	loss_value_2: 0.03705
	loss_reward_2: 0.00527
	loss_policy_3: 0.03708
	accuracy_policy_3: 0.63004
	loss_value_3: 0.03823
	loss_reward_3: 0.00558
	loss_policy_4: 0.03966
	accuracy_policy_4: 0.60535
	loss_value_4: 0.03926
	loss_reward_4: 0.00611
	loss_policy_5: 0.04222
	accuracy_policy_5: 0.58285
	loss_value_5: 0.04025
	loss_reward_5: 0.00683
	loss_policy: 0.27064
	loss_value: 0.36047
	loss_reward: 0.02927
[2024-05-05 16:53:17] nn step 56800, lr: 0.005.
	loss_policy_0: 0.08498
	accuracy_policy_0: 0.80066
	loss_value_0: 0.16966
	loss_policy_1: 0.02868
	accuracy_policy_1: 0.70492
	loss_value_1: 0.03572
	loss_reward_1: 0.00552
	loss_policy_2: 0.03318
	accuracy_policy_2: 0.66539
	loss_value_2: 0.03694
	loss_reward_2: 0.00527
	loss_policy_3: 0.03661
	accuracy_policy_3: 0.63895
	loss_value_3: 0.03809
	loss_reward_3: 0.00569
	loss_policy_4: 0.0394
	accuracy_policy_4: 0.60641
	loss_value_4: 0.03917
	loss_reward_4: 0.00615
	loss_policy_5: 0.04197
	accuracy_policy_5: 0.58781
	loss_value_5: 0.04001
	loss_reward_5: 0.00711
	loss_policy: 0.26482
	loss_value: 0.35958
	loss_reward: 0.02974
Optimization_Done 56800
[2024-05-05 16:55:06] [command] train weight_iter_56800.pkl 284 285
[2024-05-05 16:55:23] nn step 56850, lr: 0.005.
	loss_policy_0: 0.13789
	accuracy_policy_0: 0.71992
	loss_value_0: 0.17425
	loss_policy_1: 0.03662
	accuracy_policy_1: 0.64414
	loss_value_1: 0.03647
	loss_reward_1: 0.00491
	loss_policy_2: 0.04096
	accuracy_policy_2: 0.60805
	loss_value_2: 0.03774
	loss_reward_2: 0.00483
	loss_policy_3: 0.04438
	accuracy_policy_3: 0.58117
	loss_value_3: 0.03883
	loss_reward_3: 0.00541
	loss_policy_4: 0.0471
	accuracy_policy_4: 0.5534
	loss_value_4: 0.03985
	loss_reward_4: 0.00589
	loss_policy_5: 0.04949
	accuracy_policy_5: 0.53258
	loss_value_5: 0.04095
	loss_reward_5: 0.00643
	loss_policy: 0.35643
	loss_value: 0.36809
	loss_reward: 0.02749
[2024-05-05 16:55:39] nn step 56900, lr: 0.005.
	loss_policy_0: 0.10787
	accuracy_policy_0: 0.76961
	loss_value_0: 0.17927
	loss_policy_1: 0.03361
	accuracy_policy_1: 0.68016
	loss_value_1: 0.03791
	loss_reward_1: 0.00538
	loss_policy_2: 0.03885
	accuracy_policy_2: 0.6409
	loss_value_2: 0.03931
	loss_reward_2: 0.00521
	loss_policy_3: 0.04162
	accuracy_policy_3: 0.62262
	loss_value_3: 0.04026
	loss_reward_3: 0.00539
	loss_policy_4: 0.0446
	accuracy_policy_4: 0.59551
	loss_value_4: 0.04147
	loss_reward_4: 0.00598
	loss_policy_5: 0.04739
	accuracy_policy_5: 0.5716
	loss_value_5: 0.04242
	loss_reward_5: 0.0067
	loss_policy: 0.31394
	loss_value: 0.38064
	loss_reward: 0.02866
[2024-05-05 16:55:55] nn step 56950, lr: 0.005.
	loss_policy_0: 0.09315
	accuracy_policy_0: 0.78496
	loss_value_0: 0.16712
	loss_policy_1: 0.02935
	accuracy_policy_1: 0.6991
	loss_value_1: 0.03495
	loss_reward_1: 0.00487
	loss_policy_2: 0.03457
	accuracy_policy_2: 0.65328
	loss_value_2: 0.03628
	loss_reward_2: 0.00498
	loss_policy_3: 0.03842
	accuracy_policy_3: 0.62273
	loss_value_3: 0.03737
	loss_reward_3: 0.00531
	loss_policy_4: 0.04146
	accuracy_policy_4: 0.59758
	loss_value_4: 0.03846
	loss_reward_4: 0.00545
	loss_policy_5: 0.04353
	accuracy_policy_5: 0.57746
	loss_value_5: 0.03946
	loss_reward_5: 0.00629
	loss_policy: 0.28047
	loss_value: 0.35364
	loss_reward: 0.02691
[2024-05-05 16:56:11] nn step 57000, lr: 0.005.
	loss_policy_0: 0.09039
	accuracy_policy_0: 0.79516
	loss_value_0: 0.16985
	loss_policy_1: 0.03013
	accuracy_policy_1: 0.70027
	loss_value_1: 0.03548
	loss_reward_1: 0.00524
	loss_policy_2: 0.03495
	accuracy_policy_2: 0.65855
	loss_value_2: 0.03662
	loss_reward_2: 0.00508
	loss_policy_3: 0.03862
	accuracy_policy_3: 0.63184
	loss_value_3: 0.03764
	loss_reward_3: 0.00547
	loss_policy_4: 0.0413
	accuracy_policy_4: 0.60543
	loss_value_4: 0.03861
	loss_reward_4: 0.00578
	loss_policy_5: 0.04465
	accuracy_policy_5: 0.58117
	loss_value_5: 0.03963
	loss_reward_5: 0.00668
	loss_policy: 0.28004
	loss_value: 0.35783
	loss_reward: 0.02825
Optimization_Done 57000
[2024-05-05 16:58:12] [command] train weight_iter_57000.pkl 285 286
[2024-05-05 16:58:29] nn step 57050, lr: 0.005.
	loss_policy_0: 0.12433
	accuracy_policy_0: 0.71609
	loss_value_0: 0.17122
	loss_policy_1: 0.03503
	accuracy_policy_1: 0.63926
	loss_value_1: 0.03636
	loss_reward_1: 0.00536
	loss_policy_2: 0.04006
	accuracy_policy_2: 0.61008
	loss_value_2: 0.03811
	loss_reward_2: 0.00497
	loss_policy_3: 0.04375
	accuracy_policy_3: 0.58188
	loss_value_3: 0.03956
	loss_reward_3: 0.00538
	loss_policy_4: 0.04776
	accuracy_policy_4: 0.55172
	loss_value_4: 0.04091
	loss_reward_4: 0.00567
	loss_policy_5: 0.05115
	accuracy_policy_5: 0.52273
	loss_value_5: 0.04218
	loss_reward_5: 0.00668
	loss_policy: 0.34208
	loss_value: 0.36834
	loss_reward: 0.02806
[2024-05-05 16:58:45] nn step 57100, lr: 0.005.
	loss_policy_0: 0.09997
	accuracy_policy_0: 0.76316
	loss_value_0: 0.16629
	loss_policy_1: 0.03087
	accuracy_policy_1: 0.67691
	loss_value_1: 0.03521
	loss_reward_1: 0.00505
	loss_policy_2: 0.03643
	accuracy_policy_2: 0.63496
	loss_value_2: 0.03682
	loss_reward_2: 0.00491
	loss_policy_3: 0.0405
	accuracy_policy_3: 0.61148
	loss_value_3: 0.03827
	loss_reward_3: 0.00509
	loss_policy_4: 0.04409
	accuracy_policy_4: 0.58324
	loss_value_4: 0.03942
	loss_reward_4: 0.00576
	loss_policy_5: 0.04765
	accuracy_policy_5: 0.56109
	loss_value_5: 0.04075
	loss_reward_5: 0.00644
	loss_policy: 0.29951
	loss_value: 0.35675
	loss_reward: 0.02724
[2024-05-05 16:59:00] nn step 57150, lr: 0.005.
	loss_policy_0: 0.09629
	accuracy_policy_0: 0.76883
	loss_value_0: 0.16584
	loss_policy_1: 0.03052
	accuracy_policy_1: 0.68391
	loss_value_1: 0.03519
	loss_reward_1: 0.00532
	loss_policy_2: 0.03628
	accuracy_policy_2: 0.64297
	loss_value_2: 0.03669
	loss_reward_2: 0.00504
	loss_policy_3: 0.04038
	accuracy_policy_3: 0.60977
	loss_value_3: 0.03812
	loss_reward_3: 0.00537
	loss_policy_4: 0.04404
	accuracy_policy_4: 0.58215
	loss_value_4: 0.03928
	loss_reward_4: 0.00577
	loss_policy_5: 0.04676
	accuracy_policy_5: 0.56613
	loss_value_5: 0.04076
	loss_reward_5: 0.00655
	loss_policy: 0.29426
	loss_value: 0.35588
	loss_reward: 0.02804
[2024-05-05 16:59:16] nn step 57200, lr: 0.005.
	loss_policy_0: 0.08827
	accuracy_policy_0: 0.77934
	loss_value_0: 0.16418
	loss_policy_1: 0.02989
	accuracy_policy_1: 0.68832
	loss_value_1: 0.03456
	loss_reward_1: 0.00535
	loss_policy_2: 0.03499
	accuracy_policy_2: 0.64789
	loss_value_2: 0.03608
	loss_reward_2: 0.00508
	loss_policy_3: 0.03915
	accuracy_policy_3: 0.61492
	loss_value_3: 0.03761
	loss_reward_3: 0.00542
	loss_policy_4: 0.04256
	accuracy_policy_4: 0.59074
	loss_value_4: 0.03896
	loss_reward_4: 0.00562
	loss_policy_5: 0.04627
	accuracy_policy_5: 0.5707
	loss_value_5: 0.04026
	loss_reward_5: 0.00658
	loss_policy: 0.28113
	loss_value: 0.35165
	loss_reward: 0.02804
Optimization_Done 57200
[2024-05-05 17:01:04] [command] train weight_iter_57200.pkl 286 287
[2024-05-05 17:01:21] nn step 57250, lr: 0.005.
	loss_policy_0: 0.10711
	accuracy_policy_0: 0.75371
	loss_value_0: 0.1604
	loss_policy_1: 0.03155
	accuracy_policy_1: 0.67422
	loss_value_1: 0.03366
	loss_reward_1: 0.00644
	loss_policy_2: 0.03651
	accuracy_policy_2: 0.64027
	loss_value_2: 0.03529
	loss_reward_2: 0.00576
	loss_policy_3: 0.04025
	accuracy_policy_3: 0.6093
	loss_value_3: 0.03656
	loss_reward_3: 0.00655
	loss_policy_4: 0.04357
	accuracy_policy_4: 0.57984
	loss_value_4: 0.03781
	loss_reward_4: 0.00707
	loss_policy_5: 0.04668
	accuracy_policy_5: 0.55605
	loss_value_5: 0.03928
	loss_reward_5: 0.00806
	loss_policy: 0.30566
	loss_value: 0.343
	loss_reward: 0.03387
[2024-05-05 17:01:37] nn step 57300, lr: 0.005.
	loss_policy_0: 0.07898
	accuracy_policy_0: 0.78582
	loss_value_0: 0.14502
	loss_policy_1: 0.02603
	accuracy_policy_1: 0.69945
	loss_value_1: 0.03055
	loss_reward_1: 0.00579
	loss_policy_2: 0.03068
	accuracy_policy_2: 0.65816
	loss_value_2: 0.03202
	loss_reward_2: 0.0053
	loss_policy_3: 0.034
	accuracy_policy_3: 0.63551
	loss_value_3: 0.03348
	loss_reward_3: 0.00592
	loss_policy_4: 0.03725
	accuracy_policy_4: 0.6043
	loss_value_4: 0.03448
	loss_reward_4: 0.00629
	loss_policy_5: 0.04027
	accuracy_policy_5: 0.58516
	loss_value_5: 0.03585
	loss_reward_5: 0.00722
	loss_policy: 0.24722
	loss_value: 0.3114
	loss_reward: 0.03052
[2024-05-05 17:01:53] nn step 57350, lr: 0.005.
	loss_policy_0: 0.07695
	accuracy_policy_0: 0.79738
	loss_value_0: 0.15194
	loss_policy_1: 0.02664
	accuracy_policy_1: 0.70383
	loss_value_1: 0.03203
	loss_reward_1: 0.006
	loss_policy_2: 0.03134
	accuracy_policy_2: 0.66406
	loss_value_2: 0.03354
	loss_reward_2: 0.0056
	loss_policy_3: 0.03533
	accuracy_policy_3: 0.63191
	loss_value_3: 0.03498
	loss_reward_3: 0.00604
	loss_policy_4: 0.03875
	accuracy_policy_4: 0.60762
	loss_value_4: 0.03618
	loss_reward_4: 0.00621
	loss_policy_5: 0.04195
	accuracy_policy_5: 0.58527
	loss_value_5: 0.03762
	loss_reward_5: 0.00767
	loss_policy: 0.25096
	loss_value: 0.32631
	loss_reward: 0.03153
[2024-05-05 17:02:09] nn step 57400, lr: 0.005.
	loss_policy_0: 0.07104
	accuracy_policy_0: 0.80648
	loss_value_0: 0.14479
	loss_policy_1: 0.02503
	accuracy_policy_1: 0.71023
	loss_value_1: 0.0308
	loss_reward_1: 0.00588
	loss_policy_2: 0.02968
	accuracy_policy_2: 0.67133
	loss_value_2: 0.03213
	loss_reward_2: 0.00504
	loss_policy_3: 0.03359
	accuracy_policy_3: 0.63863
	loss_value_3: 0.0335
	loss_reward_3: 0.00582
	loss_policy_4: 0.03738
	accuracy_policy_4: 0.60477
	loss_value_4: 0.03461
	loss_reward_4: 0.00623
	loss_policy_5: 0.03992
	accuracy_policy_5: 0.58699
	loss_value_5: 0.03588
	loss_reward_5: 0.00743
	loss_policy: 0.23665
	loss_value: 0.31172
	loss_reward: 0.03039
Optimization_Done 57400
[2024-05-05 17:04:08] [command] train weight_iter_57400.pkl 287 288
[2024-05-05 17:04:25] nn step 57450, lr: 0.005.
	loss_policy_0: 0.09137
	accuracy_policy_0: 0.75109
	loss_value_0: 0.14067
	loss_policy_1: 0.02588
	accuracy_policy_1: 0.67684
	loss_value_1: 0.02962
	loss_reward_1: 0.00506
	loss_policy_2: 0.03004
	accuracy_policy_2: 0.64039
	loss_value_2: 0.03104
	loss_reward_2: 0.00474
	loss_policy_3: 0.03293
	accuracy_policy_3: 0.61262
	loss_value_3: 0.03226
	loss_reward_3: 0.00516
	loss_policy_4: 0.03585
	accuracy_policy_4: 0.57977
	loss_value_4: 0.03342
	loss_reward_4: 0.00537
	loss_policy_5: 0.03872
	accuracy_policy_5: 0.55977
	loss_value_5: 0.03462
	loss_reward_5: 0.00642
	loss_policy: 0.25478
	loss_value: 0.30163
	loss_reward: 0.02675
[2024-05-05 17:04:41] nn step 57500, lr: 0.005.
	loss_policy_0: 0.07229
	accuracy_policy_0: 0.79883
	loss_value_0: 0.14556
	loss_policy_1: 0.0241
	accuracy_policy_1: 0.71453
	loss_value_1: 0.03076
	loss_reward_1: 0.00533
	loss_policy_2: 0.0286
	accuracy_policy_2: 0.67223
	loss_value_2: 0.0322
	loss_reward_2: 0.00499
	loss_policy_3: 0.03192
	accuracy_policy_3: 0.64938
	loss_value_3: 0.0333
	loss_reward_3: 0.00532
	loss_policy_4: 0.0348
	accuracy_policy_4: 0.61871
	loss_value_4: 0.03439
	loss_reward_4: 0.0057
	loss_policy_5: 0.03693
	accuracy_policy_5: 0.60035
	loss_value_5: 0.03543
	loss_reward_5: 0.00642
	loss_policy: 0.22864
	loss_value: 0.31165
	loss_reward: 0.02775
[2024-05-05 17:04:57] nn step 57550, lr: 0.005.
	loss_policy_0: 0.06667
	accuracy_policy_0: 0.80602
	loss_value_0: 0.14476
	loss_policy_1: 0.02334
	accuracy_policy_1: 0.72387
	loss_value_1: 0.03072
	loss_reward_1: 0.00521
	loss_policy_2: 0.02726
	accuracy_policy_2: 0.68172
	loss_value_2: 0.03211
	loss_reward_2: 0.00489
	loss_policy_3: 0.03099
	accuracy_policy_3: 0.65367
	loss_value_3: 0.03336
	loss_reward_3: 0.00522
	loss_policy_4: 0.03373
	accuracy_policy_4: 0.62805
	loss_value_4: 0.03454
	loss_reward_4: 0.00539
	loss_policy_5: 0.03631
	accuracy_policy_5: 0.60609
	loss_value_5: 0.03574
	loss_reward_5: 0.00657
	loss_policy: 0.21829
	loss_value: 0.31123
	loss_reward: 0.02729
[2024-05-05 17:05:13] nn step 57600, lr: 0.005.
	loss_policy_0: 0.06165
	accuracy_policy_0: 0.81949
	loss_value_0: 0.14178
	loss_policy_1: 0.0224
	accuracy_policy_1: 0.72691
	loss_value_1: 0.03007
	loss_reward_1: 0.00524
	loss_policy_2: 0.02634
	accuracy_policy_2: 0.68488
	loss_value_2: 0.03148
	loss_reward_2: 0.00459
	loss_policy_3: 0.02971
	accuracy_policy_3: 0.65379
	loss_value_3: 0.03252
	loss_reward_3: 0.00515
	loss_policy_4: 0.03222
	accuracy_policy_4: 0.62824
	loss_value_4: 0.03356
	loss_reward_4: 0.00528
	loss_policy_5: 0.03489
	accuracy_policy_5: 0.60703
	loss_value_5: 0.03467
	loss_reward_5: 0.00651
	loss_policy: 0.20721
	loss_value: 0.30408
	loss_reward: 0.02676
Optimization_Done 57600
[2024-05-05 17:07:12] [command] train weight_iter_57600.pkl 288 289
[2024-05-05 17:07:30] nn step 57650, lr: 0.005.
	loss_policy_0: 0.10186
	accuracy_policy_0: 0.73426
	loss_value_0: 0.1469
	loss_policy_1: 0.02795
	accuracy_policy_1: 0.66102
	loss_value_1: 0.03104
	loss_reward_1: 0.00506
	loss_policy_2: 0.03218
	accuracy_policy_2: 0.62207
	loss_value_2: 0.03248
	loss_reward_2: 0.00476
	loss_policy_3: 0.03534
	accuracy_policy_3: 0.59977
	loss_value_3: 0.03383
	loss_reward_3: 0.00536
	loss_policy_4: 0.03851
	accuracy_policy_4: 0.57031
	loss_value_4: 0.0351
	loss_reward_4: 0.00565
	loss_policy_5: 0.04095
	accuracy_policy_5: 0.54879
	loss_value_5: 0.0363
	loss_reward_5: 0.00628
	loss_policy: 0.27679
	loss_value: 0.31565
	loss_reward: 0.02711
[2024-05-05 17:07:46] nn step 57700, lr: 0.005.
	loss_policy_0: 0.08662
	accuracy_policy_0: 0.78309
	loss_value_0: 0.15972
	loss_policy_1: 0.02779
	accuracy_policy_1: 0.69555
	loss_value_1: 0.03391
	loss_reward_1: 0.00541
	loss_policy_2: 0.03225
	accuracy_policy_2: 0.65988
	loss_value_2: 0.03549
	loss_reward_2: 0.00531
	loss_policy_3: 0.03601
	accuracy_policy_3: 0.62949
	loss_value_3: 0.03703
	loss_reward_3: 0.00579
	loss_policy_4: 0.04007
	accuracy_policy_4: 0.59871
	loss_value_4: 0.03828
	loss_reward_4: 0.00625
	loss_policy_5: 0.04279
	accuracy_policy_5: 0.57359
	loss_value_5: 0.03944
	loss_reward_5: 0.00719
	loss_policy: 0.26554
	loss_value: 0.34388
	loss_reward: 0.02996
[2024-05-05 17:08:02] nn step 57750, lr: 0.005.
	loss_policy_0: 0.07393
	accuracy_policy_0: 0.80039
	loss_value_0: 0.1517
	loss_policy_1: 0.02504
	accuracy_policy_1: 0.71004
	loss_value_1: 0.03205
	loss_reward_1: 0.00543
	loss_policy_2: 0.02958
	accuracy_policy_2: 0.66918
	loss_value_2: 0.03359
	loss_reward_2: 0.00493
	loss_policy_3: 0.03304
	accuracy_policy_3: 0.64117
	loss_value_3: 0.03487
	loss_reward_3: 0.00554
	loss_policy_4: 0.03682
	accuracy_policy_4: 0.61102
	loss_value_4: 0.03598
	loss_reward_4: 0.00581
	loss_policy_5: 0.03917
	accuracy_policy_5: 0.59773
	loss_value_5: 0.03709
	loss_reward_5: 0.00692
	loss_policy: 0.23758
	loss_value: 0.32528
	loss_reward: 0.02863
[2024-05-05 17:08:18] nn step 57800, lr: 0.005.
	loss_policy_0: 0.07075
	accuracy_policy_0: 0.80414
	loss_value_0: 0.14893
	loss_policy_1: 0.02441
	accuracy_policy_1: 0.70809
	loss_value_1: 0.0315
	loss_reward_1: 0.00519
	loss_policy_2: 0.02878
	accuracy_policy_2: 0.67434
	loss_value_2: 0.033
	loss_reward_2: 0.00509
	loss_policy_3: 0.03249
	accuracy_policy_3: 0.64562
	loss_value_3: 0.03411
	loss_reward_3: 0.00548
	loss_policy_4: 0.03595
	accuracy_policy_4: 0.6098
	loss_value_4: 0.03541
	loss_reward_4: 0.00576
	loss_policy_5: 0.03861
	accuracy_policy_5: 0.59215
	loss_value_5: 0.03651
	loss_reward_5: 0.00662
	loss_policy: 0.23099
	loss_value: 0.31945
	loss_reward: 0.02814
Optimization_Done 57800
[2024-05-05 17:10:16] [command] train weight_iter_57800.pkl 289 290
[2024-05-05 17:10:34] nn step 57850, lr: 0.005.
	loss_policy_0: 0.11921
	accuracy_policy_0: 0.72957
	loss_value_0: 0.16727
	loss_policy_1: 0.03396
	accuracy_policy_1: 0.65867
	loss_value_1: 0.03534
	loss_reward_1: 0.00623
	loss_policy_2: 0.03839
	accuracy_policy_2: 0.62195
	loss_value_2: 0.03704
	loss_reward_2: 0.00614
	loss_policy_3: 0.04228
	accuracy_policy_3: 0.58773
	loss_value_3: 0.03874
	loss_reward_3: 0.00679
	loss_policy_4: 0.04575
	accuracy_policy_4: 0.5675
	loss_value_4: 0.04023
	loss_reward_4: 0.00701
	loss_policy_5: 0.04935
	accuracy_policy_5: 0.54074
	loss_value_5: 0.04182
	loss_reward_5: 0.00818
	loss_policy: 0.32894
	loss_value: 0.36044
	loss_reward: 0.03435
[2024-05-05 17:10:50] nn step 57900, lr: 0.005.
	loss_policy_0: 0.09108
	accuracy_policy_0: 0.76164
	loss_value_0: 0.14537
	loss_policy_1: 0.02847
	accuracy_policy_1: 0.67234
	loss_value_1: 0.03102
	loss_reward_1: 0.0058
	loss_policy_2: 0.03262
	accuracy_policy_2: 0.64027
	loss_value_2: 0.03255
	loss_reward_2: 0.00542
	loss_policy_3: 0.03578
	accuracy_policy_3: 0.61953
	loss_value_3: 0.0342
	loss_reward_3: 0.00599
	loss_policy_4: 0.03929
	accuracy_policy_4: 0.58734
	loss_value_4: 0.03549
	loss_reward_4: 0.00635
	loss_policy_5: 0.04195
	accuracy_policy_5: 0.56953
	loss_value_5: 0.03685
	loss_reward_5: 0.00741
	loss_policy: 0.26919
	loss_value: 0.31547
	loss_reward: 0.03097
[2024-05-05 17:11:06] nn step 57950, lr: 0.005.
	loss_policy_0: 0.08906
	accuracy_policy_0: 0.77727
	loss_value_0: 0.15171
	loss_policy_1: 0.02863
	accuracy_policy_1: 0.68582
	loss_value_1: 0.03224
	loss_reward_1: 0.00613
	loss_policy_2: 0.03357
	accuracy_policy_2: 0.64852
	loss_value_2: 0.03408
	loss_reward_2: 0.0058
	loss_policy_3: 0.0371
	accuracy_policy_3: 0.62906
	loss_value_3: 0.03539
	loss_reward_3: 0.00613
	loss_policy_4: 0.04052
	accuracy_policy_4: 0.59523
	loss_value_4: 0.03702
	loss_reward_4: 0.00652
	loss_policy_5: 0.04346
	accuracy_policy_5: 0.57551
	loss_value_5: 0.0385
	loss_reward_5: 0.00782
	loss_policy: 0.27234
	loss_value: 0.32895
	loss_reward: 0.0324
[2024-05-05 17:11:22] nn step 58000, lr: 0.005.
	loss_policy_0: 0.08262
	accuracy_policy_0: 0.78812
	loss_value_0: 0.14874
	loss_policy_1: 0.02757
	accuracy_policy_1: 0.69273
	loss_value_1: 0.03128
	loss_reward_1: 0.00572
	loss_policy_2: 0.0325
	accuracy_policy_2: 0.64844
	loss_value_2: 0.03295
	loss_reward_2: 0.00577
	loss_policy_3: 0.03594
	accuracy_policy_3: 0.62891
	loss_value_3: 0.0343
	loss_reward_3: 0.00601
	loss_policy_4: 0.03936
	accuracy_policy_4: 0.60441
	loss_value_4: 0.03537
	loss_reward_4: 0.00647
	loss_policy_5: 0.04245
	accuracy_policy_5: 0.5798
	loss_value_5: 0.03687
	loss_reward_5: 0.00775
	loss_policy: 0.26044
	loss_value: 0.31951
	loss_reward: 0.03172
Optimization_Done 58000
[2024-05-05 17:13:19] [command] train weight_iter_58000.pkl 290 291
[2024-05-05 17:13:37] nn step 58050, lr: 0.005.
	loss_policy_0: 0.10888
	accuracy_policy_0: 0.71492
	loss_value_0: 0.13639
	loss_policy_1: 0.03233
	accuracy_policy_1: 0.62039
	loss_value_1: 0.02891
	loss_reward_1: 0.0061
	loss_policy_2: 0.03633
	accuracy_policy_2: 0.59062
	loss_value_2: 0.03066
	loss_reward_2: 0.00588
	loss_policy_3: 0.04023
	accuracy_policy_3: 0.55676
	loss_value_3: 0.03226
	loss_reward_3: 0.00633
	loss_policy_4: 0.04321
	accuracy_policy_4: 0.5357
	loss_value_4: 0.03365
	loss_reward_4: 0.00683
	loss_policy_5: 0.04603
	accuracy_policy_5: 0.51156
	loss_value_5: 0.03492
	loss_reward_5: 0.00771
	loss_policy: 0.307
	loss_value: 0.29678
	loss_reward: 0.03285
[2024-05-05 17:13:53] nn step 58100, lr: 0.005.
	loss_policy_0: 0.09654
	accuracy_policy_0: 0.74762
	loss_value_0: 0.14236
	loss_policy_1: 0.0318
	accuracy_policy_1: 0.64668
	loss_value_1: 0.03037
	loss_reward_1: 0.00644
	loss_policy_2: 0.03696
	accuracy_policy_2: 0.61645
	loss_value_2: 0.0318
	loss_reward_2: 0.00601
	loss_policy_3: 0.04047
	accuracy_policy_3: 0.58652
	loss_value_3: 0.03327
	loss_reward_3: 0.00664
	loss_policy_4: 0.04391
	accuracy_policy_4: 0.56141
	loss_value_4: 0.03459
	loss_reward_4: 0.00706
	loss_policy_5: 0.04689
	accuracy_policy_5: 0.54332
	loss_value_5: 0.03596
	loss_reward_5: 0.00829
	loss_policy: 0.29657
	loss_value: 0.30835
	loss_reward: 0.03444
[2024-05-05 17:14:09] nn step 58150, lr: 0.005.
	loss_policy_0: 0.09207
	accuracy_policy_0: 0.76121
	loss_value_0: 0.14335
	loss_policy_1: 0.03119
	accuracy_policy_1: 0.65961
	loss_value_1: 0.0305
	loss_reward_1: 0.00662
	loss_policy_2: 0.03632
	accuracy_policy_2: 0.61793
	loss_value_2: 0.03211
	loss_reward_2: 0.0063
	loss_policy_3: 0.03954
	accuracy_policy_3: 0.60078
	loss_value_3: 0.0336
	loss_reward_3: 0.00652
	loss_policy_4: 0.04322
	accuracy_policy_4: 0.57035
	loss_value_4: 0.03492
	loss_reward_4: 0.00701
	loss_policy_5: 0.04639
	accuracy_policy_5: 0.54859
	loss_value_5: 0.03639
	loss_reward_5: 0.00848
	loss_policy: 0.28874
	loss_value: 0.31087
	loss_reward: 0.03493
[2024-05-05 17:14:25] nn step 58200, lr: 0.005.
	loss_policy_0: 0.08573
	accuracy_policy_0: 0.76949
	loss_value_0: 0.13837
	loss_policy_1: 0.03073
	accuracy_policy_1: 0.65672
	loss_value_1: 0.02963
	loss_reward_1: 0.00645
	loss_policy_2: 0.03555
	accuracy_policy_2: 0.61672
	loss_value_2: 0.03111
	loss_reward_2: 0.00583
	loss_policy_3: 0.03863
	accuracy_policy_3: 0.5957
	loss_value_3: 0.03246
	loss_reward_3: 0.00653
	loss_policy_4: 0.04198
	accuracy_policy_4: 0.56977
	loss_value_4: 0.03392
	loss_reward_4: 0.0068
	loss_policy_5: 0.04488
	accuracy_policy_5: 0.54992
	loss_value_5: 0.03546
	loss_reward_5: 0.00832
	loss_policy: 0.2775
	loss_value: 0.30095
	loss_reward: 0.03392
Optimization_Done 58200
[2024-05-05 17:16:12] [command] train weight_iter_58200.pkl 291 292
[2024-05-05 17:16:30] nn step 58250, lr: 0.005.
	loss_policy_0: 0.12225
	accuracy_policy_0: 0.70148
	loss_value_0: 0.13027
	loss_policy_1: 0.03422
	accuracy_policy_1: 0.6159
	loss_value_1: 0.02768
	loss_reward_1: 0.00546
	loss_policy_2: 0.03764
	accuracy_policy_2: 0.58914
	loss_value_2: 0.02909
	loss_reward_2: 0.00523
	loss_policy_3: 0.0406
	accuracy_policy_3: 0.56281
	loss_value_3: 0.03028
	loss_reward_3: 0.00553
	loss_policy_4: 0.04282
	accuracy_policy_4: 0.54312
	loss_value_4: 0.03145
	loss_reward_4: 0.00594
	loss_policy_5: 0.04515
	accuracy_policy_5: 0.52316
	loss_value_5: 0.03256
	loss_reward_5: 0.00698
	loss_policy: 0.32268
	loss_value: 0.28134
	loss_reward: 0.02914
[2024-05-05 17:16:46] nn step 58300, lr: 0.005.
	loss_policy_0: 0.0995
	accuracy_policy_0: 0.74961
	loss_value_0: 0.12988
	loss_policy_1: 0.03153
	accuracy_policy_1: 0.64977
	loss_value_1: 0.02774
	loss_reward_1: 0.00569
	loss_policy_2: 0.03594
	accuracy_policy_2: 0.60797
	loss_value_2: 0.02902
	loss_reward_2: 0.00522
	loss_policy_3: 0.03865
	accuracy_policy_3: 0.59227
	loss_value_3: 0.03017
	loss_reward_3: 0.00567
	loss_policy_4: 0.04169
	accuracy_policy_4: 0.56609
	loss_value_4: 0.03132
	loss_reward_4: 0.00608
	loss_policy_5: 0.04417
	accuracy_policy_5: 0.54277
	loss_value_5: 0.03258
	loss_reward_5: 0.00742
	loss_policy: 0.29149
	loss_value: 0.28072
	loss_reward: 0.03008
[2024-05-05 17:17:02] nn step 58350, lr: 0.005.
	loss_policy_0: 0.09194
	accuracy_policy_0: 0.75281
	loss_value_0: 0.12588
	loss_policy_1: 0.02931
	accuracy_policy_1: 0.65422
	loss_value_1: 0.02685
	loss_reward_1: 0.00556
	loss_policy_2: 0.03382
	accuracy_policy_2: 0.61375
	loss_value_2: 0.02809
	loss_reward_2: 0.00493
	loss_policy_3: 0.03604
	accuracy_policy_3: 0.60016
	loss_value_3: 0.02922
	loss_reward_3: 0.00537
	loss_policy_4: 0.03869
	accuracy_policy_4: 0.57453
	loss_value_4: 0.03034
	loss_reward_4: 0.00599
	loss_policy_5: 0.04118
	accuracy_policy_5: 0.55617
	loss_value_5: 0.03125
	loss_reward_5: 0.00709
	loss_policy: 0.27098
	loss_value: 0.27162
	loss_reward: 0.02894
[2024-05-05 17:17:18] nn step 58400, lr: 0.005.
	loss_policy_0: 0.08139
	accuracy_policy_0: 0.75918
	loss_value_0: 0.11861
	loss_policy_1: 0.02724
	accuracy_policy_1: 0.65539
	loss_value_1: 0.02531
	loss_reward_1: 0.00537
	loss_policy_2: 0.03106
	accuracy_policy_2: 0.62148
	loss_value_2: 0.02668
	loss_reward_2: 0.00466
	loss_policy_3: 0.03375
	accuracy_policy_3: 0.60215
	loss_value_3: 0.02768
	loss_reward_3: 0.00493
	loss_policy_4: 0.03615
	accuracy_policy_4: 0.57906
	loss_value_4: 0.02852
	loss_reward_4: 0.00535
	loss_policy_5: 0.03841
	accuracy_policy_5: 0.56125
	loss_value_5: 0.02959
	loss_reward_5: 0.00676
	loss_policy: 0.248
	loss_value: 0.25638
	loss_reward: 0.02707
Optimization_Done 58400
[2024-05-05 17:19:06] [command] train weight_iter_58400.pkl 292 293
[2024-05-05 17:19:23] nn step 58450, lr: 0.005.
	loss_policy_0: 0.11988
	accuracy_policy_0: 0.70793
	loss_value_0: 0.14082
	loss_policy_1: 0.03148
	accuracy_policy_1: 0.64555
	loss_value_1: 0.03005
	loss_reward_1: 0.00452
	loss_policy_2: 0.03494
	accuracy_policy_2: 0.61633
	loss_value_2: 0.03157
	loss_reward_2: 0.00447
	loss_policy_3: 0.03795
	accuracy_policy_3: 0.58641
	loss_value_3: 0.03291
	loss_reward_3: 0.00482
	loss_policy_4: 0.0406
	accuracy_policy_4: 0.56141
	loss_value_4: 0.03423
	loss_reward_4: 0.00518
	loss_policy_5: 0.04299
	accuracy_policy_5: 0.54137
	loss_value_5: 0.03555
	loss_reward_5: 0.00583
	loss_policy: 0.30783
	loss_value: 0.30513
	loss_reward: 0.02482
[2024-05-05 17:19:39] nn step 58500, lr: 0.005.
	loss_policy_0: 0.09444
	accuracy_policy_0: 0.75594
	loss_value_0: 0.14228
	loss_policy_1: 0.02813
	accuracy_policy_1: 0.6716
	loss_value_1: 0.03023
	loss_reward_1: 0.00461
	loss_policy_2: 0.0321
	accuracy_policy_2: 0.64004
	loss_value_2: 0.03172
	loss_reward_2: 0.00452
	loss_policy_3: 0.03504
	accuracy_policy_3: 0.61539
	loss_value_3: 0.0329
	loss_reward_3: 0.0047
	loss_policy_4: 0.03722
	accuracy_policy_4: 0.5966
	loss_value_4: 0.03394
	loss_reward_4: 0.0052
	loss_policy_5: 0.04016
	accuracy_policy_5: 0.57141
	loss_value_5: 0.03515
	loss_reward_5: 0.00601
	loss_policy: 0.2671
	loss_value: 0.30623
	loss_reward: 0.02504
[2024-05-05 17:19:55] nn step 58550, lr: 0.005.
	loss_policy_0: 0.08774
	accuracy_policy_0: 0.77148
	loss_value_0: 0.14643
	loss_policy_1: 0.02775
	accuracy_policy_1: 0.67938
	loss_value_1: 0.03099
	loss_reward_1: 0.00457
	loss_policy_2: 0.03218
	accuracy_policy_2: 0.63598
	loss_value_2: 0.03219
	loss_reward_2: 0.00449
	loss_policy_3: 0.03447
	accuracy_policy_3: 0.62598
	loss_value_3: 0.03341
	loss_reward_3: 0.00484
	loss_policy_4: 0.03675
	accuracy_policy_4: 0.5998
	loss_value_4: 0.03441
	loss_reward_4: 0.00516
	loss_policy_5: 0.03925
	accuracy_policy_5: 0.58125
	loss_value_5: 0.03564
	loss_reward_5: 0.00606
	loss_policy: 0.25815
	loss_value: 0.31307
	loss_reward: 0.02513
[2024-05-05 17:20:11] nn step 58600, lr: 0.005.
	loss_policy_0: 0.08335
	accuracy_policy_0: 0.77859
	loss_value_0: 0.14804
	loss_policy_1: 0.02666
	accuracy_policy_1: 0.68859
	loss_value_1: 0.03118
	loss_reward_1: 0.00484
	loss_policy_2: 0.03087
	accuracy_policy_2: 0.65191
	loss_value_2: 0.03236
	loss_reward_2: 0.00458
	loss_policy_3: 0.03361
	accuracy_policy_3: 0.62555
	loss_value_3: 0.03364
	loss_reward_3: 0.0048
	loss_policy_4: 0.036
	accuracy_policy_4: 0.60734
	loss_value_4: 0.03483
	loss_reward_4: 0.00549
	loss_policy_5: 0.03885
	accuracy_policy_5: 0.58641
	loss_value_5: 0.0361
	loss_reward_5: 0.00626
	loss_policy: 0.24934
	loss_value: 0.31614
	loss_reward: 0.02596
Optimization_Done 58600
[2024-05-05 17:22:11] [command] train weight_iter_58600.pkl 293 294
[2024-05-05 17:22:28] nn step 58650, lr: 0.005.
	loss_policy_0: 0.14057
	accuracy_policy_0: 0.71508
	loss_value_0: 0.1655
	loss_policy_1: 0.03745
	accuracy_policy_1: 0.63742
	loss_value_1: 0.03506
	loss_reward_1: 0.00501
	loss_policy_2: 0.04122
	accuracy_policy_2: 0.60078
	loss_value_2: 0.03689
	loss_reward_2: 0.00486
	loss_policy_3: 0.04487
	accuracy_policy_3: 0.57102
	loss_value_3: 0.03839
	loss_reward_3: 0.00539
	loss_policy_4: 0.04757
	accuracy_policy_4: 0.54809
	loss_value_4: 0.03999
	loss_reward_4: 0.00573
	loss_policy_5: 0.05063
	accuracy_policy_5: 0.5248
	loss_value_5: 0.04141
	loss_reward_5: 0.00644
	loss_policy: 0.36231
	loss_value: 0.35725
	loss_reward: 0.02742
[2024-05-05 17:22:44] nn step 58700, lr: 0.005.
	loss_policy_0: 0.11298
	accuracy_policy_0: 0.75395
	loss_value_0: 0.16604
	loss_policy_1: 0.03314
	accuracy_policy_1: 0.67336
	loss_value_1: 0.03516
	loss_reward_1: 0.00523
	loss_policy_2: 0.03759
	accuracy_policy_2: 0.6407
	loss_value_2: 0.03686
	loss_reward_2: 0.00502
	loss_policy_3: 0.04109
	accuracy_policy_3: 0.61395
	loss_value_3: 0.03853
	loss_reward_3: 0.00539
	loss_policy_4: 0.04384
	accuracy_policy_4: 0.59098
	loss_value_4: 0.03993
	loss_reward_4: 0.00557
	loss_policy_5: 0.04685
	accuracy_policy_5: 0.56574
	loss_value_5: 0.0414
	loss_reward_5: 0.0066
	loss_policy: 0.31549
	loss_value: 0.35792
	loss_reward: 0.02781
[2024-05-05 17:23:00] nn step 58750, lr: 0.005.
	loss_policy_0: 0.10203
	accuracy_policy_0: 0.77156
	loss_value_0: 0.16666
	loss_policy_1: 0.03144
	accuracy_policy_1: 0.68488
	loss_value_1: 0.03528
	loss_reward_1: 0.00508
	loss_policy_2: 0.03581
	accuracy_policy_2: 0.64965
	loss_value_2: 0.03699
	loss_reward_2: 0.00476
	loss_policy_3: 0.03944
	accuracy_policy_3: 0.62148
	loss_value_3: 0.03845
	loss_reward_3: 0.00518
	loss_policy_4: 0.04273
	accuracy_policy_4: 0.59492
	loss_value_4: 0.03983
	loss_reward_4: 0.00554
	loss_policy_5: 0.04579
	accuracy_policy_5: 0.57422
	loss_value_5: 0.04127
	loss_reward_5: 0.0063
	loss_policy: 0.29724
	loss_value: 0.35847
	loss_reward: 0.02686
[2024-05-05 17:23:16] nn step 58800, lr: 0.005.
	loss_policy_0: 0.09579
	accuracy_policy_0: 0.78023
	loss_value_0: 0.16244
	loss_policy_1: 0.03008
	accuracy_policy_1: 0.68621
	loss_value_1: 0.0343
	loss_reward_1: 0.00496
	loss_policy_2: 0.03424
	accuracy_policy_2: 0.6523
	loss_value_2: 0.036
	loss_reward_2: 0.0048
	loss_policy_3: 0.03755
	accuracy_policy_3: 0.62867
	loss_value_3: 0.03754
	loss_reward_3: 0.00517
	loss_policy_4: 0.04011
	accuracy_policy_4: 0.60867
	loss_value_4: 0.03887
	loss_reward_4: 0.0056
	loss_policy_5: 0.04327
	accuracy_policy_5: 0.58539
	loss_value_5: 0.04017
	loss_reward_5: 0.00638
	loss_policy: 0.28103
	loss_value: 0.34933
	loss_reward: 0.02692
Optimization_Done 58800
[2024-05-05 17:25:15] [command] train weight_iter_58800.pkl 294 295
[2024-05-05 17:25:32] nn step 58850, lr: 0.005.
	loss_policy_0: 0.12651
	accuracy_policy_0: 0.72855
	loss_value_0: 0.16604
	loss_policy_1: 0.03725
	accuracy_policy_1: 0.64012
	loss_value_1: 0.03527
	loss_reward_1: 0.00567
	loss_policy_2: 0.04221
	accuracy_policy_2: 0.60281
	loss_value_2: 0.03678
	loss_reward_2: 0.00529
	loss_policy_3: 0.04626
	accuracy_policy_3: 0.57141
	loss_value_3: 0.0381
	loss_reward_3: 0.00542
	loss_policy_4: 0.04939
	accuracy_policy_4: 0.54242
	loss_value_4: 0.03948
	loss_reward_4: 0.006
	loss_policy_5: 0.05269
	accuracy_policy_5: 0.52336
	loss_value_5: 0.04084
	loss_reward_5: 0.00663
	loss_policy: 0.35431
	loss_value: 0.35652
	loss_reward: 0.02901
[2024-05-05 17:25:48] nn step 58900, lr: 0.005.
	loss_policy_0: 0.10762
	accuracy_policy_0: 0.76387
	loss_value_0: 0.16991
	loss_policy_1: 0.03495
	accuracy_policy_1: 0.66211
	loss_value_1: 0.03623
	loss_reward_1: 0.00565
	loss_policy_2: 0.04028
	accuracy_policy_2: 0.62379
	loss_value_2: 0.03772
	loss_reward_2: 0.0054
	loss_policy_3: 0.04389
	accuracy_policy_3: 0.5975
	loss_value_3: 0.03914
	loss_reward_3: 0.00561
	loss_policy_4: 0.04704
	accuracy_policy_4: 0.57367
	loss_value_4: 0.0405
	loss_reward_4: 0.00604
	loss_policy_5: 0.05083
	accuracy_policy_5: 0.5477
	loss_value_5: 0.04183
	loss_reward_5: 0.00699
	loss_policy: 0.32461
	loss_value: 0.36533
	loss_reward: 0.02968
[2024-05-05 17:26:04] nn step 58950, lr: 0.005.
	loss_policy_0: 0.09387
	accuracy_policy_0: 0.7768
	loss_value_0: 0.15948
	loss_policy_1: 0.03162
	accuracy_policy_1: 0.67371
	loss_value_1: 0.03379
	loss_reward_1: 0.00536
	loss_policy_2: 0.03714
	accuracy_policy_2: 0.63086
	loss_value_2: 0.03533
	loss_reward_2: 0.00507
	loss_policy_3: 0.04103
	accuracy_policy_3: 0.59887
	loss_value_3: 0.03676
	loss_reward_3: 0.00542
	loss_policy_4: 0.04412
	accuracy_policy_4: 0.57578
	loss_value_4: 0.03796
	loss_reward_4: 0.00566
	loss_policy_5: 0.04755
	accuracy_policy_5: 0.54953
	loss_value_5: 0.03925
	loss_reward_5: 0.00654
	loss_policy: 0.29534
	loss_value: 0.34255
	loss_reward: 0.02805
[2024-05-05 17:26:21] nn step 59000, lr: 0.005.
	loss_policy_0: 0.09377
	accuracy_policy_0: 0.78441
	loss_value_0: 0.1657
	loss_policy_1: 0.03188
	accuracy_policy_1: 0.67914
	loss_value_1: 0.03524
	loss_reward_1: 0.00563
	loss_policy_2: 0.03696
	accuracy_policy_2: 0.63809
	loss_value_2: 0.03674
	loss_reward_2: 0.00517
	loss_policy_3: 0.04111
	accuracy_policy_3: 0.61016
	loss_value_3: 0.03812
	loss_reward_3: 0.00545
	loss_policy_4: 0.04432
	accuracy_policy_4: 0.58547
	loss_value_4: 0.03965
	loss_reward_4: 0.00586
	loss_policy_5: 0.04744
	accuracy_policy_5: 0.56793
	loss_value_5: 0.04092
	loss_reward_5: 0.00704
	loss_policy: 0.29548
	loss_value: 0.35637
	loss_reward: 0.02915
Optimization_Done 59000
[2024-05-05 17:28:10] [command] train weight_iter_59000.pkl 295 296
[2024-05-05 17:28:27] nn step 59050, lr: 0.005.
	loss_policy_0: 0.11647
	accuracy_policy_0: 0.74918
	loss_value_0: 0.16377
	loss_policy_1: 0.03531
	accuracy_policy_1: 0.65578
	loss_value_1: 0.03443
	loss_reward_1: 0.00631
	loss_policy_2: 0.04003
	accuracy_policy_2: 0.6175
	loss_value_2: 0.03612
	loss_reward_2: 0.00583
	loss_policy_3: 0.04349
	accuracy_policy_3: 0.59156
	loss_value_3: 0.03755
	loss_reward_3: 0.00587
	loss_policy_4: 0.04621
	accuracy_policy_4: 0.57484
	loss_value_4: 0.03899
	loss_reward_4: 0.00641
	loss_policy_5: 0.04914
	accuracy_policy_5: 0.54719
	loss_value_5: 0.04042
	loss_reward_5: 0.00756
	loss_policy: 0.33066
	loss_value: 0.35127
	loss_reward: 0.03197
[2024-05-05 17:28:43] nn step 59100, lr: 0.005.
	loss_policy_0: 0.11169
	accuracy_policy_0: 0.77707
	loss_value_0: 0.18261
	loss_policy_1: 0.0364
	accuracy_policy_1: 0.67746
	loss_value_1: 0.03832
	loss_reward_1: 0.00697
	loss_policy_2: 0.04163
	accuracy_policy_2: 0.64008
	loss_value_2: 0.03997
	loss_reward_2: 0.00664
	loss_policy_3: 0.04548
	accuracy_policy_3: 0.61375
	loss_value_3: 0.04166
	loss_reward_3: 0.00669
	loss_policy_4: 0.04908
	accuracy_policy_4: 0.5934
	loss_value_4: 0.04286
	loss_reward_4: 0.00694
	loss_policy_5: 0.05201
	accuracy_policy_5: 0.5727
	loss_value_5: 0.04463
	loss_reward_5: 0.00854
	loss_policy: 0.33629
	loss_value: 0.39006
	loss_reward: 0.03578
[2024-05-05 17:28:59] nn step 59150, lr: 0.005.
	loss_policy_0: 0.09591
	accuracy_policy_0: 0.78168
	loss_value_0: 0.16693
	loss_policy_1: 0.032
	accuracy_policy_1: 0.68777
	loss_value_1: 0.03515
	loss_reward_1: 0.00616
	loss_policy_2: 0.0368
	accuracy_policy_2: 0.64617
	loss_value_2: 0.03684
	loss_reward_2: 0.00569
	loss_policy_3: 0.04047
	accuracy_policy_3: 0.62242
	loss_value_3: 0.03815
	loss_reward_3: 0.00583
	loss_policy_4: 0.0437
	accuracy_policy_4: 0.60055
	loss_value_4: 0.0395
	loss_reward_4: 0.00646
	loss_policy_5: 0.0463
	accuracy_policy_5: 0.57375
	loss_value_5: 0.04094
	loss_reward_5: 0.00742
	loss_policy: 0.29518
	loss_value: 0.35751
	loss_reward: 0.03156
[2024-05-05 17:29:16] nn step 59200, lr: 0.005.
	loss_policy_0: 0.09091
	accuracy_policy_0: 0.78941
	loss_value_0: 0.1672
	loss_policy_1: 0.03166
	accuracy_policy_1: 0.68418
	loss_value_1: 0.03553
	loss_reward_1: 0.00634
	loss_policy_2: 0.03698
	accuracy_policy_2: 0.64535
	loss_value_2: 0.03707
	loss_reward_2: 0.00585
	loss_policy_3: 0.04034
	accuracy_policy_3: 0.61551
	loss_value_3: 0.03832
	loss_reward_3: 0.00599
	loss_policy_4: 0.04306
	accuracy_policy_4: 0.60012
	loss_value_4: 0.03952
	loss_reward_4: 0.00623
	loss_policy_5: 0.04639
	accuracy_policy_5: 0.57594
	loss_value_5: 0.04101
	loss_reward_5: 0.0076
	loss_policy: 0.28934
	loss_value: 0.35865
	loss_reward: 0.03201
Optimization_Done 59200
[2024-05-05 17:31:14] [command] train weight_iter_59200.pkl 296 297
[2024-05-05 17:31:31] nn step 59250, lr: 0.005.
	loss_policy_0: 0.13258
	accuracy_policy_0: 0.73238
	loss_value_0: 0.17621
	loss_policy_1: 0.03613
	accuracy_policy_1: 0.6484
	loss_value_1: 0.03708
	loss_reward_1: 0.00609
	loss_policy_2: 0.04014
	accuracy_policy_2: 0.61477
	loss_value_2: 0.03829
	loss_reward_2: 0.00598
	loss_policy_3: 0.04343
	accuracy_policy_3: 0.58887
	loss_value_3: 0.03939
	loss_reward_3: 0.00661
	loss_policy_4: 0.04573
	accuracy_policy_4: 0.56172
	loss_value_4: 0.04058
	loss_reward_4: 0.0067
	loss_policy_5: 0.04852
	accuracy_policy_5: 0.54441
	loss_value_5: 0.0416
	loss_reward_5: 0.00803
	loss_policy: 0.34653
	loss_value: 0.37315
	loss_reward: 0.03341
[2024-05-05 17:31:47] nn step 59300, lr: 0.005.
	loss_policy_0: 0.10449
	accuracy_policy_0: 0.76805
	loss_value_0: 0.17331
	loss_policy_1: 0.03173
	accuracy_policy_1: 0.68215
	loss_value_1: 0.03659
	loss_reward_1: 0.00605
	loss_policy_2: 0.03684
	accuracy_policy_2: 0.64418
	loss_value_2: 0.03793
	loss_reward_2: 0.00577
	loss_policy_3: 0.04017
	accuracy_policy_3: 0.61285
	loss_value_3: 0.03902
	loss_reward_3: 0.00611
	loss_policy_4: 0.04298
	accuracy_policy_4: 0.58965
	loss_value_4: 0.04018
	loss_reward_4: 0.00644
	loss_policy_5: 0.04513
	accuracy_policy_5: 0.57047
	loss_value_5: 0.04122
	loss_reward_5: 0.00758
	loss_policy: 0.30135
	loss_value: 0.36824
	loss_reward: 0.03194
[2024-05-05 17:32:04] nn step 59350, lr: 0.005.
	loss_policy_0: 0.09102
	accuracy_policy_0: 0.78332
	loss_value_0: 0.16451
	loss_policy_1: 0.02914
	accuracy_policy_1: 0.6893
	loss_value_1: 0.03472
	loss_reward_1: 0.00565
	loss_policy_2: 0.03372
	accuracy_policy_2: 0.6525
	loss_value_2: 0.03606
	loss_reward_2: 0.00523
	loss_policy_3: 0.03671
	accuracy_policy_3: 0.62164
	loss_value_3: 0.03728
	loss_reward_3: 0.00592
	loss_policy_4: 0.03953
	accuracy_policy_4: 0.59816
	loss_value_4: 0.03819
	loss_reward_4: 0.00631
	loss_policy_5: 0.04204
	accuracy_policy_5: 0.57773
	loss_value_5: 0.03918
	loss_reward_5: 0.00734
	loss_policy: 0.27216
	loss_value: 0.34993
	loss_reward: 0.03046
[2024-05-05 17:32:20] nn step 59400, lr: 0.005.
	loss_policy_0: 0.09059
	accuracy_policy_0: 0.78938
	loss_value_0: 0.17156
	loss_policy_1: 0.0302
	accuracy_policy_1: 0.69266
	loss_value_1: 0.03617
	loss_reward_1: 0.00605
	loss_policy_2: 0.03502
	accuracy_policy_2: 0.65668
	loss_value_2: 0.03755
	loss_reward_2: 0.00573
	loss_policy_3: 0.03818
	accuracy_policy_3: 0.62305
	loss_value_3: 0.03904
	loss_reward_3: 0.00606
	loss_policy_4: 0.04033
	accuracy_policy_4: 0.60629
	loss_value_4: 0.03997
	loss_reward_4: 0.0067
	loss_policy_5: 0.04269
	accuracy_policy_5: 0.58645
	loss_value_5: 0.04105
	loss_reward_5: 0.00787
	loss_policy: 0.27701
	loss_value: 0.36534
	loss_reward: 0.03241
Optimization_Done 59400
[2024-05-05 17:34:19] [command] train weight_iter_59400.pkl 297 298
[2024-05-05 17:34:36] nn step 59450, lr: 0.005.
	loss_policy_0: 0.1391
	accuracy_policy_0: 0.71504
	loss_value_0: 0.17793
	loss_policy_1: 0.03657
	accuracy_policy_1: 0.63719
	loss_value_1: 0.03755
	loss_reward_1: 0.00604
	loss_policy_2: 0.0406
	accuracy_policy_2: 0.5932
	loss_value_2: 0.03914
	loss_reward_2: 0.00571
	loss_policy_3: 0.04403
	accuracy_policy_3: 0.56957
	loss_value_3: 0.04044
	loss_reward_3: 0.00608
	loss_policy_4: 0.04601
	accuracy_policy_4: 0.5502
	loss_value_4: 0.04167
	loss_reward_4: 0.00665
	loss_policy_5: 0.04875
	accuracy_policy_5: 0.52758
	loss_value_5: 0.04288
	loss_reward_5: 0.0075
	loss_policy: 0.35506
	loss_value: 0.37961
	loss_reward: 0.03198
[2024-05-05 17:34:52] nn step 59500, lr: 0.005.
	loss_policy_0: 0.1113
	accuracy_policy_0: 0.76004
	loss_value_0: 0.17684
	loss_policy_1: 0.03284
	accuracy_policy_1: 0.67418
	loss_value_1: 0.03723
	loss_reward_1: 0.00587
	loss_policy_2: 0.03796
	accuracy_policy_2: 0.62969
	loss_value_2: 0.03903
	loss_reward_2: 0.0059
	loss_policy_3: 0.04162
	accuracy_policy_3: 0.60133
	loss_value_3: 0.04028
	loss_reward_3: 0.00623
	loss_policy_4: 0.04456
	accuracy_policy_4: 0.57434
	loss_value_4: 0.04151
	loss_reward_4: 0.00661
	loss_policy_5: 0.04803
	accuracy_policy_5: 0.54305
	loss_value_5: 0.0428
	loss_reward_5: 0.00786
	loss_policy: 0.31632
	loss_value: 0.37768
	loss_reward: 0.03246
[2024-05-05 17:35:08] nn step 59550, lr: 0.005.
	loss_policy_0: 0.10536
	accuracy_policy_0: 0.77527
	loss_value_0: 0.18336
	loss_policy_1: 0.03225
	accuracy_policy_1: 0.68793
	loss_value_1: 0.03878
	loss_reward_1: 0.00586
	loss_policy_2: 0.03769
	accuracy_policy_2: 0.63793
	loss_value_2: 0.04069
	loss_reward_2: 0.00596
	loss_policy_3: 0.04179
	accuracy_policy_3: 0.60344
	loss_value_3: 0.04191
	loss_reward_3: 0.00639
	loss_policy_4: 0.04427
	accuracy_policy_4: 0.58039
	loss_value_4: 0.04332
	loss_reward_4: 0.00688
	loss_policy_5: 0.04768
	accuracy_policy_5: 0.55785
	loss_value_5: 0.04466
	loss_reward_5: 0.00798
	loss_policy: 0.30906
	loss_value: 0.39273
	loss_reward: 0.03307
[2024-05-05 17:35:23] nn step 59600, lr: 0.005.
	loss_policy_0: 0.09841
	accuracy_policy_0: 0.78785
	loss_value_0: 0.18065
	loss_policy_1: 0.03055
	accuracy_policy_1: 0.69457
	loss_value_1: 0.03829
	loss_reward_1: 0.00619
	loss_policy_2: 0.03614
	accuracy_policy_2: 0.64613
	loss_value_2: 0.03999
	loss_reward_2: 0.00598
	loss_policy_3: 0.04033
	accuracy_policy_3: 0.61512
	loss_value_3: 0.04142
	loss_reward_3: 0.00645
	loss_policy_4: 0.04328
	accuracy_policy_4: 0.58922
	loss_value_4: 0.0427
	loss_reward_4: 0.00663
	loss_policy_5: 0.04665
	accuracy_policy_5: 0.55969
	loss_value_5: 0.0439
	loss_reward_5: 0.00802
	loss_policy: 0.29537
	loss_value: 0.38696
	loss_reward: 0.03327
Optimization_Done 59600
[2024-05-05 17:37:21] [command] train weight_iter_59600.pkl 298 299
[2024-05-05 17:37:38] nn step 59650, lr: 0.005.
	loss_policy_0: 0.11614
	accuracy_policy_0: 0.75012
	loss_value_0: 0.1748
	loss_policy_1: 0.03368
	accuracy_policy_1: 0.66602
	loss_value_1: 0.0371
	loss_reward_1: 0.00571
	loss_policy_2: 0.03856
	accuracy_policy_2: 0.62559
	loss_value_2: 0.03893
	loss_reward_2: 0.00564
	loss_policy_3: 0.04265
	accuracy_policy_3: 0.59391
	loss_value_3: 0.04039
	loss_reward_3: 0.00636
	loss_policy_4: 0.04614
	accuracy_policy_4: 0.56957
	loss_value_4: 0.04188
	loss_reward_4: 0.00663
	loss_policy_5: 0.04876
	accuracy_policy_5: 0.54977
	loss_value_5: 0.04332
	loss_reward_5: 0.00755
	loss_policy: 0.32593
	loss_value: 0.37642
	loss_reward: 0.03188
[2024-05-05 17:37:54] nn step 59700, lr: 0.005.
	loss_policy_0: 0.09553
	accuracy_policy_0: 0.78719
	loss_value_0: 0.17115
	loss_policy_1: 0.02997
	accuracy_policy_1: 0.7009
	loss_value_1: 0.03616
	loss_reward_1: 0.00569
	loss_policy_2: 0.03468
	accuracy_policy_2: 0.65953
	loss_value_2: 0.03791
	loss_reward_2: 0.00557
	loss_policy_3: 0.03911
	accuracy_policy_3: 0.62234
	loss_value_3: 0.03916
	loss_reward_3: 0.00593
	loss_policy_4: 0.04245
	accuracy_policy_4: 0.59445
	loss_value_4: 0.04055
	loss_reward_4: 0.00647
	loss_policy_5: 0.04574
	accuracy_policy_5: 0.5716
	loss_value_5: 0.04189
	loss_reward_5: 0.0074
	loss_policy: 0.28748
	loss_value: 0.36682
	loss_reward: 0.03106
[2024-05-05 17:38:10] nn step 59750, lr: 0.005.
	loss_policy_0: 0.09266
	accuracy_policy_0: 0.80242
	loss_value_0: 0.17924
	loss_policy_1: 0.03079
	accuracy_policy_1: 0.70578
	loss_value_1: 0.03815
	loss_reward_1: 0.00593
	loss_policy_2: 0.0363
	accuracy_policy_2: 0.65961
	loss_value_2: 0.0399
	loss_reward_2: 0.00599
	loss_policy_3: 0.04056
	accuracy_policy_3: 0.62676
	loss_value_3: 0.04137
	loss_reward_3: 0.00635
	loss_policy_4: 0.04381
	accuracy_policy_4: 0.60129
	loss_value_4: 0.04277
	loss_reward_4: 0.00669
	loss_policy_5: 0.04744
	accuracy_policy_5: 0.57395
	loss_value_5: 0.04404
	loss_reward_5: 0.00802
	loss_policy: 0.29156
	loss_value: 0.38548
	loss_reward: 0.03299
[2024-05-05 17:38:26] nn step 59800, lr: 0.005.
	loss_policy_0: 0.08579
	accuracy_policy_0: 0.80328
	loss_value_0: 0.17116
	loss_policy_1: 0.02896
	accuracy_policy_1: 0.70207
	loss_value_1: 0.03618
	loss_reward_1: 0.00579
	loss_policy_2: 0.03382
	accuracy_policy_2: 0.66156
	loss_value_2: 0.03778
	loss_reward_2: 0.00564
	loss_policy_3: 0.03815
	accuracy_policy_3: 0.62855
	loss_value_3: 0.03937
	loss_reward_3: 0.00625
	loss_policy_4: 0.04159
	accuracy_policy_4: 0.59836
	loss_value_4: 0.04088
	loss_reward_4: 0.00642
	loss_policy_5: 0.04478
	accuracy_policy_5: 0.58098
	loss_value_5: 0.04217
	loss_reward_5: 0.00768
	loss_policy: 0.27308
	loss_value: 0.36753
	loss_reward: 0.03178
Optimization_Done 59800
[2024-05-05 17:40:15] [command] train weight_iter_59800.pkl 299 300
[2024-05-05 17:40:33] nn step 59850, lr: 0.005.
	loss_policy_0: 0.11258
	accuracy_policy_0: 0.74852
	loss_value_0: 0.16786
	loss_policy_1: 0.03285
	accuracy_policy_1: 0.66746
	loss_value_1: 0.03515
	loss_reward_1: 0.00586
	loss_policy_2: 0.03618
	accuracy_policy_2: 0.64105
	loss_value_2: 0.03682
	loss_reward_2: 0.00572
	loss_policy_3: 0.04002
	accuracy_policy_3: 0.61137
	loss_value_3: 0.03845
	loss_reward_3: 0.00601
	loss_policy_4: 0.04345
	accuracy_policy_4: 0.59227
	loss_value_4: 0.04003
	loss_reward_4: 0.0067
	loss_policy_5: 0.04599
	accuracy_policy_5: 0.57488
	loss_value_5: 0.04112
	loss_reward_5: 0.00752
	loss_policy: 0.31106
	loss_value: 0.35943
	loss_reward: 0.03181
[2024-05-05 17:40:49] nn step 59900, lr: 0.005.
	loss_policy_0: 0.09134
	accuracy_policy_0: 0.79145
	loss_value_0: 0.17131
	loss_policy_1: 0.02993
	accuracy_policy_1: 0.70875
	loss_value_1: 0.03607
	loss_reward_1: 0.00591
	loss_policy_2: 0.03474
	accuracy_policy_2: 0.6718
	loss_value_2: 0.03783
	loss_reward_2: 0.00593
	loss_policy_3: 0.03857
	accuracy_policy_3: 0.6409
	loss_value_3: 0.03965
	loss_reward_3: 0.00596
	loss_policy_4: 0.04181
	accuracy_policy_4: 0.61961
	loss_value_4: 0.04084
	loss_reward_4: 0.00649
	loss_policy_5: 0.04433
	accuracy_policy_5: 0.60238
	loss_value_5: 0.04235
	loss_reward_5: 0.00759
	loss_policy: 0.28072
	loss_value: 0.36805
	loss_reward: 0.03187
[2024-05-05 17:41:05] nn step 59950, lr: 0.005.
	loss_policy_0: 0.08419
	accuracy_policy_0: 0.80145
	loss_value_0: 0.16682
	loss_policy_1: 0.0286
	accuracy_policy_1: 0.70805
	loss_value_1: 0.03526
	loss_reward_1: 0.00597
	loss_policy_2: 0.03307
	accuracy_policy_2: 0.67391
	loss_value_2: 0.03668
	loss_reward_2: 0.00563
	loss_policy_3: 0.03643
	accuracy_policy_3: 0.64773
	loss_value_3: 0.03805
	loss_reward_3: 0.00588
	loss_policy_4: 0.03965
	accuracy_policy_4: 0.62305
	loss_value_4: 0.03926
	loss_reward_4: 0.00638
	loss_policy_5: 0.04302
	accuracy_policy_5: 0.60074
	loss_value_5: 0.04073
	loss_reward_5: 0.00726
	loss_policy: 0.26498
	loss_value: 0.35679
	loss_reward: 0.03112
[2024-05-05 17:41:21] nn step 60000, lr: 0.005.
	loss_policy_0: 0.08341
	accuracy_policy_0: 0.80902
	loss_value_0: 0.17523
	loss_policy_1: 0.02943
	accuracy_policy_1: 0.71551
	loss_value_1: 0.03702
	loss_reward_1: 0.00616
	loss_policy_2: 0.03444
	accuracy_policy_2: 0.67445
	loss_value_2: 0.03852
	loss_reward_2: 0.00579
	loss_policy_3: 0.03851
	accuracy_policy_3: 0.6502
	loss_value_3: 0.04008
	loss_reward_3: 0.00636
	loss_policy_4: 0.0416
	accuracy_policy_4: 0.62766
	loss_value_4: 0.04131
	loss_reward_4: 0.00646
	loss_policy_5: 0.04528
	accuracy_policy_5: 0.60188
	loss_value_5: 0.04287
	loss_reward_5: 0.0075
	loss_policy: 0.27266
	loss_value: 0.37504
	loss_reward: 0.03228
Optimization_Done 60000
